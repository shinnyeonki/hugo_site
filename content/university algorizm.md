---
title: university algorizm
resource-path: university algorizm.md
keywords:
tags:
  - ai-content
  - university
date: 2025-10-19T18:41:43+09:00
lastmod: 2025-10-21T20:40:22+09:00
---
### 제3장: 정렬

- 배열을 크기에 기초해 분할 후 합쳐서 정렬
   1. 크기가 1인 배열과 (n - 1)인 배열로 분할 : 삽입 정렬
   2. 크기가 𝑛/2인 두 개의 배열로 분할 : 합병 정렬
- 배열을 특정 값에 기초하여 분할 후 합쳐서 정렬
   1. 최솟값에 기초하여 분할 : 선택 정렬, 힙 정렬
   2. 기준 값(예: 첫 번째 값)에 기초하여 분할 : 빠른 정렬

![](./08.media/20251019184156-1760866916046-image.png)


네, 제공해주신 알고리즘 슬라이드(19~24장) 내용을 바탕으로 삽입 정렬(Insertion Sort)에 대해 30,000자 이상으로 매우 상세하고 단계적으로 설명해 드리겠습니다. C++ 예시 코드와 함께 각 개념을 깊이 있게 다루겠습니다.

***

### 삽입 정렬(Insertion Sort)

#### 목차
1.  **삽입 정렬이란 무엇인가? (개요)**
    *   핵심 아이디어: 정렬된 부분과 정렬되지 않은 부분
    *   가장 직관적인 비유: 카드놀이
2.  **삽입 정렬의 핵심 메커니즘: 하나의 요소 삽입하기 (Slides 19-21)**
    *   목표: 정렬되지 않은 부분의 첫 요소를 정렬된 부분의 올바른 위치에 넣기
    *   #### 1단계: 삽입 위치 찾기 (Naive 접근법과 그 문제점)
    *   #### 2단계: 문제 해결 및 올바른 알고리즘 (요소 저장과 이동)
    *   #### 그림으로 이해하는 단일 삽입 과정
3.  **전체 삽입 정렬 알고리즘 (Slide 22)**
    *   알고리즘의 구조: 외부 루프와 내부 루프
    *   #### 상세한 단계별 실행 예제
4.  **C++ 전체 구현 코드 및 해설**
    *   #### 삽입 정렬 함수 구현
    *   #### 전체 실행 코드 및 설명
5.  **시간 및 공간 복잡도 분석 (Slide 23)**
    *   #### 기본 연산(Basic Operation)의 정의
    *   #### 최악의 경우 (Worst Case) 분석
    *   #### 최선의 경우 (Best Case) 분석
    *   #### 평균적인 경우 (Average Case) 분석
    *   #### 공간 복잡도 분석
6.  **삽입 정렬의 주요 특징 및 심층 분석 (Slide 24)**
    *   #### 장점 1: 적응성(Adaptive) - 거의 정렬된 데이터에 강력함
    *   #### 장점 2: 제자리 정렬(In-place) - 추가 공간 불필요
    *   #### 장점 3: 안정 정렬(Stable Sort)
    *   #### 장점 4: 온라인(Online) 알고리즘
    *   #### 단점: 비효율적인 성능 (O(n²))
7.  **다른 정렬 알고리즘과의 비교 및 활용**
    *   #### 선택 정렬(Selection Sort)과의 비교
    *   #### 버블 정렬(Bubble Sort)과의 비교
    *   #### 고급 정렬(퀵, 병합, 힙 정렬)과의 관계
    *   #### 삽입 정렬은 언제 유용한가?
8.  **요약 및 결론**

---

#### 1. 삽입 정렬이란 무엇인가? (개요)

삽입 정렬(Insertion Sort)은 가장 단순하고 직관적인 정렬 알고리즘 중 하나입니다. 이름에서 알 수 있듯이, 정렬되지 않은 데이터를 하나씩 가져와 **이미 정렬된 부분의 올바른 위치에 삽입**하는 방식을 반복하여 전체를 정렬합니다.

##### 핵심 아이디어: 정렬된 부분과 정렬되지 않은 부분

삽입 정렬은 배열을 논리적으로 두 부분으로 나누어 생각합니다.
1.  **정렬된 부분(Sorted Sub-array)**: 배열의 앞부분으로, 이 부분의 원소들은 항상 정렬된 상태를 유지합니다.
2.  **정렬되지 않은 부분(Unsorted Sub-array)**: 배열의 뒷부분으로, 아직 처리되지 않은 원소들이 남아있습니다.

알고리즘은 정렬되지 않은 부분의 가장 앞에 있는 원소를 하나씩 꺼내어, 정렬된 부분의 끝에서부터 비교하면서 자신의 올바른 위치를 찾아 삽입합니다. 이 과정이 끝나면 정렬된 부분의 크기는 하나 늘어나고, 정렬되지 않은 부분의 크기는 하나 줄어듭니다. 이 작업을 정렬되지 않은 부분이 없어질 때까지 반복합니다.

##### 가장 직관적인 비유: 카드놀이

삽입 정렬은 우리가 손에 든 카드를 정렬하는 방식과 매우 유사합니다.
1.  바닥에 놓인 카드 뭉치(정렬되지 않은 부분)에서 카드 한 장을 집습니다.
2.  이미 손에 들고 있는 정렬된 카드들(정렬된 부분)과 비교합니다.
3.  새로 집은 카드를 손에 든 카드들 사이의 올바른 위치에 꽂아 넣습니다. 이때, 새 카드가 들어갈 자리를 만들기 위해 기존 카드들을 한 칸씩 뒤로 밀어낼 수 있습니다.
4.  바닥에 카드가 없을 때까지 이 과정을 반복합니다.

이 비유처럼, 삽입 정렬은 '하나를 꺼내서', '적절한 위치를 찾아', '삽입하는' 단순한 원리를 기반으로 동작합니다.

---

#### 2. 삽입 정렬의 핵심 메커니즘: 하나의 요소 삽입하기 (Slides 19-21)

삽입 정렬의 전체 과정을 이해하기 위해, 먼저 한 번의 반복, 즉 **하나의 요소를 정렬된 부분에 삽입하는 과정**을 깊이 있게 살펴보겠습니다. 이것이 알고리즘의 심장부입니다.

가정: 배열 `A`가 있고, `A[0...i-1]`은 이미 정렬되어 있습니다. 이제 `A[i]`를 이 정렬된 부분에 삽입하여 `A[0...i]` 전체를 정렬된 상태로 만들려고 합니다.

##### 1단계: 삽입 위치 찾기 (Naive 접근법과 그 문제점)

**슬라이드 19**의 아이디어는 `A[i]`를 삽입할 위치 `j`를 찾는 것입니다. `A[j] ≤ A[i] ≤ A[j+1]`을 만족하는 `j`를 찾아야 합니다. 이를 위해 정렬된 부분의 맨 끝(`j = i-1`)부터 시작하여 왼쪽으로 이동하며 `A[i]`보다 작은 값을 만날 때까지 비교합니다.

**슬라이드 20**은 이를 코드로 표현합니다.

```
j = i – 1
while (A[j] > A[i])
    j = j – 1
// 루프가 끝나면 A[j] <= A[i] 이므로, A[i]는 A[j+1] 위치에 들어가야 한다.
```

**문제점**: 이 코드에는 치명적인 결함이 있습니다. 만약 `A[i]`가 `A[0...i-1]`에 있는 모든 요소보다 작다면 어떻게 될까요?
예를 들어 배열이 `[5, 8, 10, 2]`이고 `i=3`일 때, `A[3]`는 `2`입니다.
1.  `j = 2`. `A[2]`(10) > `A[3]`(2). `j`는 1이 됩니다.
2.  `j = 1`. `A[1]`(8) > `A[3]`(2). `j`는 0이 됩니다.
3.  `j = 0`. `A[0]`(5) > `A[3]`(2). `j`는 -1이 됩니다.
4.  `while` 루프의 다음 조건 검사에서 `A[j]`, 즉 `A[-1]`에 접근하게 됩니다. 이는 배열의 범위를 벗어나는 접근(Out-of-Bounds Access)으로, 프로그램 오류를 유발합니다.

따라서 단순히 위치만 찾는 방식은 문제가 있습니다.

##### 2단계: 문제 해결 및 올바른 알고리즘 (요소 저장과 이동)

**슬라이드 21**은 이 문제를 해결하는 우아한 방법을 제시합니다. 위치를 먼저 찾고 나중에 삽입하는 것이 아니라, **위치를 찾는 과정과 삽입할 공간을 만드는 과정을 동시에 수행**합니다.

**핵심 전략**:
1.  **삽입할 요소 임시 저장**: 먼저 `A[i]`의 값을 `insertElement`라는 변수에 복사해 둡니다. 이렇게 하면 `A[i]`의 원래 자리는 비어있는 '구멍(hole)'으로 간주할 수 있습니다.
2.  **이동(Shift)하며 위치 찾기**: 정렬된 부분의 끝(`j = i-1`)부터 시작하여, `insertElement`보다 큰 요소들을 만나면 **오른쪽으로 한 칸씩 밀어냅니다(shift)**.
3.  **삽입**: `insertElement`보다 작거나 같은 요소를 만나거나, 배열의 맨 앞에 도달하면 `while` 루프가 멈춥니다. 바로 그 멈춘 위치의 다음 칸(`j+1`)이 `insertElement`가 들어갈 최종 위치입니다.

**알고리즘 코드 (슬라이드 21):**

```
insertElement = A[i] // 1. 삽입할 요소 저장
j = i – 1

// 2. 이동하며 위치 찾기
while (j >= 0 and A[j] > insertElement) {
    A[j+1] = A[j]  // insertElement보다 큰 요소를 오른쪽으로 한 칸 이동
    j = j - 1
}

// 3. 최종 위치에 삽입
A[j+1] = insertElement
```

이 알고리즘이 앞선 문제를 해결하는 방법:
*   `j >= 0` 조건이 `while` 루프에 추가되었습니다. 이 덕분에 `j`가 -1이 되면 루프가 즉시 종료되어 배열의 범위를 벗어나는 오류를 원천적으로 방지합니다.
*   요소들을 한 칸씩 밀어내면서 빈 공간을 만들기 때문에, 최종적으로 삽입할 때 다른 요소들을 덮어쓸 걱정이 없습니다.

##### 그림으로 이해하는 단일 삽입 과정

배열 `A = [3, 8, 12, 15, 7, ...]` 가 있고, `i=4`일 때, `A[4]=7`을 삽입하는 과정을 보겠습니다.
`A[0..]`은 `[3, 8, 12, 15]`로 정렬된 상태입니다.

1.  **초기 상태**:
    *   `insertElement = 7`
    *   `j = 3`
    *   배열: `[3, 8, 12, 15, 15]` (논리적으로 `A[4]`는 비어있다고 생각)

    ```
    A: [ 3, 8, 12, 15,  ? ]   insertElement: 7
                     ^
                     j
    ```

2.  **`while` 루프 (1차 반복)**:
    *   `j=3` >= 0 이고, `A[3]`(15) > `insertElement`(7) -> 참
    *   `A[4] = A[3]`. (15를 오른쪽으로 이동)
    *   `j`는 2가 됨.
    *   배열: `[3, 8, 12, 15, 15]`

    ```
    A: [ 3, 8, 12,  ?, 15 ]   insertElement: 7
                  ^
                  j
    ```

3.  **`while` 루프 (2차 반복)**:
    *   `j=2` >= 0 이고, `A[2]`(12) > `insertElement`(7) -> 참
    *   `A[3] = A[2]`. (12를 오른쪽으로 이동)
    *   `j`는 1이 됨.
    *   배열: `[3, 8, 12, 12, 15]`

    ```
    A: [ 3, 8,  ?, 12, 15 ]   insertElement: 7
               ^
               j
    ```

4.  **`while` 루프 (3차 반복)**:
    *   `j=1` >= 0 이고, `A[1]`(8) > `insertElement`(7) -> 참
    *   `A[2] = A[1]`. (8을 오른쪽으로 이동)
    *   `j`는 0이 됨.
    *   배열: `[3, 8, 8, 12, 15]`

    ```
    A: [ 3,  ?,  8, 12, 15 ]   insertElement: 7
            ^
            j
    ```

5.  **`while` 루프 (4차 반복)**:
    *   `j=0` >= 0 이고, `A[0]`(3) > `insertElement`(7) -> 거짓. 루프 종료!
    *   현재 `j`는 0.

6.  **삽입**:
    *   `A[j+1] = insertElement` -> `A[1] = 7`.
    *   최종 배열: `[3, 7, 8, 12, 15]`

이제 `A[0..]`가 완벽하게 정렬되었습니다. 이 과정을 전체 배열에 대해 반복하는 것이 삽입 정렬입니다.

---

#### 3. 전체 삽입 정렬 알고리즘 (Slide 22)

하나의 요소를 삽입하는 방법을 알았으니, 이제 외부 루프를 추가하여 전체 배열을 정렬하는 완전한 알고리즘을 만들 수 있습니다.

##### 알고리즘의 구조: 외부 루프와 내부 루프

**슬라이드 22**의 알고리즘은 두 개의 중첩된 루프 구조를 가집니다.

*   **외부 `for` 루프**: 정렬되지 않은 부분의 첫 번째 요소를 선택하는 역할을 합니다. 인덱스 `i`는 1부터 `n-1`까지 증가합니다. `i`가 1부터 시작하는 이유는, 크기가 1인 배열(`A[0]`)은 그 자체로 이미 정렬되어 있기 때문입니다.
*   **내부 `while` 루프**: `for` 루프에서 선택된 `A[i]`를 `A[0...i-1]`의 올바른 위치에 삽입하는 역할을 합니다. 앞서 자세히 분석한 바로 그 로직입니다.

```
InsertionSort(A[0..n−1])
// 입력: 정렬 안된 배열 A[0..n-1]
// 출력: 정렬된 배열 A[0..n-1]
1 for (i = 1; i < n; i++) {
2     insertElement = A[i]
3     j = i – 1
4     while ( j ≥ 0 and A[ j] > insertElement) {
5         A[ j + 1] = A[ j]
6         j = j – 1
      }
7     A[ j + 1] = insertElement
  }
```

##### 상세한 단계별 실행 예제

배열 `A = [5, 2, 4, 6, 1, 3]` (n=6)을 정렬하는 과정을 따라가 보겠습니다.
`|` 기호는 정렬된 부분과 정렬되지 않은 부분을 구분합니다.

**초기 상태**: `[| 5, 2, 4, 6, 1, 3]` (크기 0인 정렬된 부분. 실질적으로 `A[0]`부터 시작)

**`i = 1`**: `A[1] = 2`를 삽입.
*   `insertElement = 2`, `j = 0`
*   `while` (`j=0` >= 0 and `A[0]`(5) > 2): 참
    *   `A[1] = A[0]`. -> `[5, 5, 4, 6, 1, 3]`
    *   `j = -1`. 루프 종료.
*   `A[j+1]` 즉 `A[0] = 2`.
*   **결과**: `[2, 5 | 4, 6, 1, 3]`

**`i = 2`**: `A[2] = 4`를 삽입.
*   `insertElement = 4`, `j = 1`
*   `while` (`j=1` >= 0 and `A[1]`(5) > 4): 참
    *   `A[2] = A[1]`. -> `[2, 5, 5, 6, 1, 3]`
    *   `j = 0`.
*   `while` (`j=0` >= 0 and `A[0]`(2) > 4): 거짓. 루프 종료.
*   `A[j+1]` 즉 `A[1] = 4`.
*   **결과**: `[2, 4, 5 | 6, 1, 3]`

**`i = 3`**: `A[3] = 6`를 삽입.
*   `insertElement = 6`, `j = 2`
*   `while` (`j=2` >= 0 and `A[2]`(5) > 6): 거짓. 루프 즉시 종료.
*   `A[j+1]` 즉 `A[3] = 6`. (변화 없음)
*   **결과**: `[2, 4, 5, 6 | 1, 3]`

**`i = 4`**: `A[4] = 1`를 삽입.
*   `insertElement = 1`, `j = 3`
*   `while` 루프는 `j`가 3, 2, 1, 0일 때 모두 참. 모든 요소를 한 칸씩 오른쪽으로 민다.
    *   `[2, 4, 5, 6, 6, 3]`
    *   `[2, 4, 5, 5, 6, 3]`
    *   `[2, 4, 4, 5, 6, 3]`
    *   `[2, 2, 4, 5, 6, 3]`
*   `j`가 -1이 되어 루프 종료.
*   `A[j+1]` 즉 `A[0] = 1`.
*   **결과**: `[1, 2, 4, 5, 6 | 3]`

**`i = 5`**: `A[5] = 3`을 삽입.
*   `insertElement = 3`, `j = 4`
*   `while` (`j=4` >= 0 and `A[4]`(6) > 3): 참. `j`는 3. -> `[1, 2, 4, 5, 6, 6]`
*   `while` (`j=3` >= 0 and `A[3]`(5) > 3): 참. `j`는 2. -> `[1, 2, 4, 5, 5, 6]`
*   `while` (`j=2` >= 0 and `A[2]`(4) > 3): 참. `j`는 1. -> `[1, 2, 4, 4, 5, 6]`
*   `while` (`j=1` >= 0 and `A[1]`(2) > 3): 거짓. 루프 종료.
*   `A[j+1]` 즉 `A[2] = 3`.
*   **결과**: `[1, 2, 3, 4, 5, 6 |]`

**최종 정렬 완료**: `[1, 2, 3, 4, 5, 6]`

---

#### 4. C++ 전체 구현 코드 및 해설

슬라이드의 의사코드를 표준 C++로 구현해 보겠습니다. `std::vector`를 사용하여 동적 배열을 처리합니다.

##### 삽입 정렬 함수 구현

```cpp
#include <iostream>
#include <vector>

void insertionSort(std::vector<int>& arr) {
    // n은 배열의 크기
    int n = arr.size();
    
    // 외부 루프: i는 1부터 시작하여 배열의 끝까지 순회
    // arr[0...i-1]은 정렬된 부분, arr[i...n-1]은 정렬되지 않은 부분
    for (int i = 1; i < n; ++i) {
        // Step 1: 삽입할 요소를 키(또는 insertElement)로 저장
        int key = arr[i];
        
        // Step 2: 삽입할 위치를 찾기 위해 정렬된 부분의 끝에서부터 시작
        int j = i - 1;
        
        // Step 3: 내부 루프 - key가 들어갈 위치를 찾으면서
        // key보다 큰 요소들을 오른쪽으로 한 칸씩 이동
        // j >= 0 조건은 배열의 시작을 벗어나지 않도록 보장
        // arr[j] > key 조건은 삽입 위치를 찾는 핵심 비교
        while (j >= 0 && arr[j] > key) {
            arr[j + 1] = arr[j]; // 요소를 오른쪽으로 이동
            j = j - 1;           // 비교할 다음 요소로 이동 (왼쪽으로)
        }
        
        // Step 4: 찾은 위치(j+1)에 key를 삽입
        // 루프가 끝난 시점의 j는 key보다 작거나 같은 첫 번째 요소의 인덱스이거나 -1
        arr[j + 1] = key;
    }
}
```

##### 전체 실행 코드 및 설명

```cpp
#include <iostream>
#include <vector>

// 유틸리티 함수: 벡터를 출력
void printArray(const std::vector<int>& arr) {
    for (int val : arr) {
        std::cout << val << " ";
    }
    std::cout << std::endl;
}

// 위에서 정의한 insertionSort 함수
void insertionSort(std::vector<int>& arr) {
    int n = arr.size();
    for (int i = 1; i < n; ++i) {
        int key = arr[i];
        int j = i - 1;
        while (j >= 0 && arr[j] > key) {
            arr[j + 1] = arr[j];
            j = j - 1;
        }
        arr[j + 1] = key;
    }
}


int main() {
    std::vector<int> data = {5, 2, 4, 6, 1, 3};
    
    std::cout << "Original array: ";
    printArray(data);

    insertionSort(data);

    std::cout << "Sorted array: ";
    printArray(data);

    return 0;
}
```

*   **`main` 함수**: 정렬할 데이터를 `std::vector`로 생성하고, 정렬 전과 후의 배열 상태를 `printArray` 함수를 통해 출력하여 알고리즘의 동작을 확인합니다.
*   **`insertionSort` 함수 해설**:
    *   `key` 변수는 슬라이드의 `insertElement`에 해당합니다.
    *   외부 `for` 루프는 `i=1`부터 `n-1`까지 총 `n-1`번 실행됩니다.
    *   내부 `while` 루프는 `key`의 값과 정렬된 부분의 요소들에 따라 실행 횟수가 달라집니다. 이것이 삽입 정렬의 성능을 결정하는 가장 중요한 부분입니다.

---

#### 5. 시간 및 공간 복잡도 분석 (Slide 23)

알고리즘의 효율성을 평가하기 위해 시간과 공간 복잡도를 분석합니다.

##### 기본 연산(Basic Operation)의 정의

알고리즘의 실행 시간을 지배하는 가장 빈번하게 수행되는 연산을 '기본 연산'으로 정의합니다. **슬라이드 23**에서는 삽입 정렬의 기본 연산을 내부 `while` 루프의 조건 비교인 `A[j] > insertElement`로 정의했습니다. 우리는 이 비교 연산의 횟수를 분석할 것입니다.

##### 최악의 경우 (Worst Case) 분석

**언제 최악의 상황이 발생하는가?**
배열이 **역순으로 정렬**되어 있을 때입니다. 예를 들어, `[6, 5, 4, 3, 2, 1]`.
이 경우, `for` 루프의 각 반복(`i`)에서 `insertElement`는 항상 정렬된 부분(`A[0...i-1]`)의 모든 요소보다 작습니다. 따라서 내부 `while` 루프는 항상 `j`가 `-1`이 될 때까지, 즉 정렬된 부분의 모든 요소를 비교하고 이동시켜야 합니다.

*   `i = 1`일 때: 비교 1번
*   `i = 2`일 때: 비교 2번
*   `i = 3`일 때: 비교 3번
*   ...
*   `i = n-1`일 때: 비교 `n-1`번

따라서 기본 연산의 총 수행 횟수는 등차수열의 합입니다.
`1 + 2 + 3 + ... + (n-1) = (n-1) * (n-1 + 1) / 2 = n(n-1) / 2`

이 식은 `(n² - n) / 2` 이므로, 최고차항은 `n²`입니다.
Big-O 표기법으로는 **O(n²)**, Big-Theta 표기법으로는 **Θ(n²)** 입니다.

##### 최선의 경우 (Best Case) 분석

**언제 최선의 상황이 발생하는가?**
배열이 **이미 정렬**되어 있을 때입니다. 예를 들어, `[1, 2, 3, 4, 5, 6]`.
이 경우, `for` 루프의 각 반복(`i`)에서 `insertElement`(`A[i]`)는 `A[j]`(`A[i-1]`)보다 항상 크거나 같습니다. 따라서 내부 `while` 루프의 조건 `A[j] > insertElement`는 **항상 즉시 거짓**이 됩니다.

*   `i = 1`일 때: 비교 1번
*   `i = 2`일 때: 비교 1번
*   ...
*   `i = n-1`일 때: 비교 1번

총 비교 횟수는 `n-1`번입니다.
따라서 최선의 경우 시간 복잡도는 **O(n)**, **Θ(n)** 입니다. 이는 삽입 정렬의 매우 중요한 특징입니다.

##### 평균적인 경우 (Average Case) 분석

입력 데이터가 무작위로 분포되어 있을 경우를 가정합니다. 평균적으로, 정렬되지 않은 부분에서 가져온 `insertElement`는 정렬된 부분의 중간쯤에 삽입될 것입니다. 즉, `i`번째 요소는 정렬된 `i`개의 요소 중 절반 정도(약 `i/2`개)와 비교하고 이동해야 합니다.

총 비교 횟수는 대략 `1/2 + 2/2 + 3/2 + ... + (n-1)/2` 가 됩니다.
이는 `(1/2) * (1 + 2 + ... + (n-1)) = (1/2) * n(n-1)/2 = n(n-1)/4` 입니다.

최고차항은 여전히 `n²`이므로, 평균 시간 복잡도 또한 **O(n²)**, **Θ(n²)** 입니다.

##### 공간 복잡도 분석

삽입 정렬은 정렬을 위해 입력 배열 외에 추가적인 데이터 구조를 필요로 하지 않습니다. `i`, `j`, `key`와 같은 몇 개의 변수만을 사용하며, 이 변수들의 수는 입력 크기 `n`에 관계없이 일정합니다. 이를 **제자리 정렬(In-place sort)** 이라고 하며, 공간 복잡도는 **O(1)** 입니다.

---

#### 6. 삽입 정렬의 주요 특징 및 심층 분석 (Slide 24)

삽입 정렬은 단순한 O(n²) 알고리즘 이상의 중요한 특징들을 가지고 있습니다.

##### 장점 1: 적응성(Adaptive) - 거의 정렬된 데이터에 강력함

**슬라이드 24**에서 언급된 "값들이 거의 정렬되어 있을 때 빠르게 실행"된다는 점이 바로 적응성입니다.
만약 입력 배열이 거의 정렬되어 있다면, 각 요소를 삽입할 때 이동(shift)하는 거리가 매우 짧습니다. 내부 `while` 루프가 몇 번 실행되지 않고 금방 종료되기 때문에, 전체 실행 시간은 O(n²)이 아닌 O(n)에 가깝게 됩니다.
이러한 특성 때문에, 실시간으로 데이터가 약간씩 변경되거나 새로운 데이터가 기존 정렬된 리스트의 끝에 추가되는 경우, 삽입 정렬은 매우 효율적인 선택이 될 수 있습니다.

##### 장점 2: 제자리 정렬(In-place) - 추가 공간 불필요

앞서 분석했듯이, 삽입 정렬은 O(1)의 추가 공간만을 사용합니다. 이는 메모리 사용량이 매우 중요한 임베디드 시스템이나 특정 환경에서 큰 장점이 됩니다. 병합 정렬(Merge Sort)이 O(n)의 추가 공간을 필요로 하는 것과 대조적입니다.

##### 장점 3: 안정 정렬(Stable Sort)

안정 정렬이란, 값이 같은 요소들의 상대적인 순서가 정렬 후에도 그대로 유지되는 것을 의미합니다.
예를 들어, (값, 속성) 형태의 데이터 `[(5, 'A'), (3, 'B'), (5, 'C')]`를 값 기준으로 정렬할 때, 안정 정렬 알고리즘은 항상 `[(3, 'B'), (5, 'A'), (5, 'C')]`를 반환합니다. `(5, 'A')`가 `(5, 'C')`보다 원래 앞에 있었으므로, 정렬 후에도 이 순서가 유지됩니다.

삽입 정렬은 **안정 정렬**입니다. 그 이유는 내부 `while` 루프의 조건 `A[j] > insertElement` 때문입니다. 만약 `A[j]`와 `insertElement`의 값이 같다면(`> `가 아니므로) 루프는 멈추고, `insertElement`는 `A[j]`의 오른쪽에 삽입됩니다. 따라서 기존에 있던 요소의 위치를 침범하지 않아 상대적 순서가 보존됩니다.

##### 장점 4: 온라인(Online) 알고리즘

삽입 정렬은 '온라인' 특성을 가집니다. 즉, 모든 데이터를 미리 받지 않고, 데이터가 들어오는 대로 즉시 정렬을 수행할 수 있습니다. 이미 정렬된 `n`개의 요소 리스트가 있을 때, 새로운 데이터 `n+1`번째가 들어오면, 이 하나만 올바른 위치에 삽입하면 되므로 효율적입니다.

##### 단점: 비효율적인 성능 (O(n²))

삽입 정렬의 가장 큰 단점은 평균과 최악의 경우 시간 복잡도가 O(n²)이라는 점입니다. 데이터의 크기 `n`이 커질수록 실행 시간은 기하급수적으로 늘어납니다. 1만 개의 데이터를 정렬하는 데 1초가 걸렸다면, 10만 개의 데이터를 정렬하는 데는 약 100초(1분 40초)가 걸릴 수 있습니다. 따라서 대규모 데이터를 정렬하는 데는 적합하지 않습니다.

---

#### 7. 다른 정렬 알고리즘과의 비교 및 활용

##### 선택 정렬(Selection Sort)과의 비교
*   **공통점**: 둘 다 O(n²) 시간 복잡도를 가지며, 제자리 정렬입니다.
*   **차이점**:
    *   **비교 횟수**: 선택 정렬은 데이터의 상태와 무관하게 항상 `n(n-1)/2`번 비교합니다. 삽입 정렬은 최선의 경우 O(n)번만 비교합니다 (적응성).
    *   **교환(이동) 횟수**: 선택 정렬은 교환(swap)이 최대 `n-1`번으로 매우 적습니다. 반면 삽입 정렬은 최악의 경우 O(n²)번의 데이터 이동(shift)이 발생합니다. 데이터 이동 비용이 매우 큰 경우 선택 정렬이 더 나을 수 있습니다.
    *   **안정성**: 삽입 정렬은 안정적이지만, 일반적인 선택 정렬 구현은 불안정합니다.

##### 버블 정렬(Bubble Sort)과의 비교
*   **공통점**: 둘 다 O(n²)이며, 안정 정렬이고, 제자리 정렬입니다.
*   **차이점**: 버블 정렬도 최적화를 통해 거의 정렬된 데이터에 대해 O(n)의 성능을 낼 수 있지만(적응성), 일반적으로 삽입 정렬이 내부 루프의 구조상 더 적은 비교와 교환을 수행하여 실제 실행 시간이 더 빠른 경향이 있습니다. 대부분의 경우 삽입 정렬이 버블 정렬보다 선호됩니다.

##### 고급 정렬(퀵, 병합, 힙 정렬)과의 관계
퀵 정렬, 병합 정렬, 힙 정렬과 같은 고급 정렬 알고리즘들은 평균 O(n log n)의 시간 복잡도를 가집니다. 이는 O(n²)보다 훨씬 효율적이므로 대규모 데이터 정렬에는 이들이 사용됩니다.
하지만 재미있게도, 이들 고급 알고리즘은 종종 삽입 정렬을 부분적으로 활용합니다.

*   **하이브리드 정렬(Hybrid Sort)**: 퀵 정렬이나 병합 정렬은 재귀적으로 문제를 분할하다가, 배열의 크기가 특정 임계값(예: 16개 또는 32개) 이하로 작아지면 삽입 정렬로 전환하여 처리합니다. 작은 배열에서는 재귀 호출의 오버헤드보다 삽입 정렬의 단순함이 더 빠르기 때문입니다. 파이썬의 표준 정렬 함수(Timsort)나 C++의 `std::sort`(Introsort)가 이런 방식을 사용합니다.

##### 삽입 정렬은 언제 유용한가?

1.  **데이터의 크기 `n`이 작을 때**: 오버헤드가 적어 O(n log n) 알고리즘보다 빠를 수 있습니다.
2.  **데이터가 거의 정렬되어 있을 때**: O(n)에 가까운 성능을 보여 매우 효율적입니다.
3.  **안정 정렬이 필요하고, 추가 메모리 사용이 불가능할 때**: 이 조건을 만족하는 가장 간단한 알고리즘 중 하나입니다.

#### 8. 요약 및 결론

삽입 정렬은 정렬되지 않은 요소를 하나씩 가져와 이미 정렬된 부분의 올바른 위치에 삽입하는 과정을 반복하는 직관적이고 간단한 정렬 알고리즘입니다. 그 핵심은 `insertElement`를 임시 저장하고, `while` 루프를 통해 자신보다 큰 요소들을 오른쪽으로 한 칸씩 밀어내며 동시에 삽입 위치를 찾는 것입니다.

비록 평균 및 최악의 경우 O(n²)의 시간 복잡도로 인해 대규모 데이터 정렬에는 부적합하지만, 최선의 경우 O(n)이라는 **적응성**, O(1)의 추가 공간만 사용하는 **제자리 정렬**, 그리고 **안정성**이라는 중요한 특징을 가지고 있습니다. 이러한 장점들 덕분에 삽입 정렬은 단순히 교육용 알고리즘을 넘어, 작은 크기의 데이터를 처리하거나 고급 정렬 알고리즘의 일부로 사용되는 등 실제 세계에서도 그 가치를 인정받고 있습니다.

### 힙정렬 (heap sort)


#### **서론: 힙 정렬이란 무엇인가?**

힙 정렬은 '힙(Heap)'이라는 자료구조를 이용하여 데이터를 정렬하는 알고리즘입니다. 선택 정렬(Selection Sort)을 응용한 알고리즘이라고 볼 수 있는데, 선택 정렬이 매번 전체 배열에서 최솟값(또는 최댓값)을 찾아 자리를 교환하는 방식이라면, 힙 정렬은 힙이라는 자료구조를 통해 이 '최댓값(또는 최솟값)을 찾는 과정'을 매우 효율적으로 만들어 성능을 극대화한 방법입니다.

힙 정렬의 핵심 아이디어는 두 가지 주요 단계로 나뉩니다.

1.  **힙 구성(Build Heap) 단계:** 정렬되지 않은 입력 배열을 '힙'이라는 특정 조건을 만족하는 트리 구조로 만듭니다. 일반적으로 최댓값을 기준으로 하는 '최대 힙(Max-Heap)'을 사용합니다.
2.  **정렬(Sorting) 단계:** 힙의 루트(root) 노드는 항상 최댓값을 가집니다. 이 루트 노드의 값을 배열의 가장 마지막 요소와 교환합니다. 그리고 힙의 크기를 하나 줄인 후, 흐트러진 힙 구조를 다시 바로잡습니다. 이 과정을 힙에 요소가 하나만 남을 때까지 반복합니다. 결과적으로 배열의 끝에서부터 가장 큰 값들이 차례대로 정렬됩니다.

힙 정렬은 평균과 최악의 경우 모두 **O(n log n)**의 시간 복잡도를 가지며, 추가적인 메모리 공간을 거의 사용하지 않는 **제자리 정렬(in-place sort)**이라는 큰 장점을 가집니다. 이제 슬라이드의 내용을 따라가며 이 두 단계를 원자 단위까지 분해하여 살펴보겠습니다.

---

#### **제1부: 핵심 자료구조 '힙(Heap)'의 이해**

힙 정렬을 이해하기 위해서는 먼저 '힙'이 무엇인지 정확히 알아야 합니다.

**1. 완전 이진 트리 (Complete Binary Tree)**

힙은 기본적으로 **완전 이진 트리**의 형태를 가집니다. 완전 이진 트리란, 마지막 레벨을 제외한 모든 레벨이 완전히 채워져 있으며, 마지막 레벨의 노드들은 왼쪽부터 차례대로 채워져 있는 이진 트리를 말합니다. 이 구조 덕분에 힙은 배열을 사용하여 매우 효율적으로 표현할 수 있습니다.

**2. 배열로 힙 표현하기**

트리 구조를 배열로 표현할 때, 특정 노드의 인덱스를 알면 그 부모와 자식 노드의 인덱스를 간단한 수식으로 계산할 수 있습니다. 슬라이드에서 배열의 인덱스가 1부터 시작하는 것을 기준으로 설명하겠습니다.

*   **노드 `i`의 부모 노드 인덱스:** `floor(i / 2)`
*   **노드 `i`의 왼쪽 자식 노드 인덱스:** `2 * i`
*   **노드 `i`의 오른쪽 자식 노드 인덱스:** `2 * i + 1`

예를 들어, 3번 노드의 부모는 `floor(3/2) = 1`번 노드이고, 자식은 `2*3=6`번(왼쪽)과 `2*3+1=7`번(오른쪽) 노드가 됩니다. 이 규칙성 덕분에 포인터 없이도 배열 인덱스만으로 트리 관계를 파악할 수 있습니다.

**3. 힙 조건 (Heap Property)**

힙은 완전 이진 트리 구조에 더해 다음과 같은 '힙 조건'을 만족해야 합니다.

*   **최대 힙 (Max-Heap):** 모든 노드에 대해, 부모 노드의 값은 자식 노드의 값보다 항상 크거나 같다. (`A[부모] ≥ A[자식]`) 이 경우, 트리의 루트 노드는 전체 데이터 중 최댓값을 가지게 됩니다. 힙 정렬에서는 주로 최대 힙을 사용합니다.
*   **최소 힙 (Min-Heap):** 모든 노드에 대해, 부모 노드의 값은 자식 노드의 값보다 항상 작거나 같다. (`A[부모] ≤ A[자식]`) 이 경우, 루트 노드는 최솟값을 가집니다.

슬라이드의 내용은 최댓값을 찾아 정렬하는 것을 목표로 하므로, 이하의 모든 설명은 **최대 힙**을 기준으로 합니다.

---

#### **제2부: 힙 구성(Build Heap) 단계 상세 분석**

힙 정렬의 첫 번째 단계는 주어진 배열을 최대 힙으로 변환하는 것입니다. 이 과정을 `buildHeap` 함수가 수행합니다.

##### **슬라이드 39: 내부 노드의 힙 조건 확인**

이 슬라이드는 힙을 만들어가는 과정의 가장 기본적인 연산, 즉 특정 노드 `x`에서 힙 조건이 깨졌을 때 이를 어떻게 복구하는지에 대한 아이디어를 설명합니다. 이 연산을 보통 `heapify` 또는 슬라이드에서처럼 `pushDown`이라고 부릅니다.

어떤 내부 노드 `x`와 그 자식 노드들 `a`, `b`가 있다고 가정해 봅시다.

*   **경우 1: `A[x] ≥ A[a]` and `A[x] ≥ A[b]`**
    *   이것은 노드 `x`가 자신의 두 자식보다 모두 크거나 같다는 의미입니다. 즉, 노드 `x`를 루트로 하는 작은 부분 트리(subtree)에서는 이미 최대 힙 조건이 만족된 상태입니다. 이 경우 아무것도 할 필요가 없습니다.

*   **경우 2: 그렇지 않다면 (즉, `A[x]`가 자식 중 하나보다 작다면)**
    *   힙 조건이 깨졌습니다. `A[x]`는 `A[a]`와 `A[b]` 중 더 큰 값과 자리를 바꿔야 합니다. 예를 들어 `A[b]`가 더 크다면, `A[x]`와 `A[b]`를 교환(swap)합니다.
    *   **교환 후 문제 발생:** 교환을 하고 나면, 이제 노드 `x` 위치에는 원래 자식 노드 중 더 큰 값이 왔으므로, `x` 위치에서는 힙 조건이 만족됩니다. 하지만 원래 `A[x]`의 값이 내려간 자식 노드 위치(예: `b`)에서는 또다시 힙 조건이 깨질 수 있습니다. 그 자식 노드 `b`가 자신의 새로운 자식들보다 작을 수 있기 때문입니다.
    *   **해결책: 재귀적(또는 반복적) 처리:** 따라서, 값이 아래로 내려간 그 자식 노드를 새로운 `x`로 삼아, 힙 조건이 만족될 때까지 이 과정을 계속해서 반복해야 합니다. 값이 "아래로 가라앉는(sift-down)" 또는 "밀려 내려가는(push-down)" 과정이라고 할 수 있습니다. 이 과정은 해당 값이 리프 노드에 도달하거나, 또는 자신의 자식들보다 커져서 힙 조건을 만족하는 위치에 도달하면 멈춥니다.

이 `pushDown` 연산이 `buildHeap`과 힙 정렬의 핵심 엔진입니다.

##### **슬라이드 40 & 43: `buildHeap` 알고리즘의 구조**

`buildHeap`은 어떻게 전체 배열을 힙으로 만들까요? 모든 노드를 하나씩 검사할 필요가 없습니다. 중요한 사실은, **리프 노드(leaf node)는 그 자체로 크기 1인 힙**이라는 점입니다. 자식이 없으므로 힙 조건을 깰 일이 없기 때문입니다.

따라서 우리는 리프 노드가 아닌, **가장 마지막 내부 노드(internal node)**부터 시작해서 거꾸로 루트 노드 방향으로 올라오면서 각 노드에 대해 `pushDown` 연산을 수행하면 됩니다.

배열의 크기가 `n`일 때, 마지막 노드의 인덱스는 `n`입니다. 이 노드의 부모는 `floor(n/2)`입니다. 즉, `floor(n/2)` 인덱스를 가진 노드가 바로 가장 마지막 내부 노드입니다. 그 뒤의 `floor(n/2) + 1`부터 `n`까지의 노드들은 모두 리프 노드입니다.

`buildHeap`의 구체적인 로직을 슬라이드 코드와 함께 분석해 보겠습니다.

```c
buildHeap(A, eh) // eh는 힙의 마지막 요소 지수, 처음엔 n
  bh = (eh / 2) + 1 // 힙 내의 첫 번째 잎의 지수. 이 코드에 따르면 루프 시작 전 1을 빼야 함.
  while bh > 1 {
    bh = bh - 1
    x = bh
    pushDown(A, x, bh, eh) // x에서 pushDown 시작
  }
```

*   `eh`: `end of heap`의 약자로, 현재 힙으로 간주되는 배열의 마지막 인덱스를 나타냅니다. `buildHeap`을 처음 호출할 때는 배열 전체이므로 `n`이 됩니다.
*   `bh = (n / 2) + 1`: 슬라이드에서는 `bh`를 첫 번째 잎의 지수로 초기화한 후, 루프에 진입하면서 바로 1을 뺍니다. 따라서 실질적으로 `bh`는 `n/2`부터 시작하게 됩니다. `n/2`는 마지막 내부 노드의 인덱스입니다.
*   `while bh > 1`: `bh`가 1이 될 때까지, 즉 루트 노드까지 `pushDown`을 수행하고 나면 루프가 종료됩니다. `bh`는 1씩 감소하며 `n/2, n/2 - 1, ..., 2, 1` 순서로 진행됩니다.
*   `x = bh`: 현재 힙 조건을 확인할 노드의 인덱스를 `x`에 저장합니다.
*   `pushDown(A, x, bh, eh)`: `x`번 노드에서부터 값이 아래로 내려가야 한다면 내려보내서, `x`를 루트로 하는 서브트리를 최대 힙으로 만듭니다.

이 과정을 거치면, 아래쪽의 작은 서브트리부터 시작해서 점차 위로 올라오면서 전체 트리가 최대 힙 조건을 만족하게 됩니다. 아래쪽 서브트리들이 이미 힙으로 만들어져 있기 때문에, 상위 노드에서 `pushDown`을 수행할 때 그 아래 구조는 이미 힙이라는 가정이 성립되어 연산이 올바르게 동작합니다.

##### **슬라이드 41, 44, 45: `pushDown`과 `findLarger` 상세 분석**

`pushDown`은 `buildHeap`의 핵심 작업자입니다. `x` 위치의 값이 제자리를 찾아 아래로 내려가는 과정을 구현합니다.

```c
pushDown(A, x, bh, eh)
  y = findLarger(A, x, eh) // A[x]보다 큰 값을 가지는 x의 자식 노드 지수를 찾음
  while (A[x] < A[y] && y는 유효한 인덱스) { // y가 유효하지 않으면 루프 조건이 거짓이 되어야 함
    swap(A[x], A[y]) // 부모와 더 큰 자식을 교환
    x = y // 이제 검사 위치는 아래로 내려간 자식의 위치
    y = findLarger(A, x, eh) // 새로운 위치에서 다시 더 큰 자식을 찾음
  }
```

*   **`findLarger(A, x, eh)`:** 이 함수는 `pushDown`의 보조 함수입니다. 노드 `x`의 두 자식 중 더 큰 값을 가진 자식의 인덱스를 반환합니다. 만약 부모인 `A[x]`가 두 자식보다 모두 크다면, 교환할 필요가 없으므로 `pushDown`의 `while` 루프가 시작되지 않도록 특별한 값(예: `x` 자신 또는 유효하지 않은 인덱스)을 반환해야 합니다. 슬라이드의 코드를 좀 더 자세히 보겠습니다.

```c
findLarger(A, x, eh)
  // 자식 노드가 둘 다 힙 내에 존재하는 경우 (2x+1 <= eh)
  if 2*x + 1 <= eh {
    // 자식 둘 다 부모보다 큰 경우 (슬라이드 코드에 이 부분이 빠져있어 보강)
    if A[2*x] > A[x] || A[2*x+1] > A[x] {
      if A[2*x] >= A[2*x+1] {
        y = 2*x // 왼쪽 자식이 더 크거나 같으면 왼쪽 자식 선택
      } else {
        y = 2*x + 1 // 오른쪽 자식이 더 크면 오른쪽 자식 선택
      }
    } else {
      // 두 자식 모두 부모보다 작거나 같으면 교환 불필요
      // pushDown 루프를 멈추게 할 값을 리턴해야 함.
      // 예를 들어 y = x를 리턴하면 A[x] < A[y]가 거짓이 되어 루프 종료.
      return x; // 슬라이드에는 이 부분이 명시적이지 않음
    }
  // 자식 노드가 왼쪽 하나만 힙 내에 존재하는 경우 (2x <= eh)
  } else if 2*x <= eh && A[2*x] > A[x] {
    y = 2*x // 왼쪽 자식이 부모보다 크면 왼쪽 자식 선택
  } else {
    // 자식이 없거나, 있어도 부모보다 작으면 교환 불필요
    return x;
  }
  return y
```

*   **`while A[x] < A[y]`:** 이 조건이 `pushDown`의 핵심입니다. 부모(`A[x]`)가 더 큰 자식(`A[y]`)보다 작을 때만 루프를 실행합니다. 즉, 힙 조건이 깨졌을 때만 교환과 하강을 반복합니다. `findLarger`가 `x`를 반환했다면 `A[x] < A[x]`는 거짓이므로 루프가 돌지 않습니다.
*   `swap(A[x], A[y])`: 부모와 더 큰 자식의 값을 교환합니다.
*   `x = y`: 이제 원래 부모 값이 내려간 위치가 `y`이므로, 다음 검사는 이 위치에서 시작해야 합니다. `x`를 `y`로 업데이트합니다.
*   `y = findLarger(A, x, eh)`: 새로운 `x` 위치에서 다시 더 큰 자식을 찾아 `y`를 업데이트하고, 루프 조건을 다시 검사합니다.

##### **`buildHeap` 구체적인 예시**

배열 `A = [-, 4, 10, 3, 5, 1, 8]` (인덱스 1부터 사용)가 있다고 가정해 봅시다. `n=6`.

1.  **초기 상태:**
    *   `n = 6`. 마지막 내부 노드는 `floor(6/2) = 3`번 인덱스. `bh`는 3부터 1까지 감소하며 진행됩니다.
    *   `eh = 6`.

2.  **`bh = 3` (값: 3)일 때:**
    *   `x=3`. 자식은 `2*3=6`번(값: 8) 하나뿐입니다.
    *   `A[3]`(3) < `A[6]`(8) 이므로 `pushDown`이 동작합니다.
    *   `swap(A[3], A[6])`. 배열은 `[-, 4, 10, 8, 5, 1, 3]`이 됩니다.
    *   3번 노드를 루트로 하는 서브트리는 힙이 되었습니다.

3.  **`bh = 2` (값: 10)일 때:**
    *   `x=2`. 자식은 `2*2=4`번(값: 5)과 `2*2+1=5`번(값: 1)입니다.
    *   `A[2]`(10)은 자식들(5, 1)보다 모두 크므로 힙 조건 만족. `pushDown`은 아무것도 하지 않습니다.

4.  **`bh = 1` (값: 4)일 때:**
    *   `x=1`. 자식은 `2*1=2`번(값: 10)과 `2*1+1=3`번(값: 8)입니다.
    *   `A[1]`(4)은 두 자식보다 모두 작습니다. 더 큰 자식은 `A[2]`(10)입니다.
    *   `pushDown` 동작: `swap(A[1], A[2])`. 배열은 `[-, 10, 4, 8, 5, 1, 3]`이 됩니다.
    *   이제 원래 값 4가 2번 인덱스로 내려왔습니다. `x`는 2가 됩니다. 여기서 `pushDown`을 계속합니다.
    *   새로운 `x=2`. 자식은 `2*2=4`번(값: 5)과 `2*2+1=5`번(값: 1)입니다.
    *   `A[2]`(4)는 자식 `A[4]`(5)보다 작습니다. 더 큰 자식은 `A[4]`입니다.
    *   `swap(A[2], A[4])`. 배열은 `[-, 10, 5, 8, 4, 1, 3]`이 됩니다.
    *   원래 값 4가 4번 인덱스로 내려왔습니다. 4번 노드는 리프 노드이므로 `pushDown`이 종료됩니다.

최종적으로 `buildHeap`이 완료된 배열은 `[-, 10, 5, 8, 4, 1, 3]`이며, 이는 최대 힙 구조를 가집니다.

---

#### **제3부: 정렬(Sorting) 단계 상세 분석**

`buildHeap`을 통해 배열이 최대 힙이 되었다면, 이제 두 번째 단계인 실제 정렬을 수행합니다. 이 과정은 매우 직관적입니다.

##### **슬라이드 42: `HeapSort` 알고리즘**

```c
HeapSort(A[1..n])
  eh = n
  buildHeap(A, eh) // 1. 배열 A를 힙으로 만든다

  // 2. 힙에서 최댓값을 제거하고 남은 트리를 다시 힙으로 만든다
  while( eh > 1) {
    swap(A[1], A[eh]) // 최댓값(루트)을 힙의 마지막 요소와 교환
    eh = eh - 1 // 힙의 크기를 1 줄인다
    pushDown(A, 1, 1, eh) // 루트에서부터 힙 속성을 복원한다
  }
```

이 `while` 루프가 정렬의 핵심입니다. 루프가 한 번 돌 때마다 가장 큰 요소 하나가 제자리를 찾아갑니다.

1.  **`swap(A[1], A[eh])`:**
    *   최대 힙의 속성에 따라, 현재 힙에서 가장 큰 값은 항상 루트, 즉 `A[1]`에 있습니다.
    *   `A[eh]`는 현재 힙의 가장 마지막 요소입니다.
    *   이 둘을 교환하면, 가장 큰 값인 `A[1]`이 배열의 `eh` 위치로 이동합니다. 이 위치가 바로 이 값의 최종 정렬 위치입니다.
    *   원래 `A[eh]`에 있던 (상대적으로 작은) 값은 루트인 `A[1]`로 올라갑니다.

2.  **`eh = eh - 1`:**
    *   `eh` 위치에는 이제 최댓값이 고정되었으므로, 이 위치는 더 이상 힙의 일부가 아닙니다.
    *   힙의 유효 범위를 1부터 `eh-1`까지로 줄입니다. 이것은 마치 힙에서 최댓값을 "제거"한 것과 같은 효과를 냅니다.

3.  **`pushDown(A, 1, 1, eh)`:**
    *   `swap`으로 인해 새로운 값(원래 마지막 요소)이 루트(`A[1]`)로 올라왔습니다. 이 값은 거의 확실하게 최댓값이 아니므로, 루트에서 힙 조건이 깨졌을 것입니다.
    *   따라서 `pushDown`을 루트(인덱스 1)에서부터 호출하여 이 값을 올바른 위치까지 아래로 내려보냅니다. 이 과정을 통해 힙의 크기가 1 줄어든 새로운 힙이 다시 최대 힙 속성을 만족하게 됩니다.

4.  **`while (eh > 1)`:**
    *   이 과정을 힙의 크기가 1이 될 때까지 (즉, `eh`가 1이 될 때까지) 반복합니다. `eh`가 1이 되면 배열의 첫 번째 요소 하나만 남게 되는데, 이 요소는 자연히 가장 작은 값이므로 이미 정렬된 상태입니다.
    *   루프가 종료되면, 배열 `A`는 오름차순으로 완벽하게 정렬됩니다. `A[1]`에는 가장 작은 값, `A[n]`에는 가장 큰 값이 위치하게 됩니다.

##### **정렬 단계 구체적인 예시**

`buildHeap` 결과인 `A = [-, 10, 5, 8, 4, 1, 3]` (`n=6`)을 가지고 정렬을 시작하겠습니다.

*   **초기 힙:** `[10, 5, 8, 4, 1, 3]`, `eh = 6`

1.  **첫 번째 반복 (`eh = 6`):**
    *   `swap(A[1], A[6])`: `10`과 `3`을 교환.
        *   `A` becomes `[3, 5, 8, 4, 1, 10]`
    *   `eh = 5`. 이제 `A[6]`의 `10`은 정렬 완료된 부분입니다. 힙은 `[3, 5, 8, 4, 1]` 입니다.
    *   `pushDown(A, 1, 1, 5)`: 루트의 `3`을 `pushDown` 합니다.
        *   `3`의 자식은 `5`(2번), `8`(3번). 더 큰 자식은 `8`.
        *   `swap(A[1], A[3])`. `A` becomes `[8, 5, 3, 4, 1, 10]`.
        *   `3`이 내려간 3번 인덱스는 리프이므로 `pushDown` 종료.
    *   **결과 힙:** `[8, 5, 3, 4, 1]`, **정렬된 부분:** `[10]`

2.  **두 번째 반복 (`eh = 5`):**
    *   `swap(A[1], A[5])`: `8`과 `1`을 교환.
        *   `A` becomes `[1, 5, 3, 4, 8, 10]`
    *   `eh = 4`. 이제 `A[5]`의 `8`도 정렬 완료. 힙은 `[1, 5, 3, 4]` 입니다.
    *   `pushDown(A, 1, 1, 4)`: 루트의 `1`을 `pushDown` 합니다.
        *   `1`의 자식은 `5`(2번), `3`(3번). 더 큰 자식은 `5`.
        *   `swap(A[1], A[2])`. `A` becomes `[5, 1, 3, 4, 8, 10]`.
        *   `1`이 내려간 2번 인덱스의 자식은 `4`(4번). `1` < `4` 이므로 교환.
        *   `swap(A[2], A[4])`. `A` becomes `[5, 4, 3, 1, 8, 10]`.
    *   **결과 힙:** `[5, 4, 3, 1]`, **정렬된 부분:** `[8, 10]`

3.  **세 번째 반복 (`eh = 4`):**
    *   `swap(A[1], A[4])`: `5`와 `1`을 교환.
        *   `A` becomes `[1, 4, 3, 5, 8, 10]`
    *   `eh = 3`. 힙은 `[1, 4, 3]` 입니다.
    *   `pushDown(A, 1, 1, 3)`: 루트의 `1`을 `pushDown` 합니다.
        *   자식은 `4`(2번), `3`(3번). 더 큰 자식은 `4`.
        *   `swap(A[1], A[2])`. `A` becomes `[4, 1, 3, 5, 8, 10]`.
    *   **결과 힙:** `[4, 1, 3]`, **정렬된 부분:** `[5, 8, 10]`

4.  ... 이 과정을 `eh`가 1이 될 때까지 반복합니다. 최종적으로 배열은 `[1, 3, 4, 5, 8, 10]` 으로 정렬됩니다.

---

#### **제4부: 시간 복잡도 분석**

힙 정렬의 효율성을 이해하는 것은 매우 중요합니다. 슬라이드 46, 47의 내용을 더 깊이 있게 풀어보겠습니다.

##### **단계 1: `buildHeap`의 시간 복잡도 - O(n)**

많은 사람들이 `buildHeap`이 `n/2`개의 노드에 대해 `pushDown`을 수행하고, 각 `pushDown`은 트리의 높이(`log n`)에 비례하므로 `O(n log n)`이라고 착각하기 쉽습니다. 하지만 실제로는 더 타이트한 분석을 통해 **O(n)**임을 보일 수 있습니다.

*   **분석의 핵심:** `pushDown`의 연산량은 해당 노드가 얼마나 아래로 내려가는지에 따라 결정됩니다. 즉, 해당 노드로부터 리프 노드까지의 높이에 비례합니다.
*   트리의 높이를 `h` (약 `log n`)라고 합시다.
*   **리프 노드들 (높이 0):** 약 `n/2`개의 노드가 있습니다. 이들은 `pushDown`을 하지 않습니다. 연산량 = 0.
*   **리프 바로 위 레벨의 노드들 (높이 1):** 약 `n/4`개의 노드가 있습니다. 이들은 최악의 경우 1 레벨만 내려갑니다. 연산량 ∝ `(n/4) * 1`.
*   **높이 `k`인 노드들:** 약 `n / 2^(k+1)`개의 노드가 있고, 이들은 최악의 경우 `k` 레벨을 내려갑니다. 연산량 ∝ `(n / 2^(k+1)) * k`.
*   **루트 노드 (높이 h):** 1개의 노드가 있고, 최악의 경우 `h` 레벨을 내려갑니다. 연산량 ∝ `1 * h`.

이를 수식으로 표현하면 총 연산량은 다음과 같은 급수의 합에 비례합니다.
`S = Σ (k * (n / 2^(k+1)))` (k는 0부터 h까지)
`S = (n/2) * Σ (k / 2^k)`

여기서 `Σ (k / 2^k)` (k=0 to ∞)는 2로 수렴하는 잘 알려진 무한 등비급수입니다. 따라서 총 연산량 `S`는 `(n/2) * 2 = n`에 비례하게 됩니다.

결론적으로, **`buildHeap` 단계는 O(n)의 선형 시간**이 걸립니다. 대부분의 노드가 트리의 아래쪽에 몰려 있어 `pushDown`을 하더라도 멀리 이동하지 않기 때문입니다.

##### **단계 2: 정렬의 시간 복잡도 - O(n log n)**

이 단계의 분석은 더 직관적입니다.

*   `while` 루프는 `eh`가 `n`부터 2까지 총 `n-1`번 반복됩니다.
*   각 반복에서 가장 많은 시간을 소요하는 작업은 `pushDown(A, 1, ..., eh)` 입니다.
*   이 `pushDown`은 항상 루트에서 시작합니다. 연산량은 현재 힙의 높이에 비례합니다.
*   힙의 크기가 `k`일 때 높이는 `O(log k)`입니다.
*   루프가 진행됨에 따라 힙의 크기는 `n-1, n-2, ..., 2`로 줄어들지만, 각 `pushDown`의 시간 복잡도는 상한선인 `O(log n)`으로 볼 수 있습니다.

따라서 총 시간 복잡도는 다음과 같습니다.
`T(n) = (n-1) * (한 번의 pushDown 연산)`
`T(n) = (n-1) * O(log n) = O(n log n)`

##### **힙 정렬의 전체 시간 복잡도**

전체 시간 복잡도는 두 단계의 합입니다.
`Total = buildHeap 시간 + 정렬 시간`
`Total = O(n) + O(n log n) = O(n log n)`

지배적인 항이 `O(n log n)`이므로, 힙 정렬의 최종 시간 복잡도는 **O(n log n)**이 됩니다. 이는 평균, 최선, 최악의 경우 모두 동일하게 적용되어, 어떤 입력 데이터가 들어와도 안정적인 성능을 보장하는 장점이 있습니다.

---

#### **제5부: 힙 정렬의 특징 및 요약 (슬라이드 48 기반)**

1.  **핵심 아이디어:** 힙(Heap)이라는 자료구조를 사용하여 정렬합니다. 먼저 전체 배열을 힙으로 만들고(`buildHeap`), 그 후 힙의 루트(최댓값)를 반복적으로 제거하여 정렬된 위치에 배치합니다.

2.  **시간 복잡도 (Time Complexity):** **O(n log n)**. 입력 데이터의 초기 상태와 관계없이 항상 O(n log n)의 성능을 보장합니다. 이는 퀵 정렬이 최악의 경우 O(n^2)까지 성능이 저하될 수 있는 것과 비교되는 큰 장점입니다.

3.  **공간 복잡도 (Space Complexity):** **O(1)**. 주어진 배열 내에서 요소들의 위치를 교환하는 방식으로 정렬이 이루어지므로, 별도의 추가 메모리 공간이 거의 필요 없습니다. 이를 **제자리 정렬(In-place Sort)**이라고 하며, 메모리가 제한적인 환경에서 매우 유용합니다.

4.  **불안정 정렬 (Unstable Sort):** 힙 정렬은 안정적인 정렬이 아닙니다. 안정 정렬이란, 값이 같은 요소들의 상대적인 순서가 정렬 후에도 유지되는 것을 의미합니다. 힙 정렬에서는 `pushDown` 과정이나 루트와 마지막 노드를 교환하는 과정에서 같은 값을 가진 요소들의 순서가 뒤바뀔 수 있습니다.

**선택/삽입 정렬과의 비교**

*   **선택 정렬 (O(n^2)):** 매번 최솟값(최댓값)을 찾기 위해 `n`번의 순회를 해야 합니다. 힙 정렬은 힙 구조를 이용해 최댓값을 `O(1)`에 찾고, 구조를 재조정하는 데 `O(log n)`만 사용하므로 훨씬 효율적입니다.
*   **삽입 정렬 (최선 O(n), 최악 O(n^2)):** 이미 정렬된 데이터에 대해서는 매우 빠르지만, 역순 데이터에 대해서는 성능이 급격히 저하됩니다. 힙 정렬은 데이터 분포에 영향을 받지 않습니다.

#### **결론**

힙 정렬은 이론적으로나 실제적으로나 매우 뛰어난 정렬 알고리즘입니다. `buildHeap`의 선형 시간 복잡도, 전체적인 `O(n log n)`의 보장된 성능, 그리고 `O(1)`의 추가 공간 사용량은 힙 정렬을 다양한 상황에서 신뢰할 수 있는 선택지로 만듭니다. 제공된 슬라이드는 이러한 힙 정렬의 핵심적인 구성 요소인 `buildHeap`, `pushDown`, 그리고 반복적인 최댓값 추출 과정을 간결하면서도 정확하게 보여주고 있습니다. 이 상세 설명을 통해 알고리즘의 모든 단계를 완벽하게 이해하고, 직접 구현하거나 분석하는 데 큰 도움이 되기를 바랍니다.
### 제4장: 그래프(graph)
#### 1. 시작: 우리는 왜 그래프를 탐색해야 할까?

컴퓨터 과학에서 '그래프(Graph)'는 단순히 그림이 아닙니다. 정점(Vertex)과 그들을 잇는 간선(Edge)의 집합으로, 우리 주변의 수많은 관계와 구조를 표현하는 강력한 도구입니다.

*   **도시와 도로망:** 각 도시는 정점, 도시를 잇는 도로는 간선입니다.
*   **소셜 네트워크:** 각 사람은 정점, 친구 관계는 간선입니다.
*   **웹페이지와 링크:** 각 웹페이지는 정점, 하이퍼링크는 간선입니다.

이런 그래프가 주어졌을 때, 우리에게는 종종 이런 질문이 생깁니다.
"A 도시에서 B 도시까지 갈 수 있는 길이 있는가?"
"내 친구의 친구를 따라가면, 결국 유명인 C와 연결될 수 있을까?"
"이 웹사이트의 모든 페이지를 빠짐없이 방문하려면 어떤 순서로 링크를 클릭해야 할까?"

이 모든 질문에 답하기 위한 가장 기본적인 행위가 바로 **그래프 탐색(Graph Traversal)**입니다. 즉, 그래프의 모든 정점을 체계적으로, 빠짐없이 한 번씩 방문하는 것입니다. '깊이 우선 탐색(DFS)'은 이 그래프 탐색을 수행하는 대표적인 두 가지 방법 중 하나입니다.

#### 2. 탐색을 위한 준비: 그래프의 기본 용어와 표현

탐색을 시작하기 전에, 우리가 탐색할 '지도', 즉 그래프에 대한 몇 가지 용어를 명확히 해야 합니다. (슬라이드 8, 9, 10 내용)

##### 2.1. 그래프의 핵심 용어

*   **연결 그래프 (Connected Graph):** 그래프 내의 어떤 두 정점을 선택하더라도, 그 두 정점 사이에 항상 경로가 존재하는 그래프입니다. 즉, 모든 정점들이 직간접적으로 서로 연결되어 있어 '섬'처럼 고립된 정점이 없는 상태를 말합니다. 우리가 다룰 대부분의 탐색 문제는 이 연결 그래프를 가정합니다.
*   **경로 (Path):** 한 정점에서 다른 정점으로 간선을 따라 이동하는 길입니다. 예를 들어 {1, 3, 4, 6}은 정점 1에서 6으로 가는 경로입니다.
*   **순환 (Cycle):** 시작점으로 다시 돌아오는 경로 중, 같은 간선을 중복해서 지나지 않는 경로를 말합니다. {1, 3, 2, 1}은 순환입니다. 그래프에 순환이 있다는 것은 한 지점에서 출발해 다른 길로 갔다가 다시 원래 자리로 돌아올 수 있음을 의미합니다. 이것은 DFS 탐색에서 매우 중요한 특징이 됩니다.

##### 2.2. 컴퓨터는 그래프를 어떻게 기억할까?

우리는 그래프를 눈으로 보고 이해하지만, 컴퓨터는 그렇지 못합니다. 컴퓨터에게 그래프의 구조를 알려주기 위한 두 가지 대표적인 방법이 있습니다.

*   **인접 목록 (Adjacency List):** 각 정점마다 "나와 연결된 이웃은 누구누구야"라고 목록을 만들어주는 방식입니다.
    *   정점 1의 이웃: [2, 3, 5]
    *   정점 2의 이웃: [1, 3]
    *   정점 3의 이웃: [1, 2, 4, 5, 6]
    *   ... 이런 식으로 모든 정점에 대한 이웃 목록을 저장합니다.
    *   **장점:** 간선의 개수가 적은 '저밀도 그래프(Sparse Graph)'에서 메모리를 매우 효율적으로 사용합니다.

*   **인접 행렬 (Adjacency Matrix):** 정점의 개수가 V개일 때, V x V 크기의 표(행렬)를 만들고, 정점 i와 정점 j가 연결되어 있으면 표의 (i, j) 위치에 1을, 아니면 0을 표시하는 방식입니다.
    *   **장점:** 두 정점이 연결되어 있는지 즉시(O(1) 시간) 확인할 수 있어 '고밀도 그래프(Dense Graph)'에서 유리합니다.
    *   **단점:** 정점 개수의 제곱에 비례하는 메모리가 필요해, 정점이 많고 간선이 적으면 메모리 낭비가 심합니다.

이 표현 방법의 선택은 나중에 설명할 알고리즘의 **시간 복잡도(성능)**에 직접적인 영향을 미칩니다.

#### 3. 미로 탐험가의 전략: 깊이 우선 탐색(DFS)의 원리

이제 본격적으로 깊이 우선 탐색(DFS)에 대해 알아봅시다. DFS의 핵심 철학은 **"일단 갈 수 있는 데까지 끝까지 가본다"** 입니다. (슬라이드 11 내용)

가장 직관적인 비유는 '한쪽 벽 짚고 미로 탈출하기'입니다. 미로에 들어선 탐험가를 상상해 보세요.

1.  **시작:** 입구(시작 정점)에서 출발합니다.
2.  **전진:** 눈앞에 여러 갈래 길이 있다면, 그중 **아무 길이나 하나를 선택**해서 들어갑니다. 그리고 그 길을 따라 계속 직진합니다.
3.  **막다른 길:** 가다 보니 막다른 길(더 이상 갈 곳이 없는 정점)에 도달했습니다.
4.  **후퇴 (Backtracking):** 어쩔 수 없이, 방금 지나왔던 바로 직전의 갈림길로 되돌아갑니다.
5.  **새로운 길 탐색:** 그 갈림길에서, 아까 선택했던 길 말고 **아직 가보지 않은 다른 길이 있다면** 그 길을 선택해 다시 끝까지 가봅니다.
6.  **반복:** 이 과정을 계속 반복합니다. 한 갈림길의 모든 방향을 다 탐색했다면, 또 그 이전의 갈림길로 후퇴하여 새로운 길을 찾습니다.
7.  **종료:** 맨 처음 출발했던 입구로 돌아와, 더 이상 가보지 않은 길이 남아있지 않다면 탐색이 종료됩니다.

이것이 바로 DFS의 전부입니다. '깊이(Depth)'를 '우선(First)'으로 탐색한다는 이름 그대로, 한 경로를 최대한 깊게 파고들고, 더 이상 갈 수 없을 때 비로소 되돌아와 다른 경로를 탐색하는 방식입니다.

#### 4. 실제 탐색 과정 따라가기 (슬라이드 12, 13 예제)
![](./08.media/20251019233513-1760884513410-image.png)
![](./08.media/20251019233533-1760884533049-image.png)

![](./08.media/20251019233555-1760884555038-image.png)





이제 슬라이드의 예제 그래프를 가지고 DFS 탐험가가 되어 직접 탐색을 진행해 보겠습니다. 탐색의 기록을 위해 '방문한 정점'을 표시해 둡시다.

**(초기 상태: 모든 정점은 '방문 안함' 상태)**

1.  **시작점: 정점 1**
    *   탐험가는 정점 1에서 시작합니다. "정점 1 방문!"이라고 외치고, 방문했다고 표시합니다. (방문 순서: 1)
    *   1에서 갈 수 있는 길(인접한 정점)은 2, 3, 5입니다. 탐험가는 이 중 **임의로 정점 3**을 선택합니다.

2.  **정점 1 → 정점 3**
    *   탐험가는 3에 도착합니다. "정점 3 방문!" (방문 순서: 1, 3)
    *   3에서 갈 수 있는 길은 1, 2, 4, 5, 6입니다. 하지만 1은 방금 지나온 곳이니 이미 방문했습니다. 남은 '방문 안한' 길은 2, 4, 5, 6입니다.
    *   탐험가는 이 중 **임의로 정점 2**를 선택합니다.

3.  **정점 3 → 정점 2**
    *   탐험가는 2에 도착합니다. "정점 2 방문!" (방문 순서: 1, 3, 2)
    *   2에서 갈 수 있는 길은 1, 3입니다. 그런데 둘 다 이미 방문한 곳입니다. 더 이상 나아갈 새로운 길이 없습니다. **막다른 길입니다!**

4.  **후퇴 (Backtracking): 정점 2 → 정점 3**
    *   탐험가는 방금 왔던 길을 되짚어 정점 3으로 돌아갑니다.
    *   이제 3의 갈림길에서, 아까 선택했던 2 말고 다른 길을 찾아봅니다. 아직 방문하지 않은 길로 4, 5, 6이 남아있습니다.
    *   탐험가는 이 중 **임의로 정점 4**를 선택합니다.

5.  **정점 3 → 정점 4**
    *   탐험가는 4에 도착합니다. "정점 4 방문!" (방문 순서: 1, 3, 2, 4)
    *   4에서 갈 수 있는 길은 3과 6입니다. 3은 이미 방문했으니, 남은 길은 6뿐입니다.
    *   탐험가는 **정점 6**으로 향합니다.

6.  **정점 4 → 정점 6**
    *   탐험가는 6에 도착합니다. "정점 6 방문!" (방문 순서: 1, 3, 2, 4, 6)
    *   6에서 갈 수 있는 길은 3과 4입니다. 둘 다 이미 방문했습니다. **또 막다른 길입니다!**

7.  **후퇴 (Backtracking): 정점 6 → 정점 4**
    *   탐험가는 정점 4로 돌아갑니다.
    *   4의 갈림길에서, 6 말고 다른 가보지 않은 길이 있는지 확인합니다. 없습니다.
    *   **또 후퇴합니다.** 정점 4에서 왔던 길을 따라 정점 3으로 돌아갑니다.

8.  **후퇴 (Backtracking): 정점 4 → 정점 3**
    *   탐험가는 다시 정점 3에 섰습니다.
    *   지금까지 3에서는 2와 4 방향을 탐색했습니다. 아직 방문하지 않은 길로 5와 6이 남아있... 아, 6은 방금 4를 통해 방문했었죠. 그럼 남은 길은 **정점 5** 뿐입니다.
    *   탐험가는 5로 향합니다.

9.  **정점 3 → 정점 5**
    *   탐험가는 5에 도착합니다. "정점 5 방문!" (방문 순서: 1, 3, 2, 4, 6, 5)
    *   5에서 갈 수 있는 길은 1과 3입니다. 둘 다 이미 방문했습니다. **막다른 길입니다!**

10. **마지막 후퇴 과정**
    *   정점 5에서 정점 3으로 후퇴합니다.
    *   정점 3의 모든 이웃(2, 4, 6, 5)을 다 탐색했습니다. 이제 3에서도 더 이상 갈 곳이 없습니다. 정점 1로 후퇴합니다.
    *   정점 1에 돌아왔습니다. 1에서 출발한 3의 경로를 모두 탐색했습니다. 1의 다른 이웃인 2와 5를 확인해 보니, 탐색 과정 중에 이미 모두 방문된 상태입니다.
    *   시작점 1에서 더 이상 갈 곳이 없으므로, **탐색이 종료됩니다.**

최종적으로 모든 정점(1, 2, 3, 4, 5, 6)을 방문했으며, 방문 순서는 선택에 따라 달라질 수 있지만, 이 예제에서는 `1 → 3 → 2 → 4 → 6 → 5` 순서가 되었습니다.

#### 5. 탐험의 발자취: 깊이 우선 신장 트리(DFS Spanning Tree)

![](./08.media/20251019233758-1760884678121-image.png)

자, 이제 탐험은 끝났습니다. 우리가 이 복잡한 탐색을 마치고 나서, 탐험가가 **새로운 정점을 발견하기 위해 실제로 이동했던 경로**만 남겨보면 어떻게 될까요? 이것이 바로 '깊이 우선 신장 트리' 개념이 등장하는 이유입니다. (슬라이드 14, 15 내용)

*   1에서 3으로 갈 때, 3은 처음 발견된 정점입니다. (1-3 간선 선택)
*   3에서 2로 갈 때, 2는 처음 발견된 정점입니다. (3-2 간선 선택)
*   3에서 4로 갈 때, 4는 처음 발견된 정점입니다. (3-4 간선 선택)
*   4에서 6으로 갈 때, 6은 처음 발견된 정점입니다. (4-6 간선 선택)
*   3에서 5로 갈 때, 5는 처음 발견된 정점입니다. (3-5 간선 선택)

이 간선들 `(1,3), (3,2), (3,4), (4,6), (3,5)`를 모아 그림을 그려보면, 슬라이드 15의 그림처럼 됩니다. 이 그림은 몇 가지 중요한 특징을 가집니다.

1.  **모든 정점을 포함한다:** 원래 그래프의 모든 정점이 다 들어있습니다.
2.  **순환이 없다:** 이 간선들만으로는 시작점으로 되돌아오는 경로(사이클)를 만들 수 없습니다. 이런 구조를 우리는 **트리(Tree)**라고 부릅니다.
3.  **그래프를 '걸쳐'있다(Span):** 원래 그래프의 모든 정점을 포함하면서 트리를 이루기 때문에 **신장 트리(Spanning Tree)**라고 부릅니다.

결론적으로, **깊이 우선 신장 트리**는 복잡한 원래 그래프에서 DFS라는 특정 탐색 방법으로 길을 찾아다닌 '발자취'를 모아놓은 것입니다. 이는 원래 그래프의 모든 정점을 연결하는 핵심적인 '뼈대' 구조를 보여줍니다.


##### 5.1. 뼈대(트리 간선)와 지름길(뒤 간선)

신장 트리를 만들고 나니, 원래 그래프에 있었지만 이 뼈대에는 포함되지 않은 간선들이 있습니다. 슬라이드 15의 점선으로 표시된 `(1,2), (1,5), (3,6)` 같은 간선들입니다. 이들은 무엇일까요?

*   **트리 간선 **(Tree Edge): 실선으로 표시된 간선들. DFS 탐색 중 **새로운 정점을 발견하게 만든** 간선입니다. 즉, 트리의 부모-자식 관계를 형성하는 뼈대 간선입니다.

*   **뒤 간선 **(Back Edge): 점선으로 표시된 간선들. DFS 탐색 중, **이미 방문했지만 아직 탐색이 완료되지 않은 정점**(즉, 현재 재귀 스택에 남아 있는 조상)으로 연결되는 간선입니다. 예를 들어, 정점 2에서 인접한 정점 1을 확인했을 때, 1은 이미 방문한 상태였고 아직 함수 호출이 끝나지 않았다면, 간선 `(2,1)`은 뒤 간선입니다. 이 간선은 신장 트리에서 **자손 노드가 자신의 조상 노드**(부모 포함) 역할을 합니다.

*   **순방향 간선 **(Forward Edge): DFS 탐색 중, **이미 완전히 탐색이 끝난 자손 정점**으로 연결되는 간선입니다. 즉, 트리 경로를 따라 갈 수 있는 자손이지만, 해당 간선 자체는 탐색 과정에서 사용되지 않은 경우입니다. 예: `(1 → 3 → 4)`가 트리 경로일 때, 간선 `(1 → 4)`가 있다면 이는 순방향 간선입니다.

*   **교차 간선 **(Cross Edge): DFS 탐색 중, **서로 다른 서브트리에 속한 정점들 사이**를 연결하거나, **이미 완전히 탐색이 끝난 조상이 아닌 정점**으로 연결되는 간선입니다. 이 간선은 트리 구조 내에서 선조-자손 관계가 전혀 없는 정점 쌍을 잇습니다.

---

###### 🔍 중요한 차이: 무방향 그래프 vs. 방향 그래프

*   **무방향 그래프**(Undirected Graph)에서는 간선이 항상 양방향으로 존재하므로, DFS 과정에서 간선은 오직 **트리 간선** 또는 **뒤 간선**으로만 분류됩니다.  
    → **순방향 간선과 교차 간선은 무방향 그래프에서는 존재하지 않습니다**.

*   **방향 그래프**(Directed Graph)에서는 간선의 방향이 고정되어 있으므로, 위의 네 가지 유형(**Tree, Back, Forward, Cross**)이 모두 나타날 수 있습니다.

---

###### 🔄 매우 중요한 사실

그래프에서 **순환**(Cycle)이 존재하는지 여부는 **뒤 간선**(Back Edge)의 존재 여부와 정확히 일치합니다.  
→ **무방향 그래프**에서는 뒤 간선이 있으면 반드시 순환이 존재하며,  
→ **방향 그래프**에서도 뒤 간선이 있으면 순환이 존재합니다.  
(단, 방향 그래프에서는 순환을 구성하는 데 반드시 뒤 간선이 필요하며, 다른 간선 유형만으로는 순환을 만들 수 없습니다.)

따라서 DFS는 이 특성을 이용해 **그래프의 순환을 매우 효과적으로 탐지**하는 알고리즘으로 널리 사용됩니다.

#### 6. 알고리즘으로 정리하기 (슬라이드 16, 17)

우리가 지금까지 말로 설명한 탐험가의 행동을 컴퓨터가 알아들을 수 있는 언어, 즉 알고리즘(의사코드)으로 정리해 봅시다. 이 과정은 재귀(Recursion) 개념을 사용하면 매우 간결하게 표현됩니다.

**DFS(v): 현재 정점 v에서 탐색을 시작하라**
1.  **도착했음을 알린다:** 정점 `v`에 도착했으니, `v`를 방문했다고 표시하고, 필요하다면 `v`의 데이터를 출력한다.
2.  **주변을 살핀다:** `v`에 연결된 모든 이웃 정점 `w`들을 하나씩 살펴본다.
3.  **새로운 길이라면 가본다:** 만약 이웃 `w`가 **'방문 안함' 상태라면**, 그 길로 탐험을 떠난다. 즉, `DFS(w)`를 호출한다. 이 호출이 바로 '더 깊이 들어가는' 재귀적 행위입니다. `DFS(w)`가 모든 탐색을 마치고 돌아올 때까지(리턴될 때까지) 기다린다.
4.  **모든 길을 확인했다면 돌아간다:** `v`의 모든 이웃을 다 확인했다면, `v`에서의 탐색은 끝난 것이다. 이 함수를 호출했던 곳으로 돌아간다(리턴).

이 `DFS(v)` 함수만으로는 모든 그래프를 탐색할 수 없습니다. 만약 그래프가 여러 개의 '섬'으로 나뉘어 있다면(비연결 그래프), 하나의 섬만 탐색하고 끝나버릴 수 있습니다. 그래서 전체를 관리하는 `DFSearch(G)` 함수가 필요합니다.

**DFSearch(G): 그래프 G 전체를 탐색하라**
1.  **준비:** 모든 정점을 '방문 안함'으로 초기화한다.
2.  **모든 정점을 확인:** 그래프의 모든 정점 `v`를 하나씩 순서대로 확인한다.
3.  **아직 방문 안한 곳에서 새로 시작:** 만약 정점 `v`가 아직도 '방문 안함' 상태라면, 그곳은 아직 탐험하지 않은 새로운 '섬'이라는 뜻이다. 그곳에서부터 `DFS(v)`를 호출하여 새로운 탐색을 시작한다.

이 두 함수를 통해 어떤 형태의 그래프라도 모든 정점을 빠짐없이 방문할 수 있습니다.

#### 7. 성능 분석: 얼마나 빠를까? (슬라이드 18)

알고리즘의 효율성을 따지는 것을 시간 복잡도 분석이라고 합니다. DFS의 성능은 컴퓨터가 그래프를 어떻게 저장하고 있느냐(인접 목록 vs 인접 행렬)에 따라 달라집니다. 정점의 수를 V, 간선의 수를 E라고 합시다.

*   **인접 목록(Adjacency List) 사용 시: θ(V + E)**
    *   `DFSearch`에서 모든 정점을 한 번씩 확인합니다 (V만큼의 시간).
    *   `DFS`가 호출되는 과정에서, 모든 정점은 정확히 한 번씩 방문됩니다. 한 정점을 방문할 때마다 그 정점의 인접 목록을 쭉 훑어봅니다. 모든 정점의 인접 목록의 길이를 다 더하면, 무방향 그래프의 경우 2E가 됩니다.
    *   결과적으로, 모든 정점을 한 번씩 방문하고(V), 모든 간선을 (양방향으로) 한 번씩 확인하는(E) 셈이므로, 총 시간은 V와 E에 비례합니다. **θ(V + E)**

*   **인접 행렬(Adjacency Matrix) 사용 시: θ(V²)**
    *   한 정점 `v`에서 갈 수 있는 이웃을 찾으려면, 인접 행렬의 `v`번째 행 전체(V개의 열)를 모두 스캔해야 합니다.
    *   이 작업을 모든 정점 V개에 대해 수행해야 하므로, 총 시간은 V * V = V²에 비례합니다. **θ(V²)**

**결론:** 일반적으로 그래프는 간선이 꽉 찬 경우(Dense)보다 듬성듬성한 경우(Sparse)가 훨씬 많습니다(E << V²). 따라서 **대부분의 경우 인접 목록을 사용하는 것이 훨씬 효율적**이며, DFS의 시간 복잡도는 보통 **O(V+E)**라고 이야기합니다.

#### 최종 정리

1.  **그래프 탐색**은 연결 관계를 파악하기 위한 필수적인 과정입니다.
2.  **DFS(깊이 우선 탐색)**는 "한 우물만 파는" 전략으로, 재귀적인 방식으로 구현하기 좋습니다.
3.  DFS를 수행하면서 **새로운 정점을 발견한 경로**들을 모으면, 그래프의 뼈대인 **깊이 우선 신장 트리**가 만들어집니다.
4.  신장 트리에 포함되지 않은 **뒤 간선(Back Edge)**은 원래 그래프에 **순환(Cycle)이 있음**을 알려주는 중요한 단서입니다.
5.  DFS는 **인접 목록**으로 그래프를 표현할 때 **O(V+E)**의 매우 효율적인 시간 복잡도를 가집니다.

이처럼 DFS는 단순히 정점을 방문하는 것에서 그치지 않고, 그 과정에서 얻어지는 '신장 트리'와 '간선의 분류'라는 부산물을 통해 그래프의 구조적 특성(연결성, 순환 구조 등)을 파악하는 강력한 알고리즘입니다.
#### 6. 탐험가의 또 다른 전략: 너비 우선 탐색(BFS)의 원리

앞서 우리는 '한 우물만 파는' 깊이 우선 탐색(DFS) 탐험가를 만났습니다. 이번에는 성격이 정반대인, 매우 신중하고 체계적인 탐험가를 만나보겠습니다. 이 탐험가의 전략이 바로 **너비 우선 탐색(Breadth-First Search, BFS)**입니다. (슬라이드 20 참고)

DFS의 핵심 철학이 "일단 끝까지 가본다"였다면, BFS의 핵심 철학은 **"가까운 곳부터 차례대로 샅샅이 뒤진다"** 입니다.

이해를 돕기 위해 다른 비유를 들어보겠습니다.

*   **호수에 돌 던지기:** 잔잔한 호수 중앙(시작 정점)에 돌을 던지면, 물결(탐색)이 동심원을 그리며 바깥으로 퍼져나갑니다. 가장 가까운 곳에 첫 번째 원이 생기고, 그다음 조금 더 먼 곳에 두 번째 원이 생깁니다. BFS는 이 물결이 퍼져나가는 순서와 정확히 같습니다.
*   **SNS 친구 찾기:** 여러분(시작 정점)이 SNS에서 "나와 가장 가까운 친구들부터 모두 찾아보자"고 결심했다고 상상해 보세요.
    1.  **Level 0:** 나 자신.
    2.  **Level 1:** 먼저 나의 '직접 친구'들을 모두 찾아서 목록에 올립니다.
    3.  **Level 2:** 그다음, '직접 친구'들의 친구들(즉, 내 친구의 친구들)을 모두 찾습니다.
    4.  **Level 3:** 그 다음엔 '친구의 친구'들의 친구들을 모두 찾습니다.

이처럼, 시작점으로부터 거리가 1인 정점들을 모두 방문하고, 그다음 거리가 2인 정점들을 모두 방문하고, 그다음 거리가 3인 정점들을 모두 방문하는 방식. 이것이 바로 BFS의 본질입니다. '너비(Breadth)'를 '우선(First)'으로 탐색한다는 이름 그대로입니다.

##### 6.1. BFS의 핵심 도구: 큐(Queue)

DFS가 '막다른 길에서 되돌아오기(Backtracking)' 위해 재귀 호출(내부적으로 스택 사용)을 활용했다면, BFS는 이 '레벨(거리) 순서'를 지키기 위해 아주 특별한 자료구조를 사용합니다. 바로 **큐(Queue)**입니다.

큐는 "First-In, First-Out (FIFO)", 즉 **먼저 들어온 것이 먼저 나가는** 구조입니다. 마치 은행 창구나 놀이공원 줄서기와 같습니다.

BFS에서 큐가 어떻게 작동하는지 살펴봅시다.

1.  **탐색 대기줄:** 큐는 '다음에 방문할 정점들의 대기줄'이라고 생각할 수 있습니다.
2.  **규칙 1:** 어떤 정점을 처음 발견하면, 즉시 이 대기줄(큐)의 맨 뒤에 세웁니다.
3.  **규칙 2:** 탐색을 할 때는, 대기줄의 맨 앞에 있는 정점부터 순서대로 처리합니다.

이 간단한 두 규칙이 어떻게 '가까운 곳부터' 탐색하는 것을 보장할까요?
시작 정점 S를 큐에 넣습니다. S를 꺼내면서 S의 이웃인 A, B, C를 발견합니다. 이들을 순서대로 큐에 넣습니다. (큐 상태: [A, B, C])
이제 큐의 맨 앞인 A를 꺼냅니다. A의 이웃을 탐색합니다.
그다음 큐의 맨 앞인 B를 꺼냅니다. B의 이웃을 탐색합니다.
...
이런 식으로, 거리가 1인 A, B, C를 모두 큐에 넣고 처리한 뒤에야, 그들의 이웃인 거리 2의 정점들이 처리될 기회를 얻게 됩니다. 큐의 FIFO 특성이 자연스럽게 레벨 순서의 탐색을 만들어내는 것입니다.

#### 7. 실제 BFS 탐색 과정 따라가기 (슬라이드 20 예제)
![](./08.media/20251020040119-1760900479941-image.png)![](./08.media/20251020040138-1760900498702-image.png)
이제 슬라이드 20의 예제 그래프(DFS에서 사용했던 것과 동일한 무방향 그래프)를 가지고 BFS 탐험을 시작하겠습니다. 탐험을 위해 **큐(Queue)**와 **방문 기록부(Visited Set)**를 준비합시다.

**(초기 상태: 모든 정점 '방문 안함', 큐는 비어있음)**

1.  **시작점: 정점 1**
    *   탐험가는 정점 1에서 시작합니다. "정점 1 방문!"이라고 외치고, 방문 기록부에 표시합니다.
    *   그리고 1을 '다음에 탐색할 대기줄'인 큐에 넣습니다.
    *   **방문 기록:** {1}
    *   **큐 상태:** `[1]`

2.  **Level 1 탐색**
    *   큐가 비어있지 않으므로, 맨 앞의 정점을 꺼냅니다. **정점 1을 Dequeue** 합니다.
    *   이제 1과 연결된 이웃들을 살펴봅니다. 2, 3, 5가 있습니다.
    *   이들은 모두 아직 방문하지 않은 새로운 정점들입니다.
    *   발견한 순서대로 방문 기록부에 표시하고, 큐의 맨 뒤에 차례로 넣습니다.
    *   **방문 기록:** {1, 2, 3, 5}
    *   **큐 상태:** `[2, 3, 5]` (1은 처리되었으므로 빠짐)

3.  **Level 2 탐색 (시작)**
    *   큐의 맨 앞에 있는 **정점 2를 Dequeue** 합니다.
    *   2의 이웃은 1과 3입니다. 방문 기록부를 보니, 둘 다 이미 방문했습니다. 새로 발견한 정점이 없으므로 큐에 추가할 것도 없습니다.
    *   **방문 기록:** {1, 2, 3, 5} (변화 없음)
    *   **큐 상태:** `[3, 5]`

4.  **Level 2 탐색 (계속)**
    *   큐의 맨 앞에 있는 **정점 3을 Dequeue** 합니다.
    *   3의 이웃은 1, 2, 4, 5, 6입니다. 이 중 1, 2, 5는 이미 방문했습니다.
    *   아직 방문하지 않은 **4와 6**을 발견했습니다!
    *   4와 6을 방문 기록부에 표시하고, 큐에 차례로 넣습니다.
    *   **방문 기록:** {1, 2, 3, 5, 4, 6}
    *   **큐 상태:** `[5, 4, 6]`

5.  **Level 2 탐색 (마무리)**
    *   큐의 맨 앞에 있는 **정점 5를 Dequeue** 합니다.
    *   5의 이웃은 1과 3입니다. 둘 다 이미 방문했습니다. 새로 할 일이 없습니다.
    *   **방문 기록:** {1, 2, 3, 5, 4, 6}
    *   **큐 상태:** `[4, 6]`

6.  **Level 3 탐색**
    *   큐의 맨 앞에 있는 **정점 4를 Dequeue** 합니다.
    *   4의 이웃은 3과 6입니다. 둘 다 이미 방문했습니다.
    *   **큐 상태:** `[6]`
    *   큐의 맨 앞에 있는 **정점 6을 Dequeue** 합니다.
    *   6의 이웃은 3과 4입니다. 둘 다 이미 방문했습니다.
    *   **큐 상태:** `[]` (이제 큐가 비었습니다!)

7.  **종료**
    *   큐가 비었으므로, 더 이상 탐색할 정점이 남아있지 않습니다. 탐색을 종료합니다.

최종 방문 순서는 `1, 2, 3, 5, 4, 6`이 되었습니다. 이는 시작점 1에서부터 거리가 0인 정점(1), 거리가 1인 정점들(2, 3, 5), 거리가 2인 정점들(4, 6) 순서로 방문한 것과 정확히 일치합니다.

**중요한 특징:** BFS는 **최단 경로(Shortest Path)**를 찾는 문제의 핵심 알고리즘입니다. 간선의 가중치가 모두 1일 때, 시작점에서 특정 정점까지의 최단 거리는 BFS가 그 정점을 몇 번째 '레벨'에서 발견했는지와 같습니다.

#### 8. 너비 우선 탐색의 발자취: 너비 우선 신장 트리(BFS Spanning Tree)
![](./08.media/20251020040226-1760900546139-image.png)
DFS와 마찬가지로, BFS 탐색 과정에서 **새로운 정점을 발견하기 위해 사용된 간선**들만 모으면 **너비 우선 신장 트리(BFS Spanning Tree)**를 만들 수 있습니다. (슬라이드 22 참고)

*   1에서 2, 3, 5를 발견했습니다. (1-2), (1-3), (1-5) 간선이 트리에 포함됩니다.
*   3에서 4, 6을 발견했습니다. (3-4), (3-6) 간선이 트리에 포함됩니다.

이 간선들을 모아보면 슬라이드 22의 그림이 완성됩니다. 이 트리는 DFS 신장 트리와는 확연히 다른 모양을 가집니다.

*   **DFS 신장 트리:** 길고 얇은 모양. 깊이를 우선하기 때문입니다.
*   **BFS 신장 트리:** 짧고 넓은 모양. 너비를 우선하기 때문입니다.

#### 8.1. BFS 신장 트리에서의 간선 분류

BFS 신장 트리를 만들고 나면, 원래 그래프에 존재했지만 트리에 포함되지 않은 간선들이 남습니다. 슬라이드 22의 점선 `(2,3)`, `(4,6)` 등이 그 예입니다. 이 간선들은 DFS와는 다른 방식으로 해석됩니다.

BFS는 **레벨 순서**(level-order)로 정점을 탐색하므로, 간선의 방향성과 레벨 차이에 따라 다음과 같이 분류할 수 있습니다:

##### 1. **트리 간선 **(Tree Edge)
- BFS 탐색 중 **처음으로 정점을 발견하게 만든 간선**.
- 신장 트리의 실선으로 표현되며, 부모 → 자식 관계를 형성.
- 예: `1 → 2`, `1 → 3`, `2 → 4` 등.

##### 2. **교차 간선 **(Cross Edge)
- **가장 흔한 비트리 간선**.
- 이미 방문된 정점으로 연결되며, **두 정점이 같은 레벨이거나 인접한 레벨**(레벨 차이 ≤ 1)에 있을 때 발생.
- 무방향 그래프에서는 모든 비트리 간선이 교차 간선으로 간주됩니다.
- 예: `(2,3)` → 둘 다 레벨 1, `(4,6)` → 둘 다 레벨 2.
- 방향 그래프에서는 **다른 서브트리 간 연결** 또는 **이미 완전히 탐색된 정점**으로 가는 간선일 수 있음.

##### 3. **뒤 간선 **(Back Edge)
- **자손 → 조상**으로 향하는 간선 (DFS에서 순환 탐지의 핵심).
- **BFS에서는 거의 등장하지 않음**:
  - BFS는 레벨 단위로 탐색하므로, 자손 정점은 항상 **더 깊은 레벨**에 있고,
  - 조상 정점은 **더 낮은 레벨**에 있습니다.
  - 따라서 자손에서 조상으로 가는 간선은 **레벨이 감소하는 방향**이 되는데,
    - **무방향 그래프**: 이 간선은 이미 트리 간선으로 처리되었거나, 교차 간선으로 간주됨.
    - **방향 그래프**: 이론적으로는 가능하지만, **BFS 특성상 실제로는 교차 간선으로 분류되는 경우가 대부분**입니다.
- 결론: **BFS에서는 뒤 간선이라는 개념이 실질적으로 사용되지 않으며**, 순환 탐지는 다른 방식으로 수행됩니다.

##### 4. **순방향 간선 **(Forward Edge)
- **조상 → 자손**으로 향하되, **트리 경로가 아닌 직접 간선** (예: 레벨 0 → 레벨 2로 건너뛰는 간선).
- **BFS에서는 존재할 수 없음**:
  - BFS는 **가장 짧은 경로**(최소 레벨)로 정점을 처음 방문하므로,
  - 조상에서 자손으로 가는 더 긴 간선(예: 레벨 0 → 레벨 2)이 있더라도,
    - 해당 자손은 이미 레벨 1에서 다른 경로로 **더 빨리 방문**되었을 것이기 때문입니다.
  - 따라서 그런 간선은 **이미 방문된 정점으로 가는 간선**이 되어 **교차 간선**으로 분류됩니다.
- 결론: **BFS에서는 순방향 간선이 존재하지 않습니다**.

---

##### 🔍 요약: BFS에서의 간선 유형 (무방향 그래프 기준)

| 간선 유형      | BFS에서 등장 여부 | 설명                                    |
| ---------- | ----------- | ------------------------------------- |
| **트리 간선**  | ✅ 항상 있음     | 새로운 정점을 처음 발견할 때 사용                   |
| **교차 간선**  | ✅ 흔함        | 같은 레벨 또는 인접 레벨 간 연결                   |
| **뒤 간선**   | ❌ 실질적으로 없음  | 자손→조상 구조는 BFS 레벨 탐색과 충돌               |
| **순방향 간선** | ❌ 존재하지 않음   | BFS는 최단 레벨로 방문하므로 "건너뛰기" 간선은 비트리로 처리됨 |

> 💡 **핵심 인사이트**:  
> BFS는 **레벨 기반 탐색**이므로, **깊이**(depth) 개념에 기반한 DFS의 간선 분류(특히 Back/Forward Edge)와는 근본적으로 다릅니다.  
> 따라서 **BFS 신장 트리에서는 "트리 간선"과 "교차 간선"만 의미 있게 구분**됩니다.

이 설명은 슬라이드 22의 점선 간선들을 정확히 해석하면서도, 알고리즘 이론적 맥락도 함께 제공합니다.

#### 9. BFS 알고리즘 정리 (슬라이드 25)

말로 설명한 BFS 과정을 컴퓨터가 이해할 수 있는 알고리즘으로 정리해 봅시다.

**BFS(v): 정점 v를 시작으로 연결된 모든 곳을 너비 우선으로 탐색하라**
1.  **시작점 처리:** `v`를 '방문함'으로 표시하고, 큐에 `v`를 넣는다.
2.  **대기줄 확인:** 큐가 비어있지 않은 동안 계속 반복한다.
    1.  **처리 대상 선정:** 큐의 맨 앞에서 정점 `u`를 하나 꺼낸다. (Dequeue)
    2.  **(작업 수행):** 필요하다면 `u`의 데이터를 출력하는 등 작업을 수행한다.
    3.  **이웃 탐색:** `u`에 인접한 모든 정점 `w`에 대해 다음을 확인한다.
        *   **새로운 정점 발견:** 만약 `w`가 '방문 안함' 상태라면,
            *   `w`를 '방문함'으로 표시한다. (다음에 또 발견하지 않도록)
            *   `w`를 큐의 맨 뒤에 추가한다. (다음 레벨의 탐색 대상으로 예약)

DFS와 마찬가지로, 그래프 전체를 탐색하려면 이 `BFS(v)`를 감싸는 관리 함수가 필요합니다. (DFS의 `DFSearch`와 동일한 구조)

#### 10. BFS 성능 분석 (슬라이드 26)

BFS의 시간 복잡도는 DFS와 놀랍게도 동일합니다.

*   **인접 목록(Adjacency List) 사용 시: θ(V + E)**
    *   모든 정점은 정확히 한 번 '방문함'으로 표시되고, 정확히 한 번 큐에 들어갔다가(Enqueue) 나옵니다(Dequeue). 이 과정에서 V만큼의 시간이 걸립니다.
    *   한 정점을 큐에서 꺼낼 때마다 그 정점의 모든 이웃(간선)을 확인합니다. 탐색이 끝날 때까지 모든 간선은 (무방향 그래프의 경우) 양방향으로 총 두 번씩 확인됩니다. 이 과정에서 E만큼의 시간이 걸립니다.
    *   따라서 총 시간은 **θ(V + E)** 입니다.

*   **인접 행렬(Adjacency Matrix) 사용 시: θ(V²)**
    *   한 정점의 모든 이웃을 찾기 위해 행렬의 한 행 전체(V개)를 스캔해야 합니다. 이 작업이 V개의 정점에 대해 수행되므로, 총 시간은 **θ(V²)** 입니다.

#### 11. 탐색의 응용: 순서 찾아내기, 위상 정렬(Topological Sort)

지금까지 우리는 그래프의 모든 정점을 방문하는 '방법'에 대해 배웠습니다. 이제 이 탐색 기법을 활용하여 매우 중요하고 실용적인 문제를 해결해 보겠습니다. 바로 **위상 정렬(Topological Sort)**입니다.

##### 11.1. 문제 정의: 일의 순서 정하기 (슬라이드 27, 28)

우리 삶에는 순서가 중요한 일들이 많습니다.
*   **요리:** 재료를 손질해야 볶을 수 있고, 볶아야 접시에 담을 수 있습니다.
*   **옷 입기:** 속옷을 입고, 셔츠를 입고, 재킷을 입어야 합니다. 순서를 바꾸면 곤란합니다.
*   **대학교 수강신청:** '자료구조'를 수강해야 '알고리즘'을 수강할 수 있고, '미적분학'을 수강해야 '공학수학'을 수강할 수 있습니다.

이처럼 **선행 조건(prerequisite)**이 존재하는 작업들의 집합이 있을 때, 이 선행 조건을 모두 만족시키면서 모든 작업을 수행할 수 있는 **일렬로 된 순서**를 찾아내는 것을 **위상 정렬**이라고 합니다.

이 관계는 **방향 그래프(Directed Graph)**로 완벽하게 표현할 수 있습니다.
*   **정점(Vertex):** 각각의 작업 (예: 과목)
*   **간선(Edge):** 선행 관계. A가 B의 선수 과목이면, `A → B` 간선을 그립니다. 이 간선은 "A를 반드시 B보다 먼저 끝내야 한다"는 의미입니다.
![](./08.media/20251020040744-1760900864840-image.png)
슬라이드 28의 '과목들의 선후수 관계도'가 바로 이 예시입니다. 과목 1은 2와 4의 선수 과목이므로, `1 → 2` 와 `1 → 4` 간선이 있습니다.

##### 11.2. 위상 정렬의 필수 조건: 순환이 없어야 한다 (DAG)

위상 정렬이 가능하려면, 이 방향 그래프에 **순환(Cycle)이 절대로 없어야 합니다.** 만약 순환이 있다면 어떤 일이 벌어질까요?

`A → B` (A는 B의 선수과목)
`B → C` (B는 C의 선수과목)
`C → A` (C는 A의 선수과목)

이런 순환 관계가 있다면, A를 시작하려면 C가 끝나야 하고, C를 시작하려면 B가 끝나야 하고, B를 시작하려면 A가 끝나야 합니다. 즉, **영원히 아무것도 시작할 수 없는 모순**에 빠집니다.

따라서 위상 정렬은 **순환이 없는 방향 그래프 (Directed Acyclic Graph, DAG)** 에서만 정의됩니다.

슬라이드 29에서 설명하듯, 이 순환의 존재 여부는 우리가 앞에서 배운 **DFS**를 통해 완벽하게 찾아낼 수 있습니다. DFS 탐색 중 **뒤 간선(Back Edge)**, 즉 현재 탐색 중인 경로 상의 조상 노드로 돌아가는 간선이 발견되면, 그것이 바로 순환이 존재한다는 증거입니다.

##### 11.3. 위상 정렬 알고리즘 1: DFS와 스택의 활용 (슬라이드 30-33)

DFS의 깊이 파고드는 특성을 이용해 위상 정렬을 수행할 수 있습니다. 핵심 아이디어는 이것입니다.

> **"어떤 작업의 DFS가 종료되었다는 것은, 그 작업이 의존하는 모든 후행 작업들의 DFS가 이미 다 종료되었음을 의미한다."**

예를 들어 `A → B` 관계가 있을 때, A에서 DFS를 시작하면 반드시 B로 먼저 탐색을 떠나게 됩니다. 따라서 B와 관련된 모든 탐색이 끝나고 나서야, 비로소 A의 탐색이 '종료'될 수 있습니다.

이는 곧, **DFS가 가장 먼저 종료되는 정점은, 다른 어떤 정점의 선행 조건도 되지 않는 '맨 마지막 작업' 후보**라는 뜻입니다. DFS가 가장 늦게 종료되는 정점은, 다른 많은 작업들의 선행 조건이 되는 '맨 처음 작업' 후보가 됩니다.

이 '종료 순서'를 기록하기 위해 우리는 **스택(Stack)**을 사용합니다. 스택은 'Last-In, First-Out (LIFO)' 구조이므로, 종료된 순서대로 스택에 넣으면 나중에 꺼낼 때 종료 순서의 역순, 즉 **위상 정렬된 순서**가 됩니다.

**알고리즘 단계:**

1.  빈 **스택**과 **방문 기록부**를 준비합니다.
2.  그래프의 모든 정점을 확인하며, 아직 방문하지 않은 정점이 있다면 그 정점에서 DFS를 시작합니다.
3.  **DFS(v) 함수:**
    a. 현재 정점 `v`를 '방문함'으로 표시합니다.
    b. `v`의 모든 이웃 `w`에 대해, 만약 `w`가 아직 방문하지 않았다면 `DFS(w)`를 재귀적으로 호출합니다.
    c. **(핵심!)** `v`의 모든 이웃에 대한 탐색이 끝나면(즉, 재귀 호출이 모두 리턴되면), **그때 `v`를 스택에 push 합니다.**
4.  모든 정점에 대한 DFS가 끝나면, 스택이 빌 때까지 하나씩 pop하여 출력합니다. 이 순서가 바로 위상 정렬 결과입니다.

**예제(과목 선후수 관계도) 따라가기:**
(간결성을 위해 주된 흐름만 보입니다)

1.  임의의 정점 1에서 DFS 시작. `DFS(1)` 호출.
2.  1의 이웃 2로 이동. `DFS(2)` 호출.
3.  2의 이웃 4로 이동. `DFS(4)` 호출.
4.  4의 이웃 6으로 이동. `DFS(6)` 호출.
5.  6의 이웃 7로 이동. `DFS(7)` 호출.
6.  7은 더 이상 갈 곳이 없음. **7의 DFS 종료. 7을 스택에 Push.** [스택: 7]
7.  6으로 복귀. 더 이상 갈 곳 없음. **6의 DFS 종료. 6을 스택에 Push.** [스택: 7, 6]
8.  4로 복귀. 더 이상 갈 곳 없음. **4의 DFS 종료. 4를 스택에 Push.** [스택: 7, 6, 4]
9.  2로 복귀. 더 이상 갈 곳 없음. **2의 DFS 종료. 2를 스택에 Push.** [스택: 7, 6, 4, 2]
10. 1로 복귀. 더 이상 갈 곳 없음. **1의 DFS 종료. 1을 스택에 Push.** [스택: 7, 6, 4, 2, 1]
11. 이제 방문 안 한 정점 3에서 DFS 시작. `DFS(3)` 호출.
12. 3의 이웃 5로 이동. `DFS(5)` 호출.
13. 5의 이웃 6, 7은 이미 방문함. **5의 DFS 종료. 5를 스택에 Push.** [스택: 7, 6, 4, 2, 1, 5]
14. 3으로 복귀. 이웃 4는 이미 방문함. **3의 DFS 종료. 3을 스택에 Push.** [스택: 7, 6, 4, 2, 1, 5, 3]
15. 모든 정점 방문 완료.

**결과 출력:** 스택에서 pop: **3, 5, 1, 2, 4, 6, 7**.
이는 유효한 수강 순서 중 하나입니다. (위상 정렬의 결과는 유일하지 않을 수 있습니다. 예를 들어 1과 3은 서로 선행 관계가 없으므로 `1, 3, ...` 순서도 가능합니다.)

##### 11.4. 위상 정렬 알고리즘 2: BFS와 진입 차수의 활용 (카의 알고리즘)

(슬라이드에는 없지만, 매우 중요한 다른 접근법입니다.)
BFS를 이용한 위상 정렬 방법도 있으며, 매우 직관적입니다.

**핵심 아이디어:**

> **"선행 과목이 하나도 없는 과목은 지금 당장 수강할 수 있다."**

어떤 정점으로 들어오는 간선의 개수를 **진입 차수(In-degree)**라고 합니다. 진입 차수가 0이라는 것은 그 작업을 시작하기 위한 선행 조건이 없다는 뜻입니다.

**알고리즘 단계:**

1.  그래프의 모든 정점에 대해 **진입 차수를 계산**합니다.
2.  **큐**를 준비하고, **진입 차수가 0인 모든 정점**을 큐에 넣습니다. (이들이 맨 처음 시작할 수 있는 작업들입니다.)
3.  큐가 빌 때까지 다음을 반복합니다.
    a. 큐에서 정점 `v`를 하나 꺼냅니다. 이 `v`를 결과 리스트에 추가합니다.
    b. `v`에서 나가는 모든 간선 `v → w`를 살펴봅니다.
    c. 간선이 사라졌다고 생각하고, 이웃 `w`의 **진입 차수를 1 감소**시킵니다.
    d. 만약 `w`의 진입 차수가 0이 되었다면, `w` 역시 이제 새로운 시작점이 될 수 있으므로 **큐에 넣습니다.**
4.  반복이 끝났을 때, 결과 리스트에 모든 정점이 포함되어 있다면 위상 정렬이 성공한 것입니다. 만약 일부 정점이 빠져있다면, 이는 그래프에 순환이 존재하여 진입 차수가 0이 되지 못한 정점들이 있다는 뜻입니다.

이 방법은 마치 대학교에서 수강 가능한 과목 목록(진입 차수 0인 과목)을 보고 수강 신청을 하고, 그 과목을 이수하면 다음 학기에 새로운 과목들(진입 차수가 0이 된 과목)이 수강 가능해지는 과정과 매우 유사합니다.

##### 11.5. 시간 복잡도 (슬라이드 34)

*   **DFS 기반 알고리즘:** 이 알고리즘은 본질적으로 그래프 전체에 대한 DFS를 한 번 수행하는 것과 같습니다. 따라서 시간 복잡도는 DFS와 동일한 **θ(V + E)** (인접 목록 사용 시) 입니다.
*   **BFS 기반 알고리즘:** 진입 차수를 계산하는 데 O(V+E), 모든 정점과 간선을 한 번씩 처리하므로 이 역시 **θ(V + E)** (인접 목록 사용 시) 입니다.

#### 최종 요약 (슬라이드 35)

*   **그래프 탐색**은 정점과 간선으로 이루어진 복잡한 구조를 체계적으로 방문하는 알고리즘의 기초입니다.
*   **깊이 우선 탐색(DFS)**은 스택(재귀)을 이용해 한 경로를 끝까지 파고드는 '모험가' 방식이며, 순환 탐지나 연결 요소 찾기 등에 유용합니다.
*   **너비 우선 탐색(BFS)**은 큐를 이용해 시작점에서 가까운 순서대로 탐색하는 '전략가' 방식이며, 최단 경로 문제의 핵심입니다.
*   **위상 정렬**은 **순환이 없는 방향 그래프(DAG)**에서 작업들의 선행 순서를 결정하는 중요한 문제이며, DFS나 BFS를 응용하여 효율적으로 해결할 수 있습니다.

이러한 기본적인 그래프 알고리즘들은 수많은 컴퓨터 과학 문제들, 예를 들어 네트워크 라우팅, 소셜 네트워크 분석, 컴파일러 의존성 해결, 인공지능 탐색 등 다양한 분야의 핵심적인 문제 해결 도구로 사용됩니다.
### **제5장: 분할 정복 (Divide and Conquer)**

#### **서론: 위대한 문제 해결 전략, 분할 정복**

알고리즘 설계는 단순히 문제를 해결하는 코드를 작성하는 것을 넘어, 가장 효율적이고 우아한 해결책을 찾는 과정입니다. 수많은 문제 해결 전략 중에서 '분HAL 정복(Divide and Conquer)'은 가장 강력하고 널리 사용되는 패러다임 중 하나입니다. 이 이름은 로마 제국의 율리우스 카이사르가 사용했던 '분할하여 통치하라(Divide et Impera)'라는 정치 전략에서 유래했습니다. 거대하고 강력한 적을 한 번에 상대하기보다, 여러 개의 작은 그룹으로 분열시켜 각개격파하는 것이 훨씬 효과적이라는 이 아이디어는 컴퓨터 과학의 문제 해결에도 그대로 적용됩니다.

분할 정복 전략의 핵심 철학은 **"크고 복잡한 문제는 해결하기 어렵지만, 작고 단순한 문제는 해결하기 쉽다"**는 것입니다. 따라서 해결 불가능해 보이는 거대한 문제를 해결 가능한 작은 조각들로 나눈 뒤, 각 조각의 답을 구하고, 그 답들을 다시 현명하게 조합하여 원래 문제의 답을 찾아내는 방식입니다.

이 장에서는 분할 정복 패러다임의 기본 구조를 시작으로, 이 전략이 어떻게 최댓값/최솟값 찾기, 정렬(합병 정렬, 빠른 정렬), k번째 작은 원소 찾기(선택 문제)와 같은 고전적이고 중요한 문제들을 효율적으로 해결하는지 심도 있게 탐구할 것입니다. 또한, 분할 정복 전략이 항상 최선은 아니며, 특정 조건(부분 문제의 중복)에서는 오히려 비효율을 초래할 수 있다는 점을 피보나치 수열 예제를 통해 살펴보고, 이러한 경우 동적 계획법(Dynamic Programming)과 같은 다른 패러다임이 더 적합한 이유에 대해서도 논의할 것입니다.

---

##### **분할 정복 설계 전략의 3단계**

분할 정복 알고리즘은 일반적으로 다음과 같은 세 가지 명확한 단계로 구성됩니다. 이 3단계는 분할 정복의 심장과도 같으며, 거의 모든 분할 정복 알고리즘이 이 구조를 따릅니다.

###### **1. 분할 (Divide) 단계**

첫 번째 단계는 주어진 문제를 해결 가능한 가장 작은 단위(base case)에 도달할 때까지 더 작은 부분 문제(subproblem)들로 나누는 것입니다. 핵심은 원래 문제와 **동일한 유형**의 더 작은 문제로 나누는 것입니다. 예를 들어, n개의 숫자를 정렬하는 문제라면, 이를 n/2개의 숫자를 정렬하는 두 개의 부분 문제로 나누는 식입니다.

-   **핵심 원칙**: 부분 문제들은 서로 **독립적(independent)**이어야 이상적입니다. 즉, 한 부분 문제의 해가 다른 부분 문제의 해에 영향을 주지 않아야 합니다.
-   **분할 방법**: 문제를 어떻게 나눌 것인가는 문제의 성격에 따라 다릅니다. 배열을 정확히 반으로 나누는 것이 일반적이지만(예: 합병 정렬), 특정 기준값(pivot)을 중심으로 나누기도 합니다(예: 빠른 정렬). 이 분할 과정 자체가 알고리즘의 효율성을 결정하는 중요한 요소가 될 수 있습니다.

###### **2. 정복 (Conquer) 단계**

두 번째 단계는 분할된 각 부분 문제의 해를 구하는 것입니다. 이 과정은 주로 **재귀(Recursion)**를 통해 자연스럽게 구현됩니다. 부분 문제가 더 이상 분할할 수 없을 만큼 작아지면(이를 '기저 사례' 또는 'Base Case'라고 합니다), 바로 해를 구합니다. 예를 들어, 배열에 요소가 하나만 남았다면 그 자체가 최댓값이자 최솟값이며, 이미 정렬된 상태라고 볼 수 있습니다.

-   **재귀의 역할**: `solve(problem)` 함수가 있다면, 이 함수는 내부에서 `solve(subproblem1)`, `solve(subproblem2)` 등을 호출하는 구조를 가집니다. 이 재귀 호출의 연속이 바로 '정복' 과정입니다. 재귀는 문제를 점점 작은 단위로 쪼개 내려가다가, 기저 사례에 도달하면 해를 반환하며 다시 거슬러 올라오기 시작합니다.

###### **3. 합병 (Merge 또는 Combine) 단계**

마지막 단계는 정복 단계에서 얻은 부분 문제들의 해를 적절히 조합하여 원래 문제의 전체 해를 구하는 것입니다. 이 합병 과정의 복잡도는 알고리즘의 전체 성능에 결정적인 영향을 미칩니다.

-   **합병의 중요성**: 합병 단계가 간단한 알고리즘(예: 빠른 정렬)도 있고, 합병 단계 자체가 핵심적인 작업을 수행하는 알고리즘(예: 합병 정렬)도 있습니다. 예를 들어, 두 개의 정렬된 부분 배열을 하나의 정렬된 배열로 합치는 작업은 합병 정렬의 핵심 연산입니다. 마찬가지로, 두 부분 배열의 최댓값/최솟값들을 비교하여 전체 배열의 최댓값/최솟값을 찾는 것도 합병의 한 예입니다.

이 세 가지 단계를 거치면서, 분할 정복은 복잡한 문제를 체계적으로 해결하는 강력한 프레임워크를 제공합니다.

---

#### **5.1 최댓값과 최솟값 찾기**

배열에서 가장 큰 값(최댓값)과 가장 작은 값(최솟값)을 찾는 문제는 매우 기본적인 문제이지만, 이 문제를 어떻게 접근하느냐에 따라 연산의 효율성이 달라질 수 있습니다. 분할 정복이 어떻게 비교 횟수를 줄여 효율성을 높이는지 살펴보겠습니다.

##### 쉬운 전략 (Naive Approach)

가장 직관적이고 간단한 방법은 최댓값을 먼저 찾고, 그 다음에 최솟값을 찾는 것입니다.

1.  **최댓값 찾기**: 배열의 첫 번째 요소를 현재 최댓값(max)으로 가정합니다. 이후 배열의 두 번째 요소부터 마지막 요소까지 순회하면서, 현재 요소가 현재 최댓값보다 크면 최댓값을 갱신합니다. n개의 요소가 있다면, 첫 요소를 제외한 (n-1)개의 요소와 비교해야 하므로, **비교 횟수는 (n-1)회** 입니다.

2.  **최솟값 찾기**: 최댓값을 찾는 과정과 동일합니다. 첫 번째 요소를 현재 최솟값(min)으로 가정하고, 나머지 (n-1)개의 요소와 비교하며 최솟값을 갱신합니다. **비교 횟수는 (n-1)회** 입니다.

하지만, 슬라이드에서는 최댓값을 찾은 뒤 "남은 배열"에서 최솟값을 찾는다고 설명하여 약간의 오해를 줄 수 있습니다. 일반적인 순차 탐색에서는 최댓값을 찾는 과정과 최솟값을 찾는 과정이 별개로 진행됩니다. 따라서,

-   최댓값을 찾기 위한 비교: `n-1`회
-   최솟값을 찾기 위한 비교: `n-1`회
-   **총 비교 횟수**: `(n-1) + (n-1) = 2n - 2`회

슬라이드에서 제시한 `(n-1) + (n-2) = 2n-3`은 최댓값을 찾은 후, 그 최댓값은 최솟값이 될 수 없다는 가정 하에 남은 `n-1`개의 요소 중에서 최솟값을 찾는 경우를 상정한 것으로 보입니다. 이 경우에도 시간 복잡도는 `O(n)`으로 동일하며, 상수 배의 차이만 존재합니다. 이 방법은 매우 간단하지만, 과연 이것이 최선일까요?

##### 분할 정복 전략

분할 정복은 이 문제를 다른 각도에서 접근합니다. 비교 횟수를 줄이는 것이 목표입니다.

1.  **분할(Divide)**: 주어진 배열 `A`를 거의 같은 크기의 두 부분 배열 `A_left`와 `A_right`로 나눕니다. 예를 들어, `n`개의 요소가 있다면, 앞쪽 `n/2`개와 뒤쪽 `n/2`개로 나눕니다.

2.  **정복(Conquer)**: 두 개의 부분 배열에 대해 재귀적으로 최댓값과 최솟값을 찾습니다.
    -   `A_left`에서 최댓값(`max1`)과 최솟값(`min1`)을 찾습니다.
    -   `A_right`에서 최댓값(`max2`)과 최솟값(`min2`)을 찾습니다.
    이 과정은 배열에 요소가 하나 또는 두 개만 남을 때까지 계속됩니다 (기저 사례).

3.  **합병(Merge)**: 이제 네 개의 값(`max1`, `min1`, `max2`, `min2`)을 가지고 원래 배열 `A`의 전체 최댓값과 최솟값을 결정합니다.
    -   전체 최댓값 `max(A)` = `max(max1, max2)`  (비교 1회)
    -   전체 최솟값 `min(A)` = `min(min1, min2)`  (비교 1회)
    즉, 합병 단계에서는 단 2번의 비교만 필요합니다.

##### 최댓값/최솟값 찾기 알고리즘 (findMaxMin) 상세 분석

슬라이드 8의 `findMaxMin` 알고리즘을 한 줄씩 자세히 분석해 보겠습니다.

```c
findMaxMin(A[], i, j, min, max)
// A[i..j]의 최댓값과 최솟값을 찾는다
// 입력: 배열 A[i..j]
// 출력: min(최솟값), max(최댓값)
```

-   `A[]`: 전체 배열
-   `i`, `j`: 현재 탐색할 부분 배열의 시작 인덱스와 끝 인덱스
-   `min`, `max`: 결과(최솟값, 최댓값)를 저장할 변수 (참조에 의한 전달 방식)

**기저 사례 (Base Cases):**

```c
1 if i = j { min = A[i]; max = A[i] } // 요소가 1개인 경우
```

-   **Line 1**: 부분 배열에 요소가 하나뿐일 때 (`i == j`). 이 경우 그 요소 자체가 최솟값이자 최댓값입니다. 더 이상 분할할 필요가 없으므로 재귀가 멈춥니다. 비교 연산은 없습니다.

```c
2 else if i = j − 1 { // 요소가 2개인 경우
3   if A[i] < A[j] { min = A[i]; max = A[j] }
4   else { min = A[j]; max = A[i] }
5 }
```

-   **Line 2-5**: 부분 배열에 요소가 두 개일 때 (`i == j-1`). 이 경우 단 한 번의 비교(`A[i] < A[j]`)를 통해 둘 중 어느 것이 크고 작은지 바로 결정할 수 있습니다. 이 또한 재귀를 멈추는 기저 사례입니다. 비교 횟수는 1회입니다.

**재귀 단계 (Recursive Step):**

```c
6 else { // 요소가 2개보다 많은 경우
7   mid = ⌊(i + j) / 2⌋
8   findMaxMin(A, i, mid, min1, max1)
9   findMaxMin(A, mid + 1, j, min2, max2)
```

-   **Line 7**: **분할** 단계입니다. 현재 부분 배열의 중간 지점 `mid`를 계산하여 두 개의 작은 부분 배열 `A[i..mid]`와 `A[mid+1..j]`로 나눕니다.
-   **Line 8, 9**: **정복** 단계입니다. 두 개의 부분 배열에 대해 `findMaxMin` 함수를 재귀적으로 호출하여 각각의 최솟값/최댓값(`min1`, `max1`과 `min2`, `max2`)을 구합니다.

```c
10  if min1 < min2 { min = min1 }
11  else { min = min2 }
12  if max1 > max2 { max = max1 } // 슬라이드에서는 < 로 되어있으나, > 가 직관적
13  else { max = max2 }
14 }
```

-   **Line 10-14**: **합병** 단계입니다.
    -   왼쪽 부분 배열의 최솟값(`min1`)과 오른쪽 부분 배열의 최솟값(`min2`)을 비교하여 전체의 최솟값을 결정합니다 (비교 1회).
    -   왼쪽 부분 배열의 최댓값(`max1`)과 오른쪽 부분 배열의 최댓값(`max2`)을 비교하여 전체의 최댓값을 결정합니다 (비교 1회).
    -   따라서, 이 합병 단계에서는 총 2회의 비교가 수행됩니다.

##### 실행 예제와 비교 횟수 분석

예제 배열 `A = {22, 13, -5, -8, 15, 60, 17, 31}` (n=8)을 사용하여 알고리즘의 진행 과정을 따라가 보겠습니다.

1.  `findMaxMin(A, 0, 7)` 호출
    -   `mid = 3`.
    -   `findMaxMin(A, 0, 3)` (왼쪽)과 `findMaxMin(A, 4, 7)` (오른쪽)을 재귀 호출.

2.  `findMaxMin(A, 0, 3)` 처리 (`{22, 13, -5, -8}`)
    -   `mid = 1`.
    -   `findMaxMin(A, 0, 1)`과 `findMaxMin(A, 2, 3)`을 재귀 호출.
    -   `findMaxMin(A, 0, 1)` (`{22, 13}`): 요소 2개. `13 < 22`. 비교 1회. `min=-5, max=22` 반환. (`min1=13, max1=22`)
    -   `findMaxMin(A, 2, 3)` (`{-5, -8}`): 요소 2개. `-8 < -5`. 비교 1회. `min=-8, max=-5` 반환. (`min2=-8, max2=-5`)
    -   **합병**: `min(13, -8)` -> `-8` (비교 1회), `max(22, -5)` -> `22` (비교 1회).
    -   결과: `{22, 13, -5, -8}`의 min=-8, max=22. 총 비교 횟수 = 1+1+2 = 4회.

3.  `findMaxMin(A, 4, 7)` 처리 (`{15, 60, 17, 31}`)
    -   `mid = 5`.
    -   `findMaxMin(A, 4, 5)`와 `findMaxMin(A, 6, 7)`을 재귀 호출.
    -   `findMaxMin(A, 4, 5)` (`{15, 60}`): 비교 1회. `min=15, max=60` 반환. (`min1=15, max1=60`)
    -   `findMaxMin(A, 6, 7)` (`{17, 31}`): 비교 1회. `min=17, max=31` 반환. (`min2=17, max2=31`)
    -   **합병**: `min(15, 17)` -> `15` (비교 1회), `max(60, 31)` -> `60` (비교 1회).
    -   결과: `{15, 60, 17, 31}`의 min=15, max=60. 총 비교 횟수 = 1+1+2 = 4회.

4.  최종 합병 (1단계의 결과)
    -   `min1 = -8`, `max1 = 22` (왼쪽 전체 결과)
    -   `min2 = 15`, `max2 = 60` (오른쪽 전체 결과)
    -   `min(-8, 15)` -> `-8` (비교 1회)
    -   `max(22, 60)` -> `60` (비교 1회)
    -   최종 결과: 전체 배열의 min=-8, max=60.

**총 비교 횟수 계산**: 4회 (왼쪽) + 4회 (오른쪽) + 2회 (최종 합병) = **10회**.

**점화식을 이용한 분석**:
`T(n)`을 크기 n인 배열에 대한 비교 횟수라고 정의합시다.

-   `T(1) = 0` (요소가 하나면 비교 없음)
-   `T(2) = 1` (요소가 두 개면 비교 1회)
-   `T(n) = 2 * T(n/2) + 2` (n > 2)
    -   `2 * T(n/2)`: 크기 n/2인 두 개의 부분 문제 해결을 위한 비교 횟수
    -   `+ 2`: 두 부분 문제의 결과를 합병하기 위한 비교 횟수 (min 비교 1회, max 비교 1회)

이 점화식을 풀어보면 (n이 2의 거듭제곱이라고 가정):
`T(n) = 2 * T(n/2) + 2`
`= 2 * (2 * T(n/4) + 2) + 2 = 4 * T(n/4) + 4 + 2`
`= 4 * (2 * T(n/8) + 2) + 6 = 8 * T(n/8) + 8 + 6`
...
`= 2^k * T(n/2^k) + 2*(2^k - 1)`
`n/2^k = 2`가 될 때까지, 즉 `n = 2^(k+1)`일 때,
`T(n) = (n/2) * T(2) + 2 * (n/2 - 1)`
`= (n/2) * 1 + n - 2`
`= n/2 + n - 2 = (3n/2) - 2`

**결론**: 분할 정복을 사용한 최댓값/최솟값 찾기의 비교 횟수는 약 `(3n/2) - 2` 입니다.
-   n=8일 때: `(3*8/2) - 2 = 12 - 2 = 10`회. 위 예제와 일치합니다.
-   쉬운 전략: `2n - 2 = 2*8 - 2 = 14`회.
-   **효율성 비교**: `(3n/2) - 2`는 `2n - 2`보다 약 25% 더 효율적입니다. 데이터가 수십억 개에 달하는 경우, 이 25%의 차이는 상당한 성능 향상으로 이어집니다.

---

#### **5.2 합병 정렬 (Merge Sort)**

정렬은 컴퓨터 과학에서 가장 기본적이고 중요한 문제 중 하나입니다. 합병 정렬은 분할 정복 패러다임의 가장 대표적이고 교과서적인 예시입니다.

##### 쉬운 전략 (선택 정렬)과 그 한계

슬라이드 10에서 언급된 '쉬운 전략'은 사실상 **선택 정렬(Selection Sort)**과 유사한 아이디어입니다.

1.  전체 배열에서 최솟값을 찾습니다. (비교 `n-1`회)
2.  해당 최솟값을 배열의 첫 번째 위치에 둡니다.
3.  남은 `n-1`개의 요소에 대해 1~2번 과정을 반복합니다.
    -   두 번째 최솟값을 찾아 두 번째 위치에 둡니다. (비교 `n-2`회)
    -   ...
    -   마지막 두 요소 중 작은 것을 n-1번째 위치에 둡니다. (비교 1회)

**분석**:
-   총 비교 횟수 = `(n-1) + (n-2) + ... + 2 + 1` = `n(n-1)/2`
-   시간 복잡도: `O(n^2)`

슬라이드에서는 이 전략이 **'균형 취하기 발견법'**을 사용하지 않았다고 지적합니다. 여기서 '균형 취하기(balancing)'란 문제를 비슷한 크기의 부분 문제들로 나누는 것을 의미합니다. 선택 정렬은 크기 `n`의 문제를 크기 `1`(찾아낸 최솟값)과 크기 `n-1`(나머지)의 부분 문제로 나눕니다. 이렇게 극도로 불균형하게 문제를 분할하는 것은 분할 정복의 이점을 전혀 살리지 못하며, 결국 `O(n^2)`의 비효율적인 결과를 낳습니다.

##### 합병 정렬의 분할 정복 전략

합병 정렬은 이 불균형 문제를 정면으로 해결합니다.

1.  **분할(Divide)**: 정렬되지 않은 배열을 크기가 거의 같은 두 개의 부분 배열로 나눕니다. 이 과정은 부분 배열에 요소가 하나만 남을 때까지 재귀적으로 계속됩니다. 요소가 하나인 배열은 그 자체로 정렬된 상태입니다.

2.  **정복(Conquer)**: 각 부분 배열을 재귀적으로 정렬합니다.

3.  **합병(Merge)**: **이 단계가 합병 정렬의 핵심입니다.** 정복 단계에서 정렬된 두 개의 부분 배열을 하나의 전체 정렬된 배열로 합칩니다.

###### **합병(Merge) 과정 상세 설명**

두 개의 이미 정렬된 부분 배열 `L`과 `R`을 합병하는 과정을 생각해 봅시다.
-   새로운 임시 배열 `temp`를 준비합니다.
-   `L`의 첫 번째 요소를 가리키는 포인터 `i`와 `R`의 첫 번째 요소를 가리키는 포인터 `j`를 둡니다.
-   `L[i]`와 `R[j]`를 비교합니다.
    -   더 작은 값을 `temp` 배열에 넣고, 해당 포인터를 1 증가시킵니다.
-   `L`이나 `R` 중 어느 한쪽의 모든 요소가 `temp`에 들어갈 때까지 이 비교-복사 과정을 반복합니다.
-   남아있는 다른 쪽 배열의 모든 요소들을 `temp` 배열의 뒤에 순서대로 복사합니다.
-   마지막으로, `temp` 배열의 내용을 원래 배열의 해당 위치에 다시 복사합니다.

이 합병 과정은 두 부분 배열의 길이를 합한 만큼의 시간, 즉 선형 시간 `O(n)`이 걸립니다.

##### 합병 정렬 알고리즘 의사코드(Pseudocode)

```c
mergeSort(A[], p, r)
// 배열 A[p..r]을 정렬한다
1 if p < r {
2   q = ⌊(p + r) / 2⌋       // 분할: 중간 지점 계산
3   mergeSort(A, p, q)      // 정복: 왼쪽 부분 배열 정렬
4   mergeSort(A, q + 1, r)  // 정복: 오른쪽 부분 배열 정렬
5   merge(A, p, q, r)       // 합병: 두 정렬된 부분 배열을 합병
6 }

merge(A[], p, q, r)
// 정렬된 두 부분 배열 A[p..q]와 A[q+1..r]을 합병한다
1 // 임시 배열 temp에 A[p..r]을 복사
2 i = p; j = q + 1; t = 0;
3 while i ≤ q and j ≤ r {
4   if A[i] ≤ A[j] { temp[t++] = A[i++] }
5   else { temp[t++] = A[j++] }
6 }
7 // 남아있는 요소들을 복사
8 while i ≤ q { temp[t++] = A[i++] }
9 while j ≤ r { temp[t++] = A[j++] }
10 // temp의 내용을 A에 다시 복사
11 for i = p, t = 0; i ≤ r; i++, t++ { A[i] = temp[t] }
```

##### 합병 정렬 시간 복잡도 분석

`T(n)`을 크기 `n`인 배열을 합병 정렬하는 데 걸리는 시간이라고 합시다.

-   **분할**: `mid`를 계산하는 것은 상수 시간 `O(1)`입니다.
-   **정복**: 크기 `n/2`인 두 개의 부분 문제를 재귀적으로 해결하므로 `2 * T(n/2)`의 시간이 걸립니다.
-   **합병**: `merge` 함수는 `n`개의 모든 요소를 한 번씩 다루므로 `O(n)`의 시간이 걸립니다.

따라서 점화식은 다음과 같습니다.
`T(n) = 2 * T(n/2) + O(n)`

이 점화식은 마스터 정리(Master Theorem)에 의해 또는 직접 풀어서 `O(n log n)`임을 알 수 있습니다. 이는 최악, 평균, 최선 모든 경우에 동일하게 적용됩니다. 합병 정렬은 입력 데이터의 상태와 관계없이 항상 `O(n log n)`의 성능을 보장하는 **안정적인(stable)** 정렬 알고리즘입니다.

-   **공간 복잡도**: 합병 과정에서 `n`개의 요소를 담을 추가적인 임시 배열이 필요하므로 `O(n)`의 공간 복잡도를 가집니다.

---

#### **5.3 빠른 정렬 (Quick Sort)**

빠른 정렬은 합병 정렬과 함께 분할 정복을 사용하는 가장 유명한 정렬 알고리즘 중 하나입니다. 이름에서 알 수 있듯이, 평균적으로 매우 빠른 성능을 자랑합니다.

##### 빠른 정렬의 분할 정복 전략

빠른 정렬은 합병 정렬과 분할/합병 단계의 복잡도가 반대입니다.

1.  **분할(Divide)**: 이 단계가 빠른 정렬의 핵심입니다. 배열 내의 한 요소를 **기준(pivot)**으로 선택합니다. 그리고 기준보다 작은 모든 요소는 기준의 왼쪽으로, 큰 모든 요소는 기준의 오른쪽으로 옮기는 **분할(Partition)** 작업을 수행합니다. 이 작업이 끝나면 기준 요소는 최종적으로 정렬될 위치에 놓이게 됩니다. 합병 정렬과 달리, 이 분할 과정이 `O(n)`의 작업을 수행합니다.

2.  **정복(Conquer)**: 기준의 왼쪽 부분 배열과 오른쪽 부분 배열에 대해 재귀적으로 빠른 정렬을 수행합니다.

3.  **합병(Merge)**: **아무것도 할 필요가 없습니다.** 분할 단계에서 이미 기준을 중심으로 모든 요소가 정렬될 위치의 큰 틀(왼쪽/오른쪽)을 잡았기 때문에, 두 부분 배열이 각각 정렬되면 전체 배열이 자동으로 정렬됩니다. 이 합병 단계가 `O(1)`로 매우 간단하다는 것이 빠른 정렬의 특징입니다.

##### 빠른 정렬 시간 복잡도 분석

빠른 정렬의 성능은 **기준(pivot)을 얼마나 잘 선택하느냐**에 따라 극적으로 달라집니다.

-   **최선의 경우 (Best Case)**:
    -   매번 분할(Partition) 과정에서 기준이 배열을 정확히 절반으로 나눌 때 발생합니다.
    -   점화식: `T(n) = 2 * T(n/2) + O(n)` (분할에 `O(n)` 소요)
    -   시간 복잡도: `O(n log n)`. 합병 정렬과 동일합니다.

-   **최악의 경우 (Worst Case)**:
    -   매번 기준이 가장 작거나 가장 큰 요소로 선택될 때 발생합니다. 예를 들어, 이미 정렬된 배열에서 항상 첫 번째 요소를 기준으로 선택하는 경우입니다.
    -   이 경우, 배열은 크기 `0`과 `n-1`의 극도로 불균형한 두 부분 배열로 나뉩니다.
    -   점화식: `T(n) = T(n-1) + O(n)`
    -   시간 복잡도: `O(n^2)`. 선택 정렬만큼 비효율적이 됩니다.

-   **평균적인 경우 (Average Case)**:
    -   기준이 무작위로 선택된다면, 분할이 어느 정도 균형을 이룰 확률이 높습니다.
    -   수학적으로 분석하면, 평균 시간 복잡도는 `O(n log n)`으로 최선의 경우와 같습니다.
    -   이러한 이유로, 실제 구현에서는 기준을 무작위로 선택하거나, 세 개의 요소(처음, 중간, 끝) 중 중앙값을 선택하는 등의 기법을 사용하여 최악의 경우를 피하려 노력합니다.

빠른 정렬은 합병 정렬과 달리 추가적인 배열이 필요 없는 **제자리 정렬(in-place sort)**이 가능하여 공간 효율성이 높고, 평균 성능이 매우 뛰어나 널리 사용됩니다.

---

#### **5.4 선택 (Selection)**

선택 문제는 정렬되지 않은 배열에서 **k번째로 작은 요소**를 찾는 문제입니다.
-   k=1이면 최솟값 찾기
-   k=n이면 최댓값 찾기
-   k=⌊(n+1)/2⌋이면 **중앙값(median)** 찾기

##### 쉬운 전략

가장 간단한 방법은 배열 전체를 정렬한 뒤, k-1 인덱스에 있는 요소를 반환하는 것입니다.
-   합병 정렬이나 빠른 정렬(평균)을 사용하면 `O(n log n)`의 시간이 걸립니다.
-   하지만 우리는 전체를 정렬할 필요 없이 단지 k번째 요소만 찾으면 됩니다. 과연 더 빠르게 할 수 있을까요?

##### 분할 정복을 이용한 선택 알고리즘 (Quickselect)

이 알고리즘은 빠른 정렬(Quick Sort)의 아이디어를 차용하여 매우 효율적으로 선택 문제를 해결합니다. 그래서 종종 **퀵셀렉트(Quickselect)**라고도 불립니다.

1.  **분할(Divide)**: 빠른 정렬과 동일하게, 배열에서 기준(pivot)을 하나 선택하고 분할(Partition) 작업을 수행합니다. 이 작업이 끝나면 기준은 `p` 인덱스에 위치하게 되고, `A[p]`는 배열 내에서 `(p - first + 1)`번째로 작은 요소가 됩니다.

2.  **정복(Conquer) & 선택**:
    -   분할 작업 후 기준의 위치 `p`를 k와 비교합니다.
    -   `Small` 그룹 (기준 왼쪽)의 크기를 `s = p - first` 라고 합시다. (슬라이드에서는 `s = p - 1 - first + 1`로 표현, 즉 `p - first`와 동일)
    -   **Case 1: `k == s + 1`**: 우리가 찾던 k번째 요소가 바로 기준(pivot) 자신입니다! 탐색을 종료하고 `A[p]`를 반환합니다.
    -   **Case 2: `k <= s`**: k번째 요소는 기준보다 작으므로, `Small` 그룹 안에 있습니다. `Small` 그룹(왼쪽 부분 배열)에 대해 재귀적으로 k번째 요소를 찾습니다.
    -   **Case 3: `k > s + 1`**: k번째 요소는 기준보다 크므로, `Large` 그룹 안에 있습니다. `Large` 그룹(오른쪽 부분 배열)에서 찾아야 합니다. 단, 이때는 그냥 `k`번째를 찾는 것이 아니라, `Small` 그룹과 기준을 제외한 나머지 중에서 찾아야 하므로 `(k - s - 1)`번째 요소를 재귀적으로 찾습니다.

이 알고리즘의 핵심은 빠른 정렬처럼 양쪽을 모두 재귀 호출하는 것이 아니라, **한쪽 부분 배열에 대해서만 재귀 호출**을 한다는 점입니다. 이로 인해 시간 복잡도가 크게 향상됩니다.

##### 선택 알고리즘 예제 상세 분석 (슬라이드 42)

문제: `< 48 12 70 38 75 67 96 52 81>` (n=9) 에서 중앙값, 즉 5번째(k=5) 작은 수를 찾아라.

1.  **최초 호출**: `selection(A, 0, 8, k=5)`
    -   **분할**: 첫 번째 요소인 `48`을 기준으로 선택합니다. 분할 후 배열 상태: `< 38 12 | 48 | 70 75 67 96 52 81 >` (실제 분할 구현에 따라 순서는 다를 수 있음). 기준 `48`은 인덱스 2에 위치 (`p=2`).
    -   `Small` 그룹(`A[0.]`)의 크기 `s = p - first = 2 - 0 = 2`.
    -   **선택**:
        -   우리가 찾는 `k=5`는 `s+1=3`보다 큽니다 (`k > s+1`).
        -   따라서 답은 `Large` 그룹에 있습니다.
        -   `Large` 그룹 (`A[3.]`)에서 `k' = k - s - 1 = 5 - 2 - 1 = 2`번째 작은 요소를 찾습니다.
        -   **재귀 호출**: `selection(A, 3, 8, k=2)`

2.  **두 번째 호출**: `selection(A, 3, 8, k=2)` (`< 70 75 67 96 52 81 >` 부분)
    -   **분할**: 현재 부분 배열의 첫 요소인 `70`을 기준으로 선택합니다. 분할 후 부분 배열 상태: `< 67 52 | 70 | 96 75 81 >`. 기준 `70`은 원래 배열 인덱스 5에 위치 (`p=5`).
    -   현재 부분 배열 범위는 `first=3, last=8`입니다. `Small` 그룹(`A[3.]`)의 크기 `s = p - first = 5 - 3 = 2`.
    -   **선택**:
        -   우리가 찾는 `k=2`는 `s=2`와 같습니다 (`k <= s`).
        -   따라서 답은 `Small` 그룹에 있습니다.
        -   `Small` 그룹 (`A[3.]`)에서 여전히 `k=2`번째 작은 요소를 찾습니다.
        -   **재귀 호출**: `selection(A, 3, 4, k=2)`

3.  **세 번째 호출**: `selection(A, 3, 4, k=2)` (`< 67 52 >` 부분)
    -   **분할**: 첫 요소 `67`을 기준으로 선택합니다. 분할 후 부분 배열 상태: `< 52 | 67 >`. 기준 `67`은 원래 배열 인덱스 4에 위치 (`p=4`).
    -   현재 부분 배열 범위는 `first=3, last=4`입니다. `Small` 그룹 (`A[3.]`)의 크기 `s = p - first = 4 - 3 = 1`.
    -   **선택**:
        -   우리가 찾는 `k=2`는 `s+1=2`와 같습니다 (`k == s+1`).
        -   **찾았습니다!** 기준 요소인 `A[p] = A[4] = 67`이 바로 우리가 찾던 5번째 작은 수입니다.
        -   `67`을 반환하고 모든 재귀 호출이 종료됩니다.

(참고: 정렬된 배열: `12, 38, 48, 52, 67, 70, 75, 81, 96`. 5번째는 67이 맞습니다.)

##### 선택 알고리즘 시간 복잡도

-   **최선/평균의 경우**:
    -   매번 기준이 배열을 적절한 비율로 나눈다면 (예: 3:7), 탐색해야 할 배열의 크기는 매 단계마다 `cn` (단, `c < 1`)으로 줄어듭니다.
    -   점화식: `T(n) = T(cn) + O(n)` (분할에 `O(n)` 소요)
    -   이 점화식의 해는 `O(n)`입니다. `n + cn + c^2n + ... = n(1+c+c^2+...)`는 등비수열의 합으로 `n/(1-c)` 이므로 `O(n)`입니다.
    -   정렬(`O(n log n)`)보다 훨씬 빠른 선형 시간에 k번째 요소를 찾을 수 있습니다.

-   **최악의 경우**:
    -   빠른 정렬과 마찬가지로, 매번 기준이 가장 크거나 작은 값으로 선택되어 탐색 범위가 하나씩만 줄어드는 경우입니다.
    -   점화식: `T(n) = T(n-1) + O(n)`
    -   시간 복잡도: `O(n^2)`

---

#### **5.5 분할 정복이 부적절한 경우**

분할 정복은 매우 강력하지만 만병통치약은 아닙니다. 분할 정복이 비효율적이거나 부적절한 경우가 있는데, 가장 대표적인 사례는 **부분 문제들이 서로 중복될 때**입니다.

슬라이드 44에서 언급된 두 가지 경우는 이 문제를 잘 설명합니다.
-   **경우 1**: 크기 `n`의 문제가 거의 `n`에 가까운 크기의 두 개 이상의 부분 문제로 분할될 때.
-   **경우 2**: 크기 `n`의 문제가 `n/c` 크기의 거의 `n`개에 가까운 부분 문제로 분할될 때.

이 두 경우의 공통적인 문제는, **분할된 부분 문제들의 입력 크기의 합이 원래 문제의 입력 크기보다 훨씬 커진다**는 점입니다. 이는 같은 부분 문제를 여러 번 반복해서 풀게 될 가능성이 높다는 것을 시사합니다.

##### 대표적인 예: 피보나치 수열

피보나치 수열은 다음과 같이 정의됩니다.
`f(0) = 0`, `f(1) = 1` (슬라이드에서는 1-based index로 `f0=1, f1=1`로 정의)
`f(n) = f(n-1) + f(n-2)` for `n >= 2`

이 정의는 그 자체로 분할 정복의 재귀적 구조를 가지고 있습니다. `f(n)`이라는 문제를 `f(n-1)`과 `f(n-2)`라는 두 개의 작은 문제로 나누어 해결할 수 있기 때문입니다.

###### **재귀를 이용한 분할 정복 알고리즘**

```c
F(n)
1 if n ≤ 1 return 1
2 else return F(n - 1) + F(n - 2)
```

이 코드는 피보나치 수열의 정의를 그대로 코드로 옮긴 것입니다. 매우 간결하고 직관적입니다. 하지만 치명적인 비효율성을 가지고 있습니다.

###### **실행 트리와 중복 계산의 문제**

`F(5)`를 계산하는 과정을 생각해 봅시다 (슬라이드 48).
-   `F(5)`는 `F(4)`와 `F(3)`을 호출합니다.
-   `F(4)`는 `F(3)`과 `F(2)`를 호출합니다.
-   `F(3)`은 `F(2)`와 `F(1)`을 호출합니다.

실행 트리를 그려보면, `F(3)`은 2번, `F(2)`는 3번, `F(1)`은 5번, `F(0)`은 3번 호출됩니다 (0-based 기준). `n`이 커질수록 이 중복 호출은 기하급수적으로 늘어납니다. `F(n-1)`과 `F(n-2)`는 매우 큰 부분 문제를 공유하고 있으며, 이것이 바로 분할된 부분 문제들이 **독립적이지 않은(not independent)** 경우입니다.

이 알고리즘의 호출 횟수는 피보나치 수와 비례하며, 시간 복잡도는 약 `O(1.618^n)` (황금비의 n제곱)이라는 지수 시간(Exponential Time) 복잡도를 가집니다. n=40만 되어도 계산에 엄청난 시간이 걸립니다.

##### 더 나은 해결책: 동적 계획법 (Dynamic Programming)

이러한 중복 계산 문제를 해결하기 위한 패러다임이 바로 **동적 계획법(Dynamic Programming)**입니다. 동적 계획법의 핵심은 **한 번 계산한 부분 문제의 결과는 저장해두고(Memoization), 다시 필요할 때 재계산 없이 가져다 쓰는 것**입니다.

###### **동적 계획법 기반 알고리즘**

슬라이드 49의 알고리즘은 **상향식(Bottom-up)** 접근법을 사용합니다.
-   가장 작은 문제인 `F(0)`과 `F(1)`부터 계산을 시작합니다.
-   이 결과를 배열(또는 변수)에 저장합니다.
-   저장된 값들을 이용하여 `F(2)`, `F(3)`, ..., `F(n)`을 차례대로 계산해 나갑니다.

```c
F_DP(n)
1 F[0] = 1
2 F[1] = 1
3 for i from 2 to n
4   F[i] = F[i-1] + F[i-2]
5 return F[n]
```

이 알고리즘은 `for` 루프를 `n-1`번 반복하며, 각 반복마다 덧셈 한 번만 수행합니다. 따라서 시간 복잡도는 **`O(n)`**으로, 재귀적인 분할 정복 방식의 `O(1.618^n)`과 비교할 수 없을 정도로 효율적입니다.

---

#### **결론 및 요약**

이 장에서는 알고리즘 설계의 핵심 패러다임인 분할 정복에 대해 깊이 있게 탐구했습니다.

1.  **분할 정복의 힘**:
    -   분할 정복은 복잡한 문제를 **균형 잡힌(balanced)** 작은 부분 문제들로 나누어 해결함으로써 효율성을 극대화하는 전략입니다.
    -   '균형'은 알고리즘의 성능에 매우 중요합니다. 비슷한 크기로 나눌 때 `O(n log n)`과 같은 효율적인 시간 복잡도를 달성할 수 있습니다.
    -   최댓값/최솟값 찾기, 합병 정렬, 빠른 정렬, 선택 문제 등 다양한 문제에서 그 강력함을 확인할 수 있었습니다. 이들은 모두 **부분 문제들이 서로 독립적**이라는 공통점을 가집니다.

2.  **분할과 합병의 상호작용**:
    -   **합병 정렬**: 분할은 간단하고(`O(1)`), 합병이 핵심적인 작업(`O(n)`)을 수행합니다.
    -   **빠른 정렬**: 분할이 핵심적인 작업(`O(n)`)을 수행하고, 합병은 사실상 필요 없습니다(`O(1)`).
    -   문제의 특성에 따라 분할과 합병 단계 중 어느 쪽에 무게를 둘 것인지가 결정됩니다.

3.  **분할 정복의 한계와 대안**:
    -   **피보나치 수열** 예제에서 보았듯이, 부분 문제들이 서로 중복되어 재계산이 빈번하게 발생하는 경우, 분할 정복은 오히려 지수 시간의 비효율을 초래합니다.
    -   이러한 **중복되는 부분 문제(overlapping subproblems)** 구조를 가진 문제에는 **동적 계획법**이 훨씬 효과적인 해결책을 제공합니다.

분할 정복은 단순히 알고리즘 목록에 있는 하나의 기법이 아니라, 문제를 바라보고 구조화하는 강력한 사고방식입니다. 복잡한 현실 세계의 문제를 만났을 때, 그것을 더 작고 관리 가능한 조각으로 나누어 해결하고 다시 합치는 이 접근법은 컴퓨터 과학을 넘어 다양한 분야에서 유용하게 적용될 수 있는 지혜를 제공합니다.



네, 제공해주신 강의 자료를 바탕으로 "제6장: 동적 계획"에 대한 매우 상세하고 포괄적인 설명을 6만자 이상으로 작성해 드리겠습니다. 각 슬라이드의 내용을 심층적으로 확장하고, 동적 계획의 핵심 원리, 문제 해결 과정, 구체적인 예제(막대 자르기, 연속 행렬 곱셈, 모든 쌍 최단 경로, 배낭 채우기)의 단계별 분석을 포함하여 구성하겠습니다.

***

### **제6장: 동적 계획 (Dynamic Programming)**

네, 알겠습니다. 분할 정복(Divide and Conquer)의 한계를 살펴보고, 이를 극복하는 동적 계획법(Dynamic Programming)의 개념을 막대 자르기(Rod Cutting) 문제를 통해 매우 상세하고 이야기 형식으로 설명해 드리겠습니다.

#### 분할 정복의 한계: 중복되는 계산의 비효율성

우리는 알고리즘 설계 기법 중 하나인 **분할 정복(Divide and Conquer)**에 대해 이미 알고 있습니다. 분할 정복은 주어진 문제를 더 이상 나눌 수 없을 때까지 작은 부분 문제(subproblem)들로 나누고(Divide), 각각의 부분 문제를 해결한(Conquer) 후, 그 해들을 다시 합쳐(Combine) 원래 문제의 해를 구하는 강력한 방식입니다. 대표적인 예로 병합 정렬(Merge Sort)이나 퀵 정렬(Quick Sort)이 있습니다.

하지만 분할 정복이 모든 문제에 효율적인 해결책을 제시하는 것은 아닙니다. 분할 정복의 핵심은 '서로 독립적인' 부분 문제로 나누는 것입니다. 예를 들어 병합 정렬에서 배열의 왼쪽 절반을 정렬하는 문제와 오른쪽 절반을 정렬하는 문제는 서로 전혀 영향을 주지 않습니다.

그러나 어떤 문제들은 나누어진 부분 문제들이 서로 독립적이지 않고, 동일한 부분 문제를 여러 번 반복해서 풀어야 하는 경우가 발생합니다. 가장 고전적인 예가 **피보나치 수열**입니다.

`fib(n) = fib(n-1) + fib(n-2)`

이 점화식을 그대로 재귀 함수로 구현하면 `fib(5)`를 계산하기 위해 `fib(4)`와 `fib(3)`을 호출합니다. 그런데 `fib(4)`를 계산하는 과정에서 또다시 `fib(3)`과 `fib(2)`를 호출하게 됩니다. 결국 `fib(3)`이라는 동일한 부분 문제가 두 번 이상 계산되는 것을 볼 수 있습니다. 문제의 크기가 커질수록 이런 중복 계산은 기하급수적으로 늘어나고, 알고리즘의 효율성은 최악으로 치닫게 됩니다.

이처럼 **중복되는 부분 문제(Overlapping Subproblems)** 구조를 가진 문제에 단순한 분할 정복을 적용하면 심각한 비효율을 초래합니다.

이제 이러한 특성을 가진 또 다른 문제, 바로 '막대 자르기 문제'를 통해 이 문제를 해결하는 **동적 계획법(Dynamic Programming)**의 위력을 살펴보겠습니다.

#### 막대 자르기 문제(Rod Cutting Problem)

##### 문제 정의
길이가 `n`인 막대와 각 길이별 판매 가격표가 주어졌을 때, 막대를 어떻게 잘라야 전체 판매 금액을 최대로 할 수 있는지 찾는 문제입니다. 막대는 여러 조각으로 자를 수 있으며, 자르지 않고 통째로 파는 것도 하나의 방법입니다.

예를 들어, 다음과 같은 가격표가 있다고 가정해 보겠습니다.

| 길이 (i) | 1 | 2 | 3 | 4 |
| :------: | :-: | :-: | :-: | :-: |
| 가격 (p[i])| 2 | 5 | 9 | 10|

만약 우리에게 길이가 4인 막대가 주어진다면, 어떤 방법들이 있을까요?

*   **자르지 않음:** 길이 4짜리 한 개 -> 가격 10
*   **한 번 자름:**
    *   1 + 3으로 자름 -> `p[1] + p[3]` = 2 + 9 = 11
    *   2 + 2로 자름 -> `p[2] + p[2]` = 5 + 5 = 10
*   **두 번 자름:**
    *   1 + 1 + 2로 자름 -> `p[1] + p[1] + p[2]` = 2 + 2 + 5 = 9
*   **세 번 자름:**
    *   1 + 1 + 1 + 1로 자름 -> `p[1] * 4` = 2 * 4 = 8

이 모든 경우를 비교해보니, 1과 3으로 잘랐을 때의 판매 금액 11이 최대 이익임을 알 수 있습니다. 우리의 목표는 어떤 길이 `n`에 대해서도 이 최대 이익을 찾는 알고리즘을 설계하는 것입니다.

##### 첫 번째 시도: 분할 정복을 이용한 재귀적 접근

이 문제를 처음 접했을 때, 가장 직관적으로 떠올릴 수 있는 방법은 분할 정복 기반의 재귀적 접근입니다.

길이 `i`의 막대에서 얻을 수 있는 최대 판매 금액을 `R(i)`라고 정의해 봅시다. 길이 `i`의 막대를 얻기 위한 방법은, 첫 번째 조각을 길이 `k` (단, `1 ≤ k ≤ i`)로 자르는 것입니다. 그러면 우리는 길이 `k`짜리 조각 하나와 길이 `i-k`짜리 막대 하나를 얻게 됩니다.

*   길이 `k`짜리 조각은 가격표에 따라 `p[k]`의 가격으로 판매합니다.
*   나머지 길이 `i-k`짜리 막대는? 이 막대 또한 최적의 방법으로 잘라 팔아야 최대 이익을 얻을 수 있습니다. 즉, `R(i-k)`의 이익을 얻게 됩니다.

따라서, 첫 조각을 길이 `k`로 잘랐을 때의 총 이익은 `p[k] + R(i-k)`가 됩니다. 우리는 가능한 모든 `k` (1부터 `i`까지)에 대해 이 값을 계산하여 그중 최댓값을 찾으면 됩니다. 이를 점화식으로 표현하면 다음과 같습니다.

> **R(i) = max( p[k] + R(i-k) )** for 1 ≤ k ≤ i
>
> (단, `R(0) = 0`)

이 점화식을 코드로 구현하면 다음과 같은 재귀 함수(`cutRod_DC`)가 됩니다.

```c
// 분할 정복(재귀) 방식의 막대 자르기 알고리즘
cutRod_DC(p[], i) {
    if (i == 0) return 0; // 길이가 0이면 가격도 0

    maxSell = 0;
    for (j = 1; j <= i; j++) {
        // 첫 조각을 j로 잘랐을 때의 가격 p[j]와
        // 나머지 부분(i-j)의 최대 이익을 더한 값 중 최댓값을 찾는다.
        maxSell = MAX(maxSell, p[j] + cutRod_DC(p, i - j));
    }
    return maxSell;
}
```

###### 분할 정복 접근법의 치명적인 문제

이 코드는 정확한 답을 찾아줍니다. 하지만 피보나치 수열의 예처럼, 심각한 비효율을 내포하고 있습니다. `cutRod_DC(p, 4)`를 호출했을 때의 실행 트리를 살펴봅시다.

![Recursion Tree for R(4)|161x81](https://i.imgur.com/k6x35yW.png)

위 그림은 `R(4)`를 계산하기 위한 재귀 호출의 구조를 보여줍니다.

*   `R(4)`를 계산하기 위해 `R(3), R(2), R(1), R(0)`을 호출합니다.
*   `R(3)`을 계산하기 위해 `R(2), R(1), R(0)`을 호출합니다.
*   `R(2)`를 계산하기 위해 `R(1), R(0)`을 호출합니다.

보시다시피, `R(2)`는 `R(4)`를 계산하는 과정과 `R(3)`을 계산하는 과정에서 총 2번 호출됩니다. `R(1)`은 `R(4), R(3), R(2)`를 계산하는 과정에서 총 4번이나 호출됩니다. 이렇게 동일한 부분 문제에 대한 계산이 반복적으로 일어나는 **'중복되는 부분 문제'** 현상이 여기서도 명확하게 나타납니다.

이 알고리즘의 시간 복잡도를 분석해보면 `T(n) = Σ T(n-k) + 1` (k=1 to n) 형태가 되며, 이는 `T(n) = 2^n`에 가까운 지수 시간 복잡도(Exponential Time Complexity)를 가집니다. `n`이 조금만 커져도 계산 시간이 폭발적으로 증가하여 현실적으로 사용할 수 없는 알고리즘이 됩니다.

##### 두 번째 시도: 동적 계획법(Dynamic Programming)

동적 계획법은 바로 이 '중복되는 부분 문제'를 해결하기 위해 탄생한 알고리즘 설계 기법입니다. 아이디어는 매우 간단합니다.

> **"어떤 문제든 딱 한 번만 풀고, 그 결과를 저장해두었다가 나중에 필요할 때 다시 계산하지 말고 가져다 쓰자."**

이 기법을 적용하기 위해서는 두 가지 속성이 문제에 존재해야 합니다.

1.  **중복되는 부분 문제 (Overlapping Subproblems):** 위에서 확인했듯이, 막대 자르기 문제는 이 속성을 만족합니다.
2.  **최적 부분 구조 (Optimal Substructure):** 문제의 최적의 해가 부분 문제들의 최적의 해로부터 구성될 수 있다는 것입니다. 막대 자르기 문제에서 길이 `i`의 최적해는, 첫 조각 `k`를 자른 후 남은 `i-k` 길이 막대의 최적해에 `p[k]`를 더한 값들 중 최대값으로 구성되므로 이 속성 또한 만족합니다.

이러한 문제의 구조는 분할 정복의 트리 구조와 달리, 공유되는 부분 문제를 가진 DAG(Directed Acyclic Graph) 구조로 시각화할 수 있습니다.



(a) 분할 정복은 각 부분 문제가 독립적인 트리 구조를 가집니다.
(b) 동적 계획은 `E`와 같은 부분 문제가 여러 상위 문제(`B`, `C`)에 의해 공유되는 구조를 가지며, 이 `E`를 한 번만 계산하는 것이 핵심입니다.

###### 동적 계획 알고리즘 설계 (Bottom-up 방식)

동적 계획법은 보통 '상향식(Bottom-up)'으로 문제를 해결합니다. 즉, 가장 작은 부분 문제부터 차례대로 풀어나가면서 그 결과를 테이블에 저장하고, 더 큰 문제를 풀 때 이 테이블의 값을 활용하는 방식입니다.

막대 자르기 문제에서 가장 작은 문제는 무엇일까요? 바로 길이가 0인 막대입니다. 그 다음은 길이 1, 길이 2, ... 순서로 `n`까지 문제를 풀어 나가면 됩니다.

1.  **결과를 저장할 배열(테이블)을 만듭니다.** `maxSell[0...n]`이라는 배열을 선언하고, `maxSell[j]`에는 길이 `j` 막대의 최대 판매 금액을 저장할 것입니다.
2.  **가장 작은 문제부터 해결합니다.**
    *   `maxSell[0] = 0` (길이 0인 막대의 가격은 0)
3.  **반복문을 통해 점진적으로 큰 문제를 해결합니다.** `j`를 1부터 `n`까지 증가시키면서 `maxSell[j]`를 계산합니다.
    *   `maxSell[j]`를 계산하기 위해, 우리는 이전에 이미 계산해 둔 `maxSell[0], maxSell[1], ..., maxSell[j-1]` 값들을 사용할 수 있습니다.
    *   `R(j) = max( p[k] + R(j-k) )` 점화식을 그대로 적용하되, `R(j-k)` 대신 이미 계산된 값인 `maxSell[j-k]`를 사용합니다.

이를 코드로 구현하면 다음과 같습니다.

```c
// 동적 계획법 방식의 막대 자르기 알고리즘
cutRod_DP(p[], n) {
    // 1. 결과를 저장할 배열 선언
    배열 maxSell[0...n]을 선언한다.

    // 2. 가장 작은 문제의 해 초기화
    maxSell[0] = 0;

    // 3. 점진적으로 큰 문제 해결 (j = 1부터 n까지)
    for (j = 1; j <= n; j++) {
        maxVal = 0;
        // maxSell[j]를 계산하기 위해 모든 가능한 첫 조각 k를 고려
        for (k = 1; k <= j; k++) {
            // p[k] + maxSell[j-k] : 첫 조각을 k로 잘랐을 때의 이익
            // maxSell[j-k]는 이전에 이미 계산된 최적의 값이다.
            maxVal = MAX(maxVal, p[k] + maxSell[j - k]);
        }
        // j에 대한 최댓값을 테이블에 저장
        maxSell[j] = maxVal;
    }

    // 최종적으로 우리가 원하는 길이 n에 대한 해를 반환
    return maxSell[n];
}
```

###### 동적 계획 알고리즘 실행 과정 추적

위 알고리즘이 어떻게 동작하는지 가격표 (p[1]=2, p[2]=5, p[3]=9, p[4]=10)와 n=4를 예로 들어 단계별로 따라가 보겠습니다.

**초기 상태:**
`maxSell` 배열이 생성되고 `maxSell[0]`은 0으로 초기화됩니다.
`p = [?, 2, 5, 9, 10]`
`maxSell = [0, ?, ?, ?, ?]`

---

**j = 1 (길이 1 막대의 최대 이익 계산)**
*   `maxVal`를 0으로 초기화.
*   **k = 1:** `maxVal = MAX(0, p[1] + maxSell[1-1]) = MAX(0, 2 + maxSell[0]) = MAX(0, 2 + 0) = 2`
*   `maxSell[1] = 2`

**현재 상태:** `maxSell = [0, 2, ?, ?, ?]`

---

**j = 2 (길이 2 막대의 최대 이익 계산)**
*   `maxVal`를 0으로 초기화.
*   **k = 1:** `maxVal = MAX(0, p[1] + maxSell[2-1]) = MAX(0, 2 + maxSell[1]) = MAX(0, 2 + 2) = 4`
*   **k = 2:** `maxVal = MAX(4, p[2] + maxSell[2-2]) = MAX(4, 5 + maxSell[0]) = MAX(4, 5 + 0) = 5`
*   `maxSell[2] = 5`

**현재 상태:** `maxSell = [0, 2, 5, ?, ?]`

---

**j = 3 (길이 3 막대의 최대 이익 계산)**
*   `maxVal`를 0으로 초기화.
*   **k = 1:** `maxVal = MAX(0, p[1] + maxSell[3-1]) = MAX(0, 2 + maxSell[2]) = MAX(0, 2 + 5) = 7`
*   **k = 2:** `maxVal = MAX(7, p[2] + maxSell[3-2]) = MAX(7, 5 + maxSell[1]) = MAX(7, 5 + 2) = 7`
*   **k = 3:** `maxVal = MAX(7, p[3] + maxSell[3-3]) = MAX(7, 9 + maxSell[0]) = MAX(7, 9 + 0) = 9`
*   `maxSell[3] = 9`

**현재 상태:** `maxSell = [0, 2, 5, 9, ?]`

---

**j = 4 (길이 4 막대의 최대 이익 계산)**
*   `maxVal`를 0으로 초기화.
*   **k = 1:** `maxVal = MAX(0, p[1] + maxSell[4-1]) = MAX(0, 2 + maxSell[3]) = MAX(0, 2 + 9) = 11`
*   **k = 2:** `maxVal = MAX(11, p[2] + maxSell[4-2]) = MAX(11, 5 + maxSell[2]) = MAX(11, 5 + 5) = 11`
*   **k = 3:** `maxVal = MAX(11, p[3] + maxSell[4-3]) = MAX(11, 9 + maxSell[1]) = MAX(11, 9 + 2) = 11`
*   **k = 4:** `maxVal = MAX(11, p[4] + maxSell[4-4]) = MAX(11, 10 + maxSell[0]) = MAX(11, 10 + 0) = 11`
*   `maxSell[4] = 11`

**최종 상태:** `maxSell = [0, 2, 5, 9, 11]`

---
모든 반복이 끝나면 `maxSell[4]`에는 11이 저장되어 있습니다. 알고리즘은 이 값을 반환하고, 우리는 길이 4의 막대로 얻을 수 있는 최대 이익이 11이라는 것을 알게 됩니다.

이 과정을 슬라이드의 표와 비교해보면 정확히 일치하는 것을 볼 수 있습니다.



###### 동적 계획법의 효율성 분석

*   **시간 복잡도:** 알고리즘은 2개의 중첩된 반복문으로 구성됩니다. 바깥쪽 `j` 루프는 1부터 `n`까지 `n`번 실행됩니다. 안쪽 `k` 루프는 `j`번 실행됩니다. 따라서 기본 연산(MAX 계산 및 덧셈)의 총 실행 횟수는 `1 + 2 + 3 + ... + n`이 됩니다. 이는 `n(n+1)/2`와 같으므로, 시간 복잡도는 **Θ(n²)** 입니다.

*   **공간 복잡도:** 해를 저장하기 위해 크기가 `n+1`인 `maxSell` 배열을 사용하므로, 공간 복잡도는 **Θ(n)** 입니다.

분할 정복의 지수 시간 복잡도 `O(2^n)`과 비교해볼 때, 동적 계획법의 다항 시간 복잡도 `Θ(n²)`는 엄청나게 효율적입니다. `n`이 30일 때, `2^30`은 10억이 넘는 수지만 `30^2`은 고작 900입니다. 이 차이가 바로 동적 계획법의 힘입니다.

##### 결론: 현명한 기억의 힘

막대 자르기 문제는 분할 정복의 순진한 재귀적 접근이 왜 비효율적인지, 그리고 동적 계획법이 어떻게 그 문제를 해결하는지를 보여주는 훌륭한 예시입니다.

핵심은 **'문제를 한 번만 푸는 것'**입니다. 분할 정복은 동일한 질문을 계속해서 반복하지만, 동적 계획법은 한 번 얻은 답을 테이블에 기록해두고 필요할 때마다 꺼내 씁니다. 이 '기억(memoization)'이라는 단순한 아이디어 하나가 지수 시간을 다항 시간으로 바꾸는 극적인 성능 향상을 가져옵니다.

따라서, 어떤 문제를 해결해야 할 때 다음 두 가지 특징이 보인다면 동적 계획법을 가장 먼저 떠올려야 합니다.
1.  **중복되는 부분 문제:** 작은 문제들의 해가 여러 번 필요하다.
2.  **최적 부분 구조:** 전체 문제의 최적해가 부분 문제들의 최적해로 구성된다.

동적 계획법은 알고리즘의 효율성을 극대화하는 매우 중요하고 강력한 도구이며, 수많은 최적화 문제 해결의 기반이 됩니다.



#### 행렬들의 곱셈 순서를 정하는 문제
##### 행렬 곱셈의 특성과 새로운 문제의 발견

우리는 이전에 분할 정복의 한계와 이를 극복하는 동적 계획법의 원리를 막대 자르기 문제를 통해 살펴보았습니다. 핵심은 '중복되는 부분 문제'의 해를 한 번만 계산하고 저장하여 재사용하는 것이었죠. 이제 우리는 훨씬 더 복잡해 보이지만, 동일한 원리가 화려하게 적용되는 새로운 최적화 문제를 만나보겠습니다. 바로 연속된 행렬들의 곱셈 순서를 정하는 문제입니다.

먼저 행렬 곱셈의 기본적인 성질을 짚고 넘어가야 합니다. 두 행렬 `A`와 `B`를 곱하여 행렬 `C`를 얻는 연산 `C = A × B`는, 행렬 `A`의 열의 개수와 행렬 `B`의 행의 개수가 같을 때만 정의됩니다. 만약 `A`가 `m × p` 행렬이고 `B`가 `p × n` 행렬이라면, 결과 `C`는 `m × n` 행렬이 됩니다.

이때, `C`의 각 원소 `c_ij`를 계산하기 위해서는 `A`의 `i`번째 행과 `B`의 `j`번째 열에 있는 원소들을 각각 곱해서 더해야 합니다. 이 과정에는 총 `p`번의 스칼라 곱셈이 필요합니다. 결과 행렬 `C`는 총 `m × n`개의 원소를 가지므로, 두 행렬 `A`와 `B`를 곱하는 데 필요한 총 스칼라 곱셈 횟수는 `m × p × n`이 됩니다. 이 곱셈 횟수가 바로 우리가 앞으로 최소화하려는 '비용(cost)'이 됩니다.

자, 이제 행렬이 세 개 이상, 예를 들어 `M₁ × M₂ × M₃`과 같이 연속으로 주어졌다고 생각해 봅시다. 행렬 곱셈은 결합 법칙(associative law)이 성립하기 때문에, 곱하는 순서를 바꿔도 최종 결과는 같습니다. 즉, `(M₁ × M₂) × M₃` 와 `M₁ × (M₂ × M₃)`의 결과 행렬은 동일합니다.

**하지만, 계산 과정의 '비용'은 어떨까요?**

예를 들어 세 행렬의 크기가 다음과 같다고 가정해 봅시다.
*   `M₁` : 10 × 20
*   `M₂` : 20 × 5
*   `M₃` : 5 × 50

**경우 1: `(M₁ × M₂) × M₃`**
1.  `M₁ × M₂`를 먼저 계산합니다.
    *   비용: 10 × 20 × 5 = 1,000 번의 곱셈.
    *   결과 행렬 `(M₁₂)`의 크기: 10 × 5
2.  결과 행렬 `(M₁₂)`와 `M₃`를 곱합니다.
    *   비용: 10 × 5 × 50 = 2,500 번의 곱셈.
    *   최종 결과 행렬의 크기: 10 × 50
3.  **총 비용: 1,000 + 2,500 = 3,500**

**경우 2: `M₁ × (M₂ × M₃)`**
1.  `M₂ × M₃`를 먼저 계산합니다.
    *   비용: 20 × 5 × 50 = 5,000 번의 곱셈.
    *   결과 행렬 `(M₂₃)`의 크기: 20 × 50
2.  `M₁`과 결과 행렬 `(M₂₃)`을 곱합니다.
    *   비용: 10 × 20 × 50 = 10,000 번의 곱셈.
    *   최종 결과 행렬의 크기: 10 × 50
3.  **총 비용: 5,000 + 10,000 = 15,000**

놀랍게도 곱셈 순서에 따라 총 계산 비용이 **3,500**과 **15,000**으로 엄청난 차이를 보입니다. 행렬의 개수가 늘어날수록 가능한 곱셈 순서의 조합은 기하급수적으로 증가(카탈란 수, Catalan number를 따름)하며, 최적의 순서를 찾는 것은 매우 중요한 문제가 됩니다. 이것이 바로 **연속 행렬 곱셈 문제(Matrix Chain Multiplication Problem)**입니다.

> **문제 정의:** `n`개의 행렬 체인 `M₁ × M₂ × ... × Mₙ`이 주어졌을 때, 총 스칼라 곱셈 횟수를 최소화하는 곱셈 순서(괄호 치는 방법)를 찾는 것.

> 컴퓨터가 두 행렬을 곱할 때 실제로 무슨 일을 하는지 들여다봅시다.
> A (2x**3** 크기) 와 B (**3**x2 크기) 두 행렬이 있다고 상상해 보세요.
> 결과 행렬 C의 딱 한 칸, C\[1\]\[1\] (1행 1열)을 채우려면 어떻게 해야 할까요?
> 	A의 1행 (\[1, 2, 3\])과 B의 1열 (\[7, 9, 11\])을 가져옵니다.
> 	각각의 위치에 있는 숫자끼리 **곱하고**, 그 결과들을 전부 **더합니다**.
> 	(1 × 7) -> 곱셈 1번
>     (2 × 9) -> 곱셈 1번
>     (3 × 11) -> 곱셈 1번
>     결과들을 더하기: 7 + 18 + 33 = 58
> C의 **딱 한 칸**을 채우는 데 **곱셈을 3번** 했습니다.
> 그럼 C의 모든 칸(총 2x2 = 4칸)을 채우려면 곱셈을 몇 번 해야 할까요?
> 	- 모든 칸에 대해 똑같이 곱셈을 3번씩 해야 합니다.
> 	- 총 곱셈 횟수 = (한 칸당 곱셈 횟수) × (총 칸의 개수)
> 	- 총 곱셈 횟수 = **3** × (**2 × 2**) = 12번
> 이것을 일반화한 것이 바로 그 공식입니다.
> m × p 행렬과 p × n 행렬을 곱하면,
> - 결과 행렬은 m × n 크기 (총 m × n 개의 칸)
> - 한 칸을 채우는 데 곱셈을 p번 함
> **총 곱셈 횟수(계산량) = m × p × n**
> 알고리즘에서 말하는 **"비용(Cost)" 또는 "계산량"**은 바로 이 **총 곱셈 횟수**를 의미합니다. 우리는 이 숫자를 가장 작게 만들고 싶은 것입니다.

##### 첫 번째 시도: 분할 정복의 함정

이 문제 역시 분할 정복의 관점에서 접근해 볼 수 있습니다. `Mᵢ × Mᵢ₊₁ × ... × Mⱼ`의 최소 곱셈 횟수를 구하는 문제를 생각해 봅시다. 이 행렬 체인의 마지막 곱셈은 반드시 어떤 `k` (단, `i ≤ k < j`)를 기준으로 두 부분 체인의 곱으로 이루어집니다.

`(Mᵢ × ... × Mₖ) × (Mₖ₊₁ × ... × Mⱼ)`

즉, `i`부터 `j`까지의 행렬 곱셈 문제의 최적해를 구하려면, 가능한 모든 분할 지점 `k`에 대해 문제를 나누어보고 그중 가장 비용이 적게 드는 경우를 선택하면 됩니다.

*   `Mᵢ` 행렬의 크기를 `rᵢ₋₁ × rᵢ` 라고 표준화합시다. (즉, `r` 배열은 행렬들의 차원을 저장합니다.)
*   `Mᵢ × ... × Mₖ`를 계산한 결과 행렬의 크기는 `rᵢ₋₁ × rₖ`가 됩니다.
*   `Mₖ₊₁ × ... × Mⱼ`를 계산한 결과 행렬의 크기는 `rₖ × rⱼ`가 됩니다.

따라서, 분할 지점이 `k`일 때의 총비용은 다음과 같이 세 부분으로 나뉩니다.
1.  `Mᵢ × ... × Mₖ`를 계산하는 최소 비용.
2.  `Mₖ₊₁ × ... × Mⱼ`를 계산하는 최소 비용.
3.  두 결과 행렬을 마지막으로 곱하는 비용: `rᵢ₋₁ × rₖ × rⱼ`

`matMult(i, j)`를 `Mᵢ`부터 `Mⱼ`까지 곱하는 데 필요한 최소 곱셈 횟수라고 정의하면, 다음과 같은 점화식을 세울 수 있습니다.

> **matMult(i, j) = min { matMult(i, k) + matMult(k+1, j) + rᵢ₋₁rₖrⱼ }** (단, i ≤ k < j)
>
> `matMult(i, i) = 0` (행렬이 하나일 때는 곱셈이 필요 없으므로 비용은 0)

이 점화식은 문제의 구조를 완벽하게 표현합니다. 이를 그대로 재귀 함수로 구현하면 `matMult_DC`와 같은 코드가 됩니다.

```c
// 분할 정복(재귀) 방식의 연속 행렬 곱셈 알고리즘
matMult_DC(r[], i, j) {
    if (i == j) return 0; // 곱할 행렬이 하나뿐인 경우

    minVal = ∞;
    // 가능한 모든 분할 지점 k에 대해 탐색
    for (k = i; k < j; k++) {
        // (i...k) 부분 문제 + (k+1...j) 부분 문제 + 마지막 곱셈 비용
        cost = matMult_DC(r, i, k) + matMult_DC(r, k+1, j) + r[i-1]*r[k]*r[j];
        minVal = MIN(minVal, cost);
    }
    return minVal;
}
```

하지만 이 재귀 함수는 막대 자르기 문제에서 겪었던 것과 똑같은, 아니 훨씬 심각한 비효율을 가지고 있습니다. `matMult(1, 4)`를 계산하려면 `matMult(1, 1)`, `matMult(2, 4)`... 등등을 호출하고, `matMult(1, 3)`을 계산하기 위해서도 `matMult(2, 3)` 등을 호출하는데, `matMult(2, 4)`를 계산하는 과정에서도 `matMult(2, 3)`이 또 호출됩니다.

즉, **'중복되는 부분 문제'**가 엄청나게 많이 발생하며, 시간 복잡도는 지수적으로 증가하여 `n`이 20만 되어도 계산이 거의 불가능해집니다.

##### 두 번째 시도: 동적 계획법의 설계

우리는 이미 해법을 알고 있습니다. 이 문제는 '최적 부분 구조'와 '중복되는 부분 문제'라는 동적 계획법의 전제 조건을 완벽하게 만족합니다. 따라서 재귀 대신, 가장 작은 부분 문제부터 차례로 해결하여 그 결과를 테이블에 저장하는 **상향식(Bottom-up) 동적 계획법**을 적용할 수 있습니다.

1.  **결과를 저장할 2차원 배열(테이블)을 만듭니다.** `m[i][j]`는 `Mᵢ`부터 `Mⱼ`까지 곱하는 데 필요한 최소 곱셈 횟수, 즉 `matMult(i, j)`의 값을 저장할 공간입니다.

2.  **가장 작은 문제부터 해결합니다.** 가장 작은 문제는 곱셈 체인의 길이가 1인 경우, 즉 `m[i][i]`입니다. 이 경우 곱셈이 없으므로 비용은 0입니다. `m[i][i] = 0`으로 테이블의 대각선을 모두 채웁니다.

3.  **점진적으로 문제의 크기를 키워나갑니다.** 그 다음으로 작은 문제는 체인 길이가 2인 경우(`m[i][i+1]`), 그 다음은 3인 경우(`m[i][i+2]`), ... 마지막으로 우리가 원하는 체인 길이 `n`인 경우(`m[1][n]`)까지 순서대로 계산합니다.

이 '체인의 길이'를 제어하는 것이 이 알고리즘의 핵심입니다. 변수 `l`을 체인의 길이라고 합시다. `l`은 2부터 `n`까지 증가합니다. (슬라이드에서는 `l`을 `j-i`로 정의하여 1부터 `n-1`까지 증가시켰는데, 이는 동일한 개념입니다.)

*   `l = 1`일 때: 길이가 2인 체인(`m[1,2]`, `m[2,3]`, ...)을 모두 계산합니다.
*   `l = 2`일 때: 길이가 3인 체인(`m[1,3]`, `m[2,4]`, ...)을 모두 계산합니다. 이때 `m[1,3]`을 계산하려면 이미 계산된 `m[1,1]`, `m[2,3]`, `m[1,2]`, `m[3,3]` 등의 값이 필요하며, 우리는 이미 이 값들을 테이블에 저장해 두었습니다.
*   ...
*   `l = n-1`일 때: 길이가 `n`인 유일한 체인, 즉 `m[1,n]`을 계산합니다. 이것이 최종 답입니다.

이 로직을 코드로 구현하면 다음과 같습니다.

```c
// 동적 계획법 방식의 연속 행렬 곱셈 알고리즘
matMult_DP(r[], n) {
    // 1. 결과 저장용 2차원 배열 m 선언
    배열 m[1..n, 1..n]을 선언한다.

    // 2. 가장 작은 문제(체인 길이 1) 해결
    for (i = 1; i <= n; i++) {
        m[i, i] = 0;
    }

    // 3. 점진적으로 문제 크기(체인 길이 l)를 키움
    // l은 체인의 길이 - 1 (j-i)
    for (l = 1; l <= n - 1; l++) {
        // i는 체인의 시작 인덱스
        for (i = 1; i <= n - l; i++) {
            j = i + l; // j는 체인의 끝 인덱스
            m[i, j] = ∞; // 최솟값을 찾기 위해 무한대로 초기화

            // k는 분할 지점
            for (k = i; k < j; k++) {
                // 점화식을 DP 테이블을 이용하여 계산
                cost = m[i, k] + m[k+1, j] + r[i - 1]*r[k]*r[j];
                if (cost < m[i, j]) {
                    m[i, j] = cost;
                }
            }
        }
    }

    // 최종적으로 우리가 원하는 해(m[1,n])를 반환
    return m[1, n];
}
```

##### 동적 계획 알고리즘 실행 과정 추적 (예제 풀이)

이제 슬라이드의 예제를 통해 이 알고리즘이 마법처럼 동작하는 과정을 단계별로 따라가 보겠습니다.

**문제:** `M₁ × M₂ × M₃ × M₄` 의 최소 곱셈 횟수를 구하라.
*   `M₁`: 10 × 20
*   `M₂`: 20 × 50
*   `M₃`: 50 × 1
*   `M₄`: 1 × 100

**차원 배열 `r`:** 행렬 `Mᵢ`가 `rᵢ₋₁ × rᵢ` 크기이므로, `r` 배열은 다음과 같습니다.
`r = [r₀, r₁, r₂, r₃, r₄] = [10, 20, 50, 1, 100]`

**테이블 `m`:** 4x4 크기의 테이블을 준비합니다.

---

**1단계: 초기화 (체인 길이 1, l=0)**

가장 작은 문제인 `m[i,i]`를 0으로 채웁니다.
*   `m[1,1] = 0`
*   `m[2,2] = 0`
*   `m[3,3] = 0`
*   `m[4,4] = 0`

| i\j | 1 | 2 | 3 | 4 |
| :---: | :-: | :-: | :-: | :-: |
| **1** | 0 | | | |
| **2** | | 0 | | |
| **3** | | | 0 | |
| **4** | | | | 0 |

---

**2단계: `l = 1` (체인 길이 2)**

길이가 2인 체인들(`m[1,2]`, `m[2,3]`, `m[3,4]`)의 최소 비용을 계산합니다. 이 경우 분할 지점 `k`는 하나뿐입니다.

*   **`m[1,2]` 계산 (`M₁ × M₂`)**
    *   `i=1, j=2`, 분할 지점 `k=1`
    *   비용 = `m[1,1] + m[2,2] + r₀r₁r₂`
    *   비용 = `0 + 0 + 10 × 20 × 50 = 10,000`
    *   `m[1,2] = 10000`


*   **`m[2,3]` 계산 (`M₂ × M₃`)**
    *   `i=2, j=3`, 분할 지점 `k=2`
    *   비용 = `m[2,2] + m[3,3] + r₁r₂r₃`
    *   비용 = `0 + 0 + 20 × 50 × 1 = 1,000`
    *   `m[2,3] = 1000`

*   **`m[3,4]` 계산 (`M₃ × M₄`)**
    *   `i=3, j=4`, 분할 지점 `k=3`
    *   비용 = `m[3,3] + m[4,4] + r₂r₃r₄`
    *   비용 = `0 + 0 + 50 × 1 × 100 = 5,000`
    *   `m[3,4] = 5000`

**현재 테이블 상태:**

| i\j | 1 | 2 | 3 | 4 |
| :---: | :-: | :----: | :----: | :----: |
| **1** | 0 | **10000** | | |
| **2** | | 0 | **1000** | |
| **3** | | | 0 | **5000** |
| **4** | | | | 0 |

---

**3단계: `l = 2` (체인 길이 3)**

길이가 3인 체인들(`m[1,3]`, `m[2,4]`)의 최소 비용을 계산합니다. 이제 분할 지점이 두 개씩 생깁니다.

*   **`m[1,3]` 계산 (`M₁ × M₂ × M₃`)**
    *   `i=1, j=3`. 가능한 분할 지점 `k=1, 2`.
    *   **`k=1`일 때:** `(M₁) × (M₂ × M₃)`
        *   비용 = `m[1,1] + m[2,3] + r₀r₁r₃`
        *   비용 = `0 + 1000 + 10 × 20 × 1 = 1,200`
    *   **`k=2`일 때:** `(M₁ × M₂) × (M₃)`
        *   비용 = `m[1,2] + m[3,3] + r₀r₂r₃`
        *   비용 = `10000 + 0 + 10 × 50 × 1 = 10,500`
    *   `min(1200, 10500) = 1200`. 따라서 `m[1,3] = 1200`

*   **`m[2,4]` 계산 (`M₂ × M₃ × M₄`)**
    *   `i=2, j=4`. 가능한 분할 지점 `k=2, 3`.
    *   **`k=2`일 때:** `(M₂) × (M₃ × M₄)`
        *   비용 = `m[2,2] + m[3,4] + r₁r₂r₄`
        *   비용 = `0 + 5000 + 20 × 50 × 100 = 105,000`
    *   **`k=3`일 때:** `(M₂ × M₃) × (M₄)`
        *   비용 = `m[2,3] + m[4,4] + r₁r₃r₄`
        *   비용 = `1000 + 0 + 20 × 1 × 100 = 3,000`
    *   `min(105000, 3000) = 3000`. 따라서 `m[2,4] = 3000`

**현재 테이블 상태:**

| i\j | 1 | 2 | 3 | 4 |
| :---: | :-: | :----: | :----: | :----: |
| **1** | 0 | 10000 | **1200** | |
| **2** | | 0 | 1000 | **3000** |
| **3** | | | 0 | 5000 |
| **4** | | | | 0 |

---

**4단계: `l = 3` (체인 길이 4)**

마지막으로, 우리가 구하고자 하는 전체 체인(`m[1,4]`)의 최소 비용을 계산합니다.

*   **`m[1,4]` 계산 (`M₁ × M₂ × M₃ × M₄`)**
    *   `i=1, j=4`. 가능한 분할 지점 `k=1, 2, 3`.
    *   **`k=1`일 때:** `(M₁) × (M₂ × M₃ × M₄)`
        *   비용 = `m[1,1] + m[2,4] + r₀r₁r₄`
        *   비용 = `0 + 3000 + 10 × 20 × 100 = 23,000`
    *   **`k=2`일 때:** `(M₁ × M₂) × (M₃ × M₄)`
        *   비용 = `m[1,2] + m[3,4] + r₀r₂r₄`
        *   비용 = `10000 + 5000 + 10 × 50 × 100 = 65,000`
    *   **`k=3`일 때:** `(M₁ × M₂ × M₃) × (M₄)`
        *   비용 = `m[1,3] + m[4,4] + r₀r₃r₄`
        *   비용 = `1200 + 0 + 10 × 1 × 100 = 2,200`
    *   `min(23000, 65000, 2200) = 2200`. 따라서 `m[1,4] = 2200`

**최종 테이블 상태:**

| i\j | 1 | 2 | 3 | 4 |
| :---: | :-: | :----: | :----: | :----: |
| **1** | 0 | 10000 | 1200 | **2200** |
| **2** | | 0 | 1000 | 3000 |
| **3** | | | 0 | 5000 |
| **4** | | | | 0 |

---

모든 계산이 끝났습니다. 테이블의 `m[1,4]`에 저장된 값 **2,200**이 바로 `M₁ × M₂ × M₃ × M₄`를 계산하는 데 필요한 최소 스칼라 곱셈 횟수입니다. 그리고 이 최적의 비용은 `k=3`일 때, 즉 `(M₁ × M₂ × M₃) × M₄` 순서로 곱했을 때 얻어진다는 것도 알 수 있습니다. (물론 `(M₁ × M₂ × M₃)`의 최적 순서는 `m[1,3]` 계산 과정에서 `(M₁) × (M₂ × M₃)`로 이미 결정되었습니다.)

##### 결론: 복잡성을 길들이는 체계적인 접근

*   **효율성 분석:** 동적 계획법 알고리즘은 3개의 중첩된 루프(`l`, `i`, `k`)로 구성되어 있습니다. 각 루프는 최대 `n`번 정도 반복되므로, 전체 시간 복잡도는 **Θ(n³)**입니다. 이는 분할 정복 방식의 지수 시간 복잡도와는 비교할 수 없을 정도로 효율적입니다. `n`이 100이라면, `100³`은 100만이지만, 지수 시간은 천문학적인 숫자가 됩니다.
*   **공간 복잡도:** `n × n` 크기의 테이블 `m`을 사용하므로 공간 복잡도는 **Θ(n²)** 입니다.

연속 행렬 곱셈 문제는 동적 계획법의 설계 과정을 가장 잘 보여주는 교과서적인 예제입니다.
1.  **최적화 문제의 구조를 파악하고 점화식을 세웁니다.**
2.  **가장 작은 부분 문제부터 시작하여 상향식으로 해를 구축합니다.**
3.  **테이블을 사용하여 계산된 해를 저장하고 재사용함으로써 중복 계산을 완벽하게 제거합니다.**

겉보기에는 복잡하고 수많은 경우의 수를 따져야 할 것 같은 문제였지만, 동적 계획법이라는 체계적인 접근법을 통해 다항 시간 내에 효율적으로 해결할 수 있었습니다. 이는 복잡한 문제에 직면했을 때, 문제를 올바르게 분해하고 그 관계를 파악하는 것이 얼마나 중요한지를 보여주는 강력한 증거입니다.


네, 알겠습니다. 앞서 다룬 막대 자르기 문제에 이어, 동적 계획법의 힘을 더욱 극적으로 보여주는 **연속 행렬 곱셈(Matrix Chain Multiplication)** 문제에 대해 이야기 형식으로 매우 상세하게 설명해 드리겠습니다.


네, 알겠습니다. 제공해주신 우수한 자료를 바탕으로, 독자의 이해 흐름을 최적화하기 위해 순서를 재구성하고 내용을 다듬어 이야기 형식으로 상세하게 설명해 드리겠습니다.

#### 모든 쌍 최단 경로 (All-Pairs Shortest Path)

##### 서막: 궁극의 내비게이션을 향하여

우리가 매일 사용하는 내비게이션은 "A에서 B까지 가는 가장 빠른 길"이라는 하나의 질문에 답합니다. 이는 컴퓨터 과학에서 **단일 출발점 최단 경로(Single-Source Shortest Path)** 문제로, 다익스트라 알고리즘이 훌륭한 해답을 제공합니다.

하지만 만약 우리가 이 서비스를 운영하는 회사라면 어떨까요? 우리는 수많은 사용자의 무작위적인 출발-도착점 요청에 즉각적으로 응답해야 합니다. 더 나아가, 도시 전체의 교통 흐름을 분석하거나, 물류 회사가 수백 개의 지점 사이의 최적 운송 루트를 미리 계산하거나, 항공사가 모든 공항 간의 최적 환승 경로를 파악해야 하는 상황을 상상해 봅시다.

이런 경우, 우리는 도시의 **모든 지점에서 다른 모든 지점까지**의 최단 경로를 미리 계산해 두는 것이 훨씬 효율적입니다. 이것이 바로 **모든 쌍 최단 경로(All-Pairs Shortest Path, APSP)** 문제입니다. 이는 단순히 길 하나를 찾는 것을 넘어, 네트워크 전체의 구조적인 연결성을 완벽하게 이해하려는 궁극의 내비게이션 과제와 같습니다.

##### 제1장: 문제의 공식화 - 그래프 언어로의 번역

이 거대한 과제를 컴퓨터 과학의 언어로 옮겨 보겠습니다.

*   **그래프 `G = (V, E)`**: 도시의 교차로들은 **정점(Vertex)** `V`가 되고, 교차로를 잇는 도로들은 **간선(Edge)** `E`이 됩니다.
*   **가중치(Weight)**: 각 도로는 길이, 통행 시간 등의 비용을 가집니다. 도로는 일방통행일 수 있으므로, **방향 그래프(Directed Graph)**를 가정합니다.
*   **문제 정의:** 가중치 방향 그래프 `G`가 주어졌을 때, 모든 정점 쌍 `(i, j)`에 대해, `i`에서 `j`로 가는 최단 경로의 거리를 찾는 것.

컴퓨터는 이 정보를 **인접 행렬(Adjacency Matrix)** `W`로 표현합니다. `W`는 `n x n` 크기의 행렬이며, `wᵢⱼ`는 정점 `i`에서 `j`로 *직접 가는* 간선의 가중치입니다.

*   `wᵢⱼ = 0` (만약 `i = j`, 자기 자신으로 가는 비용은 0)
*   `wᵢⱼ = 간선의 가중치` (만약 `i`에서 `j`로 가는 간선이 존재)
*   `wᵢⱼ = ∞` (만약 `i`에서 `j`로 직접 가는 간선이 없음)

우리가 최종적으로 얻고 싶은 결과물은 **거리 행렬(Distance Matrix)** `D`입니다. `dᵢⱼ`는 정점 `i`에서 `j`로 가는, *중간에 다른 정점들을 거쳐갈 수 있는* 모든 경로 중 가장 짧은 경로의 거리를 담고 있습니다.

##### 제2장: 첫 번째 시도와 절망 - 억지 기법의 한계

가장 순진한 접근법은 모든 정점 쌍 `(i, j)`에 대해, `i`에서 `j`로 갈 수 있는 모든 가능한 경로를 하나도 빠짐없이 찾아보는 것입니다. 하지만 이는 정점의 개수 `n`이 조금만 커져도 경우의 수가 기하급수적으로 폭발합니다. `i`에서 `j`로 가는 경로에 나머지 `n-2`개의 정점들을 어떤 순서로 방문하느냐에 따라 `(n-2)!`에 비례하는 경로가 생성됩니다. 이는 천문학적인 숫자로, 현실적으로 불가능한 접근법입니다.

##### 제3장: 발상의 전환 - 동적 계획법의 새로운 관점

이 막다른 길에서 로버트 플로이드(Robert Floyd)와 스티븐 워셜(Stephen Warshall)은 문제를 완전히 새로운 각도에서 바라보는 천재적인 아이디어를 제시합니다.

> **"최단 경로를 구성하는 '중간 정점'의 범위를 점진적으로 늘려가며 해를 구하자."**

이것이 바로 **플로이드-워셜 알고리즘**의 핵심 사상입니다. 우리는 부분 문제를 다음과 같이 새롭게 정의합니다.

> **`dᵢⱼ⁽ᵏ⁾`**: 정점 `i`에서 `j`로 가는 경로 중에서, **중간에 거쳐갈 수 있는 정점이 `{1, 2, ..., k}` 집합에 속하는 정점들로만 제한**되었을 때의 최단 경로의 거리.

이 정의는 마치 우리가 세상을 알아가는 과정과 같습니다.

*   `dᵢⱼ⁽⁰⁾`: 중간에 어떤 정점도 거쳐갈 수 없을 때의 최단 경로. 이는 `i`에서 `j`로 직접 가는 길 뿐입니다. 즉, **`dᵢⱼ⁽⁰⁾ = wᵢⱼ`**. 이것이 우리의 출발점입니다.
*   `dᵢⱼ⁽¹⁾`: 이제 '정점 1'을 경유지로 사용할 수 있습니다. 기존의 직접 가는 길과, '정점 1'을 거쳐가는 새로운 길(`i → 1 → j`) 중 더 짧은 길을 선택합니다.
*   `dᵢⱼ⁽²⁾`: 이제 '정점 1 또는 2'를 경유지로 사용할 수 있습니다. `dᵢⱼ⁽¹⁾`을 기반으로, 새롭게 허용된 '정점 2'를 거쳐가면 혹시 길이 더 짧아지지 않을까 검토합니다.
*   ...
*   `dᵢⱼ⁽ⁿ⁾`: 마침내 모든 정점 `{1, 2, ..., n}`을 경유지로 허용합니다. 이것이 바로 우리가 최종적으로 원하는 최단 거리 `dᵢⱼ`입니다.

이처럼 '허용된 중간 정점'의 범위를 `k=0`에서 `n`까지 점진적으로 확장하며, 거리 행렬(우리의 지도)을 계속해서 갱신해 나가는 것이 이 알고리즘의 전략입니다.

##### 제4장: 점화식의 발견 - 정점 `k`를 둘러싼 선택

이 아이디어를 구체적인 점화식으로 만들어 봅시다. `dᵢⱼ⁽ᵏ⁾`를 계산해야 하는 시점에서, 우리는 이미 그 이전 단계의 완벽한 해답, 즉 `dᵢⱼ⁽ᵏ⁻¹⁾` (중간 정점을 `k-1`까지만 허용했을 때의 최단 경로)를 알고 있습니다.

`i`에서 `j`로 가는 최단 경로(중간 정점 `k`까지 허용)는 다음 두 가지 경우 중 하나입니다.

1.  **경우 1: 최단 경로가 정점 `k`를 거쳐가지 않는다.**
    이 경로에 정점 `k`가 필요 없다면, 이 경로는 오직 `{1, ..., k-1}` 집합의 정점들만 중간에 사용합니다. 그렇다면 이 경로의 최단 거리는 `k`를 고려하기 이전의 최단 거리와 동일합니다. 즉, **`dᵢⱼ⁽ᵏ⁻¹⁾`** 입니다.

2.  **경우 2: 최단 경로가 정점 `k`를 거쳐간다.**
    만약 최단 경로가 `k`를 거쳐간다면, 그 경로는 `i → ... → k → ... → j` 와 같은 형태입니다. 최적 부분 구조 원리에 따라, 이 경로의 `i → k` 부분과 `k → j` 부분 역시 각각 최단 경로여야 합니다. 이 부분 경로들은 중간에 `k`를 또 거칠 수 없으므로, 오직 `{1, ..., k-1}` 집합의 정점들만 사용합니다.
    따라서 `k`를 거쳐가는 경로의 총거리는 **`dᵢₖ⁽ᵏ⁻¹⁾ + dₖⱼ⁽ᵏ⁻¹⁾`** 가 됩니다.

우리는 이 두 가지 경우 중 더 짧은 쪽, 즉 더 작은 값을 선택해야 합니다. 이로써 플로이드-워셜 알고리즘의 핵심 점화식이 완성됩니다.

> **`dᵢⱼ⁽ᵏ⁾ = min( dᵢⱼ⁽ᵏ⁻¹⁾, dᵢₖ⁽ᵏ⁻¹⁾ + dₖⱼ⁽ᵏ⁻¹⁾ )`**

이 점화식은 우리에게 `k-1` 단계의 해답 행렬만 있으면 `k` 단계의 해답 행렬을 만들 수 있음을 알려줍니다.

##### 제5장: 플로이드-워셜 알고리즘과 분석

###### 알고리즘 구현: 단순함의 미학

이 점화식은 놀랍도록 간결한 코드로 구현됩니다. `k-1` 단계와 `k` 단계의 행렬을 따로 저장할 필요 없이, 하나의 행렬을 계속 갱신하는 방식으로 최적화할 수 있습니다.

```c
// D는 W로 초기화되었다고 가정
for (k = 1; k <= n; k++) {
    for (i = 1; i <= n; i++) {
        for (j = 1; j <= n; j++) {
            // D[i][j]는 'dᵢⱼ⁽ᵏ⁻¹⁾'의 역할을, 갱신 후에는 'dᵢⱼ⁽ᵏ⁾'가 됨
            D[i][j] = min(D[i][j], D[i][k] + D[k][j]);
        }
    }
}
```

###### 알고리즘 분석

*   **시간 복잡도:** `k`, `i`, `j` 세 개의 중첩 반복문이 각각 `n`번씩 실행되므로, 총 시간 복잡도는 매우 직관적으로 **`Θ(n³)`** 입니다.
*   **공간 복잡도:** `n x n` 크기의 거리 행렬 하나만 저장하면 되므로, 공간 복잡도는 **`Θ(n²)`** 입니다.

##### 제6장: 알고리즘 실행 과정 추적

알고리즘이 어떻게 한 단계씩 최단 경로를 '발견'해 나가는지 예제로 따라가 보겠습니다.

**초기 인접 행렬 `W = D⁽⁰⁾`:**

| D⁽⁰⁾ | 1 | 2 | 3 | 4 |
| :--: | :-: | :-: | :-: | :-: |
| **1** | 0 | 1 | ∞ | 2 |
| **2** | 6 | 0 | 4 | ∞ |
| **3** | ∞ | ∞ | 0 | 3 |
| **4** | ∞ | 8 | ∞ | 0 |

---

**1단계: `k = 1` (정점 1을 경유지로 허용)**
`dᵢⱼ⁽¹⁾ = min( dᵢⱼ⁽⁰⁾, dᵢ₁⁽⁰⁾ + d₁ⱼ⁽⁰⁾ )`
*   `d₂₄⁽¹⁾`: `min(∞, d₂₁⁽⁰⁾ + d₁₄⁽⁰⁾) = min(∞, 6 + 2) = 8`. (`2 → 1 → 4` 경로 발견)

| D⁽¹⁾ | 1 | 2 | 3 | 4 |
| :--: | :-: | :-: | :-: | :-: |
| **1** | 0 | 1 | ∞ | 2 |
| **2** | 6 | 0 | 4 | **8** |
| **3** | ∞ | ∞ | 0 | 3 |
| **4** | ∞ | 8 | ∞ | 0 |

---

**2단계: `k = 2` (정점 1, 2를 경유지로 허용)**
`dᵢⱼ⁽²⁾ = min( dᵢⱼ⁽¹⁾, dᵢ₂⁽¹⁾ + d₂ⱼ⁽¹⁾ )`
*   `d₁₃⁽²⁾`: `min(∞, d₁₂⁽¹⁾ + d₂₃⁽¹⁾) = min(∞, 1 + 4) = 5`. (`1 → 2 → 3` 경로 발견)
*   `d₄₁⁽²⁾`: `min(∞, d₄₂⁽¹⁾ + d₂₁⁽¹⁾) = min(∞, 8 + 6) = 14`. (`4 → 2 → 1` 경로 발견)

| D⁽²⁾ | 1 | 2 | 3 | 4 |
| :--: | :-: | :-: | :-: | :-: |
| **1** | 0 | 1 | **5** | 2 |
| **2** | 6 | 0 | 4 | 8 |
| **3** | ∞ | ∞ | 0 | 3 |
| **4** | **14**| 8 | ∞ | 0 |

---

**3단계: `k = 3` (정점 1, 2, 3을 경유지로 허용)**
`dᵢⱼ⁽³⁾ = min( dᵢⱼ⁽²⁾, dᵢ₃⁽²⁾ + d₃ⱼ⁽²⁾ )`
*   `d₂₄⁽³⁾`: `min(8, d₂₃⁽²⁾ + d₃₄⁽²⁾) = min(8, 4 + 3) = 7`. (기존 `2→1→4`보다 `2→3→4`가 더 빠름)
*   `d₄₂⁽³⁾`: `min(8, d₄₃⁽²⁾ + d₃₂⁽²⁾) = min(8, ∞ + ∞) = 8`. (갱신 없음)

| D⁽³⁾ | 1 | 2 | 3 | 4 |
| :--: | :-: | :-: | :-: | :-: |
| **1** | 0 | 1 | 5 | 2 |
| **2** | 6 | 0 | 4 | **7** |
| **3** | ∞ | ∞ | 0 | 3 |
| **4** | 14| 8 | ∞ | 0 |

*(주: 원본 자료의 예시 테이블과 달리, 정확한 계산에 따라 값을 갱신하였습니다.)*

---

**4단계: `k = 4` (모든 정점을 경유지로 허용)**
`dᵢⱼ⁽⁴⁾ = min( dᵢⱼ⁽³⁾, dᵢ₄⁽³⁾ + d₄ⱼ⁽³⁾ )`
*   `d₁₃⁽⁴⁾`: `min(5, d₁₄⁽³⁾ + d₄₃⁽³⁾) = min(5, 2 + ∞) = 5`.
*   `d₃₂⁽⁴⁾`: `min(∞, d₃₄⁽³⁾ + d₄₂⁽³⁾) = min(∞, 3 + 8) = 11`. (`3 → 4 → 2` 경로 발견)
*   `d₃₁⁽⁴⁾`: `min(∞, d₃₄⁽³⁾ + d₄₁⁽³⁾) = min(∞, 3 + 14) = 17`. (`3 → 4 → 2 → 1` 경로 발견)

| D⁽⁴⁾ - 최종 결과 | 1 | 2 | 3 | 4 |
| :-------------: | :-: | :-: | :-: | :-: |
| **1** | 0 | 1 | 5 | 2 |
| **2** | 6 | 0 | 4 | 7 |
| **3** | **17**| **11**| 0 | 3 |
| **4** | 14| 8 | ∞ | 0 |

이것이 최종 거리 행렬 `D`입니다. 이제 이 도시의 어떤 두 지점 사이의 최단 거리를 즉시 알 수 있습니다.

##### 결론: 단순함 속에 담긴 깊이

모든 쌍 최단 경로 문제는 동적 계획법의 설계 패러다임을 보여주는 위대한 예시입니다. 이 문제는 우리에게 중요한 교훈을 줍니다. 때로는 문제의 차원을 다르게 바라보는 것, 즉 경로의 길이나 정점의 개수가 아닌 **'허용된 중간 정점'** 이라는 새로운 기준을 도입하는 창의적인 발상이 복잡성의 장벽을 무너뜨리는 열쇠가 될 수 있다는 것입니다. 플로이드-워셜 알고리즘의 우아함은 바로 이 지점에서 빛을 발하며, 복잡한 네트워크 문제에 대한 강력하고 체계적인 해법을 제시합니다.

#### 0-1 배낭 채우기 문제(0-1 Knapsack Problem)


##### 한 탐험가의 고민: 무엇을 챙겨야 할까?

전설 속 보물이 잠들어 있다는 미지의 유적을 탐사하러 떠나는 한 탐험가를 상상해 봅시다. 이 탐험가에게는 평생의 꿈을 이룰 단 한 번의 기회입니다. 오랜 탐사 끝에 마침내 보물 창고를 발견했지만, 기쁨도 잠시, 큰 난관에 부딪힙니다.

보물은 산더미처럼 쌓여 있지만, 탐험가가 가진 배낭의 용량은 한정되어 있습니다. 각 보물은 저마다 다른 무게를 가지고 있고, 그 가치 또한 천차만별입니다. 어떤 것은 작고 가볍지만 엄청난 가치를 지녔고, 어떤 것은 크고 무겁지만 상대적으로 가치가 낮습니다. 유적은 곧 무너질 위기에 처해 있고, 보물을 챙겨 나갈 기회는 단 한 번뿐입니다.

탐험가는 이제 인생일대의 선택을 해야 합니다.

> **"이 한정된 용량의 배낭에 어떤 보물들을 담아야 그 가치의 총합을 최대로 만들 수 있을까?"**

이것이 바로 배낭 채우기 문제입니다. 각 보물은 통째로 가져가거나(1), 아예 가져가지 않거나(0) 둘 중 하나만 선택할 수 있습니다. 보물을 쪼개서 일부만 가져갈 수는 없습니다. 그래서 이 문제를 특별히 **'0-1 배낭 채우기 문제'**라고 부릅니다.

##### 문제의 공식화와 첫 번째 시도: 모든 가능성을 시험하다 (지 기법)

탐험가의 고민을 알고리즘 문제로 바꾸어 봅시다.

*   `n`: 보물의 총개수
*   `w[i]`: `i`번째 보물의 무게
*   `v[i]`: `i`번째 보물의 가치
*   `C`: 배낭이 견딜 수 있는 최대 무게 (용량)

우리의 목표는, 선택한 보물들의 총무게가 `C`를 넘지 않으면서, 총가치가 최대가 되는 보물들의 조합을 찾는 것입니다.

가장 단순하고 무식한 방법은 무엇일까요? 바로 가능한 모든 경우의 수를 다 시험해 보는 것입니다. 보물이 `n`개 있다면, 각 보물에 대해 '가져간다'와 '가져가지 않는다' 두 가지 선택지가 있습니다. 따라서 총 경우의 수는 `2 × 2 × ... × 2` (n번), 즉 `2ⁿ`가지가 됩니다.

이 **억지 기법(Brute-force)** 알고리즘은 다음과 같이 동작합니다.
1.  `n`개 보물들의 모든 부분 집합(조합)을 하나씩 만들어 봅니다. (총 `2ⁿ`개)
2.  각 조합에 대해, 포함된 보물들의 총무게를 계산합니다.
3.  총무게가 배낭 용량 `C`를 초과하면, 그 조합은 버립니다.
4.  용량을 초과하지 않는 조합이라면, 포함된 보물들의 총가치를 계산합니다.
5.  모든 유효한 조합들의 총가치 중 가장 큰 값을 찾아냅니다.

이 방법은 보물의 개수(`n`)가 10개만 되어도 `2¹⁰ = 1024`개의 조합을, 20개면 `2²⁰`으로 백만 개가 넘는 조합을, 30개면 10억 개가 넘는 조합을 일일이 확인해야 합니다. 시간 복잡도가 **`Ω(2ⁿ)`**으로, `n`이 조금만 커져도 현실적으로 계산이 불가능한 '나쁜' 알고리즘입니다.

우리에겐 더 똑똑한 방법이 필요합니다.

##### 더 나은 길을 찾아서: 동적 계획법의 통찰

이 문제, 어딘가 익숙하지 않나요? 하나의 큰 결정(n개의 보물 선택)이 여러 개의 작은 결정들의 연속으로 이루어집니다. 그리고 그 결정들은 서로 영향을 줍니다. 이는 동적 계획법이 활약할 수 있는 좋은 무대입니다. 동적 계획법을 적용하기 위한 두 가지 핵심 속성을 다시 확인해 봅시다.

1.  **최적 부분 구조 (Optimal Substructure):** `n`개의 보물과 용량 `C`에 대한 최적의 해(최대 가치)는, 그보다 작은 부분 문제들의 최적의 해를 기반으로 구성될 수 있어야 합니다.
2.  **중복되는 부분 문제 (Overlapping Subproblems):** 전체 문제를 해결하는 과정에서 동일한 부분 문제가 반복적으로 나타나야 합니다.

배낭 문제도 이 속성을 가질까요? 그렇습니다. `n`개의 아이템에 대한 최적의 선택은, `n-1`개의 아이템에 대한 최적의 선택과 관련이 있습니다. 이 관계를 명확히 정의하는 것이 동적 계획법 설계의 첫걸음입니다.

###### 동적 계획법의 핵심: 부분 문제 정의하기

문제를 체계적으로 풀기 위해, 우리는 결과를 기록할 테이블을 만들고, 그 테이블의 각 칸이 무엇을 의미하는지 명확하게 정의해야 합니다. `K[i][j]`라는 2차원 배열을 사용해 봅시다.

> **`K[i][j]`**: 첫 번째 보물부터 `i`번째 보물까지만 고려했을 때, 용량이 `j`인 배낭에 담을 수 있는 최대 가치

이 정의가 가장 중요합니다. `i`는 우리가 고려할 보물의 범위를, `j`는 우리가 가진 배낭의 용량을 나타내는 '상태(state)'입니다. 우리의 최종 목표는 테이블의 가장 마지막 칸, 즉 `K[n][C]` (모든 `n`개의 보물을 고려했고, 배낭의 최대 용량이 `C`일 때의 최대 가치)를 알아내는 것입니다.

###### 점화식 세우기: 선택의 기로

자, 이제 테이블의 한 칸인 `K[i][j]`를 어떻게 계산할 수 있을지 생각해 봅시다. 우리는 `i`번째 보물을 앞에 두고 선택의 기로에 서 있습니다.

**"i번째 보물을 배낭에 넣을 것인가, 말 것인가?"**

이 결정에 따라 `K[i][j]`의 값이 정해집니다.

*   **경우 1: `i`번째 보물을 배낭에 넣지 않는다.**
    만약 `i`번째 보물을 넣지 않기로 결정했다면, 문제는 간단해집니다. `i`번째 보물은 그냥 무시하고, 나머지 `i-1`개의 보물을 가지고 용량 `j`의 배낭을 채우는 문제와 동일해집니다. 이 문제의 최적해는 우리가 이미 정의한 바에 따라 `K[i-1][j]`입니다.

*   **경우 2: `i`번째 보물을 배낭에 넣는다.**
    이 선택은 `i`번째 보물의 무게 `w[i]`가 현재 배낭 용량 `j`보다 작거나 같을 때만 가능합니다. 만약 `w[i] > j`라면 이 경우는 아예 고려할 수조차 없습니다.
    만약 넣는 것이 가능하다면, 우리는 `i`번째 보물의 가치 `v[i]`를 얻게 됩니다. 그리고 `i`번째 보물을 넣었으니 배낭에는 `j - w[i]` 만큼의 용량이 남게 됩니다. 이 남은 공간은 어떻게 채워야 할까요? 당연히 최적의 방법으로 채워야 합니다. 즉, 남은 `i-1`개의 보물을 가지고 용량 `j - w[i]`의 배낭을 채워 얻을 수 있는 최대 가치를 더해야 합니다. 이 값은 바로 `K[i-1][j - w[i]]`입니다.
    따라서 이 경우의 총가치는 `v[i] + K[i-1][j - w[i]]`가 됩니다.

우리는 항상 최대의 가치를 원하므로, 이 두 가지 경우 중 더 큰 값을 선택하면 됩니다. 이것이 바로 배낭 문제의 점화식입니다.

> 1.  **if `w[i] > j` (i번째 물건이 배낭 용량 j보다 무거워 담을 수 없는 경우):**
>     `K[i, j] = K[i-1, j]` (못 담으니 이전 상태와 같음)
>
> 2.  **if `w[i] <= j` (i번째 물건을 담을 수 있는 경우):**
>     `K[i, j] = max( K[i-1, j], v[i] + K[i-1, j - w[i]] )`
>     (안 담았을 때의 가치 vs 담았을 때의 가치)

이 점화식을 이용해 `i=1`부터 `n`까지, `j=1`부터 `C`까지 테이블을 순서대로 채워나가면 마침내 최종 답 `K[n][C]`에 도달할 수 있습니다.

##### 동적 계획 알고리즘 실행 과정 추적

이제 슬라이드의 예제를 통해 이 마법 같은 과정이 어떻게 이루어지는지 한 칸 한 칸 따라가 보겠습니다.

**문제 상황:**
*   배낭 용량 `C = 7`
*   물건 `n = 4`개

| 물건 `i` | 무게 `w[i]` | 가치 `v[i]` |
| :-------: | :---------: | :---------: |
| 1 | 3 | 25 |
| 2 | 1 | 15 |
| 3 | 2 | 20 |
| 4 | 4 | 30 |

**테이블 준비:** `K[0.][0.]` 크기의 테이블을 만듭니다.

**1단계: 초기화**
*   `i=0`인 행(0번째 행)은 '아무 물건도 고려하지 않았을 때'를 의미하므로, 어떤 용량의 배낭이든 최대 가치는 0입니다. `K[0, j] = 0`.
*   `j=0`인 열(0번째 열)은 '배낭 용량이 0일 때'를 의미하므로, 어떤 물건도 담을 수 없어 최대 가치는 0입니다. `K[i, 0] = 0`.

| i\j | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
| :---: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| **1** | 0 | | | | | | | |
| **2** | 0 | | | | | | | |
| **3** | 0 | | | | | | | |
| **4** | 0 | | | | | | | |

---

**2단계: `i = 1` 행 계산 (물건 1: w=3, v=25 고려)**

이제 물건 1 하나만 가지고 배낭을 채워봅니다.

| j | `w[1]>j`? | 계산: `max( K[0,j], v[1]+K[0,j-w[1]] )` | K[1,j] |
| :-: | :-------: | :------------------------------------------- | :----: |
| 1 | 3 > 1 (Yes) | `K[0,1]` = 0 | 0 |
| 2 | 3 > 2 (Yes) | `K[0,2]` = 0 | 0 |
| 3 | 3 <= 3 (No) | `max(K[0,3], 25+K[0,0]) = max(0, 25)` | 25 |
| 4 | 3 <= 4 (No) | `max(K[0,4], 25+K[0,1]) = max(0, 25)` | 25 |
| 5 | 3 <= 5 (No) | `max(K[0,5], 25+K[0,2]) = max(0, 25)` | 25 |
| 6 | 3 <= 6 (No) | `max(K[0,6], 25+K[0,3]) = max(0, 25)` | 25 |
| 7 | 3 <= 7 (No) | `max(K[0,7], 25+K[0,4]) = max(0, 25)` | 25 |

**현재 테이블:**

| i\j | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
| :---: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| **1** | 0 | 0 | 0 | **25** | **25** | **25** | **25** | **25** |
| **2** | 0 | | | | | | | |
| **3** | 0 | | | | | | | |
| **4** | 0 | | | | | | | |

---

**3단계: `i = 2` 행 계산 (물건 1, 2 고려. 물건 2: w=1, v=15)**

이제 물건 1과 2를 가지고 배낭을 채웁니다. 계산은 항상 바로 윗 행의 값을 참조합니다.

| j | `w[2]>j`? | 계산: `max( K[1,j], v[2]+K[1,j-w[2]] )` | K[2,j] |
| :-: | :-------: | :------------------------------------------- | :----: |
| 1 | 1 <= 1 (No) | `max(K[1,1], 15+K[1,0]) = max(0, 15+0)` | 15 |
| 2 | 1 <= 2 (No) | `max(K[1,2], 15+K[1,1]) = max(0, 15+0)` | 15 |
| 3 | 1 <= 3 (No) | `max(K[1,3], 15+K[1,2]) = max(25, 15+0)` | 25 |
| 4 | 1 <= 4 (No) | `max(K[1,4], 15+K[1,3]) = max(25, 15+25)` | 40 |
| 5 | 1 <= 5 (No) | `max(K[1,5], 15+K[1,4]) = max(25, 15+25)` | 40 |
| 6 | 1 <= 6 (No) | `max(K[1,6], 15+K[1,5]) = max(25, 15+25)` | 40 |
| 7 | 1 <= 7 (No) | `max(K[1,7], 15+K[1,6]) = max(25, 15+25)` | 40 |

**현재 테이블:**

| i\j | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
| :---: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| **1** | 0 | 0 | 0 | 25 | 25 | 25 | 25 | 25 |
| **2** | 0 | **15** | **15** | **25** | **40** | **40** | **40** | **40** |
| **3** | 0 | | | | | | | |
| **4** | 0 | | | | | | | |

---

**4단계: `i = 3` 행 계산 (물건 1, 2, 3 고려. 물건 3: w=2, v=20)**

| j | `w[3]>j`? | 계산: `max( K[2,j], v[3]+K[2,j-w[3]] )` | K[3,j] |
| :-: | :-------: | :------------------------------------------- | :----: |
| 1 | 2 > 1 (Yes) | `K[2,1]` = 15 | 15 |
| 2 | 2 <= 2 (No) | `max(K[2,2], 20+K[2,0]) = max(15, 20+0)` | 20 |
| 3 | 2 <= 3 (No) | `max(K[2,3], 20+K[2,1]) = max(25, 20+15)` | 35 |
| 4 | 2 <= 4 (No) | `max(K[2,4], 20+K[2,2]) = max(40, 20+15)` | 40 |
| 5 | 2 <= 5 (No) | `max(K[2,5], 20+K[2,3]) = max(40, 20+25)` | 45 |
| 6 | 2 <= 6 (No) | `max(K[2,6], 20+K[2,4]) = max(40, 20+40)` | 60 |
| 7 | 2 <= 7 (No) | `max(K[2,7], 20+K[2,5]) = max(40, 20+40)` | 60 |

**현재 테이블:**

| i\j | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
| :---: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| **1** | 0 | 0 | 0 | 25 | 25 | 25 | 25 | 25 |
| **2** | 0 | 15 | 15 | 25 | 40 | 40 | 40 | 40 |
| **3** | 0 | 15 | **20** | **35** | 40 | **45** | **60** | **60** |
| **4** | 0 | | | | | | | |

---

**5단계: `i = 4` 행 계산 (모든 물건 고려. 물건 4: w=4, v=30)**

| j | `w[4]>j`? | 계산: `max( K[3,j], v[4]+K[3,j-w[4]] )` | K[4,j] |
| :-: | :-------: | :------------------------------------------- | :----: |
| 1 | 4 > 1 (Yes) | `K[3,1]` = 15 | 15 |
| 2 | 4 > 2 (Yes) | `K[3,2]` = 20 | 20 |
| 3 | 4 > 3 (Yes) | `K[3,3]` = 35 | 35 |
| 4 | 4 <= 4 (No) | `max(K[3,4], 30+K[3,0]) = max(40, 30+0)` | 40 |
| 5 | 4 <= 5 (No) | `max(K[3,5], 30+K[3,1]) = max(45, 30+15)` | 45 |
| 6 | 4 <= 6 (No) | `max(K[3,6], 30+K[3,2]) = max(60, 30+20)` | 60 |
| 7 | 4 <= 7 (No) | `max(K[3,7], 30+K[3,3]) = max(60, 30+35)` | 65 |

**최종 테이블:**

| i\j | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
| :---: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0 (w)** | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| **1 (3)** | 0 | 0 | 0 | 25 | 25 | 25 | 25 | 25 |
| **2 (1)** | 0 | 15 | 15 | 25 | 40 | 40 | 40 | 40 |
| **3 (2)** | 0 | 15 | 20 | 35 | 40 | 45 | 60 | 60 |
| **4 (4)** | 0 | 15 | 20 | 35 | 40 | 45 | 60 | **65** |

드디어 테이블이 완성되었습니다! 우리가 찾던 최종 답, `K[4, 7]`은 **65**입니다. 탐험가는 최대 65의 가치를 배낭에 담아 유적을 탈출할 수 있습니다.

##### 해답을 넘어: 어떤 물건을 담았는가? (역추적)

최대 가치가 65라는 것은 알았지만, 탐험가에게 정말 필요한 정보는 '그래서 어떤 보물을 챙겨야 하는가?' 입니다. 이 정보 또한 우리가 만든 테이블 안에 숨겨져 있습니다. 테이블을 거꾸로 거슬러 올라가며 우리가 내렸던 선택을 복기해 봅시다. 이 과정을 **역추적(Backtracking)**이라고 합니다.

1.  **`K[4, 7] = 65`**에서 시작합니다. 이 값은 바로 윗칸 `K[3, 7] = 60`과 같습니까?
    *   다릅니다. `65 ≠ 60`. 이는 `K[4, 7]`을 계산할 때 **물건 4를 포함하는 선택**이 최적이었다는 의미입니다.
    *   **선택: 물건 4 (w=4, v=30)를 배낭에 넣는다!**
    *   이제 우리는 물건 4를 넣었으므로, 남은 용량 `7 - w[4] = 7 - 4 = 3`에 대해, 남은 물건 `1, 2, 3`으로 채우는 문제로 이동해야 합니다. 즉, `K[3, 3]`으로 이동합니다.

2.  현재 위치는 **`K[3, 3] = 35`** 입니다. 이 값은 바로 윗칸 `K[2, 3] = 25`와 같습니까?
    *   다릅니다. `35 ≠ 25`. 이는 `K[3, 3]`을 계산할 때 **물건 3을 포함하는 선택**이 최적이었다는 의미입니다.
    *   **선택: 물건 3 (w=2, v=20)를 배낭에 넣는다!**
    *   남은 용량 `3 - w[3] = 3 - 2 = 1`에 대해, 남은 물건 `1, 2`로 채우는 문제로 이동합니다. 즉, `K[2, 1]`로 이동합니다.

3.  현재 위치는 **`K[2, 1] = 15`** 입니다. 이 값은 바로 윗칸 `K[1, 1] = 0`과 같습니까?
    *   다릅니다. `15 ≠ 0`. 이는 `K[2, 1]`을 계산할 때 **물건 2를 포함하는 선택**이 최적이었다는 의미입니다.
    *   **선택: 물건 2 (w=1, v=15)를 배낭에 넣는다!**
    *   남은 용량 `1 - w[2] = 1 - 1 = 0`에 대해, 남은 물건 `1`로 채우는 문제로 이동합니다. 즉, `K[1, 0]`으로 이동합니다.

4.  현재 위치는 **`K[1, 0] = 0`** 입니다. 배낭 용량이 0이 되었으므로 역추적을 종료합니다.

역추적 결과, 탐험가가 챙겨야 할 보물은 **물건 2, 3, 4**입니다. 확인해 볼까요?
*   총무게: `w[2] + w[3] + w[4] = 1 + 2 + 4 = 7` (배낭 용량 7에 딱 맞습니다!)
*   총가치: `v[2] + v[3] + v[4] = 15 + 20 + 30 = 65` (우리가 구한 최대 가치와 일치합니다!)

##### 알고리즘 분석 및 결론

*   **시간 복잡도:** 알고리즘은 `i`를 1부터 `n`까지, `j`를 1부터 `C`까지 순회하는 2중 반복문으로 구성됩니다. 각 칸을 채우는 데 걸리는 시간은 상수 시간(`O(1)`)입니다. 따라서 총 시간 복잡도는 **`Θ(nC)`** 입니다.

*   **의사 다항 시간 (Pseudo-polynomial time):** 이 시간 복잡도는 한 가지 특이한 점이 있습니다. 문제의 입력 크기는 보통 물건의 개수 `n`으로 생각하지만, 복잡도가 배낭의 용량 `C`라는 숫자 값에 직접적으로 의존합니다. 만약 `C`가 `n`에 비해 터무니없이 큰 값이라면(예를 들어, `C`가 `2ⁿ`이라면) 이 알고리즘은 다시 지수 시간처럼 동작할 수 있습니다. 이처럼 입력으로 주어진 '숫자의 크기'에 따라 다항 시간이 되는 경우를 **의사 다항 시간**이라고 부릅니다. 하지만 대부분의 현실적인 문제에서 `C`는 적절한 크기를 가지므로 매우 효율적인 알고리즘입니다.

*   **공간 복잡도:** `(n+1) × (C+1)` 크기의 테이블을 사용하므로 공간 복잡도는 **`Θ(nC)`** 입니다.

배낭 채우기 문제는 동적 계획법의 철학을 완벽하게 보여줍니다. 복잡하고 거대한 문제를 '고려할 물건의 개수'와 '배낭의 용량'이라는 두 가지 차원으로 잘게 나누어 가장 작은 문제부터 체계적으로 해결해 나갑니다. 각 단계의 해를 테이블에 꼼꼼히 기록함으로써, 한 번 푼 문제는 다시 풀지 않고 그 결과를 즉시 활용하여 폭발적인 경우의 수를 효율적으로 제어합니다.

탐험가가 모든 조합을 머릿속으로 떠올리며 혼란에 빠지는 대신, 이처럼 체계적인 표를 그려 나갔다면, 그는 틀림없이 최적의 보물 조합을 찾아 유적을 무사히 빠져나왔을 것입니다. 이것이 바로 복잡한 최적화 문제를 해결하는 동적 계획법의 힘입니다.

### 제 7장: 탐욕 기법 (Greedy)
