#!/usr/bin/env python3
"""
Search Index Generator for Hugo Site (using hugo.toml)

Generates only search_index.json from ../content markdown files,
respecting ignoreFiles defined in hugo.toml.
"""

import os
import json
import re
from pathlib import Path
from datetime import datetime
import argparse
import fnmatch
import subprocess


# ==============================
# ğŸ”§ ì „ì—­ ì„¤ì • (Global Config)
# ==============================

# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent.resolve()
PROJECT_ROOT = SCRIPT_DIR.parent.resolve()
HUGO_CONFIG_PATH = PROJECT_ROOT / "hugo.toml"
CONTENT_DIR = PROJECT_ROOT / "content"  # ../content
DEFAULT_OUTPUT_DIR = PROJECT_ROOT / "static" / "indexing"

# âœ… í¬í•¨í•  í™•ì¥ì (ì´ í™•ì¥ìë§Œ ì¸ë±ì‹± ëŒ€ìƒ)
INCLUDE_EXTENSIONS = {'.md'}

# ë””ë ‰í„°ë¦¬ ì œì™¸ ê¸°ë³¸ê°’ (hugo.toml ì •ì˜ëœ ignoreFiles ì™¸ ì¶”ê°€ ì œì™¸ í•­ëª©ì´ í•„ìš”í•˜ë©´ ì—¬ê¸°ì— ì¶”ê°€)
ADDITIONAL_EXCLUDE_PATTERNS = ['content/08.media']


# ==============================
# ğŸ“„ í—¬í¼ í•¨ìˆ˜
# ==============================

def parse_hugo_toml_ignore_files(toml_path):
    """hugo.tomlì—ì„œ ignoreFiles ë°°ì—´ì„ íŒŒì‹±í•˜ì—¬ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    ignore_patterns = []
    if not toml_path.is_file():
        print(f"âš ï¸  Warning: {toml_path} not found. No ignore rules loaded.")
        return ignore_patterns

    try:
        with open(toml_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # TOML íŒŒì‹±: ignoreFiles = [ ... ]
        match = re.search(r'^\s*ignoreFiles\s*=\s*\[(.*?)\]', content, re.MULTILINE | re.DOTALL)
        print(f"â„¹ï¸  Parsing ignoreFiles from {toml_path}")
        if match:
            list_content = match.group(1)
            entries = re.findall(r'["\']([^"\']+)["\']', list_content)
            ignore_patterns = [entry.strip() for entry in entries if entry.strip()]
        # else:
            print("â„¹ï¸  No ignoreFiles found in hugo.toml")

    except Exception as e:
        print(f"âŒ Error parsing hugo.toml: {e}")

    return ignore_patterns


def generate_pretty_url(relative_path_str):
    """íŒŒì¼ ê²½ë¡œë¥¼ Hugoì˜ Pretty URL í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    p = Path(relative_path_str)
    slug = p.stem.lower()
    slug = re.sub(r'\s+', '-', slug)
    slug = re.sub(r'[^\w\-@]', '', slug, flags=re.UNICODE)
    slug = re.sub(r'-+', '-', slug)
    slug = slug.strip('-')
    url_path = p.parent / slug
    return f"/{url_path.as_posix().strip('./')}/"


def extract_frontmatter(content):
    match = re.match(r'^---\s*\n([\s\S]*?)\n---\s*\n?', content, re.DOTALL)
    if not match:
        return {}

    frontmatter_text = match.group(1)
    frontmatter = {}
    lines = frontmatter_text.strip().split('\n')
    current_key = None
    current_list = []

    for line in lines:
        if line.strip().startswith('- '):
            if current_key:
                current_list.append(line.strip()[2:])
            continue

        if ':' in line:
            if current_key and current_list:
                frontmatter[current_key] = current_list
                current_list = []

            key, value = line.split(':', 1)
            key = key.strip()
            value = value.strip()

            if value:
                frontmatter[key] = value.strip("'\"")
                current_key = None
                current_list = []
            else:
                current_key = key

    if current_key and current_list:
        frontmatter[current_key] = current_list

    return frontmatter


def clean_markdown_for_search(content):
    content = re.sub(r'^---\s*\n[\s\S]*?---\s*\n?', '', content, flags=re.DOTALL)
    content = re.sub(r'```[\s\S]*?```', ' ', content)
    content = re.sub(r'`[^`]+`', ' ', content)
    content = re.sub(r'!\[.*?\]\(.*?\)', ' ', content)
    content = re.sub(r'\[([^\]]+)\]\(.*?\)', r'\1', content)
    content = re.sub(r'^[#>*+-]+\s*', '', content, flags=re.MULTILINE)
    content = re.sub(r'^---\s*$', '', content, flags=re.MULTILINE)
    content = re.sub(r'(\*\*|\*|__|_|~~)', '', content)
    content = re.sub(r'\s+', ' ', content)
    return content.strip()


def should_exclude(path_obj, root_path, exclude_patterns):
    """ì£¼ì–´ì§„ ê²½ë¡œê°€ ì œì™¸ ëŒ€ìƒì¸ì§€ í™•ì¸ (fnmatch ê¸°ë°˜, í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)"""
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
    rel_path_from_project = str(path_obj.relative_to(PROJECT_ROOT))
    for pattern in exclude_patterns:
        # íŒ¨í„´ì´ '/'ë¡œ ëë‚˜ë©´ ë””ë ‰í„°ë¦¬ íŒ¨í„´ì´ë¯€ë¡œ í•˜ìœ„ ëª¨ë“  íŒŒì¼ë„ ë§¤ì¹­ë˜ë„ë¡ ì²˜ë¦¬
        if pattern.endswith('/'):
            dir_pattern = pattern.rstrip('/')
            if rel_path_from_project.startswith(dir_pattern + '/') or rel_path_from_project == dir_pattern:
                return True
        # ì¼ë°˜ íŒ¨í„´ ë§¤ì¹­
        if fnmatch.fnmatch(rel_path_from_project, pattern) or fnmatch.fnmatch('/' + rel_path_from_project, pattern):
            return True
    return False


# ==============================
# ğŸ§  ë©”ì¸ ë¡œì§
# ==============================

def build_search_index(output_dir, additional_excludes=None):
    root_path = CONTENT_DIR.resolve()
    output_path = Path(output_dir).resolve()
    output_path.mkdir(parents=True, exist_ok=True)

    hugo_ignore_patterns = parse_hugo_toml_ignore_files(HUGO_CONFIG_PATH)
    exclude_patterns = set(hugo_ignore_patterns + ADDITIONAL_EXCLUDE_PATTERNS)
    if additional_excludes:
        exclude_patterns.update(additional_excludes)

    print(f"ğŸ“ Scanning content in: {root_path}")
    print(f"ğŸš« Ignoring patterns: {sorted(exclude_patterns)}")
    print(f"âœ… Only including files with extensions: {sorted(INCLUDE_EXTENSIONS)}")

    all_file_data = []
    for dirpath, dirnames, filenames in os.walk(root_path, topdown=True):
        dirnames[:] = [d for d in dirnames if not should_exclude(Path(dirpath) / d, root_path, exclude_patterns) and not d.startswith('.')]
        for filename in sorted(filenames):
            if filename.startswith('.') or not any(filename.lower().endswith(ext) for ext in INCLUDE_EXTENSIONS):
                continue

            file_path = Path(dirpath) / filename
            if should_exclude(file_path, root_path, exclude_patterns):
                continue

            relative_path_str = str(file_path.relative_to(root_path))
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                if not content.strip(): continue

                frontmatter = extract_frontmatter(content)
                cleaned_content = clean_markdown_for_search(content)
                filename_stem = file_path.stem
                pretty_url = generate_pretty_url(relative_path_str)

                all_file_data.append({
                    "key": filename_stem, "path": pretty_url, "filename": filename_stem,
                    "frontmatter": frontmatter, "content": cleaned_content,
                })
            except Exception as e:
                print(f"âŒ Error processing {file_path}: {e}")

    files = {data['key']: {"path": data['path'], "filename": data['filename'], "content": data['content'], "frontmatter": data['frontmatter']} for data in all_file_data}
    search_index = {"generated_at": datetime.now().isoformat(), "files": files}

    # âœ¨ ë³€ê²½ì : ê¸°ì¡´ íŒŒì¼ê³¼ ë¹„êµí•˜ì—¬ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¡œì§ ì‹œì‘
    output_file = output_path / "search_index.json"
    is_changed = True # ê¸°ë³¸ì ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆë‹¤ê³  ê°€ì •

    if output_file.is_file():
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                existing_index = json.load(f)
            # 'files' ê°ì²´ë§Œ ë¹„êµí•˜ì—¬ ì‹¤ì œ ì½˜í…ì¸  ë³€ê²½ ì—¬ë¶€ í™•ì¸
            if existing_index.get('files') == search_index['files']:
                is_changed = False
        except (json.JSONDecodeError, IOError) as e:
            print(f"âš ï¸ Warning: Could not read or parse existing {output_file}. Regenerating. Error: {e}")
            is_changed = True # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ, ë³€ê²½ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼

    if is_changed:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(search_index, f, ensure_ascii=False, indent=2)
        print(f"âœ… Content changed. Saved search_index.json ({len(files)} files)")

        version_string = datetime.now().strftime("%Y%m%d%H%M%S")
        with open(output_path / "version.json", 'w', encoding='utf-8') as f:
            json.dump({"version": version_string}, f, ensure_ascii=False, indent=2)
        print(f"âœ… Updated version.json (version: {version_string})")
    else:
        print(f"âœ… No content changes detected. Skipping file updates.")


# ==============================
# ğŸš€ ì‹¤í–‰
# ==============================

def main():
    parser = argparse.ArgumentParser(description="Generate search_index.json using hugo.toml ignore rules")
    parser.add_argument('--output-dir', '-o', default=str(DEFAULT_OUTPUT_DIR),
                        help=f'Output directory (default: {DEFAULT_OUTPUT_DIR})')
    parser.add_argument('--exclude', '-e', nargs='*',
                        help='Additional glob patterns to exclude (e.g., "drafts/*")')
    args = parser.parse_args()

    build_search_index(args.output_dir, args.exclude)


if __name__ == "__main__":
    main()