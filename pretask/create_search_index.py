#!/usr/bin/env python3
"""
Search Index Generator for Hugo Site (using hugo.toml)

Generates only search_index.json from ../content markdown files,
respecting ignoreFiles defined in hugo.toml.
"""

import os
import json
import re
from pathlib import Path
from datetime import datetime
import argparse
import fnmatch
import subprocess


# ==============================
# ğŸ”§ ì „ì—­ ì„¤ì • (Global Config)
# ==============================

# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent.resolve()
PROJECT_ROOT = SCRIPT_DIR.parent.resolve()
HUGO_CONFIG_PATH = PROJECT_ROOT / "hugo.toml"
CONTENT_DIR = PROJECT_ROOT / "content"  # ../content
DEFAULT_OUTPUT_DIR = PROJECT_ROOT / "static" / "indexing"

# âœ… í¬í•¨í•  í™•ì¥ì (ì´ í™•ì¥ìë§Œ ì¸ë±ì‹± ëŒ€ìƒ)
INCLUDE_EXTENSIONS = {'.md'}

# ë””ë ‰í„°ë¦¬ ì œì™¸ ê¸°ë³¸ê°’ (hugo.toml ì •ì˜ëœ ignoreFiles ì™¸ ì¶”ê°€ ì œì™¸ í•­ëª©ì´ í•„ìš”í•˜ë©´ ì—¬ê¸°ì— ì¶”ê°€)
ADDITIONAL_EXCLUDE_PATTERNS = ['content/08.media']


# ==============================
# ğŸ“„ í—¬í¼ í•¨ìˆ˜
# ==============================

def parse_hugo_toml_ignore_files(toml_path):
    """hugo.tomlì—ì„œ ignoreFiles ë°°ì—´ì„ íŒŒì‹±í•˜ì—¬ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    ignore_patterns = []
    if not toml_path.is_file():
        print(f"âš ï¸  Warning: {toml_path} not found. No ignore rules loaded.")
        return ignore_patterns

    try:
        with open(toml_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # TOML íŒŒì‹±: ignoreFiles = [ ... ]
        match = re.search(r'^\s*ignoreFiles\s*=\s*\[(.*?)\]', content, re.MULTILINE | re.DOTALL)
        print(f"â„¹ï¸  Parsing ignoreFiles from {toml_path}")
        if match:
            list_content = match.group(1)
            entries = re.findall(r'["\']([^"\']+)["\']', list_content)
            ignore_patterns = [entry.strip() for entry in entries if entry.strip()]
        # else:
            print("â„¹ï¸  No ignoreFiles found in hugo.toml")

    except Exception as e:
        print(f"âŒ Error parsing hugo.toml: {e}")

    return ignore_patterns


def generate_pretty_url(relative_path_str):
    """íŒŒì¼ ê²½ë¡œë¥¼ Hugoì˜ Pretty URL í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê³µë°± í¬í•¨ ê²½ë¡œë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)"""
    p = Path(relative_path_str)
    
    # ëª¨ë“  ê²½ë¡œ êµ¬ì„± ìš”ì†Œ(ë””ë ‰í„°ë¦¬ + íŒŒì¼ëª…)ë¥¼ ì •ì œ
    def clean_part(part):
        part = part.lower()
        part = re.sub(r'\s+', '-', part)               # ê³µë°± â†’ í•˜ì´í”ˆ
        part = re.sub(r'[^\w\-@.+]', '', part, flags=re.UNICODE)  # í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ì ì œê±° (+ ì¶”ê°€)
        part = re.sub(r'-+', '-', part)                # ì—°ì† í•˜ì´í”ˆ â†’ í•˜ë‚˜ë¡œ
        return part.strip('-')
    
    cleaned_parts = [clean_part(part) for part in p.parts if part]
    if not cleaned_parts:
        return "/"
    
    # ë§ˆì§€ë§‰ì€ í™•ì¥ì ì—†ëŠ” íŒŒì¼ëª…(stem)ì´ì–´ì•¼ í•¨
    cleaned_parts[-1] = clean_part(p.stem)
    
    url_path = "/".join(cleaned_parts).strip('/')
    return f"/{url_path}/" if url_path else "/"


def extract_frontmatter(content):
    match = re.match(r'^---\s*\n([\s\S]*?)\n---\s*\n?', content, re.DOTALL)
    if not match:
        return {}

    frontmatter_text = match.group(1)
    frontmatter = {}
    lines = frontmatter_text.strip().split('\n')
    current_key = None
    current_list = []

    for line in lines:
        if line.strip().startswith('- '):
            if current_key:
                current_list.append(line.strip()[2:])
            continue

        if ':' in line:
            if current_key and current_list:
                frontmatter[current_key] = current_list
                current_list = []

            key, value = line.split(':', 1)
            key = key.strip()
            value = value.strip()

            if value:
                frontmatter[key] = value.strip("'\"")
                current_key = None
                current_list = []
            else:
                current_key = key

    if current_key and current_list:
        frontmatter[current_key] = current_list

    return frontmatter

def clean_markdown_for_search(content: str) -> str:
    """
    ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ ì¸ë±ì‹±ì— ìµœì í™”ëœ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ì •ì œí•©ë‹ˆë‹¤.
    'ë³´í˜¸-ì²˜ë¦¬-ë³µì›' ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ì½”ë“œ(`...` ë° ```...```, ~~~...~~~)ë¥¼ ì•ˆì „í•˜ê²Œ ê²©ë¦¬í•˜ê³ ,
    ë‚˜ë¨¸ì§€ ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ë§Œ ì œê±°í•˜ì—¬ ì˜ë¯¸ ìˆëŠ” ì½˜í…ì¸ ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    """
    if not content:
        return ""

    protected_fragments = []

    def protect_fragment(match):
        fragment = match.group(0)
        # Use unprintable control characters for robust placeholders
        placeholder = f"\x02PROTECTED{len(protected_fragments)}\x03"
        protected_fragments.append(fragment)
        return placeholder

    # 1ë‹¨ê³„: ì½”ë“œ ë¸”ë¡ ë° ì¸ë¼ì¸ ì½”ë“œ ë³´í˜¸
    content = re.sub(r'^\s*(?:> ?)*```[\s\S]+?^\s*(?:> ?)*```\s*$', protect_fragment, content, flags=re.MULTILINE)
    content = re.sub(r'^\s*(?:> ?)*~~~[\s\S]+?^\s*(?:> ?)*~~~\s*$', protect_fragment, content, flags=re.MULTILINE)
    content = re.sub(r'`[^`]+?`', protect_fragment, content)

    # 2ë‹¨ê³„: Frontmatter ì œê±°
    content = re.sub(r'\A---[\s\S]+?^---\s*', '', content, flags=re.MULTILINE)

    # 3ë‹¨ê³„: HTML íƒœê·¸ ì œê±°
    content = re.sub(r'<[^>]+>', '', content)

    # 4ë‹¨ê³„: í…Œì´ë¸” ê´€ë ¨ ë¬¸ë²• ì œê±°
    content = re.sub(r'^\s*\|?[-|: \t]+-[-|: \t]*\|?\s*$', '', content, flags=re.MULTILINE)
    content = content.replace('|', ' ')

    # 5ë‹¨ê³„: ì´ë¯¸ì§€ ë° ë§í¬ ì œê±°
    content = re.sub(r'!\[([^\]]*)\]\([^\)]*\)', r'\1', content) # ì´ë¯¸ì§€
    content = re.sub(r'\[([^\]]+)\]\([^\)]*\)', r'\1', content) # ë§í¬

    # 6ë‹¨ê³„: í—¤ë”, ë¦¬ìŠ¤íŠ¸, ì¸ìš©ë¬¸ ë“± ë¼ì¸ ì‹œì‘ ë¬¸ë²•ì„ ë°˜ë³µì ìœ¼ë¡œ ì œê±°
    lines = content.split('\n')
    cleaned_lines = []
    for line in lines:
        while True:
            new_line = re.sub(r'^\s*([#>*+-]|\d+\.)\s*', '', line)
            if new_line == line:
                break
            line = new_line
        cleaned_lines.append(line)
    content = '\n'.join(cleaned_lines)

    # 7ë‹¨ê³„: ë‚˜ë¨¸ì§€ ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì œê±°
    content = re.sub(r'(\*\*|__)(.*?)\1', r'\2', content) # Bold
    content = re.sub(r'(\*|_)(.*?)\1', r'\2', content) # Italic
    content = re.sub(r'~~(.*?)~~', r'\1', content) # Strikethrough
    content = re.sub(r'^\[\^([^\]]+)\]:.*', '', content) # Footnote definition
    content = re.sub(r'\[\^([^\]]+)\]', '', content) # Footnote reference
    content = re.sub(r'^\s*[-*_]{3,}\s*$', '', content, flags=re.MULTILINE) # Horizontal rules

    # 8ë‹¨ê³„: ë³´í˜¸ëœ ì½”ë“œ ì¡°ê° ë³µì› ë° ì •ì œ
    for i, fragment in enumerate(protected_fragments):
        placeholder = f"\x02PROTECTED{i}\x03"
        cleaned_fragment = fragment

        if '\n' in fragment: # ë¸”ë¡ ì½”ë“œë¡œ ê°„ì£¼
            lines = fragment.strip().split('\n')
            lines = lines[1:-1] # íœìŠ¤ ì œê±°
            lines = [re.sub(r'^\s*(?:> ?)*', '', l) for l in lines] # '>' ì œê±°
            cleaned_fragment = '\n'.join(lines)
        else: # ì¸ë¼ì¸ ì½”ë“œë¡œ ê°„ì£¼
            cleaned_fragment = fragment.strip('`')
        
        content = content.replace(placeholder, " " + cleaned_fragment + " ")

    # 9ë‹¨ê³„: ìµœì¢… ì •ì œ
    content = re.sub(r'\s+', ' ', content)
    return content.strip()

def should_exclude(path_obj, root_path, exclude_patterns):
    """ì£¼ì–´ì§„ ê²½ë¡œê°€ ì œì™¸ ëŒ€ìƒì¸ì§€ í™•ì¸ (fnmatch ê¸°ë°˜, í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)"""
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
    rel_path_from_project = str(path_obj.relative_to(PROJECT_ROOT))
    for pattern in exclude_patterns:
        # íŒ¨í„´ì´ '/'ë¡œ ëë‚˜ë©´ ë””ë ‰í„°ë¦¬ íŒ¨í„´ì´ë¯€ë¡œ í•˜ìœ„ ëª¨ë“  íŒŒì¼ë„ ë§¤ì¹­ë˜ë„ë¡ ì²˜ë¦¬
        if pattern.endswith('/'):
            dir_pattern = pattern.rstrip('/')
            if rel_path_from_project.startswith(dir_pattern + '/') or rel_path_from_project == dir_pattern:
                return True
        # ì¼ë°˜ íŒ¨í„´ ë§¤ì¹­
        if fnmatch.fnmatch(rel_path_from_project, pattern) or fnmatch.fnmatch('/' + rel_path_from_project, pattern):
            return True
    return False


# ==============================
# ğŸ§  ë©”ì¸ ë¡œì§
# ==============================

def build_search_index(output_dir, additional_excludes=None):
    root_path = CONTENT_DIR.resolve()
    output_path = Path(output_dir).resolve()
    output_path.mkdir(parents=True, exist_ok=True)

    hugo_ignore_patterns = parse_hugo_toml_ignore_files(HUGO_CONFIG_PATH)
    exclude_patterns = set(hugo_ignore_patterns + ADDITIONAL_EXCLUDE_PATTERNS)
    if additional_excludes:
        exclude_patterns.update(additional_excludes)

    print(f"ğŸ“ Scanning content in: {root_path}")
    print(f"ğŸš« Ignoring patterns: {sorted(exclude_patterns)}")
    print(f"âœ… Only including files with extensions: {sorted(INCLUDE_EXTENSIONS)}")

    all_file_data = []
    for dirpath, dirnames, filenames in os.walk(root_path, topdown=True):
        dirnames[:] = [d for d in dirnames if not should_exclude(Path(dirpath) / d, root_path, exclude_patterns) and not d.startswith('.')]
        for filename in sorted(filenames):
            if filename.startswith('.') or not any(filename.lower().endswith(ext) for ext in INCLUDE_EXTENSIONS):
                continue

            file_path = Path(dirpath) / filename
            if should_exclude(file_path, root_path, exclude_patterns):
                continue

            relative_path_str = str(file_path.relative_to(root_path))
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                if not content.strip(): continue

                frontmatter = extract_frontmatter(content)
                cleaned_content = clean_markdown_for_search(content)
                filename_stem = file_path.stem
                pretty_url = generate_pretty_url(relative_path_str)

                all_file_data.append({
                    "key": filename_stem, "path": pretty_url, "filename": filename_stem,
                    "frontmatter": frontmatter, "content": cleaned_content,
                })
            except Exception as e:
                print(f"âŒ Error processing {file_path}: {e}")

    files = {data['key']: {"path": data['path'], "filename": data['filename'], "content": data['content'], "frontmatter": data['frontmatter']} for data in all_file_data}
    search_index = {"generated_at": datetime.now().isoformat(), "files": files}

    # âœ¨ ë³€ê²½ì : ê¸°ì¡´ íŒŒì¼ê³¼ ë¹„êµí•˜ì—¬ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¡œì§ ì‹œì‘
    output_file = output_path / "search_index.json"
    is_changed = True # ê¸°ë³¸ì ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆë‹¤ê³  ê°€ì •

    if output_file.is_file():
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                existing_index = json.load(f)
            # 'files' ê°ì²´ë§Œ ë¹„êµí•˜ì—¬ ì‹¤ì œ ì½˜í…ì¸  ë³€ê²½ ì—¬ë¶€ í™•ì¸
            if existing_index.get('files') == search_index['files']:
                is_changed = False
        except (json.JSONDecodeError, IOError) as e:
            print(f"âš ï¸ Warning: Could not read or parse existing {output_file}. Regenerating. Error: {e}")
            is_changed = True # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ, ë³€ê²½ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼

    if is_changed:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(search_index, f, ensure_ascii=False, indent=2)
        print(f"âœ… Content changed. Saved search_index.json ({len(files)} files)")

        version_string = datetime.now().strftime("%Y%m%d%H%M%S")
        with open(output_path / "version.json", 'w', encoding='utf-8') as f:
            json.dump({"version": version_string}, f, ensure_ascii=False, indent=2)
        print(f"âœ… Updated version.json (version: {version_string})")
    else:
        print(f"âœ… No content changes detected. Skipping file updates.")


# ==============================
# ğŸš€ ì‹¤í–‰
# ==============================

def main():
    parser = argparse.ArgumentParser(description="Generate search_index.json using hugo.toml ignore rules")
    parser.add_argument('--output-dir', '-o', default=str(DEFAULT_OUTPUT_DIR),
                        help=f'Output directory (default: {DEFAULT_OUTPUT_DIR})')
    parser.add_argument('--exclude', '-e', nargs='*',
                        help='Additional glob patterns to exclude (e.g., "drafts/*")')
    args = parser.parse_args()

    build_search_index(args.output_dir, args.exclude)


if __name__ == "__main__":
    main()