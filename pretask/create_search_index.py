#!/usr/bin/env python3
"""
Search Index Generator for Hugo Site (using hugo.toml)

Generates only search_index.json from ../content markdown files,
respecting ignoreFiles defined in hugo.toml.
"""

import os
import json
import re
from pathlib import Path
from datetime import datetime
import argparse
import fnmatch
import subprocess


# ==============================
# ğŸ”§ ì „ì—­ ì„¤ì • (Global Config)
# ==============================

# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent.resolve()
PROJECT_ROOT = SCRIPT_DIR.parent.resolve()
HUGO_CONFIG_PATH = PROJECT_ROOT / "hugo.toml"
CONTENT_DIR = PROJECT_ROOT / "content"  # ../content
DEFAULT_OUTPUT_DIR = PROJECT_ROOT / "static" / "indexing"

# âœ… í¬í•¨í•  í™•ì¥ì (ì´ í™•ì¥ìë§Œ ì¸ë±ì‹± ëŒ€ìƒ)
INCLUDE_EXTENSIONS = {'.md'}

# ë””ë ‰í„°ë¦¬ ì œì™¸ ê¸°ë³¸ê°’ (hugo.toml ì •ì˜ëœ ignoreFiles ì™¸ ì¶”ê°€ ì œì™¸ í•­ëª©ì´ í•„ìš”í•˜ë©´ ì—¬ê¸°ì— ì¶”ê°€)
ADDITIONAL_EXCLUDE_PATTERNS = ['content/08.media']


# ==============================
# ğŸ“„ í—¬í¼ í•¨ìˆ˜
# ==============================

def parse_hugo_toml_ignore_files(toml_path):
    """hugo.tomlì—ì„œ ignoreFiles ë°°ì—´ì„ íŒŒì‹±í•˜ì—¬ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    ignore_patterns = []
    if not toml_path.is_file():
        print(f"âš ï¸  Warning: {toml_path} not found. No ignore rules loaded.")
        return ignore_patterns

    try:
        with open(toml_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # TOML íŒŒì‹±: ignoreFiles = [ ... ]
        match = re.search(r'^\s*ignoreFiles\s*=\s*\[(.*?)\]', content, re.MULTILINE | re.DOTALL)
        print(f"â„¹ï¸  Parsing ignoreFiles from {toml_path}")
        if match:
            list_content = match.group(1)
            entries = re.findall(r'["\']([^"\']+)["\']', list_content)
            ignore_patterns = [entry.strip() for entry in entries if entry.strip()]
        # else:
            print("â„¹ï¸  No ignoreFiles found in hugo.toml")

    except Exception as e:
        print(f"âŒ Error parsing hugo.toml: {e}")

    return ignore_patterns


def generate_pretty_url(relative_path_str):
    """íŒŒì¼ ê²½ë¡œë¥¼ Hugoì˜ Pretty URL í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê³µë°± í¬í•¨ ê²½ë¡œë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)"""
    p = Path(relative_path_str)
    
    # ëª¨ë“  ê²½ë¡œ êµ¬ì„± ìš”ì†Œ(ë””ë ‰í„°ë¦¬ + íŒŒì¼ëª…)ë¥¼ ì •ì œ
    def clean_part(part):
        part = part.lower()
        part = re.sub(r'\s+', '-', part)               # ê³µë°± â†’ í•˜ì´í”ˆ
        part = re.sub(r'[^\w\-@.]', '', part, flags=re.UNICODE)  # í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ì ì œê±°
        part = re.sub(r'-+', '-', part)                # ì—°ì† í•˜ì´í”ˆ â†’ í•˜ë‚˜ë¡œ
        return part.strip('-')
    
    cleaned_parts = [clean_part(part) for part in p.parts if part]
    if not cleaned_parts:
        return "/"
    
    # ë§ˆì§€ë§‰ì€ í™•ì¥ì ì—†ëŠ” íŒŒì¼ëª…(stem)ì´ì–´ì•¼ í•¨
    cleaned_parts[-1] = clean_part(p.stem)
    
    url_path = "/".join(cleaned_parts).strip('/')
    return f"/{url_path}/" if url_path else "/"


def extract_frontmatter(content):
    match = re.match(r'^---\s*\n([\s\S]*?)\n---\s*\n?', content, re.DOTALL)
    if not match:
        return {}

    frontmatter_text = match.group(1)
    frontmatter = {}
    lines = frontmatter_text.strip().split('\n')
    current_key = None
    current_list = []

    for line in lines:
        if line.strip().startswith('- '):
            if current_key:
                current_list.append(line.strip()[2:])
            continue

        if ':' in line:
            if current_key and current_list:
                frontmatter[current_key] = current_list
                current_list = []

            key, value = line.split(':', 1)
            key = key.strip()
            value = value.strip()

            if value:
                frontmatter[key] = value.strip("'\"")
                current_key = None
                current_list = []
            else:
                current_key = key

    if current_key and current_list:
        frontmatter[current_key] = current_list

    return frontmatter


# def clean_markdown_for_search(content):
#     content = re.sub(r'^---\s*\n[\s\S]*?---\s*\n?', '', content, flags=re.DOTALL)
#     content = re.sub(r'```[\s\S]*?```', ' ', content)
#     content = re.sub(r'`[^`]+`', ' ', content)
#     content = re.sub(r'!\[.*?\]\(.*?\)', ' ', content)
#     content = re.sub(r'\[([^\]]+)\]\(.*?\)', r'\1', content)
#     content = re.sub(r'^[#>*+-]+\s*', '', content, flags=re.MULTILINE)
#     content = re.sub(r'^---\s*$', '', content, flags=re.MULTILINE)
#     content = re.sub(r'(\*\*|\*|__|_|~~)', '', content)
#     content = re.sub(r'\s+', ' ', content)
#     return content.strip()

# def clean_markdown_for_search(content):
#     """
#     ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ ì¸ë±ì‹±ì— ì í•©í•œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ì •ì œí•©ë‹ˆë‹¤.
#     - 1ë‹¨ê³„: Frontmatter ë“± ê²€ìƒ‰ì— ë¶ˆí•„ìš”í•œ ì„¹ì…˜ì„ ì™„ì „íˆ ì œê±°í•©ë‹ˆë‹¤.
#     - 2ë‹¨ê³„: ì½”ë“œ, ì´ë¯¸ì§€, ë§í¬ ë“±ì—ì„œ ë¬¸ë²•(syntax)ì€ ì œê±°í•˜ë˜, ë‚´ìš©(content)ì€ ë³´ì¡´í•©ë‹ˆë‹¤.
#     - 3ë‹¨ê³„: í—¤ë”, ëª©ë¡, ê°•ì¡° ë“± ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ ì„œì‹ì„ ì œê±°í•©ë‹ˆë‹¤.
#     - 4ë‹¨ê³„: ê³µë°±ì„ ì •ê·œí™”í•˜ì—¬ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.
#     """
#     # 1ë‹¨ê³„: ë²„ë¦´ ê²ƒë“¤ì„ ë¨¼ì € í™•ì‹¤í•˜ê²Œ ì œê±°í•˜ê¸° (Unambiguous Removal)
#     # Frontmatter ì œê±°
#     content = re.sub(r'^---\s*\n[\s\S]*?---\s*\n?', '', content, flags=re.DOTALL)

#     # 2ë‹¨ê³„: ë‚´ìš©(Content)ì€ ì‚´ë¦¬ë˜, ë¬¸ë²•(Syntax)ë§Œ ì œê±°í•˜ê¸°
#     # (TO-BE) ì½”ë“œ ë¸”ë¡: ë‚´ìš©ë¬¼ì€ ë‚¨ê¸°ê³ , ``` êµ¬ë¬¸ë§Œ ì œê±°
#     # ```python, ``` ë“± ì½”ë“œ ë¸”ë¡ì˜ ì‹œì‘ê³¼ ë ë¼ì¸ì„ ì œê±°í•©ë‹ˆë‹¤.
#     content = re.sub(r'^```[a-zA-Z0-9-]*\s*$', '', content, flags=re.MULTILINE)

#     # (TO-BE) ì¸ë¼ì¸ ì½”ë“œ: ë‚´ìš©ë¬¼ì€ ë‚¨ê¸°ê³ , ` êµ¬ë¬¸ë§Œ ì œê±° (ìº¡ì²˜ ê·¸ë£¹ \1 ì‚¬ìš©)
#     # `code` -> code
#     content = re.sub(r'`([^`]+)`', r'\1', content)

#     # (TO-BE) ì´ë¯¸ì§€: ëŒ€ì²´ í…ìŠ¤íŠ¸(alt text)ëŠ” ë‚¨ê¸°ê³ , ë‚˜ë¨¸ì§€ êµ¬ë¬¸ ì œê±° (ìº¡ì²˜ ê·¸ë£¹ \1 ì‚¬ìš©)
#     # ![alt text](url) -> alt text
#     content = re.sub(r'!\[([^\]]*)\]\(.*?\)', r'\1', content)

#     # (ìœ ì§€) ë§í¬: ë§í¬ í…ìŠ¤íŠ¸ëŠ” ë‚¨ê¸°ê³ , ë‚˜ë¨¸ì§€ êµ¬ë¬¸ ì œê±° (ìº¡ì²˜ ê·¸ë£¹ \1 ì‚¬ìš©)
#     # [link text](url) -> link text
#     content = re.sub(r'\[([^\]]+)\]\(.*?\)', r'\1', content)

#     # 3ë‹¨ê³„: ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì„œì‹ ì œê±°í•˜ê¸° (Formatting Cleanup)
#     # í—¤ë”, ëª©ë¡, ì¸ìš© ê¸°í˜¸ ì œê±°
#     content = re.sub(r'^[#>*+-]+\s*', '', content, flags=re.MULTILINE)

#     # ìˆ˜í‰ì„  ì œê±° (---, *** ë“±)
#     content = re.sub(r'^\s*[-*_]{3,}\s*$', '', content, flags=re.MULTILINE)

#     # ê°•ì¡°(ë³¼ë“œ, ì´íƒ¤ë¦­), ì·¨ì†Œì„  ë“± ì¸ë¼ì¸ ì„œì‹ ê¸°í˜¸ ì œê±°
#     content = re.sub(r'(\*\*|\*|__|_|~~)', '', content)

#     # 4ë‹¨ê³„: ìµœì¢… ì •ì œ (Final Polishing)
#     # ì—¬ëŸ¬ ê³µë°±/ê°œí–‰ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ í†µí•©í•˜ê³ , ì•ë’¤ ê³µë°± ì œê±°
#     content = re.sub(r'\s+', ' ', content)
#     return content.strip()

def clean_markdown_for_search(content: str) -> str:
    """
    ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ ì¸ë±ì‹±ì— ìµœì í™”ëœ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ì •ì œí•©ë‹ˆë‹¤.
    'ë³´í˜¸-ì²˜ë¦¬-ë³µì›' ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ì½”ë“œ(`...` ë° ```...```, ~~~...~~~)ë¥¼ ì•ˆì „í•˜ê²Œ ê²©ë¦¬í•˜ê³ ,
    ë‚˜ë¨¸ì§€ ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ë§Œ ì œê±°í•˜ì—¬ ì˜ë¯¸ ìˆëŠ” ì½˜í…ì¸ ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì— ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ” ë¬¸ìì—´ë¡œ ë³€ê²½í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.
    í…Œì´ë¸” êµ¬ë¶„ì„  ì œê±°ë¥¼ ì¶”ê°€í•˜ê³ , ì½”ë“œ ë¸”ë¡ íŒ¨í„´ì„ ë” ìœ ì—°í•˜ê²Œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
    """
    if not content:
        return ""

    protected_fragments = []

    def protect_fragment(match):
        fragment = match.group(0)
        placeholder = f"PROTECTEDFRAGMENT{len(protected_fragments)}END"
        protected_fragments.append(fragment)
        return placeholder

    # 1ë‹¨ê³„: ì½”ë“œ ë³´í˜¸ â€” ì¸ë¼ì¸ ì½”ë“œì™€ ì½”ë“œ ë¸”ë¡ì„ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ëŒ€ì²´
    # ë¨¼ì € ~~~ ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬
    content = re.sub(r'^\s*~~~[a-zA-Z0-9+-]*\s*\n[\s\S]*?^\s*~~~', protect_fragment, content, flags=re.MULTILINE)
    # ë‹¤ìŒ ``` ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬ (ë©€í‹°ë¼ì¸, ì–¸ì–´ ì‹ë³„ì í¬í•¨ ê°€ëŠ¥, ê³µë°± í—ˆìš©)
    content = re.sub(r'^\s*```[a-zA-Z0-9+-]*\s*\n[\s\S]*?^\s*```', protect_fragment, content, flags=re.MULTILINE)
    # ì¸ë¼ì¸ ì½”ë“œ: ë‹¨ìˆœ ì²˜ë¦¬ (`code`)
    content = re.sub(r'`[^`]+?`', protect_fragment, content)

    # 2ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì œê±° (ì½”ë“œëŠ” ì´ë¯¸ ë³´í˜¸ë¨)

    # Frontmatter (YAML) ì œê±° â€” ì •í™•íˆ ë¬¸ì„œ ì‹œì‘ì—ë§Œ í—ˆìš©
    content = re.sub(r'^---\s*\n(?:.*\n)*?---\s*\n?', '', content, count=1, flags=re.MULTILINE)

    # ì´ë¯¸ì§€: ![alt](url) â†’ alt (altê°€ ë¹„ì–´ ìˆìœ¼ë©´ ì œê±°)
    content = re.sub(r'!\[([^\]]*?)\]\([^)]*?\)', lambda m: m.group(1) if m.group(1).strip() else '', content)

    # ë§í¬: [text](url) â†’ text
    content = re.sub(r'\[([^\]]+?)\]\([^)]*?\)', r'\1', content)

    # í—¤ë”, ë¦¬ìŠ¤íŠ¸, ì¸ìš©ë¬¸, íƒœìŠ¤í¬ ë¦¬ìŠ¤íŠ¸ ë“± ë¼ì¸ ì‹œì‘ ë¬¸ë²• ì œê±°
    content = re.sub(r'^\s*([#>*+-]|\d+\.)\s+', '', content, flags=re.MULTILINE)

    # í…Œì´ë¸”: íŒŒì´í”„(|)ë¥¼ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´ â†’ ì…€ ë‚´ìš©ì€ ìœ ì§€
    content = content.replace('|', ' ')

    # í…Œì´ë¸” êµ¬ë¶„ì„  ì œê±° (--- ë˜ëŠ” :--: ë“±)
    content = re.sub(r'^\s*[-:=]+(?:\s*[-:=]+)*\s*$', '', content, flags=re.MULTILINE)

    # ë³¼ë“œ/ì´íƒ¤ë¦­/ì·¨ì†Œì„  â€” ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ ì†ìƒ ë°©ì§€
    # __bold__ ë˜ëŠ” **bold**
    content = re.sub(r'(\*\*|__)(.+?)\1', r'\2', content)
    # _italic_ â€” ë‹¨ì–´ ê²½ê³„ í™•ì¸ (my_varëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
    content = re.sub(r'\b_([^_]+?)_\b', r'\1', content)
    # *italic* â€” ë‹¨ì–´ ê²½ê³„ ì—†ì´ë„ ì•ˆì „í•˜ê²Œ
    content = re.sub(r'\*([^*]+?)\*', r'\1', content)
    # ~~strikethrough~~
    content = re.sub(r'~~([^~]+?)~~', r'\1', content)

    # ìˆ˜í‰ì„  ì œê±°
    content = re.sub(r'^\s*[-*_]{3,}\s*$', '', content, flags=re.MULTILINE)

    # HTML íƒœê·¸ ì œê±°
    content = re.sub(r'<[^>]+>', '', content)

    # 3ë‹¨ê³„: ë³´í˜¸ëœ ì½”ë“œ ì¡°ê° ë³µì› â€” ì½”ë“œ ë¬¸ë²• ì œê±°, ë‚´ìš©ë§Œ ì¶”ì¶œ
    for i, fragment in enumerate(protected_fragments):
        cleaned = fragment
        # ì½”ë“œ ë¸”ë¡: ```lang\n...\n``` ë˜ëŠ” ~~~lang\n...\n~~~ â†’ ...
        if fragment.lstrip().startswith(('```', '~~~')):
            # ê³µë°±ê³¼ íœìŠ¤ ì œê±°
            lines = fragment.split('\n')
            if len(lines) >= 2:
                # ì²« ì¤„: ```lang ë˜ëŠ” ~~~lang â†’ ì œê±°
                # ë§ˆì§€ë§‰ ì¤„: ``` ë˜ëŠ” ~~~ â†’ ì œê±°
                inner = '\n'.join(lines[1:-1]).strip()
                cleaned = inner
            else:
                cleaned = ''
        elif fragment.startswith('`') and fragment.endswith('`'):
            # ì¸ë¼ì¸ ì½”ë“œ: `code` â†’ code
            cleaned = fragment[1:-1]
        # í”Œë ˆì´ìŠ¤í™€ë” êµì²´
        placeholder = f"PROTECTEDFRAGMENT{i}END"
        content = content.replace(placeholder, " " + cleaned + " ")

    # 4ë‹¨ê³„: ìµœì¢… ì •ì œ
    # ì—°ì†ëœ ê³µë°±/íƒ­/ê°œí–‰ â†’ ë‹¨ì¼ ê³µë°±
    content = re.sub(r'\s+', ' ', content)
    return content.strip()

    # 3ë‹¨ê³„: ë³´í˜¸ëœ ì½”ë“œ ì¡°ê° ë³µì› â€” ì½”ë“œ ë¬¸ë²• ì œê±°, ë‚´ìš©ë§Œ ì¶”ì¶œ
    for i, fragment in enumerate(protected_fragments):
        cleaned = fragment
        # ì½”ë“œ ë¸”ë¡: ```lang\n...\n``` â†’ ...
        if fragment.startswith('```'):
            # ì–¸ì–´ ì‹ë³„ìì™€ ë°±í‹± ì œê±°
            lines = fragment.split('\n')
            if len(lines) >= 2:
                # ì²« ì¤„: ```lang â†’ ì œê±°
                # ë§ˆì§€ë§‰ ì¤„: ``` â†’ ì œê±°
                inner = '\n'.join(lines[1:-1])
                cleaned = inner
            else:
                cleaned = ''  # ì˜ëª»ëœ ì½”ë“œ ë¸”ë¡
        elif fragment.startswith('`') and fragment.endswith('`'):
            # ì¸ë¼ì¸ ì½”ë“œ: `code` â†’ code
            cleaned = fragment[1:-1]
        # ì½”ë“œ ë‚´ë¶€ì˜ ë¶ˆí•„ìš”í•œ ê³µë°±ì€ ìœ ì§€ (ê²€ìƒ‰ ì‹œ ì˜ë¯¸ ìˆì„ ìˆ˜ ìˆìŒ)
        placeholder = f"__PROTECTED_FRAGMENT_{i}__"
        content = content.replace(placeholder, " " + cleaned + " ")

    # 4ë‹¨ê³„: ìµœì¢… ì •ì œ
    # ì—°ì†ëœ ê³µë°±/íƒ­/ê°œí–‰ â†’ ë‹¨ì¼ ê³µë°±
    content = re.sub(r'\s+', ' ', content)
    return content.strip()

def should_exclude(path_obj, root_path, exclude_patterns):
    """ì£¼ì–´ì§„ ê²½ë¡œê°€ ì œì™¸ ëŒ€ìƒì¸ì§€ í™•ì¸ (fnmatch ê¸°ë°˜, í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)"""
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
    rel_path_from_project = str(path_obj.relative_to(PROJECT_ROOT))
    for pattern in exclude_patterns:
        # íŒ¨í„´ì´ '/'ë¡œ ëë‚˜ë©´ ë””ë ‰í„°ë¦¬ íŒ¨í„´ì´ë¯€ë¡œ í•˜ìœ„ ëª¨ë“  íŒŒì¼ë„ ë§¤ì¹­ë˜ë„ë¡ ì²˜ë¦¬
        if pattern.endswith('/'):
            dir_pattern = pattern.rstrip('/')
            if rel_path_from_project.startswith(dir_pattern + '/') or rel_path_from_project == dir_pattern:
                return True
        # ì¼ë°˜ íŒ¨í„´ ë§¤ì¹­
        if fnmatch.fnmatch(rel_path_from_project, pattern) or fnmatch.fnmatch('/' + rel_path_from_project, pattern):
            return True
    return False


# ==============================
# ğŸ§  ë©”ì¸ ë¡œì§
# ==============================

def build_search_index(output_dir, additional_excludes=None):
    root_path = CONTENT_DIR.resolve()
    output_path = Path(output_dir).resolve()
    output_path.mkdir(parents=True, exist_ok=True)

    hugo_ignore_patterns = parse_hugo_toml_ignore_files(HUGO_CONFIG_PATH)
    exclude_patterns = set(hugo_ignore_patterns + ADDITIONAL_EXCLUDE_PATTERNS)
    if additional_excludes:
        exclude_patterns.update(additional_excludes)

    print(f"ğŸ“ Scanning content in: {root_path}")
    print(f"ğŸš« Ignoring patterns: {sorted(exclude_patterns)}")
    print(f"âœ… Only including files with extensions: {sorted(INCLUDE_EXTENSIONS)}")

    all_file_data = []
    for dirpath, dirnames, filenames in os.walk(root_path, topdown=True):
        dirnames[:] = [d for d in dirnames if not should_exclude(Path(dirpath) / d, root_path, exclude_patterns) and not d.startswith('.')]
        for filename in sorted(filenames):
            if filename.startswith('.') or not any(filename.lower().endswith(ext) for ext in INCLUDE_EXTENSIONS):
                continue

            file_path = Path(dirpath) / filename
            if should_exclude(file_path, root_path, exclude_patterns):
                continue

            relative_path_str = str(file_path.relative_to(root_path))
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                if not content.strip(): continue

                frontmatter = extract_frontmatter(content)
                cleaned_content = clean_markdown_for_search(content)
                filename_stem = file_path.stem
                pretty_url = generate_pretty_url(relative_path_str)

                all_file_data.append({
                    "key": filename_stem, "path": pretty_url, "filename": filename_stem,
                    "frontmatter": frontmatter, "content": cleaned_content,
                })
            except Exception as e:
                print(f"âŒ Error processing {file_path}: {e}")

    files = {data['key']: {"path": data['path'], "filename": data['filename'], "content": data['content'], "frontmatter": data['frontmatter']} for data in all_file_data}
    search_index = {"generated_at": datetime.now().isoformat(), "files": files}

    # âœ¨ ë³€ê²½ì : ê¸°ì¡´ íŒŒì¼ê³¼ ë¹„êµí•˜ì—¬ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¡œì§ ì‹œì‘
    output_file = output_path / "search_index.json"
    is_changed = True # ê¸°ë³¸ì ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆë‹¤ê³  ê°€ì •

    if output_file.is_file():
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                existing_index = json.load(f)
            # 'files' ê°ì²´ë§Œ ë¹„êµí•˜ì—¬ ì‹¤ì œ ì½˜í…ì¸  ë³€ê²½ ì—¬ë¶€ í™•ì¸
            if existing_index.get('files') == search_index['files']:
                is_changed = False
        except (json.JSONDecodeError, IOError) as e:
            print(f"âš ï¸ Warning: Could not read or parse existing {output_file}. Regenerating. Error: {e}")
            is_changed = True # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ, ë³€ê²½ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼

    if is_changed:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(search_index, f, ensure_ascii=False, indent=2)
        print(f"âœ… Content changed. Saved search_index.json ({len(files)} files)")

        version_string = datetime.now().strftime("%Y%m%d%H%M%S")
        with open(output_path / "version.json", 'w', encoding='utf-8') as f:
            json.dump({"version": version_string}, f, ensure_ascii=False, indent=2)
        print(f"âœ… Updated version.json (version: {version_string})")
    else:
        print(f"âœ… No content changes detected. Skipping file updates.")


# ==============================
# ğŸš€ ì‹¤í–‰
# ==============================

def main():
    parser = argparse.ArgumentParser(description="Generate search_index.json using hugo.toml ignore rules")
    parser.add_argument('--output-dir', '-o', default=str(DEFAULT_OUTPUT_DIR),
                        help=f'Output directory (default: {DEFAULT_OUTPUT_DIR})')
    parser.add_argument('--exclude', '-e', nargs='*',
                        help='Additional glob patterns to exclude (e.g., "drafts/*")')
    args = parser.parse_args()

    build_search_index(args.output_dir, args.exclude)


if __name__ == "__main__":
    main()