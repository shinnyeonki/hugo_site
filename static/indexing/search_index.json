{
  "generated_at": "2025-11-02T07:22:52.641376",
  "files": {
    "00.Home": {
      "path": "/00.home/",
      "filename": "00.Home",
      "content": "개인 기록 저장 보관소 일반적인 폴더 트리 구조로도 볼수 있지만 태그별로 기록을 관리하고 있음 단 주제가 밀접하다면 폴더에 묶어있음 TABLE length(rows) AS \"File Count\" FROM \"\" FLATTEN file.tags AS Tag WHERE Tag AND !contains(file.path, \"05.clipping\") AND !contains(file.path, \"89.Obsidian\") GROUP BY Tag SORT length(rows) desc mermaid 와 dataview 는 사용 필수",
      "frontmatter": {
        "date": "2025-05-26T00:14:03+09:00",
        "lastmod": "2025-06-20T02:14:28+09:00"
      }
    },
    "AOP": {
      "path": "/aop/",
      "filename": "AOP",
      "content": "AOP(Aspect-Oriented Programming, 관점 지향 프로그래밍)는 소프트웨어 설계에서 횡단 관심사(Cross-cutting Concerns)를 효과적으로 분리하고 모듈화하기 위한 프로그래밍 패러다임입니다. 🔍 핵심 아이디어 문제: 로깅, 보안, 트랜잭션, 예외 처리 같은 기능은 여러 모듈에 걸쳐 중복되며, 이로 인해 핵심 비즈니스 로직이 흐려지고, 유지보수가 어려워집니다. 해결: AOP는 이런 공통 관심사를 별도의 \"관점(Aspect)으로 추출하여, 원래 코드(핵심 로직)와 분리해서 관리할 수 있게 해줍니다. 💡 AOP는 OOP(객체 지향 프로그래밍)를 보완하는 개념입니다. OOP는 \"무엇을 하는가?\"(객체와 책임)에 집중한다면, AOP는 \"언제, 어디서 공통 동작을 수행할 것인가?\"에 집중합니다. 🧱 AOP의 주요 용어 용어 설명 Aspect (관점) 횡단 관심사를 모듈화한 단위. 예: LoggingAspect , SecurityAspect Join Point (조인 포인트) 프로그램 실행 중 관점을 삽입할 수 있는 후보 지점 (예: 메서드 호출, 예외 발생 등) Pointcut (포인트컷) 실제로 관점을 적용할 조인 포인트를 필터링하는 표현식. (예: UserService의 모든 메서드 ) Advice (어드바이스) 포인트컷에서 실행될 실제 코드. 언제 실행될지에 따라 종류가 나뉨. Weaving (위빙) Aspect를 핵심 코드에 삽입하는 과정. 컴파일 시, 로드 시, 런타임 시 가능. 📌 Advice의 종류 (실행 시점 기준) 종류 설명 Before 대상 메서드 실행 전에 실행 After 대상 메서드 실행 후 (성공/실패 상관없이) After-returning 대상 메서드가 정상적으로 반환된 후 실행 After-throwing 대상 메서드에서 예외가 발생한 후 실행 Around 대상 메서드 전후를 완전히 감싸는 가장 강력한 형태 (실행 제어 가능) 💡 예시 (Spring AOP 기준) @Aspect @Component public class LoggingAspect { @Before(\"execution(* com.example.service.UserService.*(..))\") public void logBefore(JoinPoint joinPoint) { System.out.println(\"메서드 호출 전: \" + joinPoint.getSignature().getName()); } @Around(\"@annotation(LogExecutionTime)\") public Object logExecutionTime(ProceedingJoinPoint joinPoint) throws Throwable { long start = System.currentTimeMillis(); Object result = joinPoint.proceed(); // 원래 메서드 실행 long end = System.currentTimeMillis(); System.out.println(joinPoint.getSignature() + \" 실행 시간: \" + (end - start) + \"ms\"); return result; } } → UserService 의 모든 메서드에 자동으로 로깅이 적용되며, 핵심 로직에는 로깅 코드 한 줄도 없음! ✅ AOP의 장점 핵심 로직과 횡단 관심사 분리 → 코드가 깔끔하고 집중도 높음 중복 제거 → DRY 원칙 준수 유지보수 용이 → 로깅 정책 변경 시 한 곳만 수정 유연한 확장 → 새로운 공통 기능을 쉽게 추가 ⚠️ 주의 사항 과도한 사용은 디버깅을 어렵게 함 (실행 흐름이 명시적이지 않음) 런타임 위빙은 성능 오버헤드 발생 가능 모든 언어에서 네이티브 지원하지 않음 (Java: Spring AOP, AspectJ / C#: PostSharp / Python: 데코레이터로 유사 구현 등) 📚 요약 AOP는 \"로깅, 보안, 트랜잭션 같은 공통 기능을 핵심 코드에서 분리해, 마치 레고 블록처럼 붙일 수 있게 해주는 기술\"입니다. 이를 통해 관심사의 분리(SoC), 높은 응집도, 낮은 결합도를 실현할 수 있습니다. 궁금한 부분(예: Spring에서의 사용법, 다른 언어 예시 등)이 있다면 알려주세요!",
      "frontmatter": {
        "tags": [
          "ai-content"
        ],
        "date": "2025-10-19T16:27:14+09:00",
        "lastmod": "2025-10-19T16:27:21+09:00"
      }
    },
    "Spec-Driven Development": {
      "path": "/spec-driven-development/",
      "filename": "Spec-Driven Development",
      "content": "권력의 역전 수십 년간 코드가 왕이었습니다. 명세는 코드를 보조하는 역할을 했습니다. 코딩이라는 \"진짜 작업\"이 시작되면 지었다가 허물어버리는 비계와 같았습니다. 우리는 개발을 안내하기 위해 제품 요구사항 문서(PRD)를 작성했고, 구현에 정보를 제공하기 위해 설계 문서를 만들었으며, 아키텍처를 시각화하기 위해 다이어그램을 그렸습니다. 하지만 이것들은 항상 코드 자체에 종속적이었습니다. 코드가 진실이었습니다. 다른 모든 것은 기껏해야 좋은 의도에 불과했습니다. 코드는 진실의 원천이었고, 코드가 발전함에 따라 명세는 그 속도를 거의 따라가지 못했습니다. 자산(코드)과 구현이 하나로 묶여 있기 때문에, 코드로부터 빌드하지 않고서는 병렬적인 구현을 갖기란 쉽지 않습니다. 명세 주도 개발(SDD)은 이 권력 구조를 뒤집습니다. 명세가 코드를 보조하는 것이 아니라, 코드가 명세를 보조합니다. 제품 요구사항 문서(PRD)는 구현을 위한 가이드가 아니라, 구현을 생성하는 원천입니다. 기술 계획은 코딩에 정보를 제공하는 문서가 아니라, 코드를 생산하는 정밀한 정의입니다. 이것은 우리가 소프트웨어를 구축하는 방식에 대한 점진적인 개선이 아닙니다. 무엇이 개발을 주도하는가에 대한 근본적인 재고입니다. 명세와 구현 사이의 격차는 소프트웨어 개발 초기부터 고질적인 문제였습니다. 우리는 더 나은 문서, 더 상세한 요구사항, 더 엄격한 프로세스로 이 격차를 메우려 노력했습니다. 이러한 접근 방식들은 그 격차를 필연적인 것으로 받아들이기 때문에 실패합니다. 격차를 좁히려 할 뿐, 결코 제거하지는 못합니다. SDD는 명세와 명세로부터 파생된 구체적인 구현 계획을 실행 가능하게 만듦으로써 그 격차를 제거합니다. 명세와 구현 계획이 코드를 생성할 때, 격차는 존재하지 않습니다. 오직 변환만 있을 뿐입니다. 이러한 변환은 이제 AI가 복잡한 명세를 이해하고 구현하며, 상세한 구현 계획을 만들 수 있게 되었기 때문에 가능해졌습니다. 하지만 구조 없이 원시적인 AI 생성은 혼돈을 낳습니다. SDD는 작동하는 시스템을 생성할 만큼 정밀하고, 완전하며, 모호하지 않은 명세와 그에 따른 구현 계획을 통해 그 구조를 제공합니다. 명세가 주요 산출물이 됩니다. 코드는 특정 언어와 프레임워크로 표현된 (구현 계획에 따른) 구현체가 됩니다. 이 새로운 세계에서 소프트웨어를 유지보수한다는 것은 명세를 발전시키는 것을 의미합니다. 개발팀의 의도는 자연어(\"의도 주도 개발\"), 디자인 자산, 핵심 원칙 및 기타 가이드라인으로 표현됩니다. 개발의 공용어(lingua franca)는 더 높은 수준으로 이동하고, 코드는 마지막 단계를 처리하는 접근 방식이 됩니다. 디버깅은 잘못된 코드를 생성하는 명세와 그 구현 계획을 수정하는 것을 의미합니다. 리팩토링은 명확성을 위해 명세를 재구성하는 것을 의미합니다. 전체 개발 워크플로우는 명세를 중심적인 진실의 원천으로 재편성되며, 구현 계획과 코드는 지속적으로 재생성되는 결과물이 됩니다. 새로운 기능으로 앱을 업데이트하거나, 창의적인 존재로서 새로운 병렬 구현을 만드는 것은 명세를 재검토하고 새로운 구현 계획을 만드는 것을 의미합니다. 따라서 이 프로세스는 0 -> 1, (1', ...), 2, 3, N의 과정을 거칩니다. 개발팀은 창의성, 실험, 비판적 사고에 집중하게 됩니다. 실제 SDD 워크플로우 워크플로우는 종종 모호하고 불완전한 아이디어에서 시작됩니다. AI와의 반복적인 대화를 통해 이 아이디어는 포괄적인 PRD가 됩니다. AI는 명확한 질문을 하고, 엣지 케이스를 식별하며, 정밀한 인수 기준을 정의하는 데 도움을 줍니다. 전통적인 개발에서 며칠간의 회의와 문서 작업이 필요했던 일이, 집중적인 명세 작업 몇 시간 만에 이루어집니다. 이는 전통적인 소프트웨어 개발 수명주기(SDLC)를 변화시킵니다. 요구사항과 설계는 별개의 단계가 아니라 지속적인 활동이 됩니다. 이는 팀이 검토한 명세를 표현하고 버전 관리하며, 브랜치를 생성하고 병합하는 팀 프로세스를 지원합니다. 제품 관리자가 인수 기준을 업데이트하면, 구현 계획은 영향을 받는 기술적 결정을 자동으로 표시합니다. 아키텍트가 더 나은 패턴을 발견하면, PRD는 새로운 가능성을 반영하여 업데이트됩니다. 이 명세 과정 전반에 걸쳐, 리서치 에이전트가 중요한 컨텍스트를 수집합니다. 라이브러리 호환성, 성능 벤치마크, 보안 영향을 조사합니다. 조직의 제약 조건은 자동으로 발견되고 적용됩니다. 회사의 데이터베이스 표준, 인증 요구사항, 배포 정책 등이 모든 명세에 원활하게 통합됩니다. PRD로부터 AI는 요구사항을 기술적 결정에 매핑하는 구현 계획을 생성합니다. 모든 기술 선택에는 문서화된 근거가 있습니다. 모든 아키텍처 결정은 특정 요구사항으로 거슬러 올라갑니다. 이 과정 전반에 걸쳐, 일관성 검증이 지속적으로 품질을 향상시킵니다. AI는 일회성 관문으로서가 아니라 지속적인 개선 과정으로서 명세의 모호성, 모순, 격차를 분석합니다. 코드 생성은 명세와 그 구현 계획이 충분히 안정화되는 즉시 시작되지만, \"완전\"할 필요는 없습니다. 초기 생성물은 명세가 실제로 의미가 있는지 테스트하는 탐색적일 수 있습니다. 도메인 개념은 데이터 모델이 됩니다. 사용자 스토리는 API 엔드포인트가 됩니다. 인수 시나리오는 테스트가 됩니다. 이는 명세를 통해 개발과 테스트를 통합합니다. 테스트 시나리오는 코드 작성 후에 작성되는 것이 아니라, 구현과 테스트를 모두 생성하는 명세의 일부입니다. 피드백 루프는 초기 개발을 넘어 확장됩니다. 프로덕션 환경의 지표와 장애는 단순히 긴급 수정(hotfix)을 유발하는 데 그치지 않고, 다음 재생성을 위해 명세를 업데이트합니다. 성능 병목 현상은 새로운 비기능적 요구사항이 됩니다. 보안 취약점은 모든 미래 생성물에 영향을 미치는 제약 조건이 됩니다. 명세, 구현, 그리고 운영 현실 사이의 이러한 반복적인 상호작용이야말로 진정한 이해가 나타나는 지점이며, 전통적인 SDLC가 지속적인 진화로 변모하는 곳입니다. 지금 SDD가 중요한 이유 세 가지 트렌드가 SDD를 가능하게 할 뿐만 아니라 필수적으로 만듭니다. 첫째, AI 역량이 자연어 명세를 통해 안정적으로 작동하는 코드를 생성할 수 있는 임계점에 도달했습니다. 이는 개발자를 대체하는 것이 아니라, 명세에서 구현으로의 기계적인 번역을 자동화함으로써 개발자의 효율성을 증폭시키는 것입니다. 이는 탐색과 창의성을 증폭시키고, \"처음부터 다시 시작하기\"를 쉽게 지원하며, 추가, 삭제, 비판적 사고를 지원할 수 있습니다. 둘째, 소프트웨어의 복잡성은 기하급수적으로 계속 증가하고 있습니다. 현대 시스템은 수십 개의 서비스, 프레임워크, 의존성을 통합합니다. 이 모든 조각들을 수동 프로세스를 통해 원래의 의도와 일치시키는 것은 점점 더 어려워지고 있습니다. SDD는 명세 주도 생성을 통해 체계적인 일관성을 제공합니다. 프레임워크는 인간 우선이 아닌 AI 우선 지원을 제공하거나, 재사용 가능한 컴포넌트를 중심으로 설계되도록 진화할 수 있습니다. 셋째, 변화의 속도가 가속화되고 있습니다. 오늘날 요구사항은 과거 어느 때보다 훨씬 빠르게 변화합니다. 방향 전환(Pivoting)은 더 이상 예외적인 일이 아니라 예상되는 일입니다. 현대 제품 개발은 사용자 피드백, 시장 상황, 경쟁 압력에 기반한 빠른 반복을 요구합니다. 전통적인 개발은 이러한 변화를 방해 요소로 취급합니다. 각 방향 전환은 문서, 설계, 코드를 통해 수동으로 변경 사항을 전파해야 합니다. 그 결과는 속도를 제한하는 느리고 신중한 업데이트이거나, 기술 부채를 축적하는 빠르고 무모한 변경입니다. SDD는 \"만약 우리가 티셔츠를 더 많이 팔기 위한 비즈니스 요구를 촉진하기 위해 애플리케이션을 재구현하거나 변경해야 한다면, 그것을 어떻게 구현하고 실험할 것인가?\"와 같은 가상/시뮬레이션 실험을 지원할 수 있습니다. SDD는 요구사항 변경을 장애물에서 정상적인 워크플로우로 변환합니다. 명세가 구현을 주도할 때, 방향 전환은 수동 재작성이 아닌 체계적인 재생성이 됩니다. PRD에서 핵심 요구사항을 변경하면, 영향을 받는 구현 계획이 자동으로 업데이트됩니다. 사용자 스토리를 수정하면, 해당 API 엔드포인트가 재생성됩니다. 이것은 단지 초기 개발에 관한 것이 아니라, 필연적인 변화 속에서도 엔지니어링 속도를 유지하는 것에 관한 것입니다. 핵심 원칙 명세가 공용어(Lingua Franca): 명세가 주요 산출물이 됩니다. 코드는 특정 언어와 프레임워크로 표현된 구현체가 됩니다. 소프트웨어를 유지보수한다는 것은 명세를 발전시키는 것을 의미합니다. 실행 가능한 명세: 명세는 작동하는 시스템을 생성할 만큼 정밀하고, 완전하며, 모호하지 않아야 합니다. 이는 의도와 구현 사이의 격차를 제거합니다. 지속적인 개선: 일관성 검증은 일회성 관문이 아니라 지속적으로 발생합니다. AI는 지속적인 프로세스로서 명세의 모호성, 모순, 격차를 분석합니다. 리서치 기반 컨텍스트: 리서치 에이전트는 명세 과정 전반에 걸쳐 기술적 옵션, 성능 영향, 조직의 제약 조건을 조사하며 중요한 컨텍스트를 수집합니다. 양방향 피드백: 프로덕션 환경의 현실이 명세 진화에 정보를 제공합니다. 지표, 장애, 운영상의 교훈이 명세 개선을 위한 입력이 됩니다. 탐색을 위한 브랜칭: 동일한 명세에서 여러 구현 접근 방식을 생성하여 성능, 유지보수성, 사용자 경험, 비용 등 다양한 최적화 목표를 탐색합니다. 구현 접근법 오늘날 SDD를 실천하려면 기존 도구들을 조합하고 프로세스 전반에 걸쳐 규율을 유지해야 합니다. 이 방법론은 다음을 통해 실천할 수 있습니다. 반복적인 명세 개발을 위한 AI 어시스턴트 기술적 컨텍스트 수집을 위한 리서치 에이전트 명세를 구현으로 변환하기 위한 코드 생성 도구 명세 우선 워크플로우에 맞게 조정된 버전 관리 시스템 명세 문서의 AI 분석을 통한 일관성 검사 핵심은 명세를 진실의 원천으로 취급하고, 코드는 그 반대가 아니라 명세를 보조하는 생성된 결과물로 다루는 것입니다. 명령어를 통한 SDD 간소화 SDD 방법론은 명세 → 계획 → 작업화 워크플로우를 자동화하는 세 가지 강력한 명령어를 통해 크게 향상됩니다. /speckit.specify 명령어 이 명령어는 간단한 기능 설명(사용자 프롬프트)을 자동 리포지토리 관리가 포함된 완전하고 구조화된 명세로 변환합니다. 자동 기능 번호 매기기: 기존 명세를 스캔하여 다음 기능 번호(예: 001, 002, 003)를 결정합니다. 브랜치 생성: 설명에서 시맨틱 브랜치 이름을 생성하고 자동으로 생성합니다. 템플릿 기반 생성: 기능 명세 템플릿을 복사하고 사용자의 요구사항에 맞게 커스터마이즈합니다. 디렉토리 구조: 모든 관련 문서를 위해 적절한 specs/[branch-name]/ 구조를 생성합니다. /speckit.plan 명령어 기능 명세가 존재하면, 이 명령어는 포괄적인 구현 계획을 생성합니다. 명세 분석: 기능 요구사항, 사용자 스토리, 인수 기준을 읽고 이해합니다. 헌법 준수: 프로젝트 헌법 및 아키텍처 원칙과의 일관성을 보장합니다. 기술적 변환: 비즈니스 요구사항을 기술 아키텍처 및 구현 세부사항으로 변환합니다. 상세 문서화: 데이터 모델, API 계약, 테스트 시나리오를 위한 지원 문서를 생성합니다. 빠른 시작 검증: 주요 검증 시나리오를 담은 빠른 시작 가이드를 생성합니다. /speckit.tasks 명령어 계획이 생성된 후, 이 명령어는 계획 및 관련 설계 문서를 분석하여 실행 가능한 작업 목록을 생성합니다. 입력: plan.md (필수)를 읽고, data-model.md , contracts/ , research.md 가 있는 경우 함께 읽습니다. 작업 도출: 계약, 엔티티, 시나리오를 특정 작업으로 변환합니다. 병렬화: 독립적인 작업을 [P] 로 표시하고 안전하게 병렬 처리할 수 있는 그룹을 개략적으로 설명합니다. 출력: 작업 에이전트가 실행할 수 있도록 기능 디렉토리에 tasks.md 를 작성합니다. 예시: 채팅 기능 구축하기 이 명령어들이 전통적인 개발 워크플로우를 어떻게 변화시키는지 보여줍니다. 전통적인 접근 방식: 1. 문서에 PRD 작성 (2-3시간) 2. 설계 문서 작성 (2-3시간) 3. 프로젝트 구조 수동 설정 (30분) 4. 기술 명세 작성 (3-4시간) 5. 테스트 계획 작성 (2시간) 총: ~12시간의 문서 작업 명령어를 사용한 SDD 접근 방식: # 1단계: 기능 명세 생성 (5분) /speckit.specify 실시간 채팅 시스템 (메시지 기록 및 사용자 접속 상태 표시 기능 포함) # 이 명령어는 자동으로 다음을 수행합니다: # - \"003-chat-system\" 브랜치 생성 # - specs/003-chat-system/spec.md 생성 # - 구조화된 요구사항으로 파일 채우기 # 2단계: 구현 계획 생성 (5분) /speckit.plan 실시간 메시징은 WebSocket, 기록은 PostgreSQL, 접속 상태는 Redis 사용 # 3단계: 실행 가능한 작업 생성 (5분) /speckit.tasks # 이 명령어는 자동으로 다음을 생성합니다: # - specs/003-chat-system/plan.md # - specs/003-chat-system/research.md (WebSocket 라이브러리 비교) # - specs/003-chat-system/data-model.md (Message 및 User 스키마) # - specs/003-chat-system/contracts/ (WebSocket 이벤트, REST 엔드포인트) # - specs/003-chat-system/quickstart.md (주요 검증 시나리오) # - specs/003-chat-system/tasks.md (계획에서 도출된 작업 목록) 15분 안에 다음을 얻을 수 있습니다: 사용자 스토리와 인수 기준이 포함된 완전한 기능 명세 기술 선택과 근거가 포함된 상세한 구현 계획 코드 생성을 위한 준비된 API 계약 및 데이터 모델 자동 및 수동 테스트를 위한 포괄적인 테스트 시나리오 기능 브랜치에 올바르게 버전 관리된 모든 문서 구조화된 자동화의 힘 이러한 명령어들은 단지 시간을 절약하는 것뿐만 아니라, 일관성과 완전성을 강제합니다: 누락되는 세부사항 없음: 템플릿은 비기능적 요구사항부터 오류 처리에 이르기까지 모든 측면을 고려하도록 보장합니다. 추적 가능한 결정: 모든 기술적 선택은 특정 요구사항으로 연결됩니다. 살아있는 문서: 명세가 코드를 생성하기 때문에 코드와 동기화 상태를 유지합니다. 신속한 반복: 며칠이 아닌 몇 분 만에 요구사항을 변경하고 계획을 재생성합니다. 이 명령어들은 명세를 정적인 문서가 아닌 실행 가능한 산출물로 취급함으로써 SDD 원칙을 구현합니다. 명세 프로세스를 필요악에서 개발의 원동력으로 변화시킵니다. 템플릿 주도 품질: 구조가 LLM을 제약하여 더 나은 결과를 만드는 방법 이 명령어들의 진정한 힘은 단순한 자동화가 아니라, 템플릿이 LLM의 행동을 더 높은 품질의 명세로 유도하는 방식에 있습니다. 템플릿은 LLM의 출력을 생산적인 방향으로 제약하는 정교한 프롬프트 역할을 합니다. 섣부른 구현 세부사항 방지 기능 명세 템플릿은 명시적으로 지시합니다: - ✅ 사용자가 무엇을(WHAT) 왜(WHY) 필요로 하는지에 집중하세요. - ❌ 어떻게(HOW) 구현할지는 피하세요 (기술 스택, API, 코드 구조 언급 금지). 이 제약은 LLM이 적절한 추상화 수준을 유지하도록 강제합니다. LLM이 자연스럽게 \"리액트와 리덕스를 사용하여 구현\"으로 넘어갈 수 있는 상황에서, 템플릿은 \"사용자는 데이터의 실시간 업데이트가 필요하다\"에 집중하게 만듭니다. 이러한 분리는 구현 기술이 변경되더라도 명세가 안정적으로 유지되도록 보장합니다. 명시적인 불확실성 표시 강제 두 템플릿 모두 [명확화 필요] 마커 사용을 의무화합니다: 사용자 프롬프트로부터 이 명세를 생성할 때: 1. **모든 모호함을 표시**: [명확화 필요: 구체적인 질문]을 사용하세요. 2. **추측하지 마세요**: 프롬프트에 명시되지 않은 것이 있다면 표시하세요. 이는 그럴듯하지만 잠재적으로 잘못된 가정을 하는 일반적인 LLM의 행동을 방지합니다. \"로그인 시스템\"이 이메일/비밀번호 인증을 사용한다고 추측하는 대신, LLM은 [명확화 필요: 인증 방식이 명시되지 않음 - 이메일/비밀번호, SSO, OAuth?] 와 같이 표시해야 합니다. 체크리스트를 통한 구조화된 사고 템플릿에는 명세를 위한 \"단위 테스트\" 역할을 하는 포괄적인 체크리스트가 포함되어 있습니다: ### 요구사항 완전성 - [ ] [명확화 필요] 마커가 남아있지 않음 - [ ] 요구사항은 테스트 가능하고 모호하지 않음 - [ ] 성공 기준은 측정 가능함 이 체크리스트는 LLM이 자신의 결과물을 체계적으로 자체 검토하도록 강제하여, 그렇지 않으면 놓칠 수 있는 격차를 잡아냅니다. 이는 LLM에게 품질 보증 프레임워크를 제공하는 것과 같습니다. 게이트를 통한 헌법 준수 구현 계획 템플릿은 단계별 게이트를 통해 아키텍처 원칙을 강제합니다: ### -1단계: 사전 구현 게이트 #### 단순성 게이트 (제7조) - [ ] 3개 이하의 프로젝트를 사용하고 있는가? - [ ] 미래를 대비한 과도한 설계(future-proofing)는 없는가? #### 추상화 방지 게이트 (제8조) - [ ] 프레임워크를 직접 사용하고 있는가? - [ ] 단일 모델 표현을 사용하는가? 이러한 게이트는 LLM이 복잡성을 명시적으로 정당화하도록 만들어 과도한 엔지니어링을 방지합니다. 게이트를 통과하지 못하면, LLM은 \"복잡성 추적\" 섹션에 그 이유를 문서화해야 하며, 이는 아키텍처 결정에 대한 책임감을 만듭니다. 계층적 세부 정보 관리 템플릿은 적절한 정보 아키텍처를 강제합니다: **중요**: 이 구현 계획은 높은 수준에서 읽기 쉽게 유지되어야 합니다. 모든 코드 샘플, 상세 알고리즘, 또는 광범위한 기술 명세는 반드시 적절한 `implementation-details/` 파일에 위치해야 합니다. 이는 명세가 읽을 수 없는 코드 덤프가 되는 일반적인 문제를 방지합니다. LLM은 적절한 세부 수준을 유지하는 법을 배우고, 복잡성을 별도의 파일로 추출하면서 주요 문서는 탐색하기 쉽게 유지합니다. 테스트 우선 사고 구현 템플릿은 테스트 우선 개발을 강제합니다: ### 파일 생성 순서 1. API 명세를 포함하는 `contracts/` 생성 2. 계약 → 통합 → 종단간(e2e) → 단위 테스트 순서로 테스트 파일 생성 3. 테스트를 통과시키기 위한 소스 파일 생성 이 순서 제약은 LLM이 구현 전에 테스트 가능성과 계약에 대해 생각하도록 보장하여, 더 견고하고 검증 가능한 명세를 이끌어 냅니다. 추측성 기능 방지 템플릿은 추측을 명시적으로 금지합니다: - [ ] 추측성이거나 \"필요할지도 모르는\" 기능 없음 - [ ] 모든 단계에는 명확한 전제 조건과 결과물이 있음 이는 LLM이 구현을 복잡하게 만드는 \"있으면 좋은\" 기능을 추가하는 것을 막습니다. 모든 기능은 명확한 인수 기준이 있는 구체적인 사용자 스토리로 거슬러 올라가야 합니다. 복합 효과 이러한 제약들은 함께 작용하여 다음과 같은 명세를 생성합니다: 완전함: 체크리스트가 누락되는 것을 방지합니다. 모호하지 않음: 강제된 명확화 마커가 불확실성을 강조합니다. 테스트 가능함: 테스트 우선 사고가 프로세스에 내장되어 있습니다. 유지보수 가능함: 적절한 추상화 수준과 정보 계층 구조를 가집니다. 구현 가능함: 구체적인 결과물이 있는 명확한 단계로 구성됩니다. 템플릿은 LLM을 창의적인 작가에서 규율 있는 명세 엔지니어로 변모시켜, 그 능력을 일관되게 고품질이고 실행 가능한, 진정으로 개발을 주도하는 명세를 생산하는 방향으로 유도합니다. 헌법적 기반: 아키텍처 규율 강제 SDD의 핵심에는 명세가 코드가 되는 방식을 지배하는 불변의 원칙 집합인 헌법이 있습니다. 헌법( memory/constitution.md )은 시스템의 아키텍처 DNA 역할을 하여, 생성된 모든 구현이 일관성, 단순성, 품질을 유지하도록 보장합니다. 개발의 9개 조항 헌법은 개발 프로세스의 모든 측면을 형성하는 9개의 조항을 정의합니다. 제1조: 라이브러리 우선 원칙 모든 기능은 예외 없이 독립적인 라이브러리로 시작해야 합니다. 이는 처음부터 모듈식 설계를 강제합니다. Specify의 모든 기능은 반드시 독립적인 라이브러리로 시작해야 한다. 어떤 기능도 재사용 가능한 라이브러리 컴포넌트로 먼저 추상화되지 않고 애플리케이션 코드 내에서 직접 구현되어서는 안 된다. 이 원칙은 명세가 모놀리식 애플리케이션이 아닌 모듈식이고 재사용 가능한 코드를 생성하도록 보장합니다. LLM이 구현 계획을 생성할 때, 명확한 경계와 최소한의 의존성을 가진 라이브러리로 기능을 구조화해야 합니다. 제2조: CLI 인터페이스 의무화 모든 라이브러리는 명령줄 인터페이스(CLI)를 통해 기능을 노출해야 합니다. 모든 CLI 인터페이스는 다음을 반드시 준수해야 한다: - 텍스트를 입력으로 받는다 (stdin, 인수, 또는 파일을 통해) - 텍스트를 출력으로 생성한다 (stdout을 통해) - 구조화된 데이터 교환을 위해 JSON 형식을 지원한다 이는 관찰 가능성과 테스트 가능성을 강제합니다. LLM은 기능을 불투명한 클래스 내부에 숨길 수 없으며, 모든 것은 텍스트 기반 인터페이스를 통해 접근하고 검증할 수 있어야 합니다. 제3조: 테스트 우선 원칙 가장 혁신적인 조항으로, 테스트 없이는 코드도 없습니다. 이것은 협상 불가능하다: 모든 구현은 엄격한 테스트 주도 개발(TDD)을 따라야 한다. 다음이 선행되지 않고는 어떠한 구현 코드도 작성되어서는 안 된다: 1. 단위 테스트가 작성된다. 2. 테스트가 사용자에 의해 검증되고 승인된다. 3. 테스트가 실패하는 것(Red 단계)이 확인된다. 이는 전통적인 AI 코드 생성을 완전히 뒤집습니다. 코드를 생성하고 작동하기를 바라는 대신, LLM은 먼저 행동을 정의하는 포괄적인 테스트를 생성하고, 승인을 받은 후, 그 다음에야 구현을 생성해야 합니다. 제7조 & 제8조: 단순성과 추상화 방지 이 두 조항은 과도한 엔지니어링에 맞서 싸웁니다. 제7조 3항: 최소한의 프로젝트 구조 - 초기 구현 시 최대 3개의 프로젝트 - 추가 프로젝트는 문서화된 정당화 필요 제8조 1항: 프레임워크 신뢰 - 프레임워크 기능을 감싸지 말고 직접 사용한다 LLM이 자연스럽게 정교한 추상화를 만들 수 있는 상황에서, 이 조항들은 모든 복잡성 계층에 대한 정당화를 강제합니다. 구현 계획 템플릿의 \"-1단계 게이트\"는 이러한 원칙을 직접적으로 강제합니다. 제9조: 통합 우선 테스트 고립된 단위 테스트보다 실제 환경 테스트를 우선시합니다. 테스트는 반드시 현실적인 환경을 사용해야 한다: - 모의(mock) 객체보다 실제 데이터베이스 선호 - 스텁(stub)보다 실제 서비스 인스턴스 사용 - 구현 전 계약(contract) 테스트 의무화 이는 생성된 코드가 이론만이 아닌 실제 환경에서 작동함을 보장합니다. 템플릿을 통한 헌법 강제 구현 계획 템플릿은 구체적인 체크포인트를 통해 이러한 조항들을 실행합니다. ### -1단계: 사전 구현 게이트 #### 단순성 게이트 (제7조) - [ ] 3개 이하의 프로젝트를 사용하고 있는가? - [ ] 미래를 대비한 과도한 설계는 없는가? #### 추상화 방지 게이트 (제8조) - [ ] 프레임워크를 직접 사용하고 있는가? - [ ] 단일 모델 표현을 사용하는가? #### 통합 우선 게이트 (제9조) - [ ] 계약이 정의되었는가? - [ ] 계약 테스트가 작성되었는가? 이러한 게이트는 아키텍처 원칙에 대한 컴파일 타임 체크 역할을 합니다. LLM은 게이트를 통과하거나, \"복잡성 추적\" 섹션에 정당한 예외를 문서화하지 않고는 진행할 수 없습니다. 불변의 원칙의 힘 헌법의 힘은 그 불변성에 있습니다. 구현 세부사항은 진화할 수 있지만, 핵심 원칙은 일정하게 유지됩니다. 이는 다음을 제공합니다. 시간에 따른 일관성: 오늘 생성된 코드는 내년에 생성될 코드와 동일한 원칙을 따릅니다. LLM 간의 일관성: 다른 AI 모델도 아키텍처적으로 호환되는 코드를 생성합니다. 아키텍처 무결성: 모든 기능이 시스템 설계를 훼손하는 대신 강화합니다. 품질 보증: 테스트 우선, 라이브러리 우선, 단순성 원칙이 유지보수 가능한 코드를 보장합니다. 헌법의 진화 원칙은 불변이지만, 그 적용은 진화할 수 있습니다. 제4조 2항: 개정 절차 이 헌법을 수정하려면 다음이 필요하다: - 변경 이유에 대한 명시적인 문서화 - 프로젝트 유지보수자의 검토 및 승인 - 하위 호환성 평가 이는 안정성을 유지하면서 방법론이 학습하고 개선될 수 있도록 합니다. 헌법은 날짜가 기록된 개정안을 통해 원칙이 실제 경험을 바탕으로 어떻게 개선될 수 있는지를 보여줍니다. 규칙을 넘어: 개발 철학 헌법은 단순한 규칙집이 아니라, LLM이 코드 생성에 대해 어떻게 생각하는지를 형성하는 철학입니다. 불투명성보다 관찰 가능성: 모든 것은 CLI 인터페이스를 통해 검사 가능해야 합니다. 기교보다 단순성: 단순하게 시작하고, 필요성이 입증될 때만 복잡성을 추가합니다. 고립보다 통합: 인공적인 환경이 아닌 실제 환경에서 테스트합니다. 모놀리스보다 모듈성: 모든 기능은 명확한 경계를 가진 라이브러리입니다. 이러한 원칙들을 명세 및 계획 프로세스에 내장함으로써, SDD는 생성된 코드가 단지 기능적인 것을 넘어 유지보수 가능하고, 테스트 가능하며, 아키텍처적으로 건전하도록 보장합니다. 헌법은 AI를 단순한 코드 생성기에서 시스템 설계 원칙을 존중하고 강화하는 아키텍처 파트너로 변모시킵니다. 변혁 이는 개발자를 대체하거나 창의성을 자동화하는 것에 관한 것이 아닙니다. 기계적인 번역을 자동화함으로써 인간의 능력을 증폭시키는 것입니다. 명세, 리서치, 코드가 함께 진화하며, 각 반복이 더 깊은 이해와 의도와 구현 간의 더 나은 일치를 가져오는 긴밀한 피드백 루프를 만드는 것입니다. 소프트웨어 개발은 의도와 구현 간의 일치를 유지하기 위한 더 나은 도구가 필요합니다. SDD는 단순히 코드를 안내하는 것이 아니라 코드를 생성하는 실행 가능한 명세를 통해 이러한 일치를 달성하기 위한 방법론을 제공합니다. 실제 speckit 도구 구현 예시 명확한 요구사항 → 기술 설계 → 실행 가능한 작업 → 구현이라는 구조를 따르며, 소프트웨어 개발의 품질과 일관성을 높이는 데 초점을 둡니다. /speckit.constitution 목적: 프로젝트의 핵심 원칙, 개발 가이드라인, 의사결정 기준을 문서화합니다. 이것은 프로젝트의 “헌법” 역할을 하며, 나중에 기술 선택이나 설계 논쟁이 있을 때 참조됩니다. 예시: /speckit.constitution - 모든 코드는 테스트 가능해야 하며, 단위 테스트 커버리지는 최소 80% 이상이어야 한다. - 사용자 인터페이스는 접근성(WCAG 2.1 AA)을 준수해야 한다. - 백엔드는 stateless 설계를 따르며, 세션은 JWT 기반으로 관리한다. - 데이터베이스 마이그레이션은 버전 관리되고, 롤백 가능해야 한다. - 한국어와 영어를 동시에 지원하며, 모든 문자열은 i18n 레이어를 통해 관리된다. 💡 이 단계는 프로젝트 초기에 한 번 정의되지만, 필요시 업데이트 가능합니다. /speckit.specify 목적: 무엇을 만들 것인가에 대한 요구사항과 사용자 스토리를 명확히 정의합니다. 기술 구현보다는 비즈니스 가치와 사용자 관점에 집중합니다. 예시: /speckit.specify 사용자 스토리: - 사용자로서, 나는 로그인 없이도 게스트 모드로 앱을 사용할 수 있어야 한다. - 관리자로서, 나는 사용자 계정을 비활성화할 수 있어야 한다. - 사용자로서, 나는 내 프로필 사진을 업로드하고 자르기 기능을 사용할 수 있어야 한다. 비기능적 요구사항: - 시스템은 1초 이내에 프로필 이미지 업로드 응답을 반환해야 한다. - 모든 API는 HTTPS만 허용하며, CORS 정책은 명시적으로 설정된다. 💡 이 단계에서 모호한 요구사항(예: “빠르게 작동해야 한다”)은 /speckit.clarify 로 사전에 해소하는 것이 좋습니다. /speckit.clarify (선택적, 권장됨) 목적: /speckit.specify 에서 모호하거나 불완전한 요구사항을 명확히 합니다. 질문을 통해 명세의 품질을 높입니다. 예시: /speckit.clarify 질문: - “게스트 모드”에서 사용자는 어떤 데이터를 저장할 수 있나요? 로컬 스토리지만 허용되나요? - 프로필 사진 자르기 기능은 서버에서 처리되나요, 클라이언트에서 처리되나요? - 비활성화된 계정은 완전히 삭제되나요, 아니면 상태만 변경되나요? 💡 이 단계를 거치면 나중에 설계나 구현에서 혼선을 줄일 수 있습니다. /speckit.plan 목적: 명세를 바탕으로 기술적 구현 계획을 세웁니다. 기술 스택, 아키텍처, 데이터 흐름, API 설계 등을 포함합니다. 예시: /speckit.plan 기술 스택: - 프론트엔드: React + TypeScript + Vite - 백엔드: Node.js + Express - DB: PostgreSQL - 인증: JWT + bcrypt - 파일 저장: AWS S3 (프로필 이미지) 아키텍처: - 클라이언트는 REST API를 통해 /api/v1/profile/upload 엔드포인트에 이미지 전송 - 서버는 sharp 라이브러리로 이미지 크롭 후 S3에 저장 - DB에는 S3 URL만 저장 보안: - multipart/form-data 업로드 시 파일 타입 검증 (image/jpeg, image/png만 허용) - 최대 파일 크기: 5MB 💡 이 단계는 개발자가 실제로 코드를 작성하기 전의 “청사진”입니다. /speckit.tasks 목적: 계획을 바탕으로 실행 가능한 작업 목록을 생성합니다. 작업은 작고, 독립적이며, 검증 가능해야 합니다. 예시: /speckit.tasks 1. [FE] 프로필 페이지에 이미지 업로드 UI 추가 (React 컴포넌트) 2. [FE] 이미지 미리보기 및 크롭 영역 선택 기능 구현 (react-easy-crop 사용) 3. [BE] /api/v1/profile/upload 엔드포인트 생성 4. [BE] multer로 multipart/form-data 파싱 설정 5. [BE] sharp로 이미지 리사이징 및 크롭 (300x300 정사각형) 6. [BE] AWS SDK 설정 및 S3 업로드 함수 구현 7. [BE] DB에 사용자 프로필 URL 업데이트 8. [TEST] 프로필 이미지 업로드 통합 테스트 작성 💡 각 작업은 PR 단위로 분리 가능하며, CI/CD 파이프라인과 연동하기 좋습니다. /speckit.analyze (선택적) 목적: 생성된 명세, 계획, 작업 간의 일관성과 커버리지를 분석합니다. 예: “모든 사용자 스토리가 작업으로 분해되었는가?” “보안 요구사항이 구현 계획에 반영되었는가?” 예시 출력: /speckit.analyze ✅ 모든 사용자 스토리는 최소 1개 이상의 작업에 매핑됨. ⚠️ “게스트 모드 데이터 저장” 관련 작업이 누락됨 → 작업 #9 추가 필요. ✅ 비기능적 요구사항(1초 응답 시간)은 성능 테스트 항목으로 포함됨. 💡 이 단계는 구현 전 품질 보증(QA)의 일환입니다. /speckit.checklist (선택적) 목적: 요구사항의 완전성, 명확성, 일관성을 검증하는 커스텀 체크리스트를 생성합니다. 말 그대로 “영어를 위한 유닛 테스트”처럼 작동합니다. 예시: /speckit.checklist [ ] 모든 사용자 스토리는 “사용자로서, 나는 ~할 수 있다” 형식인가? [ ] 각 비기능적 요구사항은 측정 가능한 지표(예: 1초, 99.9% 가용성)를 포함하는가? [ ] 기술 계획에서 언급된 모든 외부 서비스(S3, DB 등)는 보안 설정이 명시되었는가? [ ] 한국어 경로 처리(예: /사용자/이미지.jpg)에 대한 테스트 케이스가 포함되었는가? 💡 특히 국제화(i18n)나 특수 환경(예: ARM64, macOS/Linux 호환성)이 중요한 경우 유용합니다. /speckit.implement 목적: 모든 작업을 자동 또는 수동으로 실행하여 실제 코드를 생성합니다. 이 명령은 앞선 단계들의 산출물을 기반으로 완전한 기능 구현을 수행합니다. 예시 동작: 위의 /speckit.tasks 목록을 순차적으로 실행 각 작업에 대해 코드 생성, 테스트 작성, 문서화 수행 최종 결과: 프로필 이미지 업로드 기능이 완전히 작동하는 상태 💡 실제 구현은 개발자가 직접 할 수도 있고, AI 어시스턴트가 코드 스니펫을 제안할 수도 있습니다. 요약: Spec-Driven Development 워크플로우 흐름 constitution → specify → [clarify] → plan → tasks → [analyze] → [checklist] → implement 이 접근법은 “생각하고 → 설계하고 → 검증하고 → 구현한다”는 원칙을 따르며, 특히 복잡한 시스템, 팀 협업, 장기 유지보수가 필요한 프로젝트에 매우 효과적입니다. 필요하시면, 실제 프로젝트(예: C++ 데몬, VS Code 디버깅 설정 등)에 이 워크플로우를 적용해 함께 진행해 드릴 수도 있습니다.",
      "frontmatter": {
        "tags": [
          "ai-content"
        ],
        "date": "2025-10-13T13:36:25+09:00",
        "lastmod": "2025-10-13T13:45:24+09:00"
      }
    },
    "Toss technical writing guide": {
      "path": "/toss-technical-writing-guide/",
      "filename": "Toss technical writing guide",
      "content": "학습 문서 ( learning ) 시작하기 문서 : 처음 접하는 기술의 주요 흐름과 개념을 이해하도록 돕는 문서 튜토리얼 문서 : 명확한 목표와 결과물이 있는 단계별 학습 문서 => FAQ 섹션 제공 => 접어둘 수 있는 컴포넌트 활용 문제 해결을 위한 문서 작성 가이드 🛠 ( troubleshooting ) How-to 가이드 : 특정 기능 구현이나 작업 수행을 위한 단계별 절차 트러블슈팅 문서 : 발생한 문제를 진단하고 해결하기 위한 디버깅 과정 안내 참조 문서 작성 가이드 📑 ( reference ) 정확성과 완전성 확보 일관된 구조 유지 쉬운 검색 및 탐색 지원 실용적인 예제 코드 제공 깊은 이해를 위한 설명 문서 작성 가이드 📚 ( explanation ) 배경 및 맥락 제공 시각 자료 적극 활용 학습을 위한 문서 작성 가이드 📚 학습을 위한 문서는 독자가 새로운 기술이나 도구를 배울 수 있도록 돕는 문서입니다. 이 문서를 읽은 후 독자가 무엇을 할 수 있는지에 대한 명확한 목표를 제시할 수 있어야 합니다. 막힘없는 진행 보장 독자가 따라하다가 막히거나 오류가 발생하지 않도록 안정적인 학습 환경을 만들어야 합니다. 모든 예제 코드는 실제로 실행하여 검증하고, 필요한 준비 사항도 꼼꼼히 안내해야 합니다. 단계별 설명 제공 독자가 차근차근 진행할 수 있도록 문서를 단계별로 구조화하세요. 예제도 간단한 것부터 점진적으로 난이도를 높여 제시하는 것이 효과적입니다. 실행 가능한 코드 예제 포함 단순히 개념만 설명하는 것이 아니라, 실제 실행 가능한 예제 코드를 포함해야 합니다. 예제는 핵심 기능을 잘 드러내야 하며, 독자가 직접 실행해보며 학습할 수 있도록 해야 합니다. 실행 가능한 예제는 코드 사용법에 대한 직관적인 이해를 돕고, 실제 프로젝트에 바로 적용할 수 있어 학습 경험을 크게 향상시킵니다. 이해를 돕기 위한 예시 예시를 보면서 학습을 위한 문서 작성 방법을 간단히 익혀봅시다. 첫 번째 예시는 React 시작하기 문서입니다. React 시작하기 React는 컴포넌트 기반으로 UI를 만들 수 있는 JavaScript 라이브러리입니다. 여기서는 가장 기본적인 React 프로젝트를 실행해보고 동작 방식을 이해하는 것을 목표로 합니다. React 프로젝트 실행하기 1단계: 프로젝트 생성 터미널을 열고 다음 명령어를 입력하여 React 프로젝트를 생성합니다. my-app 대신 원하는 프로젝트 이름을 사용할 수 있습니다. Bash npx create-react-app my-app cd my-app 2단계: 개발 서버 실행 생성된 프로젝트 폴더로 이동한 후, 다음 명령어를 실행하여 개발 서버를 시작합니다. Bash npm start 개발 서버가 실행되면 브라우저에서 http://localhost:3000 을 열어 React 기본 화면을 확인하세요. React에서 화면을 만드는 방법 React에서는 컴포넌트라는 개념을 사용하여 화면을 구성합니다. 컴포넌트는 UI의 가장 작은 단위입니다. 기본 컴포넌트 수정하기 src/App.js 파일을 열고 내용을 아래처럼 수정해 보세요. JavaScript function App() { return <h1>안녕하세요! React를 시작해 봅시다.</h1>; } export default App; 파일을 저장한 후, 브라우저를 새로고침하면 React 기본 화면 대신 \"안녕하세요! React를 시작해 봅시다.\"라는 문구가 표시됩니다. 직접 컴포넌트 만들기 새로운 컴포넌트를 직접 만들어 App.js 에 추가해 봅시다. src 폴더 안에 Welcome.js 파일을 생성하세요. 다음 코드를 입력하고 저장하세요. function Welcome({ name }) { return <h2>안녕하세요, {name}님!</h2>; } export Welcome; // 'default' 키워드를 사용하여 내보내기 (권장) 참고: 위 코드에서 export default Welcome; 으로 수정하는 것을 권장합니다. React 컴포넌트는 일반적으로 default export 로 내보내어 다른 파일에서 쉽게 가져올 수 있도록 합니다. 이제 App.js 에서 새로운 Welcome 컴포넌트를 가져와 추가해보세요. import Welcome from \"./Welcome\"; // Welcome.js에서 Welcome 컴포넌트 가져오기 function App() { return ( <div> <h1>React 학습을 시작해봅시다!</h1> <Welcome name=\"주연\" /> {/* Welcome 컴포넌트 사용 */} </div> ); } export default App; 파일을 저장하면, 브라우저에 \"안녕하세요, 주연님!\"이라는 문구가 추가된 것을 확인할 수 있습니다. '시작하기'와 '튜토리얼' 문서의 차이점 학습을 위한 문서는 크게 두 가지 유형, '시작하기'와 '튜토리얼'로 나눌 수 있습니다. 비슷해 보이지만 목적에 약간의 차이가 있습니다. 시작하기 문서: 처음 접하는 독자가 기술의 주요 흐름 및 개념을 이해할 수 있도록 돕는 문서입니다. 간단한 설치 및 설정 안내, 필수 흐름 및 개념 소개를 통해 전체적인 흐름을 이해하는 데 중점을 둡니다. 튜토리얼 문서: 명확한 목표와 결과물이 있어서 단순히 흐름을 익히는 '시작하기'보다 더 구체적입니다. 각 단계를 따라 하면서 자연스럽게 개념을 익히고, 코드를 실행하며 학습할 수 있도록 구성하는 것이 중요합니다. 참고: 이 분류가 엄격한 것은 아닙니다. 만약 주요 흐름이나 개념, 설치와 설정이 아주 간단하다면 튜토리얼 문서의 초반에 포함되기도 합니다. 학습 문서의 효율성 높이기 학습을 위한 문서는 성공적인 학습 경험에 집중하는 문서라서 실제 프로젝트에서 발생하는 문제와는 거리가 멀 수 있습니다. 또한, 모든 정보를 한 문서에 담으려다 보면 복잡도가 높아지고 집중하기 어려워질 수 있습니다. 이 문제를 어떻게 개선할 수 있을까요? FAQ 섹션 제공 튜토리얼에는 가장 중요한 핵심 과정만 포함하고, 자주 발생하는 문제와 해결책을 문서 마지막에 FAQ 섹션으로 추가합니다. 이렇게 하면 튜토리얼 본문은 성공적인 학습 경험을 중심으로 유지하면서 동시에 독자가 부딪힐 수 있는 문제를 해결할 방법을 제공할 수 있습니다. 접어둘 수 있는 컴포넌트 활용 튜토리얼 문서가 길어지면 독자가 핵심 내용을 빠르게 따라갈 수 있도록 일부 내용을 접어두는 방식이 유용할 수 있습니다. 각 단계에서 발생할 수 있는 문제뿐만 아니라 부가 설명, 추가 예제, 심화 개념 등을 숨겼다가 필요할 때 열어볼 수 있도록 제공할 수 있습니다. 체크리스트 학습을 위한 문서를 작성할 때 다음 사항들을 확인해 보세요. 학습 목표를 명확하게 제시했나요? 직접 따라 해 봤을 때 오류가 생기지는 않나요? 단계별로 설명되어 있나요? 독자가 직접 실행할 수 있는 코드 예제가 포함되어 있나요? 문제 해결을 위한 문서 작성 가이드 🛠 문제 해결을 위한 문서는 독자가 직면한 문제를 빠르고 효과적으로 해결할 수 있도록 돕는 문서입니다. 따라서 이 문서는 독자가 현재 겪고 있는 문제를 해결할 수 있는지 여부가 가장 중요합니다. 문제의 원인과 해결 방법을 논리적으로 제공하고, 바로 적용할 수 있는 해결책과 실용적인 예시를 포함해야 합니다. 명확한 문제 상황 정의 문제가 발생한 원인과 그로 인해 나타난 현상을 구분하여 설명하세요. 에러 메시지, 로그 예시를 포함하면 독자가 문제를 더 쉽게 이해할 수 있습니다. 바로 적용 가능한 해결 방법 제공 해결 방법은 명확하고 즉시 적용 가능해야 합니다. 코드 예제, 명령어, 설정 방법을 포함하세요. 해결책이 어떤 원리로 문제를 해결하는지 언급하면 좋습니다. 환경별 차이 고려 같은 문제가 다른 환경(예: OS, 라이브러리 버전)이나 설정에서 어떻게 나타날 수 있는지도 다루세요. 이해를 돕기 위한 예시 문제 해결을 위한 문서 중 목표 달성을 위한 단계별 절차를 안내하여 독자가 특정 작업을 성공적으로 수행할 수 있도록 돕는 How-to 가이드를 어떻게 작성하는지 살펴보겠습니다. How-to 가이드 How-to 가이드에는 단계별 실행 절차와 실행 가능한 코드 예제가 포함됩니다. 튜토리얼과 달리 전체적인 흐름이나 개념 이해보다는 특정 작업을 성공적으로 수행하는 데 중점을 둡니다. React에서 자동 재시도 기능 통합 가이드 이 가이드는 자동 재시도 로직을 React 컴포넌트에 통합하여 API 요청 실패 시 자동으로 재시도하는 기능을 구현하는 방법을 설명합니다. 이 기능을 통해 네트워크 불안정 상황에서도 안정적인 데이터 요청을 보장하여 사용자 경험을 개선할 수 있습니다. UI 구현하기 다음 예제는 자동 재시도 로직( fetchWithRetry 함수는 별도로 정의되어 있다고 가정)을 활용해 API 데이터를 불러오고, 로딩 상태와 오류 처리를 포함한 UI를 구현하는 코드입니다. import { useEffect, useState } from \"react\"; // fetchWithRetry 함수는 이 가이드의 범위를 넘어선다고 가정합니다. // 예시: 네트워크 요청 실패 시 지정된 횟수만큼 재시도하는 로직을 포함합니다. async function fetchWithRetry(url, options, retries, delay) { for (let i = 0; i < retries; i++) { try { const response = await fetch(url, options); if (!response.ok) { throw new Error(`HTTP error! status: ${response.status}`); } return await response.json(); } catch (error) { if (i < retries - 1) { console.warn(`Retry attempt ${i + 1} for ${url}. Retrying in ${delay}ms...`); await new Promise(resolve => setTimeout(resolve, delay)); } else { throw error; // 마지막 시도에서도 실패하면 에러 발생 } } } } function App() { const [data, setData] = useState(null); const [error, setError] = useState(null); const [loading, setLoading] = useState(true); useEffect(() => { fetchWithRetry(\"https://jsonplaceholder.typicode.com/todos/1\", {}, 3, 1000) .then(json => { setData(json); setLoading(false); }) .catch(err => { setError(err.message); setLoading(false); }); }, []); return ( <div> {loading ? ( <p>데이터 로딩 중...</p> ) : error ? ( <p style={{ color: \"red\" }}>{error}</p> ) : ( <div> <h2>API 데이터</h2> <pre>{JSON.stringify(data, null, 2)}</pre> </div> )} </div> ); } export default App; 트러블슈팅 문서 \"Module not found: Can't resolve 'react'\" 에러 해결 가이드 \"Module not found: Can't resolve 'react'\" 에러가 발생했을 때 해결 방법을 알려드립니다. 패키지 설치 여부 확인 이 에러는 React 패키지가 설치되어 있지 않거나, node_modules 디렉토리 내에 해당 모듈이 존재하지 않을 때 발생합니다. 터미널에서 아래 명령어를 실행하여 React 패키지가 설치되어 있는지 확인하세요. Bash npm list react 패키지 재설치 및 환경 점검 문제가 계속된다면, React 및 React-DOM 패키지를 재설치해 보세요. node_modules 디렉토리와 package-lock.json 파일을 삭제한 후 다시 설치하면, 환경 관련 문제가 해결될 가능성이 높습니다. Bash # React 및 React-DOM 재설치 npm install react react-dom # 또는, 재설치 절차: # 1. 기존 node_modules 디렉토리 및 package-lock.json 파일 삭제 rm -rf node_modules package-lock.json # 2. 모든 의존성 다시 설치 npm install # 3. 이후 프로젝트 실행 npm start \\[선택\\] Node.js 버전 확인 및 조정 Node.js 버전이 호환되지 않는 경우에도 이 에러가 발생할 수 있습니다. 현재 Node.js 버전을 확인하고, 필요하다면 호환되는 버전으로 전환하세요. Bash # 현재 Node.js 버전 확인 node -v # 예시: Node Version Manager(nvm)를 사용하여 버전 전환 nvm use 18 How-to 가이드와 트러블슈팅 문서의 차이점 문제 해결을 위한 문서는 크게 두 가지 유형, How-to 가이드와 트러블슈팅 문서로 나눌 수 있습니다. 비슷해 보이지만 목적에 차이가 있습니다. How-to 가이드: 특정 기능을 성공적으로 구현하거나 특정 작업을 수행하는 단계별 절차에 초점을 맞춥니다. 트러블슈팅 문서: 이미 발생한 문제를 진단하고 해결하기 위한 디버깅 과정에 중점을 둡니다. 체크리스트 문제 해결 문서를 작성할 때 다음 사항들을 확인해 보세요. 단순히 에러 원인만 설명하는 것이 아니라, 에러에 대한 기본적인 지식도 충분히 제공했나요? 즉시 적용할 수 있는 해결 방법이 포함되어 있나요? 환경별(운영체제, 라이브러리 버전 등) 차이를 고려한 설명도 포함되어 있나요? 궁금한 점이 있다면 언제든지 다시 질문해주세요! 참조 문서 작성 가이드 📑 참조 문서는 독자가 특정 기술, 도구 또는 시스템에 대한 정확하고 완전한 정보를 빠르게 찾을 수 있도록 설계됩니다. 핵심은 정확성, 완전성, 검색 용이성이며, 독자가 필요할 때 즉시 원하는 정보를 찾아 적용할 수 있어야 합니다. 또한, 함수, 매개변수, 반환값, 사용 예제 같은 구성을 정하고 일관되게 작성해야 합니다. 정확성과 완전성 확보 문서에 포함된 모든 정보는 정확해야 하며, 누락된 부분이 없어야 합니다. 기술적 오류나 모호한 설명은 피하고, 항상 최신 상태를 유지해야 합니다. 일관된 구조 유지 일관된 포맷과 구조는 가독성을 높입니다. API 문서라면 '함수 이름 → 매개변수 → 반환값 → 예제 코드'와 같은 표준화된 구조를 만들어 보세요. 쉬운 검색 및 탐색 지원 독자가 필요한 정보를 빠르게 찾을 수 있도록 문서를 체계적으로 구성해야 합니다. 목차, 키워드 검색, 앵커 링크 등을 활용하여 정보를 쉽게 탐색할 수 있도록 돕습니다. 실용적인 예제 코드 제공 명확한 설명과 함께 예제 코드를 함께 제공해야 합니다. 예제 코드는 특정 함수나 API를 어떻게 사용하는지 직관적으로 이해하는 데 큰 도움이 됩니다. 예시: fetch API 참조 문서 fetch API 문서를 통해 참조 문서 작성 방법을 간단히 익혀봅시다. fetch API fetch 함수는 네트워크 리소스를 요청하고 응답을 처리하는 비동기 API입니다. Promise<Response> 객체를 반환하며, 클라이언트와 서버 간 데이터를 효율적으로 주고받을 수 있어 REST API와 같은 서비스 통신에 유용합니다. XMLHttpRequest 보다 간결한 문법을 제공하고, async/await 와 함께 사용하면 가독성이 뛰어나다는 장점이 있습니다. 시그니처 TypeScript fetch(input: RequestInfo, init?: RequestInit): Promise<Response> 매개변수 input (필수): 요청할 URL 또는 Request 객체. init (선택): 요청의 옵션을 담은 객체. method : HTTP 요청 방식 (GET, POST, PUT, DELETE 등) headers : 요청에 포함할 헤더 정보 (예: { 'Content-Type': 'application/json' } ) body : 요청 본문 (예: JSON.stringify({ name: 'John' }) ) mode : 요청 모드 (cors, no-cors, same-origin) credentials : 쿠키 포함 여부 (omit, same-origin, include) cache : 캐시 정책 (default, no-store, reload, force-cache 등) redirect : 리디렉션 처리 방식 (follow, error, manual) 반환값 fetch 는 Promise<Response> 객체를 반환합니다. Response 객체의 주요 속성은 다음과 같습니다: ok : 응답 성공 여부 (200~299 상태 코드인 경우 true ) status : HTTP 상태 코드 (예: 200, 404, 500) headers : 응답 헤더 ( Headers 객체) json() : 응답 본문을 JSON 객체로 변환 ( Promise<object> ) text() : 응답 본문을 문자열로 변환 ( Promise<string> ) blob() : 응답 본문을 Blob 객체로 변환 ( Promise<Blob> ) 사용 예제 기본 GET 요청 JavaScript fetch('https://jsonplaceholder.typicode.com/posts/1') .then(response => { if (!response.ok) { throw new Error('네트워크 응답이 올바르지 않습니다.'); } return response.json(); }) .then(data => console.log(data)) .catch(error => console.error('오류 발생:', error)); POST 요청 예제 JavaScript fetch('https://api.example.com/user', { method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify({ name: 'John', age: 30 }), }) .then(response => { if (!response.ok) { throw new Error('요청이 실패했습니다.'); } return response.json(); }) .then(data => console.log('서버 응답:', data)) .catch(error => console.error('오류 발생:', error)); async/await 사용 예제 JavaScript async function fetchData() { try { const response = await fetch('https://jsonplaceholder.typicode.com/posts/1'); if (!response.ok) { throw new Error('네트워크 응답이 올바르지 않습니다.'); } const data = await response.json(); console.log(data); } catch (error) { console.error('오류 발생:', error); } } fetchData(); 더 나아가기 참조 문서는 방대한 양의 정보를 포함하기 때문에 독자가 원하는 내용을 빠르게 찾기 어려울 수 있습니다. 이를 해결하기 위해 주제별로 섹션을 나누고 목차를 제공하면 탐색이 쉬워집니다. 예를 들어, 결제 도메인을 다루는 문서라면 주제별 섹션과 함께 API별 목차를 제공할 수 있습니다. 또한, 독자가 참조 문서를 활용하기 전에 반드시 알아야 할 정보의 배치에도 신경 쓰면 좋습니다. 독자가 항상 참조 문서를 활용하는 시점에만 문서를 보는 것은 아니므로, 사전에 알아야 할 정보가 있을 수 있습니다. 예를 들어, API 키 설정, 인증 방식, 요청 헤더 구성 같은 핵심 정보를 문서의 앞부분에 두어 눈에 잘 띄도록 하면 독자가 문서를 처음 접했을 때 필요한 내용을 놓치지 않게 됩니다. 체크리스트 참조 문서를 작성할 때 다음 사항들을 확인해 보세요. 검색 및 탐색이 쉬운 구조인가요? 정보가 정확하고 완전한가요? 문서가 일관된 구조와 형식으로 작성되어 있나요? 실용적인 예시 코드가 포함되어 있나요? 궁금한 점이나 추가적으로 다루고 싶은 내용이 있다면 언제든지 알려주세요! 깊은 이해를 위한 설명 문서 작성 가이드 📚 깊은 이해를 위한 설명 문서는 독자가 특정 기술이나 개념을 깊이 있게 이해할 수 있도록 돕는 것을 목표로 합니다. 핵심은 배경과 맥락을 충분히 설명하고, 의사결정 과정을 명확히 공유하는 것입니다. 이 문서를 통해 독자는 기술의 원리와 철학을 이해할 수 있어야 합니다. 배경 및 맥락 제공 기술이 등장한 이유와 해결하려는 문제를 먼저 설명하세요. 독자가 해당 기술을 왜 선택해야 하는지 납득할 수 있도록 설득력 있게 서술하세요. 시각 자료 적극 활용 복잡한 개념은 다이어그램, 흐름도, 표 등을 사용해 시각화하세요. 전체적인 구조, 데이터 흐름 또는 컴포넌트 구조는 시각적으로 보여주면 더 직관적으로 이해할 수 있습니다. 이해를 돕기 위한 예시 개념 이해를 위한 문서와 도메인 지식을 다루는 문서 두 가지 예시를 통해 자세히 살펴보겠습니다. 개념 이해를 위한 문서 예시 특정 개념의 원리와 작동 방식을 설명하는 문서는 기본적인 원리부터 응용까지 다루며, 독자가 개념을 깊이 이해하도록 돕는 것이 핵심입니다. 첫 번째 예시는 React의 가상 DOM 작동 원리를 설명하는 문서입니다. React의 가상 DOM 작동 원리 React의 가상 DOM(Virtual DOM)은 UI 변경을 효율적으로 감지하고 최소한의 변경만 실제 DOM에 반영하는 방식을 통해 성능을 최적화하는 핵심 기술입니다. 이를 통해 불필요한 렌더링을 방지하고 빠른 UI 업데이트를 제공합니다. 가상 DOM이 등장한 배경 웹 애플리케이션이 복잡해지면서, 기존의 DOM 조작 방식에는 다음과 같은 문제가 발생했습니다. DOM 조작 비용이 크다: 직접적인 DOM 변경이 많아질수록 브라우저의 렌더링 성능이 저하됩니다. 전체 페이지 리렌더링 문제: 특정 부분만 변경해도 전체 UI가 다시 그려지는 경우가 많습니다. UI 성능 저하: 많은 DOM 업데이트가 발생하면 프레임 속도가 떨어지고 사용자 경험(UX)이 저하될 가능성이 높습니다. React는 이러한 문제를 해결하기 위해 가상 DOM을 도입했습니다. 가상 DOM을 활용하면 변경 사항을 먼저 계산하고, 최소한의 연산으로 실제 DOM을 업데이트할 수 있습니다. 개념 가상 DOM(Virtual DOM)은 실제 DOM의 경량화된 JavaScript 객체 모델입니다. React는 UI 변경이 발생하면 이 가상 DOM을 업데이트한 후, 변경된 부분만 실제 DOM에 반영합니다. 이 방식의 장점은 다음과 같습니다: 빠른 연산 가능: 가상 DOM은 메모리에서 동작하므로 계산 속도가 빠릅니다. 효율적인 업데이트: 변경 사항을 비교하여 최소한의 DOM 업데이트만 수행합니다. 예측 가능성 향상: 선언적 UI 모델을 유지하면서도 최적화된 성능을 제공합니다. 작동 방식 가상 DOM은 다음과 같은 과정을 거쳐 렌더링을 최적화합니다. UI 변경 감지: React는 컴포넌트의 상태(state)나 속성(props)이 변경되면 새로운 가상 DOM을 생성합니다. Diffing 알고리즘 적용: 이전 가상 DOM과 새로운 가상 DOM을 비교하여 변경된 요소를 찾습니다. 최소한의 변경만 반영: 변경된 부분만 실제 DOM에 적용하여 성능을 최적화합니다. 이 과정은 React의 핵심 알고리즘인 Reconciliation(조정 과정)을 기반으로 작동합니다. 시각적 다이어그램 다음 다이어그램은 가상 DOM의 작동 과정을 나타냅니다. 🖥️ UI 변경 감지 ┌─────────────────────────────────┐ │ UI 변경 감지 │ │ (컴포넌트의 상태/props 변경 감지) │ └─────────────────────────────────┘ │ ▼ ⚙️ 가상 DOM 업데이트 ┌─────────────────────────────────┐ │ 가상 DOM 생성 및 업데이트 │ └─────────────────────────────────┘ │ ▼ 🔍 Diffing 알고리즘 적용 ┌─────────────────────────────────┐ │ 이전 가상 DOM과 비교하여 │ │ 변경된 요소 도출 │ └─────────────────────────────────┘ │ ▼ 💻 최소 변경 반영 (실제 DOM) ┌─────────────────────────────────┐ │ 변경된 부분만 실제 DOM에 반영 │ └─────────────────────────────────┘ 위 과정에서 가장 중요한 것은 Diffing 알고리즘입니다. React는 key 속성을 활용하여 변경된 노드를 빠르게 찾고, 효율적으로 업데이트할 수 있도록 설계되어 있습니다. 코드 예제 React의 가상 DOM을 활용하는 간단한 예제입니다. JavaScript import React, { useState } from 'react'; function Counter() { const [count, setCount] = useState(0); return ( <div> <p>현재 카운트: {count}</p> <button onClick={() => setCount(count + 1)}>증가</button> </div> ); } export default Counter; 이 코드에서 setCount 를 호출하면 React는 새로운 가상 DOM을 생성하고, 이전 상태와 비교하여 변경된 부분만 실제 DOM에 반영합니다. 가상 DOM은 Redux나 MobX 같은 상태 관리 라이브러리와 함께 사용하면 더욱 강력한 성능을 발휘할 수 있습니다. 이러한 라이브러리는 상태(state) 변경을 추적하고, 변경된 데이터를 기반으로 UI를 효율적으로 업데이트하는 역할을 합니다. 도메인 문서 예시 다음은 특정 도메인에 대한 이해를 돕는 문서입니다. 독자가 도메인을 깊이 이해할 수 있도록 돕는 것이 핵심입니다. 아래 예시를 참고하여 본인의 서비스가 다루는 도메인(예: 커머스, 금융, 소셜 미디어 등)에 대한 문서를 작성해 보세요. 커머스 도메인 이해하기 이 문서는 커머스 도메인의 개념과 흐름을 설명합니다. 커머스 도메인은 전자상거래 시스템에서 상품 판매, 주문 처리, 결제, 배송 등 거래 과정을 다루는 핵심 개념입니다. 효율적인 커머스 시스템을 구축하려면 도메인의 구조와 주요 요소를 이해하는 것이 중요합니다. 기본 개념 커머스 도메인은 다음과 같은 핵심 요소로 구성됩니다. 상품(Product): 판매하는 물품이나 서비스 주문(Order): 소비자가 상품을 구매하는 행위 결제(Payment): 거래 대금을 처리하는 과정 배송(Shipping): 상품을 소비자에게 전달하는 절차 소비자(Consumer): 상품이나 서비스를 구매하는 사람 판매자(Seller): 상품이나 서비스를 제공하는 사람 이 요소들은 서로 긴밀하게 연결되어 있습니다. 예를 들어, 주문(Order)이 생성되면 결제(Payment)가 진행되고, 결제가 완료되면 배송(Shipping) 절차가 시작됩니다. 유저 행동 흐름 소비자가 상품을 구매할 때의 주요 과정은 다음과 같습니다. 상품 탐색 및 장바구니 추가: 소비자는 원하는 상품을 선택하고 장바구니에 추가합니다. 주문 및 결제 진행: 장바구니에서 주문을 확정하고 결제를 진행합니다. 결제 승인 및 주문 확정: 결제가 승인되면 주문이 확정됩니다. 배송 준비 및 출고: 판매자가 주문을 확인하고 상품을 포장한 후 출고합니다. 소비자에게 상품 도착: 택배 또는 다른 배송 수단을 통해 소비자에게 상품이 전달됩니다. 이 과정에서 결제 승인 오류, 재고 부족, 배송 지연 등의 예외 상황이 발생할 수 있습니다. 개발자는 이런 예외 처리를 고려해 로직을 설계해야 합니다. 도메인 흐름도 아래 흐름도는 커머스 도메인의 주요 요소들이 어떻게 연결되는지 보여줍니다. 🛍️ 소비자 (Consumer) │ ┌─────────────┴─────────────┐ ▼ ▼ ┌───────────────┐ ┌───────────────┐ │ 장바구니 (Cart) │ ──▶ │ 주문 (Order) │ └───────────────┘ └───────────────┘ │ │ ▼ ▼ ┌───────────────┐ ┌───────────────┐ │ 상품 (Product) │ │ 결제 (Payment) │ └───────────────┘ └───────────────┘ │ │ ▼ ▼ ┌───────────────┐ ┌───────────────┐ │재고 (Inventory)│ │ 배송 (Shipping)│ └───────────────┘ └───────────────┘ │ ▼ ┌───────────────┐ │ 판매자 (Seller) │ └───────────────┘",
      "frontmatter": {
        "date": "2025-07-02T04:32:34+09:00",
        "lastmod": "2025-08-21T13:24:32+09:00"
      }
    },
    "android studio build sequence": {
      "path": "/android-studio-build-sequence/",
      "filename": "android studio build sequence",
      "content": "네, 제공해주신 Android Gradle 빌드 로그의 각 단계를 매우 자세하게 설명해 드리겠습니다. 이 로그는 Android Studio에서 'Run' 버튼을 누르거나 터미널에서 ./gradlew assembleDebug 명령을 실행했을 때 나타나는 과정입니다. 핵심 개념: 증분 빌드 (Incremental Build) 와 UP-TO-DATE 로그를 이해하기 전에 가장 중요한 개념은 Gradle의 증분 빌드 기능입니다. 정의: Gradle은 각 작업(Task)의 입력(input)과 출력(output)을 기억합니다. 다음 빌드를 실행할 때, Gradle은 각 작업의 입력 파일(예: 소스 코드, 리소스 파일)이 마지막 빌드 이후 변경되었는지 확인합니다. UP-TO-DATE 의 의미: 만약 입력 파일에 아무런 변경이 없다면, Gradle은 해당 작업을 다시 실행하는 대신 이전에 생성했던 출력물을 그대로 재사용합니다. 이때 로그에 UP-TO-DATE 라고 표시됩니다. 결과: 이 기능 덕분에 코드를 전혀 수정하지 않고 다시 빌드하면, 모든 작업이 UP-TO-DATE 가 되어 빌드가 매우 빠르게(로그에서는 1초) 완료됩니다. 빌드 단계별 상세 설명 빌드 과정은 크게 리소스 처리 → 매니페스트 처리 → 코드 컴파일 → DEX 변환 → APK 패키징의 순서로 진행됩니다. 1단계: 준비 및 설정 (Preparation & Setup) > Task :app:preBuild & > Task :app:preDebugBuild 역할: 본격적인 빌드 전에 필요한 준비 작업을 수행합니다. 예를 들어, 빌드 과정에서 생성될 파일들을 저장할 폴더( build/ 디렉토리)를 만들거나 초기 환경 설정을 합니다. preDebugBuild 는 'debug' 빌드 유형에 특화된 준비 작업입니다. > Task :app:checkKotlinGradlePluginConfigurationErrors 역할: Kotlin Gradle 플러그인 설정에 오류가 없는지 확인합니다. SKIPPED : 이 작업은 특정 조건에서만 실행되거나, 필요 없다고 판단되면 건너뜁니다. 문제가 있는 상태가 아닙니다. 2단계: 리소스 처리 (Resource Processing) 이 단계에서는 앱의 모든 리소스(레이아웃 XML, 이미지, 문자열 등)를 처리합니다. 주로 aapt2 도구가 사용됩니다. > Task :app:checkDebugAarMetadata 역할: 프로젝트가 의존하는 라이브러리( .aar 파일)들의 메타데이터가 올바른지 확인합니다. > Task :app:processDebugNavigationResources & > Task :app:compileDebugNavigationResources 역할: Android Jetpack의 Navigation Component를 사용하는 경우, res/navigation 폴더 안의 XML 파일들을 처리하고 컴파일합니다. > Task :app:generateDebugResValues & > Task :app:generateDebugResources 역할: build.gradle 파일에 정의된 resValue 같은 동적으로 생성되는 리소스 값들을 실제 리소스 파일로 만듭니다. > Task :app:mergeDebugResources 역할: 매우 중요한 단계입니다. 프로젝트의 기본 리소스( src/main/res ), 디버그용 리소스( src/debug/res ), 그리고 모든 라이브러리(AAR)에 포함된 리소스들을 하나로 합쳐서 단일 폴더에 모읍니다. 만약 같은 이름의 리소스가 여러 곳에 있다면, 정해진 우선순위에 따라 하나를 선택합니다. > Task :app:packageDebugResources 역할: aapt2 가 본격적으로 동작하는 핵심 단계입니다. mergeDebugResources 에서 합쳐진 모든 리소스를 컴파일하고 연결(link)합니다. 이 과정에서 R.java 파일이 생성되고, 모든 리소스가 바이너리 형식으로 포함된 resources.ap_ 라는 중간 결과물이 만들어집니다. > Task :app:parseDebugLocalResources 역할: packageDebugResources 이후 생성된 리소스들을 파싱하여 다음 단계를 준비합니다. 3단계: 매니페스트 처리 (Manifest Processing) > Task :app:createDebugCompatibleScreenManifests 역할: 앱의 화면 호환성(screen compatibility)과 관련된 매니페스트 조각을 생성합니다. > Task :app:extractDeepLinksDebug 역할: 매니페스트와 코드 내 어노테이션에서 딥링크(Deep Link) 정보를 추출합니다. > Task :app:processDebugMainManifest , > Task :app:processDebugManifest , > Task :app:processDebugManifestForPackage 역할: 리소스 병합과 유사하게 AndroidManifest.xml 파일을 병합합니다. src/main/AndroidManifest.xml 을 기본으로, 라이브러리의 매니페스트와 src/debug/ 폴더의 매니페스트 조각들을 합칩니다. 이 과정에서 build.gradle 에 정의된 applicationId , versionCode 같은 플레이스홀더 값들을 실제 값으로 교체하여 최종 매니페스트를 완성합니다. 4단계: 코드 컴파일 및 DEX 변환 (Code Compilation & Dexing) 이 단계에서는 Kotlin/Java 소스 코드를 컴파일하고, Android 런타임이 이해할 수 있는 DEX 파일로 변환합니다. > Task :app:compileDebugKotlin 역할: Kotlin 컴파일러( kotlinc )를 사용하여 프로젝트의 모든 .kt 파일을 .class 파일(자바 바이트코드)로 컴파일합니다. > Task :app:compileDebugJavaWithJavac 역할: Java 컴파일러( javac )를 사용하여 .java 파일을 .class 파일로 컴파일합니다. NO-SOURCE : 이 작업은 실행되었지만, 컴파일할 .java 소스 파일이 하나도 없었다는 의미입니다. 프로젝트가 100% Kotlin으로 작성되었을 가능성이 높습니다. > Task :app:mergeDebugShaders , > Task :app:compileDebugShaders 역hal: OpenGL 셰이더( .glsl 파일)가 있다면 컴파일하고 병합합니다. (여기서는 NO-SOURCE 이므로 셰이더 파일이 없습니다.) > Task :app:mergeDebugAssets 역할: assets 폴더의 내용물을 병합합니다. > Task :app:checkDebugDuplicateClasses 역할: 컴파일된 클래스 파일과 라이브러리들 사이에 중복된 클래스가 있는지 검사합니다. 중복 클래스는 앱 실행 시 충돌을 일으킬 수 있으므로 중요한 검증 과정입니다. > Task :app:desugarDebugFileDependencies 역할: Java 8 이상의 최신 언어 기능(람다, 스트림 API 등)을 이전 버전의 Android에서도 동작하도록 변환(Desugaring)하는 준비 작업을 합니다. d8 도구가 이 역할을 수행합니다. > Task :app:dexBuilderDebug , > Task :app:mergeProjectDexDebug , > Task :app:mergeExtDexDebug , > Task :app:mergeLibDexDebug 역할: DEXing 단계입니다. d8 도구를 사용하여 프로젝트의 모든 .class 파일(내 코드 + 라이브러리 코드)을 Dalvik Executable( .dex ) 파일로 변환합니다. .dex 파일은 Android 런타임(ART)이 실행하는 파일 형식입니다. 클래스 파일이 매우 많으면 여러 개의 .dex 파일로 나뉠 수 있으며, 이 작업들은 그것들을 병합하는 역할도 합니다. 5단계: 네이티브 코드 및 최종 패키징 (Native Code & Final Packaging) 이제 모든 재료(컴파일된 리소스, DEX 파일, 매니페스트 등)를 모아 최종 APK 파일을 만듭니다. > Task :app:mergeDebugJniLibFolders & > Task :app:mergeDebugNativeLibs 역할: C/C++로 작성된 네이티브 라이브러리( .so 파일)가 있다면, 모든 소스(내 프로젝트, 라이브러리)에서 가져와 아키텍처별(arm64-v8a, x86_64 등)로 정리하여 병합합니다. > Task :app:stripDebugDebugSymbols 역할: 네이티브 라이브러리에서 디버깅 심볼을 제거하여 파일 크기를 줄입니다. (디버그 빌드에서는 보통 이 작업이 최소한으로 수행되거나 건너뜁니다.) > Task :app:validateSigningDebug 역할: APK에 서명할 준비가 되었는지 확인합니다. 디버그 빌드의 경우, 보통 SDK에 포함된 기본 디버그 키스토어로 자동 서명됩니다. > Task :app:packageDebug 역할: 최종 APK 생성 단계입니다. 지금까지 만들어진 모든 결과물—컴파일된 리소스( resources.ap_ ), DEX 파일, 최종 매니페스트, 네이티브 라이브러리( .so ), assets —을 하나의 압축 파일( .apk )로 묶습니다. 이 단계에서 apksigner 를 사용하여 디버그 키로 APK에 서명하고, zipalign 으로 최적화하는 과정도 포함됩니다. > Task :app:createDebugApkListingFileRedirect 역할: 생성된 APK 파일의 내용물 목록을 담은 메타데이터 파일을 생성합니다. > Task :app:assembleDebug 역할: 최종 집계(aggregator) 작업입니다. 이 작업 자체는 아무 일도 하지 않지만, 디버그 APK를 만드는 데 필요한 위의 모든 작업들에 대한 의존성을 가지고 있습니다. 따라서 assembleDebug 작업이 성공적으로 완료되었다는 것은 디버그 APK가 성공적으로 생성되었음을 의미합니다. 최종 결과 BUILD SUCCESSFUL in 1s : 빌드가 1초 만에 성공적으로 완료되었습니다. 36 actionable tasks: 36 up-to-date : Gradle이 실행해야 할 작업은 총 36개였지만, 입력 파일이 변경되지 않았기 때문에 36개 모두 이전 결과물을 재사용하여 작업을 완료했다는 의미입니다. Build Analyzer results available : Android Studio는 이 빌드 과정의 성능을 분석한 결과를 제공합니다. 빌드가 느릴 때 원인을 파악하는 데 유용한 도구입니다.",
      "frontmatter": {
        "tags": [
          "ai-content"
        ],
        "date": "2025-09-08T15:10:44+09:00",
        "lastmod": "2025-10-13T13:22:05+09:00"
      }
    },
    "android 이벤트 처리 코드 분리": {
      "path": "/android-이벤트-처리-코드-분리/",
      "filename": "android 이벤트 처리 코드 분리",
      "content": "네, 말씀하신 내용이 바로 '관심사의 분리(Separation of Concerns)' 원칙을 UI 계층에 적용하는 아주 훌륭한 접근 방식입니다. Activity/Fragment는 오케스트라의 지휘자처럼 어떤 뷰와 어떤 로직 핸들러(ViewModel)를 연결할지 '조율'만 하고, 실제 이벤트 처리 로직은 모두 ViewModel에, UI 내부의 세부 이벤트 감지는 커스텀 뷰 자체에 위임하는 구조입니다. 이 목표를 달성하기 위한 가장 이상적이고 현대적인 방법을 단계별로 설명해 드리겠습니다. 핵심 전략: Custom View + Callback Interface + ViewModel Custom View: 내부에 포함된 여러 위젯(ImageView, TextView 등)의 이벤트를 감지합니다. Callback Interface: Custom View에서 발생한 이벤트를 외부로 알리기 위한 '약속(Contract)'입니다. \"프로필 이미지가 눌렸다\", \"팔로우 버튼이 눌렸다\" 와 같은 의미 있는(semantic) 이벤트로 추상화합니다. ViewModel: Callback Interface를 구현하여 실제 비즈니스 로직을 처리합니다. Activity/Fragment: Custom View와 ViewModel을 연결(setup)하는 역할만 담당합니다. 단계별 구현 방법 간단한 '프로필 뷰'를 예시로 들어보겠습니다. 이 뷰는 프로필 이미지와 팔로우 버튼으로 구성됩니다. Step 1: 이벤트 콜백 인터페이스 정의 (계약서 작성) 어떤 이벤트가 발생할 수 있는지 명확하게 정의하는 인터페이스를 만듭니다. // ProfileViewEvents.kt interface ProfileViewEvents { fun onProfileImageClicked(userId: String) fun onFollowButtonClicked(userId: String) } onClick 같은 일반적인 이름 대신 onProfileImageClicked 처럼 의미를 담아 정의하는 것이 중요합니다. Step 2: 커스텀 뷰(Custom View) 제작 이 뷰는 내부의 클릭 이벤트를 감지하고, 외부로 약속된 인터페이스(ProfileViewEvents)를 통해 이벤트를 전달하는 역할만 합니다. <!-- view_profile.xml (커스텀 뷰의 레이아웃) --> <merge xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:app=\"http://schemas.android.com/apk/res-auto\" xmlns:tools=\"http://schemas.android.com/tools\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" tools:parentTag=\"androidx.constraintlayout.widget.ConstraintLayout\"> <ImageView android:id=\"@+id/profile_image\" android:layout_width=\"80dp\" android:layout_height=\"80dp\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" tools:src=\"@tools:sample/avatars\" /> <Button android:id=\"@+id/follow_button\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"Follow\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" /> </merge> // ProfileView.kt (커스텀 뷰 클래스) class ProfileView @JvmOverloads constructor( context: Context, attrs: AttributeSet? = null, defStyleAttr: Int = 0 ) : ConstraintLayout(context, attrs, defStyleAttr) { private val binding: ViewProfileBinding private var eventListener: ProfileViewEvents? = null private var userId: String = \"default_user\" // 예시 데이터 init { binding = ViewProfileBinding.inflate(LayoutInflater.from(context), this, true) setupInternalClicks() } // 뷰 내의 세부적인 클릭 리스너 설정 private fun setupInternalClicks() { binding.profileImage.setOnClickListener { // 외부로 약속된 이벤트를 호출 eventListener?.onProfileImageClicked(userId) } binding.followButton.setOnClickListener { // 외부로 약속된 이벤트를 호출 eventListener?.onFollowButtonClicked(userId) } } // 데이터를 설정하는 함수 fun setProfileData(user: User) { this.userId = user.id // Glide, Coil 등으로 이미지 로드 // binding.userName.text = user.name } // 외부에서 이벤트 핸들러(ViewModel)를 주입받는 통로 fun setEventListener(listener: ProfileViewEvents) { this.eventListener = listener } } Step 3: ViewModel에서 이벤트 핸들러 구현 ViewModel은 View가 무슨 모양인지 전혀 몰라도 됩니다. 오직 ProfileViewEvents 라는 약속만 보고 로직을 구현합니다. // ProfileViewModel.kt class ProfileViewModel : ViewModel(), ProfileViewEvents { // 인터페이스 구현 private val _toastMessage = MutableLiveData<Event<String>>() val toastMessage: LiveData<Event<String>> = _toastMessage // 약속된 이벤트가 호출되면 실행될 실제 로직 override fun onProfileImageClicked(userId: String) { _toastMessage.value = Event(\"프로필 이미지 클릭: $userId\") // ex) 프로필 상세 화면으로 이동하는 로직 트리거 } override fun onFollowButtonClicked(userId: String) { // ex) API 호출하여 팔로우/언팔로우 처리 _toastMessage.value = Event(\"팔로우 버튼 클릭: $userId\") } } Step 4: Activity/Fragment에서 '조율' 작업 Activity는 그저 지휘자입니다. ProfileView 와 ProfileViewModel 을 연결해 주기만 합니다. 이벤트 처리 로직은 단 한 줄도 없습니다. <!-- activity_main.xml --> <LinearLayout ...> <com.example.yourapp.ProfileView android:id=\"@+id/profile_view\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" /> </LinearLayout> // MainActivity.kt class MainActivity : AppCompatActivity() { private lateinit var binding: ActivityMainBinding private val viewModel: ProfileViewModel by viewModels() override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) binding = ActivityMainBinding.inflate(layoutInflater) setContentView(binding.root) // --- 지휘자의 역할: 뷰와 뷰모델 연결 (Setup) --- binding.profileView.setEventListener(viewModel) // --------------------------------------------- // 데이터가 있다면 뷰에 설정 val user = User(id = \"user-123\", name = \"Android\") binding.profileView.setProfileData(user) observeViewModel() } // ViewModel의 상태 변화를 감지하여 UI에 반영 private fun observeViewModel() { viewModel.toastMessage.observe(this, EventObserver { message -> Toast.makeText(this, message, Toast.LENGTH_SHORT).show() }) } } 이 구조의 강력한 장점 완벽한 책임 분리: Activity: 뷰와 로직의 연결, 생명주기 관리. Custom View: 자기 내부의 UI 이벤트 감지 및 외부로 추상화된 이벤트 전달. ViewModel: \"무엇을 할 것인가\"에 대한 비즈니스 로직만 담당. 재사용성 극대화: ProfileView 는 어떤 화면에서든 가져다 쓸 수 있는 독립적인 부품이 됩니다. ProfileViewModel 의 로직도 다른 뷰에서 재사용될 수 있습니다. 테스트 용이성: ProfileViewModel 은 Android 프레임워크에 대한 의존성이 거의 없으므로(인터페이스만 구현), JVM 환경에서 손쉽게 유닛 테스트를 할 수 있습니다. 가독성 및 유지보수성: MainActivity 는 매우 간결해지며, 전체적인 구조를 파악하기 쉬워집니다. 새로운 기능을 추가하거나 수정할 때 어디를 봐야 할지 명확해집니다. 심화: 데이터 바인딩으로 연결(Setup) 코드도 제거하기 데이터 바인딩을 사용하면 Activity의 setEventListener 호출 코드마저 XML로 옮겨 완전히 자동화할 수 있습니다. BindingAdapter 생성 // ProfileViewBindingAdapter.kt @BindingAdapter(\"onProfileEvent\") fun setOnProfileEvent(view: ProfileView, listener: ProfileViewEvents) { view.setEventListener(listener) } XML 레이아웃 수정 <!-- activity_main.xml --> <layout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:app=\"http://schemas.android.com/apk/res-auto\"> <data> <variable name=\"viewModel\" type=\"com.example.yourapp.ProfileViewModel\" /> </data> <LinearLayout ...> <com.example.yourapp.ProfileView android:id=\"@+id/profile_view\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" app:onProfileEvent=\"@{viewModel}\" /> <!-- ViewModel을 리스너로 직접 바인딩 --> </LinearLayout> </layout> Activity 수정 binding.profileView.setEventListener(viewModel) 코드가 필요 없어집니다. // MainActivity.kt // ... binding.viewModel = viewModel // 바인딩 변수에 뷰모델 할당 // binding.profileView.setEventListener(viewModel) <- 이 줄 삭제! 이 방법을 사용하면 Activity는 그야말로 뷰-모델 바인딩과 생명주기 관리만 하는, 매우 이상적인 'Dumb View'가 됩니다.",
      "frontmatter": {
        "date": "2025-10-16T16:04:39+09:00",
        "lastmod": "2025-10-16T16:09:26+09:00"
      }
    },
    "git checkout에서 restore, switch 변경": {
      "path": "/git-checkout에서-restore-switch-변경/",
      "filename": "git checkout에서 restore, switch 변경",
      "content": "",
      "frontmatter": {
        "tags": [
          "git"
        ],
        "date": "2025-10-23T05:24:22+09:00",
        "lastmod": "2025-10-23T05:24:44+09:00"
      }
    },
    "github workflow": {
      "path": "/github-workflow/",
      "filename": "github workflow",
      "content": "workflow 기본 event : 하나 이상의 이벤트 job : 하나 이상의 일 ( job 개별로 runner(가상머신) ) 워크플로 저장소에서 발생하는 이벤트 GitHub 외부에서 발생하여 repository_dispatch GitHub에서 이벤트를 트리거하는 이벤트 예정된 시간 수동",
      "frontmatter": {
        "date": "2025-10-27T07:17:22+09:00",
        "lastmod": "2025-10-28T04:36:07+09:00"
      }
    },
    "html 화면 z 축": {
      "path": "/html-화면-z-축/",
      "filename": "html 화면 z 축",
      "content": "✦ 네, 더 자세히 설명해 드리겠습니다. 이 현상은 웹 브라우저가 요소를 화면에 그리는 방식, 특히 스태킹 컨텍스트(Stacking Context)와 z-index 라는 개념 때문에 발생합니다. 그리고 검색 결과가 가리지 않는 이유는 검색창과 검색 결과 항목의 CSS position 속성 차이 때문입니다. 현상이 발생하는 원리 - 왜 검색창만 가리는가? 웹 페이지의 요소들은 단순히 2차원 평면에 있는 것이 아니라, 3차원 공간처럼 서로 겹쳐질 수 있는 '레이어(layer)' 구조를 가집니다. 어떤 요소가 다른 요소의 위나 아래에 그려질지 결정하는 규칙이 바로 '스태킹(쌓임) 순서'입니다. 기본 쌓임 순서: 기본적으로 HTML 코드에서 나중에 나오는 요소가 먼저 나온 요소보다 위에 쌓입니다. 스태킹 컨텍스트의 형성: 특정 CSS 속성은 요소와 그 자식들을 위한 '독립된 레이어 그룹'을 만드는데, 이를 스태킹 컨텍스트라고 합니다. 대표적으로 position 속성이 relative , absolute , fixed , sticky 이면서 z-index 가 auto 가 아닌 값을 가질 때 (또는 z-index 가 없더라도 특정 조건에서), 또는 opacity 가 1 미만일 때 등이 스태킹 컨텍스트를 형성합니다. 중요한 점은 position: relative 만으로도 z-index 없이 스태킹 컨텍스트를 형성할 수 있으며, 이는 때때로 예상치 못한 쌓임 순서를 야기할 수 있다는 것입니다. 현재 문제 상황 분석: 탭 바 ( <nav> 또는 div ): position: sticky 속성을 가지고 있습니다. 이 속성은 스크롤될 때 화면 특정 위치에 '달라붙게' 만듭니다. 하지만 z-index 값이 명시적으로 지정되지 않았습니다. sticky 요소는 스태킹 컨텍스트를 형성하지만, z-index: auto 일 경우, 다른 요소들과의 쌓임 순서가 모호해질 수 있습니다. 특히 아래 설명할 position: relative 요소와 만났을 때 브라우저 렌더링 엔진이 헷갈리는 경우가 발생합니다. 검색창 ( <input> ): .src/js/ui/search-ui.js 에서 생성되는 검색창은 다음과 같은 구조를 가집니다: <div class=\"mb-4\"> <div class=\"relative\"> <!-- ★★★ 바로 이 부분입니다! position: relative이 적용됨 ★★★ --> <input type=\"text\" id=\"searchInput\" ...> ... </div> </div> class=\"relative\" 는 해당 div 에 position: relative; 속성을 적용합니다. 이 position: relative 속성은 z-index 가 명시되지 않더라도 새로운 스태킹 컨텍스트를 형성합니다. 검색 결과 ( <div> ): 검색 결과 항목들은 position 속성이 명시적으로 지정되어 있지 않습니다. 즉, 기본값인 position: static 을 가집니다. static 요소는 새로운 스태킹 컨텍스트를 형성하지 않으며, 일반적인 문서 흐름과 쌓임 순서를 따릅니다. 겹침 발생 원인 (검색창만 해당): 사이드바 내부에서 스크롤이 발생하면, '스크롤되는 콘텐츠' 영역(검색창 포함)과 '상단에 고정된' 영역(탭 바)이 겹치게 됩니다. 이때, z-index 가 없는 position: sticky 탭 바와 z-index 가 없는 position: relative 검색창 컨테이너가 만나게 됩니다. 이 두 요소 모두 스태킹 컨텍스트를 형성하지만, 어느 쪽이 더 높은지 명확한 우선순위가 없습니다. 이러한 모호한 상황에서 브라우저의 렌더링 엔진은 position: relative 를 가진 요소를 (비록 z-index 가 auto 여도) position: sticky 만 있는 요소보다 더 '앞으로' 가져오려는 경향이 있습니다. 따라서 스크롤되어 움직이는 검색창이 sticky로 고정만 되어있는 탭 바보다 높은 레이어로 판단되어 위로 올라타는 현상이 발생하는 것입니다. 검색 결과는 position: static 이므로 스태킹 컨텍스트를 형성하지 않고, 일반적인 문서 흐름에 따라 sticky 요소의 아래로 자연스럽게 지나갑니다. 그래서 검색 결과는 탭 바를 가리지 않는 것입니다. 쉽게 비유하자면, 투명한 필름(요소) 여러 장을 겹쳐 놓았는데, 각 필름의 상하 관계( z-index )를 명확히 지정해주지 않은 상황과 같습니다. 특히, '강력한 접착제'( position: relative 또는 absolute 같은 속성)를 사용한 필름이 '약한 접착제'( position: sticky 만 있는 경우)를 사용한 필름보다 더 튀어나와 보이는 경향이 있는 것입니다. 코드를 변경하면 해결되는 원리 제안 드린 해결책은 탭 바의 쌓임 순서를 가장 명확하고 강력하게 지정하는 것입니다. z-10 클래스 추가: index.html 의 sticky 속성을 가진 탭 바 div 에 z-10 클래스를 추가하면, 이 요소에는 z-index: 10; 이라는 CSS 속성이 적용됩니다. 명시적인 스태킹 컨텍스트 형성: position: sticky 와 z-index: 10 이 함께 적용되면서, 탭 바는 '레벨 10'의 가장 확실하고 강력한 독립된 스태킹 컨텍스트를 형성합니다. 쌓임 순서 정리: 탭 바: 이제 '레벨 10'에 위치하게 됩니다. 검색창 컨테이너: position: relative 로 스태킹 컨텍스트를 형성하지만, z-index 가 명시되지 않았으므로 '레벨 0' (기본 레벨)에 머무릅니다. 브라우저의 기본 규칙에 따라 z-index: auto 는 명시된 z-index 값보다 낮은 것으로 간주됩니다. 검색 결과: position: static 이므로 스태킹 컨텍스트를 형성하지 않으며, z-index 는 적용되지 않습니다. '레벨 0'보다도 더 기본층에 속합니다. 결과: 이제 브라우저는 명확한 지시를 받았습니다. 스크롤이 일어나 검색창이나 검색 결과가 탭 바 위치로 올라오더라도, '레벨 0' 이하에 있는 검색창과 검색 결과는 '레벨 10'에 있는 탭 바의 아래를 지나가게 됩니다. 따라서 더 이상 탭 바를 덮는 현상이 발생하지 않습니다. 결론적으로, z-index 를 추가하는 것은 \"이 탭 바는 다른 어떤 요소보다 항상 위에 있어야 해!\"라고 브라우저에게 명확하게 알려주는 행위이며, 이를 통해 position 속성으로 인해 발생할 수 있는 모든 모호했던 요소 간의 상하 관계를 가장 확실하게 바로잡는 것입니다. position: relative 가 적용된 검색창뿐만 아니라, 향후 다른 position 속성을 가진 요소들이 추가되더라도 탭 바는 항상 최상단에 유지될 것입니다.",
      "frontmatter": {
        "tags": [
          "web"
        ],
        "date": "2025-09-11T13:46:07+09:00",
        "lastmod": "2025-10-13T13:36:25+09:00"
      }
    },
    "hugo URL 관련 method": {
      "path": "/hugo-url-관련-method/",
      "filename": "hugo URL 관련 method",
      "content": "Hugo의 baseURL 은 사이트의 기본 URL을 설정하는 중요한 구성 옵션이지만, 실제 템플릿 작성이나 다양한 배포 환경을 관리할 때는 baseURL 을 직접 사용하는 것보다 더 유연하고 효율적인 여러 방법들이 있습니다. baseURL 을 대체하거나 함께 사용할 수 있는 모든 메소드들은 크게 템플릿 함수, 사이트 구성 설정, 그리고 환경 변수 및 명령줄 플래그로 나눌 수 있습니다. 템플릿 함수 (Template Functions) 템플릿 내에서 URL을 생성할 때, 하드코딩된 경로 대신 Hugo가 제공하는 내장 함수를 사용하는 것이 가장 좋은 방법입니다. 이 함수들은 baseURL 설정을 자동으로 참조하여 올바른 URL을 생성해 줍니다. ` | 함수 | 설명 | 사용 예시 | | | -------------------- | -------------------------------------------------------------- | ------------------------------------------------------- | -------------------- | absURL ** | 입력된 경로를 절대 URL로 변환합니다. 사이트의 baseURL 을 기준으로 전체 URL을 생성합니다. | {{ \"css/style.css\" absURL }} relURL ** | 입력된 경로를 상대 URL로 변환합니다. baseURL 에 서브디렉토리가 포함된 경우에도 올바르게 작동합니다. | {{ \"about/\" relURL }} absLangURL ** | 다국어 사이트에서 현재 언어 코드(예: /en/ )를 포함한 절대 URL을 생성합니다. | {{ \"posts/my-post/\" absLangURL }} relLangURL ** | 다국어 사이트에서 현재 언어 코드를 포함한 상대 URL을 생성합니다. | {{ \"categories/tech/\" relLangURL }} ref ** | 다른 페이지에 대한 절대 퍼머링크(permalink)를 생성합니다. 페이지의 경로를 인수로 받습니다. | {{ ref . \"about.md\" }} relRef ** | 다른 페이지에 대한 상대 퍼머링크를 생성합니다. | {{ relRef . \"about.md\" }} .Permalink ** | 현재 페이지의 절대 URL을 반환합니다. | {{ .Title }} .RelPermalink ** | 현재 페이지의 상대 URL을 반환합니다. | {{ .Title }} urls.JoinPath ** | 여러 경로 요소를 결합하여 하나의 URL 문자열로 만듭니다. | {{ urls.JoinPath .Site.BaseURL \"images\" \"logo.png\" }} urls.Parse ** | 주어진 URL을 파싱하여 구조화된 객체로 반환합니다. | {{ $url := urls.Parse \"https://example.com/docs\" }} urls.URLize ** | 주어진 문자열을 URL에 사용하기 안전한 형태로 변환합니다. | {{ .Title urls.URLize }} urls.Anchorize ** | 주어진 문자열을 HTML의 id 속성값으로 사용하기 적합하게 변환합니다. | hugo.toml hugo.toml (또는 config.toml ) 파일에서 URL 동작을 제어하는 몇 가지 설정이 있습니다. 이 설정들은 baseURL canonifyURLs **: true 로 설정하면 모든 상대 URL을 baseURL 을 기준으로 절대 URL로 변환합니다. 하지만 이 옵션은 레거시 기능으로 간주되며, absURL relativeURLs **: true 로 설정하면 모든 상대 URL을 현재 콘텐츠에 대한 상대적인 경로로 재작성합니다. 예를 들어, /about/ 페이지에 있는 /css/style.css 링크는 ../css/style.css baseURL \u0002PROTECTED0\u0003 ### 환경 변수 및 명령줄 플래그 baseURL -b 또는 --baseURL )**: hugo 또는 hugo server 명령을 실행할 때 baseURL 을 직접 지정할 수 있습니다. 이 방법은 hugo.toml \u0002PROTECTED1\u0003 HUGOBASEURL )**: HUGOBASEURL 환경 변수를 설정하여 baseURL \u0002PROTECTED2\u0003 baseURL 을 중심으로 다양한 URL 관리 방법을 제공하여, 개발자가 어떤 환경에서도 유연하게 대처할 수 있도록 지원합니다. 템플릿에서는 가급적 absURL 이나 relURL`과 같은 함수를 사용하고, 배포 환경에 따라서는 명령줄 플래그나 환경 변수를 활용하는 것이 현대적인 Hugo 개발 방식입니다.",
      "frontmatter": {
        "date": "2025-10-29T13:49:24+09:00",
        "lastmod": "2025-10-29T13:58:00+09:00"
      }
    },
    "hugo template 시스템 경우의 수로 비교한 자세한 설명": {
      "path": "/hugo-template-시스템-경우의-수로-비교한-자세한-설명/",
      "filename": "hugo template 시스템 경우의 수로 비교한 자세한 설명",
      "content": "전제 hugo 는 기본적으로 md 파일을 html 로 렌더링(컴파일) 하여 정적인 사이트를 만들수 있는 도구이다 이중 template 관련 개념이 복잡하여 이를 제대로 정리한다 md->html 변환 과정중 적절한 template 를 선택하여 적절한 html 로 만들때 사용하는 것이 template 이다 hugo 기본 전제 를 먼저 보고 오자 우선순위 일단 이해가 안되더라도 이것부터 보고 넘어가자 이 순서대로 우선순위가 적용된다 커스텀 레이아웃 프런트머터에 설정된 layout 값 페이지 종류(Page kinds) home , section , taxonomy , term , page 중 하나 표준 레이아웃 1 list 또는 single 출력 형식(Output format) html , rss 등 표준 레이아웃 2 all 언어(Language) en 등 미디어 타입(Media type) text/html 등 페이지 경로(Page path) 예: /blog/mypost 타입(Type) 프런트머터에 설정된 type 값 모든 경우의 수로 알아본 template 선택 시스템 실제 코드 template 우선순위 판단 코드 분석 이 시스템은 크게 주요 점수( w1 )와 두 단계의 동점자 처리 점수( w2 , w3 )로 나뉩니다. w1 점수: 주요 우선순위표 (The Main Scoreboard) w1 은 템플릿의 핵심적인 우선순위를 결정하는 가장 중요한 점수입니다. 이 점수가 높은 템플릿이 거의 항상 선택됩니다. 점수는 아래 항목들이 일치할 때마다 누적됩니다. (모든 유효한 템플릿은 기본 점수 1점으로 시작합니다)* 우선순위 추가 점수 일치 조건 (Attribute Match) 코드 내 상수 설명 1 +6점 커스텀 layout (Front Matter) weightcustomLayout 페이지의 Front Matter에 layout: \"mylayout\" 이 있고, 템플릿명이 mylayout.html 일 때. 가장 강력한 점수. 1 +6점 Render Hook 종류 (Variant 1) weightVariant1 Render Hook( _markup ) 템플릿이 렌더링 대상의 종류와 일치할 때. (예: 링크 렌더링 시 render-link.html ) 2 +5점 페이지 Kind weightKind 페이지의 종류( home , section , page 등)가 템플릿명과 일치할 때. (예: 홈페이지가 home.html 을 만난 경우) 3 +4점 표준 Layout ( single , list ) weightLayoutStandard 페이지의 표준 레이아웃( single , list )이 템플릿명과 일치할 때. (예: 단일 페이지가 single.html 을 만난 경우) 3 +4점 Output Format weightOutputFormat 페이지의 출력 형식( rss , json 등)이 템플릿명과 일치할 때. (예: RSS 피드가 *.rss.xml 을 만난 경우) 3 +4점 Render Hook 세부사항 (Variant 2) weightVariant2 Render Hook이 렌더링 대상의 세부 속성과 일치할 때. (예: Go 코드 블록이 render-codeblock-go.html 을 만난 경우) 4 +2점 all 레이아웃 weightLayoutAll 템플릿명이 만능 레이아웃인 all.html 일 때. 5 +1점 Language weightLang 페이지의 언어( en , ko 등)가 템플릿명과 일치할 때. 5 +1점 Media Type weightMediaType 페이지의 미디어 타입( text/html 등)이 템플릿의 확장자와 일치할 때. (예: HTML 페이지가 .html 템플릿을 만난 경우) 기본 +1점 (모든 유효 템플릿) w.w1++ 실격 처리 라운드를 통과한 모든 템플릿이 기본으로 받는 점수. w2 점수: 1차 동점자 처리표 (The First Tie-Breaker) w2 는 w1 점수가 동일할 때 사용되는 첫 번째 동점자 처리 점수입니다. 이 점수는 경로 특이성(Path Specificity)과 함께 사용되어, \"핵심적인 속성이 일치하는 템플릿이 경로 싸움에서 약간의 우위를 가진다\"는 의미를 부여합니다. w2 점수는 누적되지 않고, 아래 조건에 따라 값이 설정됩니다. 우선순위 w2 설정값 일치 조건 (Attribute Match) 코드 내 상수 역할 및 설명 1 2점 커스텀 layout (Front Matter) weight2Group2 layout Front Matter로 지정된, 가장 구체적인 종류의 일치가 발생했음을 나타냅니다. 2 1점 페이지 Kind 또는 표준 Layout weight2Group1 home , section , single , list , all 등 일반적이지만 중요한 속성이 일치했음을 나타냅니다. w3 점수: 2차 동점자 처리표 (The Final Tie-Breaker) w3 는 w1 점수와 경로 특이성까지 모두 동일한, 아주 드문 경우에 사용되는 최종 동점자 처리 점수입니다. w3 는 부가적인 속성들이 얼마나 많이 일치하는지를 세는 카운터 역할을 하며, 점수는 누적됩니다. 추가 점수 일치 조건 (Attribute Match) 코드 내 상수 역할 및 설명 1점 Language weight3 페이지와 템플릿의 언어가 일치하면 1점 추가. 1점 Output Format weight3 페이지와 템플릿의 출력 형식이 일치하면 1점 추가. 1점 Media Type weight3 페이지와 템플릿의 미디어 타입이 일치하면 1점 추가. 종합적인 결정 과정 요약 w1 점수 계산: 모든 후보 템플릿의 w1 점수를 계산합니다. 최고 w1 점수 확인: w1 점수가 가장 높은 템플릿(들)을 찾습니다. 동점자 처리 1 (경로): 최고 점수 템플릿이 여러 개일 경우, 페이지 경로와 가장 가까운(더 구체적인) 템플릿이 승리합니다. ( w2 점수는 이 과정에 영향을 줄 수 있습니다.) 동점자 처리 2 ( w3 ): 경로까지 완전히 동일한 템플릿이 있다면, w3 점수가 더 높은 템플릿이 최종 승자가 됩니다.",
      "frontmatter": {
        "tags": [
          "hugo"
        ],
        "series": "hugo",
        "series_weight": "5",
        "date": "2025-10-27T15:27:43+09:00",
        "lastmod": "2025-10-27T18:04:10+09:00"
      }
    },
    "hugo 기본 전제": {
      "path": "/hugo-기본-전제/",
      "filename": "hugo 기본 전제",
      "content": "hugo 는 기본적으로 md 파일을 html 로 렌더링(기본 렌더러 goldmark) 하여 정적인 사이트를 만들수 있는 도구 마크다운에는 frontmatter ( toml yaml 등 여러가지 형식을 지원 )를 통해 값을 넣을 수 있다 기본적으로 해석해서 사용하는 frontmatter 가 있으며 사용자가 자유롭게 정해서 사용할 수 있다 hugo 의 페이지의 종류는 5가지가 있다( home , section , taxonomy , term , page ) html 로 렌더링시에 template.html 을 만들어 분리해서 만들수 있다 hugo가 기본적으로 해석하는 frontmatter 세부적인것들은 차차 이해하기로 하고 이런 것들이 있다 이 정도로만 알아두자 핵심 메타데이터 title : 콘텐츠의 제목을 정의합니다. 이 값은 페이지의 제목 태그( <title> )와 콘텐츠 목록 등 다양한 곳에서 사용됩니다. description : 콘텐츠에 대한 간략한 설명입니다. 이 값은 주로 검색 엔진 최적화(SEO)를 위해 HTML의 메타 설명 태그에 사용됩니다. draft : true 로 설정하면 해당 콘텐츠는 hugo 명령어로 사이트를 빌드할 때 생성되지 않습니다. 아직 완성되지 않은 초안 상태의 글을 관리하는 데 유용합니다. 없어도 빌드됨 summary : 콘텐츠의 요약 내용을 정의합니다. 목록 페이지 등에서 미리보기 텍스트로 사용될 수 있습니다. 날짜 관련 변수 date : 콘텐츠의 발행 날짜를 지정합니다. 이 값을 기준으로 콘텐츠가 정렬될 수 있습니다. lastmod : 콘텐츠의 마지막 수정 날짜를 의미합니다. 이 값을 명시적으로 설정하거나, Git의 커밋 기록을 통해 자동으로 설정할 수도 있습니다. publishDate : 콘텐츠가 공개될 날짜를 예약합니다. 이 날짜 이전에는 hugo 명령어를 실행해도 해당 콘텐츠가 빌드되지 않습니다. expiryDate : 콘텐츠의 유효 기간을 설정하여, 지정된 날짜 이후에는 사이트에서 보이지 않게 됩니다. 콘텐츠 분류 (Taxonomies) tags : 콘텐츠에 하나 이상의 태그를 지정하여 분류합니다. categories : 콘텐츠가 속할 카테고리를 정의합니다. 이는 기본적으로 분류되는 값들이면 사용자가 추가적으로 정의할 수 있음 기본값은 아니지만 사람들이 많이 사용하는 분류 기준 series : 연재물과 같이 여러 콘텐츠가 하나의 시리즈에 속할 경우 사용됩니다. 콘텐츠 제어 및 렌더링 layout : 특정 콘텐츠에 적용할 템플릿(레이아웃) 파일을 지정할 수 있습니다. 이를 통해 기본 템플릿 규칙을 무시하고 원하는 디자인을 적용할 수 있습니다. type : 콘텐츠의 유형을 지정합니다. 지정하지 않으면 콘텐츠 파일이 위치한 디렉토리(섹션) 이름으로 자동 설정됩니다. weight : 콘텐츠 목록의 정렬 순서를 지정하는 데 사용되는 숫자 값입니다. 값이 작을수록 목록의 앞쪽에 위치합니다. slug : URL의 일부로 사용될 문자열을 직접 지정합니다. 설정하지 않으면 일반적으로 파일 이름에서 자동으로 생성됩니다. url : 페이지의 전체 URL 경로를 덮어씁니다. slug 보다 더 포괄적인 제어가 필요할 때 사용됩니다. aliases : 이전 URL 주소를 이 콘텐츠로 리디렉션할 때 사용됩니다. 콘텐츠의 URL이 변경되었을 때 유용합니다. outputs : 해당 콘텐츠에 대해 생성할 출력 포맷을 지정합니다. 예를 들어, HTML과 함께 JSON이나 AMP 버전을 생성할 수 있습니다. headless : true 로 설정하면 해당 페이지는 자체 URL 없이 다른 페이지에서만 참조할 수 있는 \"헤드리스 번들\"이 됩니다. cascade : 특정 섹션의 모든 하위 콘텐츠에 Front Matter 값을 한 번에 적용(상속)할 때 사용됩니다. SEO 및 메뉴 관련 keywords : SEO를 위한 메타 키워드를 배열 형태로 지정합니다. linkTitle : 메뉴 등에서 표시될 짧은 제목을 지정합니다. title 이 너무 길 경우에 유용합니다. menu : 페이지를 특정 메뉴에 추가합니다. 메뉴 이름, 가중치(weight), 부모 메뉴 등을 지정할 수 있습니다. 기타 유용한 변수들 isCJKLanguage (또는 hasCJKLanguage ): 콘텐츠에 한중일 문자가 포함되어 있는지 여부를 수동으로 설정합니다. .WordCount 나 .Summary 생성에 영향을 줍니다. params : 위에서 언급된 예약된 변수 외에 사용자 정의 변수를 추가할 때 사용됩니다. 예를 들어, author 나 show_comments 와 같은 임의의 필드를 추가하고 템플릿에서 .Params.author 와 같이 접근할 수 있습니다. build : 빌드 옵션을 설정하는 맵(map)입니다. 예를 들어, 특정 페이지를 빌드에서 제외하거나 다른 처리를 지시할 수 있습니다. 이러한 Front Matter 변수들은 TOML, YAML, JSON 형식 중 하나로 작성할 수 있다 페이지 kind 종류 hugo 의 모든 페이지는 기본적으로 종류가 존재한다 home , section , taxonomy , term , page / 에 대응되는 곳이 바로 home 이라는 종류고 / 에 매핑된 템플릿이다 md -> html 로 변환된 가장 기본적인 것의 종류가 바로 page 이다. 기본적으로 content/test/hello.md 로 만들면 public/test/hello.html 로 만들어진다. 물론 추가 설정을 통해 변경할 수도 있다 hugo template 시스템 경우의 수로 비교한 자세한 설명 폴더 하위에 _index.md 파일이 존재하면 그 폴더가 section 으로 취급이 된다 경로또한 content/test_folder/_index.md 이면 public/test_folder 로 만들어지고 https://{domain}/test_folder 으로 접근 가능하다 만약 자신이 md 파일 frontmatter 영역에 tags 를 넣어서 문서들을 관리한다고 가정할 때 hugo 가 기본적으로 taxonomy 로 분류하는 것은 tags , categories 이므로 tags 라는 분류 목록이 생성된다 이것이 taxonomy 이다 이어서 tags 가 바로 term 이다 만약 hugo.toml 과 같은 전역 설정에 다른 분류체계를 넣고 자신이 분류에 author 라는 변수를 문서 곳곳에 넣어다면 taxonomy 에 author 가 잡히게 되고 author 라는 키에 대입된 값들이 바로 term 이 된다 위의 전제를 그대로 이어서 말하면 /tags 접근하면 모든 frontmatter tags 에 값으로 넣은 것들이 보이게 되고 어떤 값 1개 (hugo 라는 이름의 태그) 를 누르게 되면 해당 값을 가진 모든 파일(hugo 라는 값을 가진 모든 파일들)이 보이게 된다 section 관련 : 나의 경우는 모든 폴더를 폴더로 취급하기 위해 모든 폴더 하위에 스크립트로 _index.md 를 두도록 하였다 하지만 이렇게 하지 않고 특정 주제의 기록을 특정 폴더 1개에 담는 방식으로 보관하는 사람은 이것을 시리즈 처럼 활용할 수 있다 예를 들어 배터리 관련 공부를 한다고 가정할 때 battery/ 폴더 하위에 관련 문서들을 저장하고 _index.md 를 생성한다면 hugo 의 section 시스템을 일종의 주제분류로 활용할 수 있을 것이다 사람마다 다르게 사용할 수 있으므로 적절히 사용하자 section 관련2: 잘 읽은 사람은 알겠지만 만약 content/_index.md 로 만든다면 public/ 으로 만들어지고 https://{domain}/ 으로 접근가능해야 하는데 이러면 home과 겹치지 않는가? template 관련 우선순위 문서를 읽는다면 이해할 수 있다",
      "frontmatter": {
        "tags": [
          "hugo"
        ],
        "series": "hugo",
        "series_weight": "1",
        "date": "2025-10-27T15:34:18+09:00",
        "lastmod": "2025-10-27T20:20:19+09:00"
      }
    },
    "java type 확인법": {
      "path": "/java-type-확인법/",
      "filename": "java type 확인법",
      "content": "Java에는 JavaScript나 C++ 같은 언어에서 사용하는 typeof 연산자와 정확히 동일한 문법적 연산자가 없습니다. Java는 정적 타입 언어(statically typed language)이기 때문에, 컴파일 시점에 변수의 타입이 결정되며, 런타임에 타입을 \"확인\"하는 것이 일반적인 용도가 아닙니다. 하지만 Java에서는 런타임에 객체의 타입을 확인할 수 있는 여러 가지 방법이 있습니다. 아래에 Java에서 타입(type)을 확인할 수 있는 모든 주요 방법을 정리해 드립니다. ✅ 1. instanceof 연산자 — 가장 흔한 타입 확인 방법 Object obj = \"Hello\"; if (obj instanceof String) { System.out.println(\"obj는 String 타입입니다.\"); } else if (obj instanceof Integer) { System.out.println(\"obj는 Integer 타입입니다.\"); } 특징: null 과 비교하면 항상 false 를 반환. 컴파일 시점에 타입 호환성이 없는 경우 컴파일 에러 발생 (예: String instanceof Integer ) 상속 관계도 인식: instanceof 는 부모 클래스/인터페이스에도 true 를 반환함. class Animal {} class Dog extends Animal {} Dog dog = new Dog(); System.out.println(dog instanceof Animal); // true System.out.println(dog instanceof Dog); // true 📌 JavaScript의 typeof 와 유사한 용도로 자주 사용됨 ✅ 2. getClass() 메서드 — 실제 실행 타입 얻기 Object obj = new ArrayList<String>(); Class<?> clazz = obj.getClass(); System.out.println(clazz.getName()); // java.util.ArrayList 특징: Object 클래스의 메서드로, 모든 객체에 사용 가능 정확한 런타임 타입(concrete class)을 반환 instanceof 와 달리 부모 타입은 포함하지 않음 Dog dog = new Dog(); System.out.println(dog.getClass().getName()); // Dog (Animal 아님!) 활용 예: 타입 이름 비교 if (obj.getClass().equals(String.class)) { System.out.println(\"String입니다\"); } ⚠️ == 대신 .equals() 를 써야 함. Class 객체는 참조 비교가 아니라 내용 비교가 필요. ✅ 3. Class.isInstance() 메서드 — instanceof 의 반대 버전 Class<String> stringClass = String.class; Object obj = \"hello\"; if (stringClass.isInstance(obj)) { System.out.println(\"obj는 String입니다\"); } 특징: instanceof 의 반대 방향으로 사용 (클래스 객체 → 객체 확인) 리플렉션(Reflection) 상황에서 유용 null 처리: isInstance(null) → false ✅ 4. Class.isAssignableFrom() — 상속/구현 관계 확인 Class<Animal> animalClass = Animal.class; Class<Dog> dogClass = Dog.class; System.out.println(animalClass.isAssignableFrom(dogClass)); // true System.out.println(dogClass.isAssignableFrom(animalClass)); // false 특징: A.isAssignableFrom(B) → B가 A의 하위 타입(또는 같음)인가? 상속 계층 구조를 체크할 때 유용 List.class.isAssignableFrom(ArrayList.class) → true ✅ 5. Type 및 ParameterizedType — 제네릭 타입 정보 확인 (리플렉션) import java.lang.reflect.*; import java.util.List; public class GenericTypeExample { public List<String> list; public static void main(String[] args) throws Exception { Field field = GenericTypeExample.class.getDeclaredField(\"list\"); Type genericType = field.getGenericType(); if (genericType instanceof ParameterizedType) { ParameterizedType pt = (ParameterizedType) genericType; Type rawType = pt.getRawType(); // java.util.List Type[] typeArgs = pt.getActualTypeArguments(); // [java.lang.String] System.out.println(\"Raw type: \" + rawType); System.out.println(\"Type arg: \" + typeArgs[0]); // class java.lang.String } } } 특징: 제네릭 타입 파라미터(예: List<String> )의 실제 타입을 런타임에 알 수 있음 타입 추출(Type Erasure) 후에도 제네릭 정보가 필드/메서드/생성자에 저장되어 있을 때만 가능 컴파일된 .class 파일에 제네릭 정보가 남아 있어야 함 (디버그 옵션 등) ✅ 6. switch 문 + instanceof (Java 17+ 패턴 매칭) Java 17부터 instanceof 와 switch 를 결합한 패턴 매칭 기능이 도입되었습니다: Object obj = \"Hello\"; switch (obj) { case String s -> System.out.println(\"문자열: \" + s.toUpperCase()); case Integer i -> System.out.println(\"정수: \" + i * 2); case null -> System.out.println(\"null입니다\"); default -> System.out.println(\"알 수 없는 타입: \" + obj.getClass().getSimpleName()); } 이 방식은 instanceof + 캐스팅을 자동으로 해주는 안전하고 간결한 문법입니다. ✅ 7. Objects.toString() 또는 String.valueOf() — 타입 정보 출력 Object obj = new HashMap<>(); System.out.println(obj); // { } → toString() 호출 System.out.println(obj.getClass().getSimpleName()); // HashMap System.out.println(String.valueOf(obj.getClass())); // class java.util.HashMap toString() 은 보통 클래스명을 포함하므로 간접적으로 타입 확인 가능 ✅ 8. Class.getSimpleName() / getCanonicalName() / getTypeName() Class<?> c = String.class; System.out.println(c.getSimpleName()); // String System.out.println(c.getCanonicalName()); // java.lang.String System.out.println(c.getTypeName()); // java.lang.String getSimpleName() : 단순 클래스명 getCanonicalName() : 패키지 포함 표준 이름 getTypeName() : 제네릭 타입까지 표현 가능 (Java 8+) ❌ Java에는 typeof 가 없다! 언어 typeof JavaScript typeof x === \"string\" ✅ C# typeof(string) ✅ (컴파일 타임) Java ❌ 없음 Java는 컴파일 타임 타입 검사가 강력하기 때문에, typeof 처럼 변수의 선언 타입을 문자열로 얻는 기능이 없습니다. 💡 Java에서 String.class 는 클래스 객체이고, typeof 처럼 타입 이름 문자열을 얻는 게 아님. 🔍 요약: Java에서 타입 확인 방법 정리 방법 설명 사용 목적 null 안전? instanceof 객체가 특정 타입 또는 하위 타입인지 확인 일반적인 타입 체크 ✅ ( null → false ) getClass() 실제 런타임 클래스 객체 가져오기 정확한 클래스 비교 ✅ Class.isInstance() 클래스 객체로 객체 타입 확인 리플렉션 환경 ✅ Class.isAssignableFrom() 상속/구현 관계 확인 API 설계, 프레임워크 ✅ switch + instanceof (Java 17+) 다중 타입 분기 간결한 타입 분기 로직 ✅ ParameterizedType 제네릭 타입 파라미터 추출 커스텀 직렬화, ORM 등 ✅ (정보 존재 시) getSimpleName() , getTypeName() 타입 이름 출력 디버깅, 로깅 - 💡 실무 팁 instanceof → 대부분의 경우 최선의 선택 getClass() → 정확한 클래스 비교가 필요할 때 (예: HashMap vs LinkedHashMap ) 리플렉션 → 프레임워크 개발 시 사용 (Spring, Jackson, Hibernate 등) 패턴 매칭 → Java 17 이상이라면 switch 로 타입 분기를 깔끔하게! ✅ 예시: instanceof vs getClass() 차이 class Animal {} class Cat extends Animal {} Cat cat = new Cat(); System.out.println(cat instanceof Animal); // true System.out.println(cat.getClass() == Animal.class); // false System.out.println(cat.getClass() == Cat.class); // true → instanceof 는 상속 포함, getClass() 는 정확한 클래스만 필요에 따라 적절한 메서드를 선택하세요! Java는 typeof 가 없지만, 위의 다양한 도구들로 충분히 타입을 안전하고 정밀하게 다룰 수 있습니다.",
      "frontmatter": {
        "date": "2025-09-12T20:51:17+09:00",
        "lastmod": "2025-09-12T21:00:28+09:00",
        "share_link": "https://share.note.sx/tj6qxwcw#PRwNTJvX39gW2hDdS96kHdqOnAc+chVkWGF/HET2ofY",
        "share_updated": "2025-09-12T20:53:26+09:00"
      }
    },
    "linux 커널 업그레이드 방지": {
      "path": "/linux-커널-업그레이드-방지/",
      "filename": "linux 커널 업그레이드 방지",
      "content": "kernel-headers 설치 (없을 경우) sudo apt install raspberrypi-kernel-headers 핵심 패키지 hold sudo apt-mark hold \\ raspberrypi-kernel \\ raspberrypi-kernel-headers \\ raspberrypi-bootloader \\ libraspberrypi0 \\ libraspberrypi-bin hold 상태 확인 sudo apt-mark showhold # 1. kernel-headers 설치 (없을 경우) sudo apt install raspberrypi-kernel-headers # 2. 핵심 패키지 hold sudo apt-mark hold \\ raspberrypi-kernel \\ raspberrypi-kernel-headers \\ raspberrypi-bootloader \\ libraspberrypi0 \\ libraspberrypi-bin # 3. hold 상태 확인 sudo apt-mark showhold",
      "frontmatter": {
        "date": "2025-09-02T15:56:20+09:00",
        "lastmod": "2025-10-23T04:34:06+09:00"
      }
    },
    "long vs float": {
      "path": "/long-vs-float/",
      "filename": "long vs float",
      "content": "좋은 질문입니다! “long vs float 중 어떤 것이 더 큰 수를 저장할 수 있는가?” — 이 질문은 자주 혼동되는 핵심 개념입니다. 결론부터 말하면: ✅ float 이 더 큰 수치(magnitude)를 표현할 수 있지만, long 은 더 큰 정확한 정수를 저장합니다. 즉, “더 큰 수”의 의미에 따라 답이 달라집니다. 🔍 핵심 요약표 항목 long float 최대 표현 가능한 수치 (절댓값) ~9.2 × 10¹⁸ ~3.4 × 10³⁸ ← 훨씬 큼 정확히 표현 가능한 최대 정수 9,223,372,036,854,775,807 (정확함) 약 16,777,216 (2²⁴)까지만 정확함 소수 표현 가능? ❌ 불가능 ✅ 가능 정밀도 완벽한 정수 약 6~7자리 십진수 사용 용도 ID, 금액, 카운터 등 정수 과학 계산, 그래픽, 근사값 🧠 상세 설명: “더 큰 수”는 무엇을 의미하나요? ✅ 경우 1: “더 큰 숫자 자체(절댓값)”을 의미한다면 → float 이 이김 long 최대값: 9,223,372,036,854,775,807 ≈ 9.2e18 float 최대값: ~3.4 × 10³⁸ = 340,000,000,000,000,000,000,000,000,000,000,000,000 → float 이 약 10²⁰배 더 큰 수치를 표현 가능! System.out.println(Long.MAX_VALUE); // 9223372036854775807 System.out.println(Float.MAX_VALUE); // 3.4028235E38 👉 수치 크기만 보면 float 이 훨씬 큼 ✅ 경우 2: “더 큰 정수를 정확하게 저장할 수 있는가?” 라면 → long 이 압승 float 은 정수도 부동소수점으로 저장하기 때문에, 큰 정수는 정확히 표현 불가능! IEEE 754 단정도( float )는 가수부(mantissa)가 23비트 + 숨겨진 1비트 = 24비트 → 정확히 표현 가능한 정수는 2²⁴ = 16,777,216 까지 예시: float 이 정수를 정확히 표현 못하는 경우 float f1 = 16_777_216f; // 2^24 → 정확히 저장됨 float f2 = 16_777_217f; // 다음 정수 → 저장되긴 하지만... float f3 = 16_777_218f; System.out.println(f1 == 16_777_216); // true System.out.println(f2 == 16_777_217); // false → 실제로는 16,777,216 으로 저장됨! System.out.println(f2); // 출력: 1.6777216E7 → 16777216 // 1억 이상은 모두 정확하지 않음 float big = 100_000_000f; System.out.println(big + 1 == big); // true → 1을 더해도 값이 변하지 않음! → float 은 1677만 초과하는 정수는 이미 ‘반올림’되어 정확히 저장되지 않습니다. 반면 long 은: long l = 9_223_372_036_854_775_807L; System.out.println(l + 1 == l); // false → 정확히 증가함! 👉 long 은 10¹⁸까지 모든 정수를 정확히 저장 가능 📊 시각화: 정수 표현 가능성 비교 정수 축: 0 ──── 10⁷ ─────── 10⁹ ─────────── 10¹⁸ ────────────────> │ │ │ │ float 정확 float 정확 float 정확 long 정확 (2²⁴=16M) (아직 OK) (이미 틀림!) (완전 정확) ↓ float은 여기서부터 \"정수\"로 쓸 수 없음! 💡 float 은 수치는 크지만, 정수로 쓰려면 1677만 이하만 안전! ✅ 결론: “더 큰 수를 저장할 수 있는 타입”은? 질문 답변 더 큰 수치를 표현할 수 있는가? ✅ float (3.4e38 > 9.2e18) 더 큰 정확한 정수를 저장할 수 있는가? ✅ long (9.2e18까지 정확, float은 1.6e7까지밖에 안 됨) 실제 프로그래밍에서 “큰 수”라고 하면 보통 어떤 의미인가? 👉 대부분 정확한 정수를 의미 → 따라서 long 이 더 유용하고 안전 🚨 추가 팁: 그럼 진짜 “더 큰 수”가 필요하면? 요구사항 적합한 타입 정수인데 long 보다 커야 함 ✅ java.math.BigInteger 소수점 포함된 큰 수 ✅ java.math.BigDecimal 그냥 실수인데 정확히 쓰고 싶음 ✅ double (64비트 부동소수점) 메모리 절약이 목적이며 소수점 사용 ⚠️ float (32비트) — 정수로 쓰지 마세요! 예: BigInteger로 1000! 계산하기 import java.math.BigInteger; BigInteger fact = BigInteger.ONE; for (int i = 2; i <= 1000; i++) { fact = fact.multiply(BigInteger.valueOf(i)); } System.out.println(fact.toString().length() + \" 자리\"); // 2568자리! → long , float , double 모두 오버플로우 → BigInteger 만 가능 ✅ 최종 정리: long vs float — 누가 더 큰 수를 저장할 수 있나? 기준 승자 이유 절댓값이 가장 큰 수를 표현 float 3.4×10³⁸ > 9.2×10¹⁸ 정확한 정수를 가장 크게 저장 long float 은 1677만 이상부터 정수 정확도 잃음 정수 계산에 안전한가? long float 은 큰 정수에서 계산 결과가 완전히 틀릴 수 있음 실수 계산에 적합한가? float long 은 소수점 없음 → 비교 불가 ✅ 실무에서는: 정수 → long 실수 → double float 은 거의 쓰지 마세요. 정수인데 long 보다 커야 한다면 → BigInteger 💡 记住这个口诀: “float는 숫자는 크지만, 정수는 허접; long은 숫자는 작지만, 정수는 확실.”",
      "frontmatter": {
        "date": "2025-09-12T21:00:27+09:00",
        "lastmod": "2025-09-12T21:22:48+09:00",
        "share_link": "https://share.note.sx/x7492ke9#d7PBhWhl51Qa3i5gN1bXPqb1wav1i5spTjc9M6M5A4k",
        "share_updated": "2025-09-12T21:00:42+09:00"
      }
    },
    "markdown frontmatter 규칙": {
      "path": "/markdown-frontmatter-규칙/",
      "filename": "markdown frontmatter 규칙",
      "content": "obsidian key hugo key type description aliases keyword string obsidian 에서 기본적으로 사용하기를 원하는 key 로 검색시 사용 hugo 에서는 meta 에 keyword 로 사용 tags tags string 가장 기본적인 주제 분류 방식이다 categories categories string 제한된 분류 값 고정 : learning , explanation , reference , date date date 생성일자 ISO 8601 준수 파일시스템 독립 lasmod lastmod date 마지막 수정일자 ISO 8601 준수 파일시스템 독립 아래부터는 선택 series series string 순서가 존재하는 시리즈 형태 seriesweight seriesweight int 순서 => 일반적으로 100 형태로 시작 하위 순서를 나타내기 위해 publish draft t/f 서로 반대로 처리 publish 이면 배포 categories 참조 : Toss technical writing guide aliases : obsidian 에서 기본적으로 사용하기를 원하는 key 로 검색시 사용하거나 keyword 로 사용할 수",
      "frontmatter": {
        "date": "2025-11-02T06:54:34+09:00",
        "lastmod": "2025-11-02T06:54:55+09:00"
      }
    },
    "python default type": {
      "path": "/python-default-type/",
      "filename": "python default type",
      "content": "default type 자료형 그룹 자료형 (Type) 설명 숫자형 (Numeric Types) int 정수 (정밀도 무제한) float 부동소수점 숫자 complex 복소수 불리언형 (Boolean Type) bool True 또는 False 값을 가지며, int 의 하위 클래스입니다. 시퀀스형 (Sequence Types) list 변경 가능한(mutable) 시퀀스. tuple 변경 불가능한(immutable) 시퀀스. range 변경 불가능한(immutable) 숫자 시퀀스. str (문자열) 변경 불가능한(immutable) 유니코드 코드 포인트의 시퀀스. 바이너리 시퀀스형 (Binary Sequence Types) bytes 변경 불가능한(immutable) 단일 바이트의 시퀀스. bytearray 변경 가능한(mutable) bytes 버전. memoryview 복사 없이 다른 바이너리 객체의 메모리에 접근합니다. 세트형 (Set Types) set 순서가 없고, 중복되지 않는 해시 가능한(hashable) 객체들의 변경 가능한(mutable) 컬렉션. frozenset 변경 불가능한(immutable) set . 매핑형 (Mapping Types) dict (사전) 해시 가능한(hashable) 값을 임의의 객체에 매핑하는 변경 가능한(mutable) 객체. 컨텍스트 관리자형 (Context Manager Types) - with 문에 의해 정의된 런타임 컨텍스트를 지원하는 형식. 타입 어노테이션형 (Type Annotation Types) GenericAlias 클래스를 구독하여 생성되는 제네릭 타입의 프록시 ( list[int] 등). Union 여러 타입을 허용하는 타입 어노테이션을 위한 형식 ( module class and instance function method code object type object NoneType ( None ellipsis ( ... NotImplementedType ( NotImplemented | 분류 (Category) | 내장 타입 (Built-in Type) | 설명 (Description) | 출처 (Source) | | :-------------------------------------- | :------------------------------------------------- | :---------------------------------------------------------------------------------------- | :------------------------------------- | int float complex bool ** | 진리값을 나타내는 타입으로, int 의 서브타입입니다. True 와 False list tuple range ** | 불변 숫자 시퀀스를 나타내며, for str bytes bytearray ** | bytes memoryview set ** | 해시 가능한 고유 객체의 순서 없는 컬렉션입니다. add() 및 remove() frozenset dict Dictionary view objects ** | dict.keys() , dict.values() , dict.items() Generic Alias ** | 클래스를 서브스크립션하여 생성되며, 주로 타입 어노테이션( list[int] Union ** | 여러 타입 객체에 대한 (비트 OR) 연산의 값을 보유하며, 주로 타입 어노테이션( int 기타 내장 타입 (Other Built-in Types) Modules 모듈의 심볼 테이블에 정의된 이름에 접근할 수 있습니다 (예: m.name ). Classes and Class Instances 객체, 값 및 타입, 그리고 클래스 정의에 대한 정보를 포함합니다. Functions 함수 정의에 의해 생성되며, 호출하는 것이 유일한 연산입니다. 내장 함수와 사용자 정의 함수가 있습니다. Methods 속성 표기법을 사용하여 호출되는 함수입니다. 내장 메서드와 클래스 인스턴스 메서드(바운드 메서드)가 있습니다. Code Objects 함수 본문과 같은 \"의사 컴파일된\" 실행 가능한 Python 코드를 나타내는 데 구현에서 사용됩니다. Type Objects 다양한 객체 타입을 나타내며, 객체의 타입은 내장 함수 type() 을 통해 접근할 수 있습니다. The Null Object ( None ) 함수가 명시적으로 값을 반환하지 않을 때 반환되는 객체입니다. 정확히 하나의 널 객체 None 이 있습니다. The Ellipsis Object ( Ellipsis 또는 ... ) 슬라이싱( Slicings )에 일반적으로 사용되는 객체입니다. 정확히 하나의 생략 객체가 있습니다. The NotImplemented Object ( NotImplemented ) 비교 및 이진 연산이 지원하지 않는 타입에 대해 작동하도록 요청받을 때 반환되는 객체입니다. Internal Objects 스택 프레임, 트레이스백, 슬라이스 객체 등을 포함합니다. collection collections 모듈은 파이썬의 기본 내장 컨테이너( dict , list , set , tuple )를 확장하여 더 전문화된 데이터 구조를 제공 클래스 (Class) 설명 namedtuple() 각 위치에 이름(필드)을 부여하여 인덱스뿐만 아니라 이름으로도 데이터에 접근할 수 있는 튜플을 생성하는 팩토리 함수입니다. deque 'double-ended queue'의 약자로, 리스트와 유사하지만 양쪽 끝에서 항목을 추가하고 제거하는(append/pop) 속도가 매우 빠릅니다. ChainMap 여러 개의 딕셔너리나 다른 매핑(mapping)들을 하나의 뷰(view)로 묶어줍니다. 검색은 여러 매핑에서 순차적으로 이루어집니다. Counter 해시 가능한(hashable) 객체의 개수를 세는 데 특화된 딕셔너리 서브클래스입니다. 요소(key)와 그 요소의 개수(value)를 매핑 형태로 저장합니다. OrderedDict 항목이 추가된 순서를 기억하는 딕셔너리 서브클래스입니다. (참고: Python 3.7부터는 기본 dict 도 삽입 순서를 유지하지만, OrderedDict 는 순서를 재정렬하는 메서드 등 추가 기능을 가집니다.) defaultdict 딕셔너리에서 존재하지 않는 키를 조회할 때, 미리 지정된 기본값(예: 0, 빈 리스트)을 자동으로 생성해주는 딕셔너리 서브클래스입니다. KeyError 를 방지하는 데 유용합니다. UserDict 딕셔너리 객체를 감싸는(wrapper) 클래스로, 기존 딕셔너리를 상속받아 새로운 클래스를 만들 때 더 편리하게 사용할 수 있도록 도와줍니다. UserList 리스트 객체를 감싸는 래퍼 클래스입니다. UserDict 와 마찬가지로 리스트를 상속받아 커스텀 클래스를 만들 때 유용합니다. UserString 문자열 객체를 감싸는 래퍼 클래스입니다. nametuple 네, namedtuple 에 대해 자세히 설명해 드리겠습니다. 말씀하신 정의는 정확합니다. namedtuple 은 '이름이 붙은 튜플'을 만드는 아주 유용한 도구입니다. 간단히 비유하자면, 메서드(기능)는 필요 없고 데이터만 담을 아주 가벼운 '클래스(Class)'를 즉석에서 만드는 것과 같습니다. 왜 namedtuple 을 사용할까요? 일반 튜플은 인덱스로만 값에 접근할 수 있습니다. # 일반 튜플 point = (10, 20) print(point[0]) # x 좌표 print(point[1]) # y 좌표 이 코드는 point[0] 이 무엇을 의미하는지 바로 알기 어렵습니다. 코드가 길어지면 이 값들이 x, y 좌표라는 것을 잊기 쉽죠. namedtuple 은 이 문제를 해결합니다. namedtuple 사용법 namedtuple 은 두 단계를 거쳐 사용됩니다. 1단계: namedtuple '타입(Type)' 정의하기 collections.namedtuple() 함수를 호출하여 새로운 클래스(타입)를 만듭니다. namedtuple('타입이름', ['필드이름1', '필드이름2', ...]) 2단계: 정의된 타입으로 '객체(Instance)' 생성하기 마치 클래스로 객체를 만들 듯이, 위에서 정의한 타입에 값을 넣어 객체를 생성합니다. 자세한 코드 예제 from collections import namedtuple # 1. 'Point'라는 이름의 namedtuple 타입을 정의합니다. # 이 타입은 'x'와 'y'라는 필드 이름을 가집니다. Point = namedtuple('Point', ['x', 'y']) # 2. Point 타입을 사용하여 객체를 생성합니다. p1 = Point(10, 20) p2 = Point(x=30, y=40) # 키워드 인자로도 생성 가능 # --- 주요 특징 --- # 1. 이름으로 값에 접근 (가장 큰 장점!) print(f\"p1의 x좌표: {p1.x}\") # 출력: p1의 x좌표: 10 print(f\"p1의 y좌표: {p1.y}\") # 출력: p1의 y좌표: 20 # 2. 인덱스로도 값에 접근 (튜플의 특성 유지) print(f\"p2의 첫 번째 값: {p2[0]}\") # 출력: p2의 첫 번째 값: 30 print(f\"p2의 두 번째 값: {p2[1]}\") # 출력: p2의 두 번째 값: 40 # 3. 불변성(Immutable) - 값 변경 불가 (튜플의 특성 유지) try: p1.x = 100 except AttributeError as e: print(f\"값 변경 시도 시 에러 발생: {e}\") # 출력: 값 변경 시도 시 에러 발생: can't set attribute # 4. 언패킹(Unpacking) 가능 (튜플의 특성 유지) x_val, y_val = p1 print(f\"언패킹된 값: x={x_val}, y={y_val}\") # 출력: 언패킹된 값: x=10, y=20 # 5. 객체 표현이 명확함 print(p1) # 출력: Point(x=10, y=20) 언제 사용하면 좋을까요? CSV 파일이나 데이터베이스의 행(Row)을 처리할 때: row[0] , row[1] 대신 row.id , row.name 처럼 명확하게 데이터를 다룰 수 있습니다. 함수에서 여러 값을 반환할 때: return (id, name, email) 대신, namedtuple 객체를 반환하면 어떤 값이 무엇을 의미하는지 명확해집니다. 좌표(x, y, z), RGB 색상(r, g, b) 등 명확한 이름이 있는 데이터 묶음을 표현할 때 유용합니다. 다른 자료형과의 비교 비교 대상 namedtuple 의 장점 일반 tuple 가독성이 월등히 좋습니다. point[0] 대신 point.x 로 의미를 명확히 할 수 있습니다. dict (딕셔셔리) 불변성(immutable)을 가집니다. 값이 실수로 변경되는 것을 막을 수 있습니다. 더 가볍고(메모리 효율적) 빠릅니다. class 데이터 필드만 필요한 경우, class 를 직접 정의하는 것보다 코드가 훨씬 간결합니다. 요약 namedtuple 은 데이터를 담기 위한 간단한 '객체'가 필요하지만, 완전한 class 를 정의하기는 번거로울 때 사용하는 최고의 도구입니다. 튜플의 특성(불변성, 효율성)과 객체지향의 장점(가독성)을 모두 가지고 있어 코드를 훨씬 깔끔하고 이해하기 쉽게 만들어 줍니다. 완전히 정리된 상태입니다! ✅ 아래는 리스트 컴프리헨션을 자연스럽게 읽는 방법, 그리고 주석(docstring, 코드 주석 등)에 사용할 수 있는 한글 / 영어 표현 패턴을 체계적으로 정리한 내용이에요. list comprehension 기본 구조 [표현식 for 항목 in 반복대상 if 조건] → 이걸 영어 문장처럼 읽으면: \"expression for item in iterable if condition\" 즉, \"조건이 있다면 해당 조건을 만족하는 item 들 중에서, 각각에 대해 expression 을 적용해서 리스트를 만들자\" 기본 리스트 생성 [x for x in range(5)] 👉 한글: \"0부터 4까지의 숫자로 구성된 리스트\" 👉 영어: \"a list of numbers from 0 to 4\" 제곱수 리스트 [x**2 for x in range(1, 6)] 👉 한글: \"1부터 5까지 각 숫자의 제곱으로 구성된 리스트\" 👉 영어: \"a list of squares of numbers from 1 to 5\" 짝수만 필터링 [x for x in range(10) if x % 2 == 0] 👉 한글: \"0부터 9까지 중 짝수만 포함하는 리스트\" 👉 영어: \"a list of even numbers from 0 to 9\" 문자열로 변환 [str(x) for x in range(2, 11)] 👉 한글: \"2부터 10까지의 숫자를 문자열로 변환한 리스트\" 👉 영어: \"a list of strings converted from numbers 2 to 10\" 조건 + 변환 동시 적용 [x.upper() for x in ['apple', 'banana', 'cherry'] if len(x) > 5] 👉 한글: \"길이가 5보다 큰 단어만 대문자로 바꾼 리스트\" 👉 영어: \"a list of words converted to uppercase where length is greater than 5\" 중첩된 예시 (딕셔너리 키 추출 등) [key for key, value in my_dict.items() if value > 10] 👉 한글: \"값이 10보다 큰 항목들의 키만 포함하는 리스트\" 👉 영어: \"a list of keys where the corresponding value is greater than 10\"",
      "frontmatter": {
        "tags": [
          "python"
        ],
        "date": "2025-07-01T06:52:17+09:00",
        "lastmod": "2025-07-01T23:01:01+09:00"
      }
    },
    "signal 차이 unix(macos) vs linux": {
      "path": "/signal-차이-unixmacos-vs-linux/",
      "filename": "signal 차이 unix(macos) vs linux",
      "content": "macOS와 Linux의 신호(Signal) 상세 분석 시그널 이름 macOS 기본 동작 macOS 설명 Linux 기본 동작 Linux 설명 (Comment) 차이점 / 비고 SIGHUP 프로세스 종료 터미널 라인 끊김 Term 제어 터미널에서 끊김 감지 또는 제어 프로세스 종료 설명은 유사하며, 둘 다 프로세스를 종료합니다. SIGINT 프로세스 종료 프로그램 인터럽트 Term 키보드로부터의 인터럽트 유사하며, 둘 다 프로세스를 종료하며, 주로 Ctrl+C로 발생합니다. SIGQUIT 코어 이미지 생성 프로그램 종료 Core 키보드로부터의 종료 둘 다 코어 덤프를 생성하고 종료합니다. SIGILL 코어 이미지 생성 불법 명령어 Core 불법 명령어 (Illegal Instruction) 둘 다 코어 덤프를 생성합니다. SIGTRAP 코어 이미지 생성 트레이스 트랩 Core 트레이스/브레이크포인트 트랩 둘 다 코어 덤프를 생성합니다. SIGABRT 코어 이미지 생성 프로그램 중단 (이전 SIGIOT) Core abort(3)로부터의 중단 시그널 둘 다 코어 덤프를 생성합니다. Linux는 abort(3) 로부터 발생한다고 명시합니다. SIGEMT 코어 이미지 생성 명령어 에뮬레이트 실행 Term 에뮬레이터 트랩 차이점: macOS는 코어 이미지를 생성하지만, Linux는 프로세스를 종료합니다. 이는 \"에뮬레이터 트랩\" 조건에 대한 다른 처리를 나타낼 수 있습니다. Linux는 이 시그널에 대한 명시적인 코어 덤프 동작이 없습니다. SIGFPE 코어 이미지 생성 부동 소수점 예외 Core 부동 소수점 예외 둘 다 코어 덤프를 생성합니다. SIGKILL 프로세스 종료 프로그램 강제 종료 Term 강제 종료 시그널 둘 다 프로세스를 종료하며, 포착하거나 무시할 수 없습니다. SIGBUS 코어 이미지 생성 버스 에러 Core 버스 에러 (잘못된 메모리 접근) 둘 다 코어 덤프를 생성합니다. SIGSEGV 코어 이미지 생성 세그멘테이션 위반 Core 유효하지 않은 메모리 참조 둘 다 코어 덤프를 생성합니다. SIGSYS 코어 이미지 생성 존재하지 않는 시스템 콜 호출 Core 잘못된 시스템 콜 (SVr4); seccomp(2) 참조 둘 다 코어 덤프를 생성합니다. Linux는 seccomp(2) 에 대한 추가 컨텍스트를 제공합니다. SIGPIPE 프로세스 종료 리더 없는 파이프에 쓰기 Term 파이프 깨짐: 리더 없는 파이프에 쓰기; pipe(7) 참조 둘 다 프로세스를 종료합니다. SIGALRM 프로세스 종료 실시간 타이머 만료 Term alarm(2)로부터의 타이머 시그널 둘 다 프로세스를 종료합니다. SIGTERM 프로세스 종료 소프트웨어 종료 시그널 Term 종료 시그널 둘 다 프로세스를 종료합니다. 이것은 기본 종료 시그널입니다. SIGURG 시그널 무시 소켓에 긴급 조건 발생 Ign 소켓의 긴급 조건 (4.2BSD) 둘 다 시그널을 무시합니다. SIGSTOP 프로세스 중지 중지 (포착하거나 무시할 수 없음) Stop 프로세스 중지 둘 다 프로세스를 중지하며 포착하거나 무시할 수 없습니다. SIGTSTP 프로세스 중지 키보드로부터 발생한 중지 시그널 Stop 터미널에서 입력된 중지 둘 다 프로세스를 중지하며, 주로 Ctrl+Z로 발생합니다. SIGCONT 시그널 무시 중지 후 계속 진행 Cont 중지 후 계속 진행 둘 다 중지된 프로세스를 계속 진행합니다. SIGCHLD 시그널 무시 자식 상태 변경됨 Ign 자식 프로세스 중지 또는 종료 둘 다 기본적으로 시그널을 무시합니다. SIGTTIN 프로세스 중지 제어 터미널에서 백그라운드 읽기 시도 Stop 백그라운드 프로세스를 위한 터미널 입력 둘 다 프로세스를 중지합니다. SIGTTOU 프로세스 중지 제어 터미널에 백그라운드 쓰기 시도 Stop 백그라운드 프로세스를 위한 터미널 출력 둘 다 프로세스를 중지합니다. SIGIO 시그널 무시 디스크립터에서 I/O 가능 (fcntl(2) 참조) Term I/O 이제 가능 (4.2BSD) 차이점: macOS는 기본적으로 이 시그널을 무시하는 반면, Linux는 프로세스를 종료합니다. Linux는 또한 SIGPOLL 의 동의어라고 명시합니다. 이는 중요한 행동 차이입니다. SIGXCPU 프로세스 종료 CPU 시간 제한 초과 (setrlimit(2) 참조) Core CPU 시간 제한 초과 (4.2BSD); setrlimit(2) 참조 차이점: macOS는 프로세스를 종료하는 반면, Linux는 코어 덤프를 생성합니다. 둘 다 setrlimit(2) 를 인정합니다. SIGXFSZ 프로세스 종료 파일 크기 제한 초과 (setrlimit(2) 참조) Core 파일 크기 제한 초과 (4.2BSD); setrlimit(2) 참조 차이점: macOS는 프로세스를 종료하는 반면, Linux는 코어 덤프를 생성합니다. 둘 다 setrlimit(2) 를 인정합니다. SIGVTALRM 프로세스 종료 가상 시간 알람 (setitimer(2) 참조) Term 가상 알람 시계 (4.2BSD) 둘 다 프로세스를 종료합니다. SIGPROF 프로세스 종료 프로파일링 타이머 알람 (setitimer(2) 참조) Term 프로파일링 타이머 만료 둘 다 프로세스를 종료합니다. SIGWINCH 시그널 무시 윈도우 크기 변경 Ign 윈도우 크기 변경 시그널 (4.3BSD, Sun) 둘 다 시그널을 무시합니다. SIGINFO 시그널 무시 키보드로부터의 상태 요청 - ( SIGPWR 의 동의어) SIGPWR 의 동의어 (기본적으로 Term) 차이점: macOS는 SIGINFO 를 상태 요청(예: Ctrl+T)을 위한 별개의 무시 가능한 시그널로 처리합니다. Linux는 SIGINFO 를 SIGPWR 의 동의어로 나열하며, 이는 기본적으로 종료됩니다. 이름과 기본 동작 모두에서 중요한 차이가 있습니다. SIGUSR1 프로세스 종료 사용자 정의 시그널 1 Term 사용자 정의 시그널 1 둘 다 프로세스를 종료합니다. SIGUSR2 프로세스 종료 사용자 정의 시그널 2 Term 사용자 정의 시그널 2 둘 다 프로세스를 종료합니다. SIGCLD (macOS에 없음) Ign SIGCHLD 의 동의어 Linux 전용: SIGCHLD 의 동의어입니다. macOS에는 SIGCLD 가 별개의 시그널로 나열되지 않습니다. SIGIOT (macOS에 없음) Core IOT 트랩. SIGABRT 의 동의어 Linux 전용: SIGABRT 의 동의어로 나열됩니다. macOS는 SIGABRT 가 \"이전 SIGIOT\"였다고 언급하지만, SIGIOT 를 별개의 시그널로 나열하지는 않습니다. SIGLOST (macOS에 없음) Term 파일 잠금 손실 (사용되지 않음) Linux 전용: 파일 잠금과 관련된 시그널이며 \"사용되지 않음\"으로 나열됩니다. macOS에는 없습니다. SIGPOLL (macOS에 없음) Term 폴링 가능한 이벤트 (Sys V); SIGIO 의 동의어 Linux 전용: SIGIO 의 동의어입니다. macOS에는 없습니다. SIGPWR (macOS에 없음) Term 전원 장애 (System V) Linux 전용: 전원 장애 이벤트에 대한 시그널입니다. Linux의 SIGINFO 는 이것의 동의어이며, 이는 macOS의 SIGINFO 와는 큰 차이입니다. SIGSTKFLT (macOS에 없음) Term 보조 프로세서의 스택 오류 (사용되지 않음) Linux 전용: 보조 프로세서의 스택 오류에 대한 시그널이며 \"사용되지 않음\"으로 나열됩니다. macOS에는 없습니다. SIGUNUSED (macOS에 없음) Core SIGSYS 와 동의어 Linux 전용: SIGSYS 의 동의어입니다. macOS에는 없습니다. 주요 차이점 요약: 특정 시그널의 기본 동작: SIGEMT: macOS (코어 덤프) vs. Linux (종료). SIGIO: macOS (무시) vs. Linux (종료). SIGXCPU: macOS (종료) vs. Linux (코어 덤프). SIGXFSZ: macOS (종료) vs. Linux (코어 덤프). SIGINFO 와 SIGPWR : macOS SIGINFO : 상태 요청(예: Ctrl+T)을 위한 별개의 시그널이며, 기본적으로 무시됩니다. Linux SIGINFO : SIGPWR (전원 장애)의 동의어이며, 기본적으로 종료됩니다. 이는 기능적으로 중요한 차이입니다. Linux 전용 시그널 (macOS에는 없음): SIGCLD: SIGCHLD 의 동의어. SIGIOT: SIGABRT 의 동의어 (macOS는 SIGABRT 가 이전에 SIGIOT 였다고 언급하지만 별개의 시그널로 나열하지는 않습니다). SIGLOST: 파일 잠금 손실 (사용되지 않음). SIGPOLL: SIGIO 의 동의어. SIGPWR: 전원 장애. SIGSTKFLT: 보조 프로세서의 스택 오류 (사용되지 않음). SIGUNUSED: SIGSYS 의 동의어. 시그널 번호: 일반적인 시그널의 이름은 대부분 일치하지만, 일부 시그널의 숫자 값은 두 운영 체제 간에 다를 수 있습니다. (제공된 목록에는 macOS의 번호만 표시되어 있습니다.) 예를 들어, SIGINFO 는 macOS에서 29입니다. 신호(Signal)란 무엇인가? 컴퓨팅에서 신호(Signal)는 운영체제(커널)가 특정 프로세스에게 비동기적인 이벤트가 발생했음을 알리기 위해 사용하는 제한된 형태의 프로세스 간 통신(IPC, Inter-Process Communication) 메커니즘입니다. 신호는 종종 \"소프트웨어 인터럽트(Software Interrupt)\"라고도 불리며, 프로세스가 정상적인 실행 흐름을 잠시 멈추고 해당 이벤트를 처리하도록 유도합니다. 신호의 발생 원인 신호는 다양한 상황에서 발생할 수 있습니다. 사용자의 직접적인 요청: 사용자가 키보드 조합(예: Ctrl+C -> SIGINT, Ctrl+Z -> SIGTSTP)을 통해 현재 실행 중인 프로세스에 신호를 보낼 때. 하드웨어 예외: 프로세스가 잘못된 연산(예: 0으로 나누기 -> SIGFPE)을 하거나, 허용되지 않은 메모리 공간에 접근(-> SIGSEGV)하는 등 하드웨어 수준의 오류가 발생했을 때. 다른 프로세스의 요청: 한 프로세스가 kill() 시스템 콜을 사용하여 다른 프로세스에 특정 신호를 보낼 때 (예: kill -9 [PID] ). 운영체제(커널)의 알림: 자식 프로세스가 종료되었을 때(-> SIGCHLD), 파이프의 읽기 쪽이 닫혔는데 쓰려고 할 때(-> SIGPIPE), 알람 타이머가 만료되었을 때(-> SIGALRM) 등 커널이 특정 상태 변화를 프로세스에 알릴 때. 신호에 대한 프로세스의 반응 (Signal Disposition) 프로세스는 신호를 수신했을 때 다음 세 가지 행동 중 하나를 취할 수 있습니다. 기본 행동(Default Action) 수행: 각 신호에는 미리 정해진 기본 행동이 있습니다. 이 행동은 프로세스 종료, 코어 덤프 생성 후 종료, 프로세스 중지, 신호 무시 등 다양합니다. 신호 잡기(Catch the Signal): 프로그래머가 특정 신호에 대한 처리 함수(Signal Handler)를 미리 등록해두면, 해당 신호가 도착했을 때 프로세스는 실행을 잠시 멈추고 등록된 핸들러 함수를 실행합니다. 핸들러 실행이 끝나면 원래 실행 흐름으로 복귀합니다. 이를 통해 프로세스는 종료되지 않고 특정 상황에 능동적으로 대처할 수 있습니다 (예: SIGINT 수신 시 임시 파일 정리 후 종료). 신호 무시(Ignore the Signal): 프로세스는 특정 신호를 무시하도록 설정할 수 있습니다. 이 경우 해당 신호가 도착해도 아무런 행동도 취하지 않습니다. 단, 두 가지 예외적인 신호가 있습니다. SIGKILL 과 SIGSTOP 은 어떤 경우에도 잡거나(Catch) 무시할(Ignore) 수 없습니다. 이들은 커널이 프로세스를 확실하게 제어하기 위한 최후의 수단으로, 항상 기본 행동(각각 프로세스 강제 종료, 프로세스 정지)을 수행합니다. 주요 신호(Signal) 상세 설명 두 운영체제에서 공통적으로 사용되는 중요한 신호들을 기능별로 묶어 상세히 설명하겠습니다. (1) 프로세스 종료 관련 신호 이 신호들은 주로 프로세스를 정상적으로 또는 강제적으로 종료시키는 데 사용됩니다. SIGHUP (1): Hang Up. 과거 모뎀 시절, 통신 연결이 끊겼을 때 보내던 신호에서 유래했습니다. 현대에는 터미널 세션이 종료되거나, 데몬(daemon) 프로세스에게 설정 파일을 다시 읽어오도록 지시하는 용도로 널리 사용됩니다. systemctl reload 나 service nginx reload 와 같은 명령어들이 내부적으로 SIGHUP을 보냅니다. SIGINT (2): Interrupt. 사용자가 키보드에서 Ctrl+C 를 눌렀을 때 터미널이 전송하는 신호입니다. \"프로그램을 중단해달라\"는 정중한 요청으로, 대부분의 프로그램은 이 신호를 받으면 진행 중인 작업을 정리하고(예: 임시 파일 삭제) 종료합니다. SIGQUIT (3): Quit. 사용자가 Ctrl+\\\\ 를 눌렀을 때 전송됩니다. SIGINT보다 더 강한 종료 요청이며, 기본 동작은 코어 덤프(Core Dump).md)를 생성하고 종료하는 것입니다. 코어 덤프는 프로세스가 비정상 종료될 당시의 메모리 상태를 담은 파일로, 디버깅에 매우 유용합니다. SIGTERM (15): Terminate. 가장 일반적인 \"소프트웨어 종료 신호\"입니다. kill 명령어에 PID만 입력하면 기본적으로 이 신호가 전송됩니다. SIGINT와 마찬가지로 \"정리하고 종료하라\"는 요청이며, 프로세스가 이 신호를 잡아서 안전하게 종료할 시간을 가질 수 있습니다. 가장 표준적이고 권장되는 종료 방식입니다. SIGKILL (9): Kill. \"절대적이고 무자비한 종료 명령\"입니다. 이 신호는 프로세스가 무시하거나 핸들러로 잡을 수 없습니다. 커널이 직접 프로세스의 실행을 중단시키기 때문에, 프로세스는 어떠한 정리 작업도 수행할 수 없습니다. 좀비 프로세스나 응답 없는 프로세스를 강제로 제거할 때 최후의 수단으로 사용됩니다. kill -9 로 유명합니다. (2) 하드웨어 및 소프트웨어 예외 관련 신호 프로세스가 잘못된 연산을 시도했을 때 커널에 의해 생성되는 신호들입니다. SIGSEGV (11): Segmentation Violation. \"세그멘테이션 오류\"로, C/C++ 개발자에게 가장 익숙한 신호입니다. 프로세스가 자신에게 할당되지 않은 메모리 영역에 접근하거나, 읽기 전용 영역에 쓰려고 할 때 발생합니다. 주로 포인터 관련 버그로 인해 발생합니다. SIGILL (4): Illegal Instruction. 프로세스가 CPU가 이해할 수 없는 기계어 코드(예: 손상된 코드, 존재하지 않는 명령어)를 실행하려고 할 때 발생합니다. SIGFPE (8): Floating-Point Exception. 부동소수점 연산에서 예외가 발생했을 때(예: 0으로 나누기, 오버플로우) 전송됩니다. 이름과 달리 정수 나눗셈에서 0으로 나눌 때도 이 신호가 발생할 수 있습니다. SIGBUS (10, 7 on Linux): Bus Error. SIGSEGV와 유사하지만, 원인이 다릅니다. SIGSEGV가 논리적인 메모리 접근 권한 위반이라면, SIGBUS는 물리적으로 유효하지 않은 주소에 접근하려 할 때 발생합니다. (예: CPU의 정렬(alignment) 요구사항 위반, 존재하지 않는 물리 메모리 주소 접근). SIGABRT (6): Abort. abort() 함수 호출을 통해 프로세스가 자기 자신에게 보내는 신호입니다. 복구 불가능한 심각한 내부 오류를 감지했을 때, 프로그램 스스로 비정상 종료를 선택하는 용도로 사용됩니다. 기본 동작은 코어 덤프 생성 후 종료입니다. (3) 작업 제어(Job Control) 관련 신호 터미널 셸 환경에서 프로세스의 실행을 일시 중지하거나 재개할 때 사용됩니다. SIGTSTP (18, 20 on Linux): Stop from Terminal. 사용자가 키보드에서 Ctrl+Z 를 눌렀을 때 전송되는 신호입니다. SIGSTOP과 달리 \"정중한 중지 요청\"이므로 프로세스가 무시하거나 처리할 수 있습니다. SIGSTOP (17, 19 on Linux): Stop. SIGKILL 의 중지 버전입니다. 프로세스가 이 신호를 무시하거나 핸들러로 잡을 수 없습니다. 운영체제는 이 신호를 통해 어떤 프로세스든 강제로 실행을 중지시킬 수 있습니다. SIGCONT (19, 18 on Linux): Continue. SIGSTOP 이나 SIGTSTP 에 의해 중지된 프로세스의 실행을 재개시키는 신호입니다. 셸에서 fg 또는 bg 명령어를 사용하면 이 신호가 전송됩니다. SIGTTIN (21) / SIGTTOU (22): 백그라운드 프로세스가 제어 터미널에 대한 입/출력을 시도할 때 발생합니다. 기본 동작은 프로세스를 중지시키는 것입니다. 이는 여러 백그라운드 작업이 동시에 터미널을 사용하려 할 때 발생하는 혼란을 방지하기 위함입니다. (4) 기타 주요 신호 SIGCHLD (20, 17 on Linux): Child Status Changed. 자식 프로세스가 종료되거나, 중지되거나, 재개될 때 부모 프로세스에게 전송되는 신호입니다. 부모 프로세스는 이 신호를 통해 자식의 상태 변화를 감지하고, wait() 계열 함수를 호출하여 좀비 프로세스가 되는 것을 방지할 수 있습니다. SIGPIPE (13): Pipe. 파이프(pipe)의 읽기 쪽(reader)이 닫혔는데 쓰기 쪽(writer)에서 계속 쓰려고 할 때 발생합니다. 예를 들어, ls -R / | head -n 10 명령어에서 head 가 10줄을 읽고 종료되면, ls 는 더 이상 데이터를 쓸 곳이 없어지고 SIGPIPE 신호를 받고 종료됩니다. SIGALRM (14): Alarm. alarm() 함수 호출로 설정된 실시간 타이머가 만료되었을 때 발생하는 신호입니다. SIGUSR1 (30, 10 on Linux) / SIGUSR2 (31, 12 on Linux): User-defined Signal. 이 두 신호는 시스템에 의해 특정 용도가 정해져 있지 않습니다. 개발자가 애플리케이션의 필요에 따라 자유롭게 의미를 부여하고 프로세스 간 통신에 사용할 수 있도록 예약된 신호입니다. macOS와 Linux의 신호(Signal) 차이점 두 운영체제는 POSIX 표준을 준수하기 때문에 대부분의 핵심 신호(SIGHUP, SIGINT, SIGKILL, SIGTERM 등)는 이름과 기능이 동일합니다. 하지만 각 운영체제의 역사적 배경(macOS는 BSD 기반, Linux는 System V와 독자적 발전)으로 인해 몇 가지 미묘하지만 중요한 차이점이 존재합니다. (1) 신호 번호(Signal Number)의 차이 가장 눈에 띄는 차이점 중 하나는 일부 신호의 번호가 다르다는 것입니다. Signal Name macOS Number Linux Number SIGBUS 10 7 SIGCHLD 20 17 SIGSTOP 17 19 SIGTSTP 18 20 SIGCONT 19 18 SIGUSR1 30 10 SIGUSR2 31 12 중요성: 이 차이점 때문에 프로그래머는 절대로 코드에 신호 번호를 하드코딩해서는 안 됩니다. 예를 들어, kill(pid, 9) 대신 kill(pid, SIGKILL) 과 같이 항상 표준 헤더 파일에 정의된 심볼릭 이름(symbolic name)을 사용해야 이식성 있는 코드를 작성할 수 있습니다. (2) 기본 동작(Default Action)의 차이 제공된 표에서 몇몇 신호의 기본 동작이 다르게 명시되어 있습니다. SIGXCPU (CPU 시간 초과), SIGXFSZ (파일 크기 제한 초과): macOS: terminate process (프로세스 종료) Linux: Core (코어 덤프 생성 후 종료) 분석: 이는 운영체제의 기본 철학 차이를 보여줍니다. Linux는 자원 제한을 초과한 경우, 원인 분석을 위한 디버깅 정보(코어 덤프)를 남기는 것을 기본으로 합니다. 반면 macOS는 더 간결하게 프로세스를 종료시키는 것을 기본 동작으로 설정했습니다. 물론 이 동작은 setrlimit() 시스템 콜을 통해 변경할 수 있습니다. SIGIO (I/O 가능): macOS: discard signal (신호 무시) Linux: Term (프로세스 종료) 분석: 이 신호는 비동기 I/O를 위해 사용되는데, 기본 동작이 다르다는 것은 이 기능을 사용할 때 운영체제별로 핸들러를 반드시 등록해야 함을 시사합니다. Linux의 경우 핸들러를 등록하지 않으면 프로세스가 예기치 않게 종료될 수 있습니다. (3) 운영체제 고유 또는 역사적 신호의 차이 macOS (BSD 계열)의 특징적 신호: SIGINFO (29): BSD 시스템에서 유래한 신호로, 터미널에서 Ctrl+T 를 누르면 전송됩니다. 실행 중인 프로세스의 상태 정보(예: dd 명령어의 진행 상황)를 출력하도록 요청하는 데 사용됩니다. Linux에는 기본적으로 같은 이름의 신호가 없지만, 일부 시스템에서는 SIGPWR 에 매핑되기도 합니다. SIGEMT (7): Emulate Instruction Trap. PDP-11과 같은 구형 하드웨어에서 사용되던 에뮬레이터 트랩 신호입니다. 현대적인 시스템에서는 거의 사용되지 않지만, BSD의 역사적 유산으로 macOS 목록에 남아있습니다. Linux 목록에도 존재는 하지만 비표준으로 취급됩니다. Linux의 특징적 신호: SIGPWR (Power Failure): System V에서 유래한 신호로, 시스템 전원에 문제가 생겼을 때(예: UPS 배터리 부족) 시스템 관리 데몬에게 알려 안전한 종료 절차를 밟도록 하는 데 사용됩니다. SIGSTKFLT (Stack Fault on coprocessor): 과거 수치 연산 보조 프로세서(coprocessor)의 스택 오류를 위한 신호였으나, 현대 x86 아키텍처에서는 사용되지 않아 사실상 사장되었습니다. SIGPOLL: System V의 SIGIO 와 동일한 역할을 하는 신호입니다. Linux는 두 이름 모두를 지원하여 호환성을 높였습니다. SIGCLD, SIGIOT: 각각 SIGCHLD , SIGABRT 의 오래된 동의어(synonym)입니다. 과거 다른 Unix 버전과의 호환성을 위해 남아있습니다. 결론 macOS와 Linux의 신호 시스템은 POSIX라는 강력한 표준 아래에서 높은 수준의 호환성을 보입니다. 개발자들은 SIGINT , SIGTERM , SIGSEGV 와 같은 핵심 신호들이 두 플랫폼에서 거의 동일하게 작동할 것이라고 기대할 수 있습니다. 하지만 그 기저에는 BSD(macOS)와 System V(Linux)라는 서로 다른 UNIX 계보의 영향이 남아있습니다. 이로 인해 일부 비표준적이거나 역사적인 신호( SIGINFO , SIGPWR )의 존재 유무, 특정 오류 상황에서의 기본 행동( SIGXCPU 의 코어 덤프 여부), 그리고 동의어의 사용 등에서 미세한 차이가 나타납니다. 따라서 높은 이식성이 요구되는 시스템 프로그래밍을 할 때는 POSIX 표준에 정의된 신호들을 중심으로 코드를 작성하고, 특정 운영체제에만 의존적인 신호의 사용은 가급적 피하는 것이 바람직합니다. 제공된 두 표는 이러한 차이점과 공통점을 명확하게 보여주는 훌륭한 자료입니다.",
      "frontmatter": {
        "tags": [
          "ai-content",
          "system-programing"
        ],
        "date": "2025-09-09T23:07:14+09:00",
        "lastmod": "2025-10-23T04:34:07+09:00"
      }
    },
    "spring 외부설정": {
      "path": "/spring-외부설정/",
      "filename": "spring 외부설정",
      "content": "application.properties 설정 우선순위 (가장 높음 → 낮음) 우선순위 소스 1 Devtools 전역 설정 ( .spring-boot-devtools.properties ) 2 테스트 애너테이션 ( @TestPropertySource ) 3 커맨드 라인 아규먼트 ( --server.port=8080 ) 4 JVM 시스템 프로퍼티 ( -Dproperty=value ) 5 OS 환경 변수 6 RandomValuePropertySource ( ${random.*} ) 7 JAR 외부의 application-{profile}.properties 8 JAR 외부의 application.properties 9 JAR 내부의 application-{profile}.properties 10 JAR 내부의 application.properties 11 @PropertySource / @ConfigurationProperties application.properties application.properties 우선 순위 application.properties 파일은 아래 4가지 경로에 만들 수 있지만, 우선순위에 따라 덮어쓰기가 된다. file:./config/ file:./ classpath:/config/ classpath:/ file 경로 : src 폴더 하위에 application.properties classpath 경로 : 기본적으로 만들어지는 resources 폴더 하위 application.properties application.properties 설정 파일 로딩 원리 Spring Boot는 시작 시 Environment 객체를 생성합니다. 이후 정해진 우선순위에 따라 application.properties, 환경 변수 등 다양한 소스(Source)의 설정 값을 읽어 Environment에 채워 넣습니다. 애플리케이션의 다른 부분들은 이 Environment를 통해 설정 값을 사용합니다. 설정 우선순위 핵심 원칙은 외부 설정이 내부 설정을 덮어쓴다(override)는 것입니다. 즉, 애플리케이션을 실행하는 시점에 주입하는 설정이 코드에 포함된 설정보다 우선순위가 높습니다. 실무 핵심 순서 (높은 순): 테스트 코드 내 설정 (@TestPropertySource) 커맨드 라인 인수 (java -jar app.jar --server.port=9000) OS 환경 변수 (export SERVER_PORT=9002) JAR 외부의 설정 파일 (./config/application.properties) JAR 내부의 설정 파일 (classpath:/application.properties) @PropertySource 로드 파일 SpringApplication 기본값 @ConfigurationProperties 고급 활용 프로퍼티 값을 타입-세이프(Type-safe)하게 객체로 바인딩하는 기능입니다. 구조: 중첩 객체, 리스트, 맵 등 복잡한 구조의 프로퍼티를 객체에 자동으로 매핑할 수 있습니다. 유효성 검사: @Validated 애너테이션과 spring-boot-starter-validation 의존성을 추가하면, 로드된 값에 대해 @NotNull, @Min 등 JSR-303 유효성 검사를 적용할 수 있습니다. 외부 설정 파일 주입 spring.config.location: 지정된 위치의 파일만 사용합니다. (기본 경로 무시) spring.config.additional-location: 기본 경로를 유지하면서 지정된 파일을 추가로 로드합니다. spring.config.import: application.properties 내에서 다른 설정 파일을 불러옵니다. optional: 접두사를 붙이면 파일이 없어도 오류가 발생하지 않습니다. spring.config.import=optional:classpath:aws.properties @PropertySource vs application.properties 우선순위: @PropertySource로 로드한 설정은 application.properties보다 우선순위가 낮습니다. 즉, 두 파일에 동일한 키가 있으면 application.properties의 값이 적용됩니다. 용도: @PropertySource는 특정 기능에 대한 설정을 모듈화하거나 레거시 설정을 통합할 때 유용합니다. 주 설정 파일은 application.properties를 사용하는 것이 표준입니다. IDE 지원 (자동 완성) spring-boot-configuration-processor 의존성을 추가하면, @ConfigurationProperties로 정의한 설정 키에 대해 IDE가 자동 완성 및 설명을 제공하여 개발 생산성을 크게 향상시킵니다. 민감 정보 관리 민감한 정보(DB 비밀번호, API 키 등)는 코드에 직접 작성하지 않는 것이 원칙입니다. 관리 방법: OS 환경 변수나 외부 설정 파일을 사용합니다. 권장 방식: 클라우드 환경에서는 Vault, AWS SSM, GCP Secret Manager와 같은 외부 보안 저장소와 연동하는 것이 가장 안전합니다. 프로필과 조건부 설정 @Profile: \"dev\", \"prod\" 와 같이 특정 프로필이 활성화될 때만 특정 빈(Bean)을 등록하도록 제어합니다. @ConditionalOnProperty: 특정 프로퍼티 값에 따라 빈의 등록 여부를 결정합니다. 기능 플래그(Feature Flag)를 구현할 때 매우 유용합니다. 테스트 설정 설정 분리: src/test/resources/application-test.properties와 같이 테스트용 설정 파일을 분리하여 관리합니다. @TestPropertySource: 특정 테스트 케이스에만 적용할 설정을 높은 우선순위로 주입할 수 있어, 독립적인 테스트 환경을 구성하는 데 효과적입니다. .properties vs .yml 우선순위: 동일한 경로에 두 파일이 모두 존재할 경우, application.properties 파일이 .yml 파일보다 항상 우선순위가 높습니다. 특징: application.properties: key=value 형식. 우선순위가 더 높음. application.yml: YAML 형식. 계층 구조 표현이 용이해 가독성이 좋음. 핵심 실무 팁 보안: 민감 정보(비밀번호, API 키 등)는 절대로 Git에 커밋하지 마세요. 분리: 테스트 환경을 위한 프로필(application-test.properties)을 분리하여 관리하세요. 컨벤션: 팀 내에서 .properties와 .yml 중 하나를 주력으로 사용하기로 컨벤션을 정하여 혼란을 방지하세요. (최근에는 가독성이 좋은 .yml이 선호되는 추세입니다.)",
      "frontmatter": {
        "date": "2025-08-09T16:26:56+09:00",
        "lastmod": "2025-10-22T03:20:38+09:00"
      }
    },
    "universal clipboard": {
      "path": "/universal-clipboard/",
      "filename": "universal clipboard",
      "content": "중앙 서버 설계 중앙 서버는 핵심적인 역할을 하며, 다음과 같은 기능을 제공해야 합니다. 사용자 인증(Authentication) : 사용자를 식별하고 데이터를 보호하기 위해 OAuth 2.0 또는 JWT(JSON Web Token) 기반의 인증 시스템을 사용. 각 사용자의 클립보드 데이터는 개인적으로 격리되어야 함. 클립보드 데이터 저장 : 데이터베이스에 클립보드 내용을 저장. 텍스트, 이미지 등 다양한 형식의 데이터를 처리할 수 있도록 설계. 단순한 db 를 사용 sqlite, mongoDB 실시간 동기화 : 클라이언트가 클립보드 데이터를 변경하면 즉시 서버로 전송. 다른 클라이언트는 서버로부터 최신 데이터를 받아옴. sse 방식으로 동기화 빠른 재연결 수립 데이터 보안 : 이후 설계 : HTTPS를 통해 데이터를 암호화하여 전송. 이후 설계 : 민감한 데이터는 AES 또는 RSA와 같은 암호화 알고리즘으로 암호화. 클라이언트 애플리케이션 설계 각 플랫폼별로 클라이언트 애플리케이션을 개발해야 합니다. 각 플랫폼의 특성을 고려하여 구현해야 합니다. macOS/iOS : 클립보드 모니터링 : NSPasteboard (macOS), UIPasteboard (iOS)를 사용하여 클립보드 변경을 감지. 백그라운드 실행 : macOS에서는 앱이 백그라운드에서 실행되도록 설정. 네트워크 통신 : URLSession 또는 Alamofire를 사용하여 서버와 통신. Android : 클립보드 모니터링 : ClipboardManager 를 사용하여 클립보드 변경을 감지. 서비스 실행 : Foreground Service를 사용하여 앱이 백그라운드에서도 동작하도록 설정. 네트워크 통신 : Retrofit 또는 OkHttp를 사용하여 서버와 통신. Windows : 클립보드 모니터링 : System.Windows.Forms.Clipboard 또는 WinAPI를 사용하여 클립보드 변경을 감지. 백그라운드 실행 : Windows 서비스 또는 BackgroundWorker를 사용. 네트워크 통신 : HttpClient를 사용하여 서버와 통신.",
      "frontmatter": {
        "date": "2025-04-17T00:43:00+09:00",
        "lastmod": "2025-06-03T10:17:57+09:00"
      }
    },
    "university algorizm": {
      "path": "/university-algorizm/",
      "filename": "university algorizm",
      "content": "제3장: 정렬 배열을 크기에 기초해 분할 후 합쳐서 정렬 크기가 1인 배열과 (n - 1)인 배열로 분할 : 삽입 정렬 크기가 𝑛/2인 두 개의 배열로 분할 : 합병 정렬 배열을 특정 값에 기초하여 분할 후 합쳐서 정렬 최솟값에 기초하여 분할 : 선택 정렬, 힙 정렬 기준 값(예: 첫 번째 값)에 기초하여 분할 : 빠른 정렬 네, 제공해주신 알고리즘 슬라이드(19~24장) 내용을 바탕으로 삽입 정렬(Insertion Sort)에 대해 30,000자 이상으로 매우 상세하고 단계적으로 설명해 드리겠습니다. C++ 예시 코드와 함께 각 개념을 깊이 있게 다루겠습니다. 삽입 정렬(Insertion Sort) 목차 삽입 정렬이란 무엇인가? (개요) 핵심 아이디어: 정렬된 부분과 정렬되지 않은 부분 가장 직관적인 비유: 카드놀이 삽입 정렬의 핵심 메커니즘: 하나의 요소 삽입하기 (Slides 19-21) 목표: 정렬되지 않은 부분의 첫 요소를 정렬된 부분의 올바른 위치에 넣기 1단계: 삽입 위치 찾기 (Naive 접근법과 그 문제점) 2단계: 문제 해결 및 올바른 알고리즘 (요소 저장과 이동) 그림으로 이해하는 단일 삽입 과정 전체 삽입 정렬 알고리즘 (Slide 22) 알고리즘의 구조: 외부 루프와 내부 루프 상세한 단계별 실행 예제 C++ 전체 구현 코드 및 해설 삽입 정렬 함수 구현 전체 실행 코드 및 설명 시간 및 공간 복잡도 분석 (Slide 23) 기본 연산(Basic Operation)의 정의 최악의 경우 (Worst Case) 분석 최선의 경우 (Best Case) 분석 평균적인 경우 (Average Case) 분석 공간 복잡도 분석 삽입 정렬의 주요 특징 및 심층 분석 (Slide 24) 장점 1: 적응성(Adaptive) - 거의 정렬된 데이터에 강력함 장점 2: 제자리 정렬(In-place) - 추가 공간 불필요 장점 3: 안정 정렬(Stable Sort) 장점 4: 온라인(Online) 알고리즘 단점: 비효율적인 성능 (O(n²)) 다른 정렬 알고리즘과의 비교 및 활용 선택 정렬(Selection Sort)과의 비교 버블 정렬(Bubble Sort)과의 비교 고급 정렬(퀵, 병합, 힙 정렬)과의 관계 삽입 정렬은 언제 유용한가? 요약 및 결론 삽입 정렬이란 무엇인가? (개요) 삽입 정렬(Insertion Sort)은 가장 단순하고 직관적인 정렬 알고리즘 중 하나입니다. 이름에서 알 수 있듯이, 정렬되지 않은 데이터를 하나씩 가져와 이미 정렬된 부분의 올바른 위치에 삽입하는 방식을 반복하여 전체를 정렬합니다. 핵심 아이디어: 정렬된 부분과 정렬되지 않은 부분 삽입 정렬은 배열을 논리적으로 두 부분으로 나누어 생각합니다. 정렬된 부분(Sorted Sub-array): 배열의 앞부분으로, 이 부분의 원소들은 항상 정렬된 상태를 유지합니다. 정렬되지 않은 부분(Unsorted Sub-array): 배열의 뒷부분으로, 아직 처리되지 않은 원소들이 남아있습니다. 알고리즘은 정렬되지 않은 부분의 가장 앞에 있는 원소를 하나씩 꺼내어, 정렬된 부분의 끝에서부터 비교하면서 자신의 올바른 위치를 찾아 삽입합니다. 이 과정이 끝나면 정렬된 부분의 크기는 하나 늘어나고, 정렬되지 않은 부분의 크기는 하나 줄어듭니다. 이 작업을 정렬되지 않은 부분이 없어질 때까지 반복합니다. 가장 직관적인 비유: 카드놀이 삽입 정렬은 우리가 손에 든 카드를 정렬하는 방식과 매우 유사합니다. 바닥에 놓인 카드 뭉치(정렬되지 않은 부분)에서 카드 한 장을 집습니다. 이미 손에 들고 있는 정렬된 카드들(정렬된 부분)과 비교합니다. 새로 집은 카드를 손에 든 카드들 사이의 올바른 위치에 꽂아 넣습니다. 이때, 새 카드가 들어갈 자리를 만들기 위해 기존 카드들을 한 칸씩 뒤로 밀어낼 수 있습니다. 바닥에 카드가 없을 때까지 이 과정을 반복합니다. 이 비유처럼, 삽입 정렬은 '하나를 꺼내서', '적절한 위치를 찾아', '삽입하는' 단순한 원리를 기반으로 동작합니다. 삽입 정렬의 핵심 메커니즘: 하나의 요소 삽입하기 (Slides 19-21) 삽입 정렬의 전체 과정을 이해하기 위해, 먼저 한 번의 반복, 즉 하나의 요소를 정렬된 부분에 삽입하는 과정을 깊이 있게 살펴보겠습니다. 이것이 알고리즘의 심장부입니다. 가정: 배열 A 가 있고, A[0...i-1] 은 이미 정렬되어 있습니다. 이제 A[i] 를 이 정렬된 부분에 삽입하여 A[0...i] 전체를 정렬된 상태로 만들려고 합니다. 1단계: 삽입 위치 찾기 (Naive 접근법과 그 문제점) 슬라이드 19의 아이디어는 A[i] 를 삽입할 위치 j 를 찾는 것입니다. A[j] ≤ A[i] ≤ A[j+1] 을 만족하는 j 를 찾아야 합니다. 이를 위해 정렬된 부분의 맨 끝( j = i-1 )부터 시작하여 왼쪽으로 이동하며 A[i] 보다 작은 값을 만날 때까지 비교합니다. 슬라이드 20은 이를 코드로 표현합니다. j = i – 1 while (A[j] > A[i]) j = j – 1 // 루프가 끝나면 A[j] <= A[i] 이므로, A[i]는 A[j+1] 위치에 들어가야 한다. 문제점: 이 코드에는 치명적인 결함이 있습니다. 만약 A[i] 가 A[0...i-1] 에 있는 모든 요소보다 작다면 어떻게 될까요? 예를 들어 배열이 [5, 8, 10, 2] 이고 i=3 일 때, A[3] 는 2 입니다. j = 2 . A[2] (10) > A[3] (2). j 는 1이 됩니다. j = 1 . A[1] (8) > A[3] (2). j 는 0이 됩니다. j = 0 . A[0] (5) > A[3] (2). j 는 -1이 됩니다. while 루프의 다음 조건 검사에서 A[j] , 즉 A[-1] 에 접근하게 됩니다. 이는 배열의 범위를 벗어나는 접근(Out-of-Bounds Access)으로, 프로그램 오류를 유발합니다. 따라서 단순히 위치만 찾는 방식은 문제가 있습니다. 2단계: 문제 해결 및 올바른 알고리즘 (요소 저장과 이동) 슬라이드 21은 이 문제를 해결하는 우아한 방법을 제시합니다. 위치를 먼저 찾고 나중에 삽입하는 것이 아니라, 위치를 찾는 과정과 삽입할 공간을 만드는 과정을 동시에 수행합니다. 핵심 전략: 삽입할 요소 임시 저장: 먼저 A[i] 의 값을 insertElement 라는 변수에 복사해 둡니다. 이렇게 하면 A[i] 의 원래 자리는 비어있는 '구멍(hole)'으로 간주할 수 있습니다. 이동(Shift)하며 위치 찾기: 정렬된 부분의 끝( j = i-1 )부터 시작하여, insertElement 보다 큰 요소들을 만나면 오른쪽으로 한 칸씩 밀어냅니다(shift). 삽입: insertElement 보다 작거나 같은 요소를 만나거나, 배열의 맨 앞에 도달하면 while 루프가 멈춥니다. 바로 그 멈춘 위치의 다음 칸( j+1 )이 insertElement 가 들어갈 최종 위치입니다. 알고리즘 코드 (슬라이드 21): insertElement = A[i] // 1. 삽입할 요소 저장 j = i – 1 // 2. 이동하며 위치 찾기 while (j >= 0 and A[j] > insertElement) { A[j+1] = A[j] // insertElement보다 큰 요소를 오른쪽으로 한 칸 이동 j = j - 1 } // 3. 최종 위치에 삽입 A[j+1] = insertElement 이 알고리즘이 앞선 문제를 해결하는 방법: j >= 0 조건이 while 루프에 추가되었습니다. 이 덕분에 j 가 -1이 되면 루프가 즉시 종료되어 배열의 범위를 벗어나는 오류를 원천적으로 방지합니다. 요소들을 한 칸씩 밀어내면서 빈 공간을 만들기 때문에, 최종적으로 삽입할 때 다른 요소들을 덮어쓸 걱정이 없습니다. 그림으로 이해하는 단일 삽입 과정 배열 A = [3, 8, 12, 15, 7, ...] 가 있고, i=4 일 때, A[4]=7 을 삽입하는 과정을 보겠습니다. A[0..] 은 [3, 8, 12, 15] 로 정렬된 상태입니다. 초기 상태: insertElement = 7 j = 3 배열: [3, 8, 12, 15, 15] (논리적으로 A[4] 는 비어있다고 생각) A: [ 3, 8, 12, 15, ? ] insertElement: 7 ^ j while 루프 (1차 반복): j=3 >= 0 이고, A[3] (15) > insertElement (7) -> 참 A[4] = A[3] . (15를 오른쪽으로 이동) j 는 2가 됨. 배열: [3, 8, 12, 15, 15] A: [ 3, 8, 12, ?, 15 ] insertElement: 7 ^ j while 루프 (2차 반복): j=2 >= 0 이고, A[2] (12) > insertElement (7) -> 참 A[3] = A[2] . (12를 오른쪽으로 이동) j 는 1이 됨. 배열: [3, 8, 12, 12, 15] A: [ 3, 8, ?, 12, 15 ] insertElement: 7 ^ j while 루프 (3차 반복): j=1 >= 0 이고, A[1] (8) > insertElement (7) -> 참 A[2] = A[1] . (8을 오른쪽으로 이동) j 는 0이 됨. 배열: [3, 8, 8, 12, 15] A: [ 3, ?, 8, 12, 15 ] insertElement: 7 ^ j while 루프 (4차 반복): j=0 >= 0 이고, A[0] (3) > insertElement (7) -> 거짓. 루프 종료! 현재 j 는 0. 삽입: A[j+1] = insertElement -> A[1] = 7 . 최종 배열: [3, 7, 8, 12, 15] 이제 A[0..] 가 완벽하게 정렬되었습니다. 이 과정을 전체 배열에 대해 반복하는 것이 삽입 정렬입니다. 전체 삽입 정렬 알고리즘 (Slide 22) 하나의 요소를 삽입하는 방법을 알았으니, 이제 외부 루프를 추가하여 전체 배열을 정렬하는 완전한 알고리즘을 만들 수 있습니다. 알고리즘의 구조: 외부 루프와 내부 루프 슬라이드 22의 알고리즘은 두 개의 중첩된 루프 구조를 가집니다. 외부 for 루프: 정렬되지 않은 부분의 첫 번째 요소를 선택하는 역할을 합니다. 인덱스 i 는 1부터 n-1 까지 증가합니다. i 가 1부터 시작하는 이유는, 크기가 1인 배열( A[0] )은 그 자체로 이미 정렬되어 있기 때문입니다. 내부 while 루프: for 루프에서 선택된 A[i] 를 A[0...i-1] 의 올바른 위치에 삽입하는 역할을 합니다. 앞서 자세히 분석한 바로 그 로직입니다. InsertionSort(A[0..n−1]) // 입력: 정렬 안된 배열 A[0..n-1] // 출력: 정렬된 배열 A[0..n-1] 1 for (i = 1; i < n; i++) { 2 insertElement = A[i] 3 j = i – 1 4 while ( j ≥ 0 and A[ j] > insertElement) { 5 A[ j + 1] = A[ j] 6 j = j – 1 } 7 A[ j + 1] = insertElement } 상세한 단계별 실행 예제 배열 A = [5, 2, 4, 6, 1, 3] (n=6)을 정렬하는 과정을 따라가 보겠습니다. | 기호는 정렬된 부분과 정렬되지 않은 부분을 구분합니다. 초기 상태: [| 5, 2, 4, 6, 1, 3] (크기 0인 정렬된 부분. 실질적으로 A[0] 부터 시작) i = 1 : A[1] = 2 를 삽입. insertElement = 2 , j = 0 while ( j=0 >= 0 and A[0] (5) > 2): 참 A[1] = A[0] . -> [5, 5, 4, 6, 1, 3] j = -1 . 루프 종료. A[j+1] 즉 A[0] = 2 . 결과: [2, 5 | 4, 6, 1, 3] i = 2 : A[2] = 4 를 삽입. insertElement = 4 , j = 1 while ( j=1 >= 0 and A[1] (5) > 4): 참 A[2] = A[1] . -> [2, 5, 5, 6, 1, 3] j = 0 . while ( j=0 >= 0 and A[0] (2) > 4): 거짓. 루프 종료. A[j+1] 즉 A[1] = 4 . 결과: [2, 4, 5 | 6, 1, 3] i = 3 : A[3] = 6 를 삽입. insertElement = 6 , j = 2 while ( j=2 >= 0 and A[2] (5) > 6): 거짓. 루프 즉시 종료. A[j+1] 즉 A[3] = 6 . (변화 없음) 결과: [2, 4, 5, 6 | 1, 3] i = 4 : A[4] = 1 를 삽입. insertElement = 1 , j = 3 while 루프는 j 가 3, 2, 1, 0일 때 모두 참. 모든 요소를 한 칸씩 오른쪽으로 민다. [2, 4, 5, 6, 6, 3] [2, 4, 5, 5, 6, 3] [2, 4, 4, 5, 6, 3] [2, 2, 4, 5, 6, 3] j 가 -1이 되어 루프 종료. A[j+1] 즉 A[0] = 1 . 결과: [1, 2, 4, 5, 6 | 3] i = 5 : A[5] = 3 을 삽입. insertElement = 3 , j = 4 while ( j=4 >= 0 and A[4] (6) > 3): 참. j 는 3. -> [1, 2, 4, 5, 6, 6] while ( j=3 >= 0 and A[3] (5) > 3): 참. j 는 2. -> [1, 2, 4, 5, 5, 6] while ( j=2 >= 0 and A[2] (4) > 3): 참. j 는 1. -> [1, 2, 4, 4, 5, 6] while ( j=1 >= 0 and A[1] (2) > 3): 거짓. 루프 종료. A[j+1] 즉 A[2] = 3 . 결과: [1, 2, 3, 4, 5, 6 |] 최종 정렬 완료: [1, 2, 3, 4, 5, 6] C++ 전체 구현 코드 및 해설 슬라이드의 의사코드를 표준 C++로 구현해 보겠습니다. std::vector 를 사용하여 동적 배열을 처리합니다. 삽입 정렬 함수 구현 #include <iostream> #include <vector> void insertionSort(std::vector<int>& arr) { // n은 배열의 크기 int n = arr.size(); // 외부 루프: i는 1부터 시작하여 배열의 끝까지 순회 // arr[0...i-1]은 정렬된 부분, arr[i...n-1]은 정렬되지 않은 부분 for (int i = 1; i < n; ++i) { // Step 1: 삽입할 요소를 키(또는 insertElement)로 저장 int key = arr[i]; // Step 2: 삽입할 위치를 찾기 위해 정렬된 부분의 끝에서부터 시작 int j = i - 1; // Step 3: 내부 루프 - key가 들어갈 위치를 찾으면서 // key보다 큰 요소들을 오른쪽으로 한 칸씩 이동 // j >= 0 조건은 배열의 시작을 벗어나지 않도록 보장 // arr[j] > key 조건은 삽입 위치를 찾는 핵심 비교 while (j >= 0 && arr[j] > key) { arr[j + 1] = arr[j]; // 요소를 오른쪽으로 이동 j = j - 1; // 비교할 다음 요소로 이동 (왼쪽으로) } // Step 4: 찾은 위치(j+1)에 key를 삽입 // 루프가 끝난 시점의 j는 key보다 작거나 같은 첫 번째 요소의 인덱스이거나 -1 arr[j + 1] = key; } } 전체 실행 코드 및 설명 #include <iostream> #include <vector> // 유틸리티 함수: 벡터를 출력 void printArray(const std::vector<int>& arr) { for (int val : arr) { std::cout << val << \" \"; } std::cout << std::endl; } // 위에서 정의한 insertionSort 함수 void insertionSort(std::vector<int>& arr) { int n = arr.size(); for (int i = 1; i < n; ++i) { int key = arr[i]; int j = i - 1; while (j >= 0 && arr[j] > key) { arr[j + 1] = arr[j]; j = j - 1; } arr[j + 1] = key; } } int main() { std::vector<int> data = {5, 2, 4, 6, 1, 3}; std::cout << \"Original array: \"; printArray(data); insertionSort(data); std::cout << \"Sorted array: \"; printArray(data); return 0; } main 함수: 정렬할 데이터를 std::vector 로 생성하고, 정렬 전과 후의 배열 상태를 printArray 함수를 통해 출력하여 알고리즘의 동작을 확인합니다. insertionSort 함수 해설: key 변수는 슬라이드의 insertElement 에 해당합니다. 외부 for 루프는 i=1 부터 n-1 까지 총 n-1 번 실행됩니다. 내부 while 루프는 key 의 값과 정렬된 부분의 요소들에 따라 실행 횟수가 달라집니다. 이것이 삽입 정렬의 성능을 결정하는 가장 중요한 부분입니다. 시간 및 공간 복잡도 분석 (Slide 23) 알고리즘의 효율성을 평가하기 위해 시간과 공간 복잡도를 분석합니다. 기본 연산(Basic Operation)의 정의 알고리즘의 실행 시간을 지배하는 가장 빈번하게 수행되는 연산을 '기본 연산'으로 정의합니다. 슬라이드 23에서는 삽입 정렬의 기본 연산을 내부 while 루프의 조건 비교인 A[j] > insertElement 로 정의했습니다. 우리는 이 비교 연산의 횟수를 분석할 것입니다. 최악의 경우 (Worst Case) 분석 언제 최악의 상황이 발생하는가? 배열이 역순으로 정렬되어 있을 때입니다. 예를 들어, [6, 5, 4, 3, 2, 1] . 이 경우, for 루프의 각 반복( i )에서 insertElement 는 항상 정렬된 부분( A[0...i-1] )의 모든 요소보다 작습니다. 따라서 내부 while 루프는 항상 j 가 -1 이 될 때까지, 즉 정렬된 부분의 모든 요소를 비교하고 이동시켜야 합니다. i = 1 일 때: 비교 1번 i = 2 일 때: 비교 2번 i = 3 일 때: 비교 3번 ... i = n-1 일 때: 비교 n-1 번 따라서 기본 연산의 총 수행 횟수는 등차수열의 합입니다. 1 + 2 + 3 + ... + (n-1) = (n-1) * (n-1 + 1) / 2 = n(n-1) / 2 이 식은 (n² - n) / 2 이므로, 최고차항은 n² 입니다. Big-O 표기법으로는 O(n²), Big-Theta 표기법으로는 Θ(n²) 입니다. 최선의 경우 (Best Case) 분석 언제 최선의 상황이 발생하는가? 배열이 이미 정렬되어 있을 때입니다. 예를 들어, [1, 2, 3, 4, 5, 6] . 이 경우, for 루프의 각 반복( i )에서 insertElement ( A[i] )는 A[j] ( A[i-1] )보다 항상 크거나 같습니다. 따라서 내부 while 루프의 조건 A[j] > insertElement 는 항상 즉시 거짓이 됩니다. i = 1 일 때: 비교 1번 i = 2 일 때: 비교 1번 ... i = n-1 일 때: 비교 1번 총 비교 횟수는 n-1 번입니다. 따라서 최선의 경우 시간 복잡도는 O(n), Θ(n) 입니다. 이는 삽입 정렬의 매우 중요한 특징입니다. 평균적인 경우 (Average Case) 분석 입력 데이터가 무작위로 분포되어 있을 경우를 가정합니다. 평균적으로, 정렬되지 않은 부분에서 가져온 insertElement 는 정렬된 부분의 중간쯤에 삽입될 것입니다. 즉, i 번째 요소는 정렬된 i 개의 요소 중 절반 정도(약 i/2 개)와 비교하고 이동해야 합니다. 총 비교 횟수는 대략 1/2 + 2/2 + 3/2 + ... + (n-1)/2 가 됩니다. 이는 (1/2) * (1 + 2 + ... + (n-1)) = (1/2) * n(n-1)/2 = n(n-1)/4 입니다. 최고차항은 여전히 n² 이므로, 평균 시간 복잡도 또한 O(n²), Θ(n²) 입니다. 공간 복잡도 분석 삽입 정렬은 정렬을 위해 입력 배열 외에 추가적인 데이터 구조를 필요로 하지 않습니다. i , j , key 와 같은 몇 개의 변수만을 사용하며, 이 변수들의 수는 입력 크기 n 에 관계없이 일정합니다. 이를 제자리 정렬(In-place sort) 이라고 하며, 공간 복잡도는 O(1) 입니다. 삽입 정렬의 주요 특징 및 심층 분석 (Slide 24) 삽입 정렬은 단순한 O(n²) 알고리즘 이상의 중요한 특징들을 가지고 있습니다. 장점 1: 적응성(Adaptive) - 거의 정렬된 데이터에 강력함 슬라이드 24에서 언급된 \"값들이 거의 정렬되어 있을 때 빠르게 실행\"된다는 점이 바로 적응성입니다. 만약 입력 배열이 거의 정렬되어 있다면, 각 요소를 삽입할 때 이동(shift)하는 거리가 매우 짧습니다. 내부 while 루프가 몇 번 실행되지 않고 금방 종료되기 때문에, 전체 실행 시간은 O(n²)이 아닌 O(n)에 가깝게 됩니다. 이러한 특성 때문에, 실시간으로 데이터가 약간씩 변경되거나 새로운 데이터가 기존 정렬된 리스트의 끝에 추가되는 경우, 삽입 정렬은 매우 효율적인 선택이 될 수 있습니다. 장점 2: 제자리 정렬(In-place) - 추가 공간 불필요 앞서 분석했듯이, 삽입 정렬은 O(1)의 추가 공간만을 사용합니다. 이는 메모리 사용량이 매우 중요한 임베디드 시스템이나 특정 환경에서 큰 장점이 됩니다. 병합 정렬(Merge Sort)이 O(n)의 추가 공간을 필요로 하는 것과 대조적입니다. 장점 3: 안정 정렬(Stable Sort) 안정 정렬이란, 값이 같은 요소들의 상대적인 순서가 정렬 후에도 그대로 유지되는 것을 의미합니다. 예를 들어, (값, 속성) 형태의 데이터 [(5, 'A'), (3, 'B'), (5, 'C')] 를 값 기준으로 정렬할 때, 안정 정렬 알고리즘은 항상 [(3, 'B'), (5, 'A'), (5, 'C')] 를 반환합니다. (5, 'A') 가 (5, 'C') 보다 원래 앞에 있었으므로, 정렬 후에도 이 순서가 유지됩니다. 삽입 정렬은 안정 정렬입니다. 그 이유는 내부 while 루프의 조건 A[j] > insertElement 때문입니다. 만약 A[j] 와 insertElement 의 값이 같다면( > 가 아니므로) 루프는 멈추고, insertElement 는 A[j] 의 오른쪽에 삽입됩니다. 따라서 기존에 있던 요소의 위치를 침범하지 않아 상대적 순서가 보존됩니다. 장점 4: 온라인(Online) 알고리즘 삽입 정렬은 '온라인' 특성을 가집니다. 즉, 모든 데이터를 미리 받지 않고, 데이터가 들어오는 대로 즉시 정렬을 수행할 수 있습니다. 이미 정렬된 n 개의 요소 리스트가 있을 때, 새로운 데이터 n+1 번째가 들어오면, 이 하나만 올바른 위치에 삽입하면 되므로 효율적입니다. 단점: 비효율적인 성능 (O(n²)) 삽입 정렬의 가장 큰 단점은 평균과 최악의 경우 시간 복잡도가 O(n²)이라는 점입니다. 데이터의 크기 n 이 커질수록 실행 시간은 기하급수적으로 늘어납니다. 1만 개의 데이터를 정렬하는 데 1초가 걸렸다면, 10만 개의 데이터를 정렬하는 데는 약 100초(1분 40초)가 걸릴 수 있습니다. 따라서 대규모 데이터를 정렬하는 데는 적합하지 않습니다. 다른 정렬 알고리즘과의 비교 및 활용 선택 정렬(Selection Sort)과의 비교 공통점: 둘 다 O(n²) 시간 복잡도를 가지며, 제자리 정렬입니다. 차이점: 비교 횟수: 선택 정렬은 데이터의 상태와 무관하게 항상 n(n-1)/2 번 비교합니다. 삽입 정렬은 최선의 경우 O(n)번만 비교합니다 (적응성). 교환(이동) 횟수: 선택 정렬은 교환(swap)이 최대 n-1 번으로 매우 적습니다. 반면 삽입 정렬은 최악의 경우 O(n²)번의 데이터 이동(shift)이 발생합니다. 데이터 이동 비용이 매우 큰 경우 선택 정렬이 더 나을 수 있습니다. 안정성: 삽입 정렬은 안정적이지만, 일반적인 선택 정렬 구현은 불안정합니다. 버블 정렬(Bubble Sort)과의 비교 공통점: 둘 다 O(n²)이며, 안정 정렬이고, 제자리 정렬입니다. 차이점: 버블 정렬도 최적화를 통해 거의 정렬된 데이터에 대해 O(n)의 성능을 낼 수 있지만(적응성), 일반적으로 삽입 정렬이 내부 루프의 구조상 더 적은 비교와 교환을 수행하여 실제 실행 시간이 더 빠른 경향이 있습니다. 대부분의 경우 삽입 정렬이 버블 정렬보다 선호됩니다. 고급 정렬(퀵, 병합, 힙 정렬)과의 관계 퀵 정렬, 병합 정렬, 힙 정렬과 같은 고급 정렬 알고리즘들은 평균 O(n log n)의 시간 복잡도를 가집니다. 이는 O(n²)보다 훨씬 효율적이므로 대규모 데이터 정렬에는 이들이 사용됩니다. 하지만 재미있게도, 이들 고급 알고리즘은 종종 삽입 정렬을 부분적으로 활용합니다. 하이브리드 정렬(Hybrid Sort): 퀵 정렬이나 병합 정렬은 재귀적으로 문제를 분할하다가, 배열의 크기가 특정 임계값(예: 16개 또는 32개) 이하로 작아지면 삽입 정렬로 전환하여 처리합니다. 작은 배열에서는 재귀 호출의 오버헤드보다 삽입 정렬의 단순함이 더 빠르기 때문입니다. 파이썬의 표준 정렬 함수(Timsort)나 C++의 std::sort (Introsort)가 이런 방식을 사용합니다. 삽입 정렬은 언제 유용한가? 데이터의 크기 n 이 작을 때: 오버헤드가 적어 O(n log n) 알고리즘보다 빠를 수 있습니다. 데이터가 거의 정렬되어 있을 때: O(n)에 가까운 성능을 보여 매우 효율적입니다. 안정 정렬이 필요하고, 추가 메모리 사용이 불가능할 때: 이 조건을 만족하는 가장 간단한 알고리즘 중 하나입니다. 요약 및 결론 삽입 정렬은 정렬되지 않은 요소를 하나씩 가져와 이미 정렬된 부분의 올바른 위치에 삽입하는 과정을 반복하는 직관적이고 간단한 정렬 알고리즘입니다. 그 핵심은 insertElement 를 임시 저장하고, while 루프를 통해 자신보다 큰 요소들을 오른쪽으로 한 칸씩 밀어내며 동시에 삽입 위치를 찾는 것입니다. 비록 평균 및 최악의 경우 O(n²)의 시간 복잡도로 인해 대규모 데이터 정렬에는 부적합하지만, 최선의 경우 O(n)이라는 적응성, O(1)의 추가 공간만 사용하는 제자리 정렬, 그리고 안정성이라는 중요한 특징을 가지고 있습니다. 이러한 장점들 덕분에 삽입 정렬은 단순히 교육용 알고리즘을 넘어, 작은 크기의 데이터를 처리하거나 고급 정렬 알고리즘의 일부로 사용되는 등 실제 세계에서도 그 가치를 인정받고 있습니다. 힙정렬 (heap sort) 서론: 힙 정렬이란 무엇인가? 힙 정렬은 '힙(Heap)'이라는 자료구조를 이용하여 데이터를 정렬하는 알고리즘입니다. 선택 정렬(Selection Sort)을 응용한 알고리즘이라고 볼 수 있는데, 선택 정렬이 매번 전체 배열에서 최솟값(또는 최댓값)을 찾아 자리를 교환하는 방식이라면, 힙 정렬은 힙이라는 자료구조를 통해 이 '최댓값(또는 최솟값)을 찾는 과정'을 매우 효율적으로 만들어 성능을 극대화한 방법입니다. 힙 정렬의 핵심 아이디어는 두 가지 주요 단계로 나뉩니다. 힙 구성(Build Heap) 단계: 정렬되지 않은 입력 배열을 '힙'이라는 특정 조건을 만족하는 트리 구조로 만듭니다. 일반적으로 최댓값을 기준으로 하는 '최대 힙(Max-Heap)'을 사용합니다. 정렬(Sorting) 단계: 힙의 루트(root) 노드는 항상 최댓값을 가집니다. 이 루트 노드의 값을 배열의 가장 마지막 요소와 교환합니다. 그리고 힙의 크기를 하나 줄인 후, 흐트러진 힙 구조를 다시 바로잡습니다. 이 과정을 힙에 요소가 하나만 남을 때까지 반복합니다. 결과적으로 배열의 끝에서부터 가장 큰 값들이 차례대로 정렬됩니다. 힙 정렬은 평균과 최악의 경우 모두 O(n log n)의 시간 복잡도를 가지며, 추가적인 메모리 공간을 거의 사용하지 않는 제자리 정렬(in-place sort)이라는 큰 장점을 가집니다. 이제 슬라이드의 내용을 따라가며 이 두 단계를 원자 단위까지 분해하여 살펴보겠습니다. 제1부: 핵심 자료구조 '힙(Heap)'의 이해 힙 정렬을 이해하기 위해서는 먼저 '힙'이 무엇인지 정확히 알아야 합니다. 완전 이진 트리 (Complete Binary Tree) 힙은 기본적으로 완전 이진 트리의 형태를 가집니다. 완전 이진 트리란, 마지막 레벨을 제외한 모든 레벨이 완전히 채워져 있으며, 마지막 레벨의 노드들은 왼쪽부터 차례대로 채워져 있는 이진 트리를 말합니다. 이 구조 덕분에 힙은 배열을 사용하여 매우 효율적으로 표현할 수 있습니다. 배열로 힙 표현하기 트리 구조를 배열로 표현할 때, 특정 노드의 인덱스를 알면 그 부모와 자식 노드의 인덱스를 간단한 수식으로 계산할 수 있습니다. 슬라이드에서 배열의 인덱스가 1부터 시작하는 것을 기준으로 설명하겠습니다. 노드 i 의 부모 노드 인덱스: floor(i / 2) 노드 i 의 왼쪽 자식 노드 인덱스: 2 * i 노드 i 의 오른쪽 자식 노드 인덱스: 2 * i + 1 예를 들어, 3번 노드의 부모는 floor(3/2) = 1 번 노드이고, 자식은 2*3=6 번(왼쪽)과 2*3+1=7 번(오른쪽) 노드가 됩니다. 이 규칙성 덕분에 포인터 없이도 배열 인덱스만으로 트리 관계를 파악할 수 있습니다. 힙 조건 (Heap Property) 힙은 완전 이진 트리 구조에 더해 다음과 같은 '힙 조건'을 만족해야 합니다. 최대 힙 (Max-Heap): 모든 노드에 대해, 부모 노드의 값은 자식 노드의 값보다 항상 크거나 같다. ( A[부모] ≥ A[자식] ) 이 경우, 트리의 루트 노드는 전체 데이터 중 최댓값을 가지게 됩니다. 힙 정렬에서는 주로 최대 힙을 사용합니다. 최소 힙 (Min-Heap): 모든 노드에 대해, 부모 노드의 값은 자식 노드의 값보다 항상 작거나 같다. ( A[부모] ≤ A[자식] ) 이 경우, 루트 노드는 최솟값을 가집니다. 슬라이드의 내용은 최댓값을 찾아 정렬하는 것을 목표로 하므로, 이하의 모든 설명은 최대 힙을 기준으로 합니다. 제2부: 힙 구성(Build Heap) 단계 상세 분석 힙 정렬의 첫 번째 단계는 주어진 배열을 최대 힙으로 변환하는 것입니다. 이 과정을 buildHeap 함수가 수행합니다. 슬라이드 39: 내부 노드의 힙 조건 확인 이 슬라이드는 힙을 만들어가는 과정의 가장 기본적인 연산, 즉 특정 노드 x 에서 힙 조건이 깨졌을 때 이를 어떻게 복구하는지에 대한 아이디어를 설명합니다. 이 연산을 보통 heapify 또는 슬라이드에서처럼 pushDown 이라고 부릅니다. 어떤 내부 노드 x 와 그 자식 노드들 a , b 가 있다고 가정해 봅시다. 경우 1: A[x] ≥ A[a] and A[x] ≥ A[b] 이것은 노드 x 가 자신의 두 자식보다 모두 크거나 같다는 의미입니다. 즉, 노드 x 를 루트로 하는 작은 부분 트리(subtree)에서는 이미 최대 힙 조건이 만족된 상태입니다. 이 경우 아무것도 할 필요가 없습니다. 경우 2: 그렇지 않다면 (즉, A[x] 가 자식 중 하나보다 작다면) 힙 조건이 깨졌습니다. A[x] 는 A[a] 와 A[b] 중 더 큰 값과 자리를 바꿔야 합니다. 예를 들어 A[b] 가 더 크다면, A[x] 와 A[b] 를 교환(swap)합니다. 교환 후 문제 발생: 교환을 하고 나면, 이제 노드 x 위치에는 원래 자식 노드 중 더 큰 값이 왔으므로, x 위치에서는 힙 조건이 만족됩니다. 하지만 원래 A[x] 의 값이 내려간 자식 노드 위치(예: b )에서는 또다시 힙 조건이 깨질 수 있습니다. 그 자식 노드 b 가 자신의 새로운 자식들보다 작을 수 있기 때문입니다. 해결책: 재귀적(또는 반복적) 처리: 따라서, 값이 아래로 내려간 그 자식 노드를 새로운 x 로 삼아, 힙 조건이 만족될 때까지 이 과정을 계속해서 반복해야 합니다. 값이 \"아래로 가라앉는(sift-down)\" 또는 \"밀려 내려가는(push-down)\" 과정이라고 할 수 있습니다. 이 과정은 해당 값이 리프 노드에 도달하거나, 또는 자신의 자식들보다 커져서 힙 조건을 만족하는 위치에 도달하면 멈춥니다. 이 pushDown 연산이 buildHeap 과 힙 정렬의 핵심 엔진입니다. 슬라이드 40 & 43: buildHeap 알고리즘의 구조 buildHeap 은 어떻게 전체 배열을 힙으로 만들까요? 모든 노드를 하나씩 검사할 필요가 없습니다. 중요한 사실은, 리프 노드(leaf node)는 그 자체로 크기 1인 힙이라는 점입니다. 자식이 없으므로 힙 조건을 깰 일이 없기 때문입니다. 따라서 우리는 리프 노드가 아닌, 가장 마지막 내부 노드(internal node)부터 시작해서 거꾸로 루트 노드 방향으로 올라오면서 각 노드에 대해 pushDown 연산을 수행하면 됩니다. 배열의 크기가 n 일 때, 마지막 노드의 인덱스는 n 입니다. 이 노드의 부모는 floor(n/2) 입니다. 즉, floor(n/2) 인덱스를 가진 노드가 바로 가장 마지막 내부 노드입니다. 그 뒤의 floor(n/2) + 1 부터 n 까지의 노드들은 모두 리프 노드입니다. buildHeap 의 구체적인 로직을 슬라이드 코드와 함께 분석해 보겠습니다. buildHeap(A, eh) // eh는 힙의 마지막 요소 지수, 처음엔 n bh = (eh / 2) + 1 // 힙 내의 첫 번째 잎의 지수. 이 코드에 따르면 루프 시작 전 1을 빼야 함. while bh > 1 { bh = bh - 1 x = bh pushDown(A, x, bh, eh) // x에서 pushDown 시작 } eh : end of heap 의 약자로, 현재 힙으로 간주되는 배열의 마지막 인덱스를 나타냅니다. buildHeap 을 처음 호출할 때는 배열 전체이므로 n 이 됩니다. bh = (n / 2) + 1 : 슬라이드에서는 bh 를 첫 번째 잎의 지수로 초기화한 후, 루프에 진입하면서 바로 1을 뺍니다. 따라서 실질적으로 bh 는 n/2 부터 시작하게 됩니다. n/2 는 마지막 내부 노드의 인덱스입니다. while bh > 1 : bh 가 1이 될 때까지, 즉 루트 노드까지 pushDown 을 수행하고 나면 루프가 종료됩니다. bh 는 1씩 감소하며 n/2, n/2 - 1, ..., 2, 1 순서로 진행됩니다. x = bh : 현재 힙 조건을 확인할 노드의 인덱스를 x 에 저장합니다. pushDown(A, x, bh, eh) : x 번 노드에서부터 값이 아래로 내려가야 한다면 내려보내서, x 를 루트로 하는 서브트리를 최대 힙으로 만듭니다. 이 과정을 거치면, 아래쪽의 작은 서브트리부터 시작해서 점차 위로 올라오면서 전체 트리가 최대 힙 조건을 만족하게 됩니다. 아래쪽 서브트리들이 이미 힙으로 만들어져 있기 때문에, 상위 노드에서 pushDown 을 수행할 때 그 아래 구조는 이미 힙이라는 가정이 성립되어 연산이 올바르게 동작합니다. 슬라이드 41, 44, 45: pushDown 과 findLarger 상세 분석 pushDown 은 buildHeap 의 핵심 작업자입니다. x 위치의 값이 제자리를 찾아 아래로 내려가는 과정을 구현합니다. pushDown(A, x, bh, eh) y = findLarger(A, x, eh) // A[x]보다 큰 값을 가지는 x의 자식 노드 지수를 찾음 while (A[x] < A[y] && y는 유효한 인덱스) { // y가 유효하지 않으면 루프 조건이 거짓이 되어야 함 swap(A[x], A[y]) // 부모와 더 큰 자식을 교환 x = y // 이제 검사 위치는 아래로 내려간 자식의 위치 y = findLarger(A, x, eh) // 새로운 위치에서 다시 더 큰 자식을 찾음 } findLarger(A, x, eh) : 이 함수는 pushDown 의 보조 함수입니다. 노드 x 의 두 자식 중 더 큰 값을 가진 자식의 인덱스를 반환합니다. 만약 부모인 A[x] 가 두 자식보다 모두 크다면, 교환할 필요가 없으므로 pushDown 의 while 루프가 시작되지 않도록 특별한 값(예: x 자신 또는 유효하지 않은 인덱스)을 반환해야 합니다. 슬라이드의 코드를 좀 더 자세히 보겠습니다. findLarger(A, x, eh) // 자식 노드가 둘 다 힙 내에 존재하는 경우 (2x+1 <= eh) if 2*x + 1 <= eh { // 자식 둘 다 부모보다 큰 경우 (슬라이드 코드에 이 부분이 빠져있어 보강) if A[2*x] > A[x] || A[2*x+1] > A[x] { if A[2*x] >= A[2*x+1] { y = 2*x // 왼쪽 자식이 더 크거나 같으면 왼쪽 자식 선택 } else { y = 2*x + 1 // 오른쪽 자식이 더 크면 오른쪽 자식 선택 } } else { // 두 자식 모두 부모보다 작거나 같으면 교환 불필요 // pushDown 루프를 멈추게 할 값을 리턴해야 함. // 예를 들어 y = x를 리턴하면 A[x] < A[y]가 거짓이 되어 루프 종료. return x; // 슬라이드에는 이 부분이 명시적이지 않음 } // 자식 노드가 왼쪽 하나만 힙 내에 존재하는 경우 (2x <= eh) } else if 2*x <= eh && A[2*x] > A[x] { y = 2*x // 왼쪽 자식이 부모보다 크면 왼쪽 자식 선택 } else { // 자식이 없거나, 있어도 부모보다 작으면 교환 불필요 return x; } return y while A[x] < A[y] : 이 조건이 pushDown 의 핵심입니다. 부모( A[x] )가 더 큰 자식( A[y] )보다 작을 때만 루프를 실행합니다. 즉, 힙 조건이 깨졌을 때만 교환과 하강을 반복합니다. findLarger 가 x 를 반환했다면 A[x] < A[x] 는 거짓이므로 루프가 돌지 않습니다. swap(A[x], A[y]) : 부모와 더 큰 자식의 값을 교환합니다. x = y : 이제 원래 부모 값이 내려간 위치가 y 이므로, 다음 검사는 이 위치에서 시작해야 합니다. x 를 y 로 업데이트합니다. y = findLarger(A, x, eh) : 새로운 x 위치에서 다시 더 큰 자식을 찾아 y 를 업데이트하고, 루프 조건을 다시 검사합니다. buildHeap 구체적인 예시 배열 A = [-, 4, 10, 3, 5, 1, 8] (인덱스 1부터 사용)가 있다고 가정해 봅시다. n=6 . 초기 상태: n = 6 . 마지막 내부 노드는 floor(6/2) = 3 번 인덱스. bh 는 3부터 1까지 감소하며 진행됩니다. eh = 6 . bh = 3 (값: 3)일 때: x=3 . 자식은 2*3=6 번(값: 8) 하나뿐입니다. A[3] (3) 💡 핵심 인사이트: BFS는 레벨 기반 탐색이므로, 깊이(depth) 개념에 기반한 DFS의 간선 분류(특히 Back/Forward Edge)와는 근본적으로 다릅니다. 따라서 BFS 신장 트리에서는 \"트리 간선\"과 \"교차 간선\"만 의미 있게 구분됩니다. 이 설명은 슬라이드 22의 점선 간선들을 정확히 해석하면서도, 알고리즘 이론적 맥락도 함께 제공합니다. BFS 알고리즘 정리 (슬라이드 25) 말로 설명한 BFS 과정을 컴퓨터가 이해할 수 있는 알고리즘으로 정리해 봅시다. BFS(v): 정점 v를 시작으로 연결된 모든 곳을 너비 우선으로 탐색하라 시작점 처리: v 를 '방문함'으로 표시하고, 큐에 v 를 넣는다. 대기줄 확인: 큐가 비어있지 않은 동안 계속 반복한다. 처리 대상 선정: 큐의 맨 앞에서 정점 u 를 하나 꺼낸다. (Dequeue) (작업 수행): 필요하다면 u 의 데이터를 출력하는 등 작업을 수행한다. 이웃 탐색: u 에 인접한 모든 정점 w 에 대해 다음을 확인한다. 새로운 정점 발견: 만약 w 가 '방문 안함' 상태라면, w 를 '방문함'으로 표시한다. (다음에 또 발견하지 않도록) w 를 큐의 맨 뒤에 추가한다. (다음 레벨의 탐색 대상으로 예약) DFS와 마찬가지로, 그래프 전체를 탐색하려면 이 BFS(v) 를 감싸는 관리 함수가 필요합니다. (DFS의 DFSearch 와 동일한 구조) BFS 성능 분석 (슬라이드 26) BFS의 시간 복잡도는 DFS와 놀랍게도 동일합니다. 인접 목록(Adjacency List) 사용 시: θ(V + E) 모든 정점은 정확히 한 번 '방문함'으로 표시되고, 정확히 한 번 큐에 들어갔다가(Enqueue) 나옵니다(Dequeue). 이 과정에서 V만큼의 시간이 걸립니다. 한 정점을 큐에서 꺼낼 때마다 그 정점의 모든 이웃(간선)을 확인합니다. 탐색이 끝날 때까지 모든 간선은 (무방향 그래프의 경우) 양방향으로 총 두 번씩 확인됩니다. 이 과정에서 E만큼의 시간이 걸립니다. 따라서 총 시간은 θ(V + E) 입니다. 인접 행렬(Adjacency Matrix) 사용 시: θ(V²) 한 정점의 모든 이웃을 찾기 위해 행렬의 한 행 전체(V개)를 스캔해야 합니다. 이 작업이 V개의 정점에 대해 수행되므로, 총 시간은 θ(V²) 입니다. 탐색의 응용: 순서 찾아내기, 위상 정렬(Topological Sort) 지금까지 우리는 그래프의 모든 정점을 방문하는 '방법'에 대해 배웠습니다. 이제 이 탐색 기법을 활용하여 매우 중요하고 실용적인 문제를 해결해 보겠습니다. 바로 위상 정렬(Topological Sort)입니다. 문제 정의: 일의 순서 정하기 (슬라이드 27, 28) 우리 삶에는 순서가 중요한 일들이 많습니다. 요리: 재료를 손질해야 볶을 수 있고, 볶아야 접시에 담을 수 있습니다. 옷 입기: 속옷을 입고, 셔츠를 입고, 재킷을 입어야 합니다. 순서를 바꾸면 곤란합니다. 대학교 수강신청: '자료구조'를 수강해야 '알고리즘'을 수강할 수 있고, '미적분학'을 수강해야 '공학수학'을 수강할 수 있습니다. 이처럼 선행 조건(prerequisite)이 존재하는 작업들의 집합이 있을 때, 이 선행 조건을 모두 만족시키면서 모든 작업을 수행할 수 있는 일렬로 된 순서를 찾아내는 것을 위상 정렬이라고 합니다. 이 관계는 방향 그래프(Directed Graph)로 완벽하게 표현할 수 있습니다. 정점(Vertex): 각각의 작업 (예: 과목) 간선(Edge): 선행 관계. A가 B의 선수 과목이면, A → B 간선을 그립니다. 이 간선은 \"A를 반드시 B보다 먼저 끝내야 한다\"는 의미입니다. 슬라이드 28의 '과목들의 선후수 관계도'가 바로 이 예시입니다. 과목 1은 2와 4의 선수 과목이므로, 1 → 2 와 1 → 4 간선이 있습니다. 위상 정렬의 필수 조건: 순환이 없어야 한다 (DAG) 위상 정렬이 가능하려면, 이 방향 그래프에 순환(Cycle)이 절대로 없어야 합니다. 만약 순환이 있다면 어떤 일이 벌어질까요? A → B (A는 B의 선수과목) B → C (B는 C의 선수과목) C → A (C는 A의 선수과목) 이런 순환 관계가 있다면, A를 시작하려면 C가 끝나야 하고, C를 시작하려면 B가 끝나야 하고, B를 시작하려면 A가 끝나야 합니다. 즉, 영원히 아무것도 시작할 수 없는 모순에 빠집니다. 따라서 위상 정렬은 순환이 없는 방향 그래프 (Directed Acyclic Graph, DAG) 에서만 정의됩니다. 슬라이드 29에서 설명하듯, 이 순환의 존재 여부는 우리가 앞에서 배운 DFS를 통해 완벽하게 찾아낼 수 있습니다. DFS 탐색 중 뒤 간선(Back Edge), 즉 현재 탐색 중인 경로 상의 조상 노드로 돌아가는 간선이 발견되면, 그것이 바로 순환이 존재한다는 증거입니다. 위상 정렬 알고리즘 1: DFS와 스택의 활용 (슬라이드 30-33) DFS의 깊이 파고드는 특성을 이용해 위상 정렬을 수행할 수 있습니다. 핵심 아이디어는 이것입니다. \"어떤 작업의 DFS가 종료되었다는 것은, 그 작업이 의존하는 모든 후행 작업들의 DFS가 이미 다 종료되었음을 의미한다.\" 예를 들어 A → B 관계가 있을 때, A에서 DFS를 시작하면 반드시 B로 먼저 탐색을 떠나게 됩니다. 따라서 B와 관련된 모든 탐색이 끝나고 나서야, 비로소 A의 탐색이 '종료'될 수 있습니다. 이는 곧, DFS가 가장 먼저 종료되는 정점은, 다른 어떤 정점의 선행 조건도 되지 않는 '맨 마지막 작업' 후보라는 뜻입니다. DFS가 가장 늦게 종료되는 정점은, 다른 많은 작업들의 선행 조건이 되는 '맨 처음 작업' 후보가 됩니다. 이 '종료 순서'를 기록하기 위해 우리는 스택(Stack)을 사용합니다. 스택은 'Last-In, First-Out (LIFO)' 구조이므로, 종료된 순서대로 스택에 넣으면 나중에 꺼낼 때 종료 순서의 역순, 즉 위상 정렬된 순서가 됩니다. 알고리즘 단계: 빈 스택과 방문 기록부를 준비합니다. 그래프의 모든 정점을 확인하며, 아직 방문하지 않은 정점이 있다면 그 정점에서 DFS를 시작합니다. DFS(v) 함수: a. 현재 정점 v 를 '방문함'으로 표시합니다. b. v 의 모든 이웃 w 에 대해, 만약 w 가 아직 방문하지 않았다면 DFS(w) 를 재귀적으로 호출합니다. c. (핵심!) v 의 모든 이웃에 대한 탐색이 끝나면(즉, 재귀 호출이 모두 리턴되면), 그때 v 를 스택에 push 합니다. 모든 정점에 대한 DFS가 끝나면, 스택이 빌 때까지 하나씩 pop하여 출력합니다. 이 순서가 바로 위상 정렬 결과입니다. 예제(과목 선후수 관계도) 따라가기: (간결성을 위해 주된 흐름만 보입니다) 임의의 정점 1에서 DFS 시작. DFS(1) 호출. 1의 이웃 2로 이동. DFS(2) 호출. 2의 이웃 4로 이동. DFS(4) 호출. 4의 이웃 6으로 이동. DFS(6) 호출. 6의 이웃 7로 이동. DFS(7) 호출. 7은 더 이상 갈 곳이 없음. 7의 DFS 종료. 7을 스택에 Push. [스택: 7] 6으로 복귀. 더 이상 갈 곳 없음. 6의 DFS 종료. 6을 스택에 Push. [스택: 7, 6] 4로 복귀. 더 이상 갈 곳 없음. 4의 DFS 종료. 4를 스택에 Push. [스택: 7, 6, 4] 2로 복귀. 더 이상 갈 곳 없음. 2의 DFS 종료. 2를 스택에 Push. [스택: 7, 6, 4, 2] 1로 복귀. 더 이상 갈 곳 없음. 1의 DFS 종료. 1을 스택에 Push. [스택: 7, 6, 4, 2, 1] 이제 방문 안 한 정점 3에서 DFS 시작. DFS(3) 호출. 3의 이웃 5로 이동. DFS(5) 호출. 5의 이웃 6, 7은 이미 방문함. 5의 DFS 종료. 5를 스택에 Push. [스택: 7, 6, 4, 2, 1, 5] 3으로 복귀. 이웃 4는 이미 방문함. 3의 DFS 종료. 3을 스택에 Push. [스택: 7, 6, 4, 2, 1, 5, 3] 모든 정점 방문 완료. 결과 출력: 스택에서 pop: 3, 5, 1, 2, 4, 6, 7. 이는 유효한 수강 순서 중 하나입니다. (위상 정렬의 결과는 유일하지 않을 수 있습니다. 예를 들어 1과 3은 서로 선행 관계가 없으므로 1, 3, ... 순서도 가능합니다.) 위상 정렬 알고리즘 2: BFS와 진입 차수의 활용 (카의 알고리즘) (슬라이드에는 없지만, 매우 중요한 다른 접근법입니다.) BFS를 이용한 위상 정렬 방법도 있으며, 매우 직관적입니다. 핵심 아이디어: \"선행 과목이 하나도 없는 과목은 지금 당장 수강할 수 있다.\" 어떤 정점으로 들어오는 간선의 개수를 진입 차수(In-degree)라고 합니다. 진입 차수가 0이라는 것은 그 작업을 시작하기 위한 선행 조건이 없다는 뜻입니다. 알고리즘 단계: 그래프의 모든 정점에 대해 진입 차수를 계산합니다. 큐를 준비하고, 진입 차수가 0인 모든 정점을 큐에 넣습니다. (이들이 맨 처음 시작할 수 있는 작업들입니다.) 큐가 빌 때까지 다음을 반복합니다. a. 큐에서 정점 v 를 하나 꺼냅니다. 이 v 를 결과 리스트에 추가합니다. b. v 에서 나가는 모든 간선 v → w 를 살펴봅니다. c. 간선이 사라졌다고 생각하고, 이웃 w 의 진입 차수를 1 감소시킵니다. d. 만약 w 의 진입 차수가 0이 되었다면, w 역시 이제 새로운 시작점이 될 수 있으므로 큐에 넣습니다. 반복이 끝났을 때, 결과 리스트에 모든 정점이 포함되어 있다면 위상 정렬이 성공한 것입니다. 만약 일부 정점이 빠져있다면, 이는 그래프에 순환이 존재하여 진입 차수가 0이 되지 못한 정점들이 있다는 뜻입니다. 이 방법은 마치 대학교에서 수강 가능한 과목 목록(진입 차수 0인 과목)을 보고 수강 신청을 하고, 그 과목을 이수하면 다음 학기에 새로운 과목들(진입 차수가 0이 된 과목)이 수강 가능해지는 과정과 매우 유사합니다. 시간 복잡도 (슬라이드 34) DFS 기반 알고리즘: 이 알고리즘은 본질적으로 그래프 전체에 대한 DFS를 한 번 수행하는 것과 같습니다. 따라서 시간 복잡도는 DFS와 동일한 θ(V + E) (인접 목록 사용 시) 입니다. BFS 기반 알고리즘: 진입 차수를 계산하는 데 O(V+E), 모든 정점과 간선을 한 번씩 처리하므로 이 역시 θ(V + E) (인접 목록 사용 시) 입니다. 최종 요약 (슬라이드 35) 그래프 탐색은 정점과 간선으로 이루어진 복잡한 구조를 체계적으로 방문하는 알고리즘의 기초입니다. 깊이 우선 탐색(DFS)은 스택(재귀)을 이용해 한 경로를 끝까지 파고드는 '모험가' 방식이며, 순환 탐지나 연결 요소 찾기 등에 유용합니다. 너비 우선 탐색(BFS)은 큐를 이용해 시작점에서 가까운 순서대로 탐색하는 '전략가' 방식이며, 최단 경로 문제의 핵심입니다. 위상 정렬은 순환이 없는 방향 그래프(DAG)에서 작업들의 선행 순서를 결정하는 중요한 문제이며, DFS나 BFS를 응용하여 효율적으로 해결할 수 있습니다. 이러한 기본적인 그래프 알고리즘들은 수많은 컴퓨터 과학 문제들, 예를 들어 네트워크 라우팅, 소셜 네트워크 분석, 컴파일러 의존성 해결, 인공지능 탐색 등 다양한 분야의 핵심적인 문제 해결 도구로 사용됩니다. 제5장: 분할 정복 (Divide and Conquer) 서론: 위대한 문제 해결 전략, 분할 정복 알고리즘 설계는 단순히 문제를 해결하는 코드를 작성하는 것을 넘어, 가장 효율적이고 우아한 해결책을 찾는 과정입니다. 수많은 문제 해결 전략 중에서 '분HAL 정복(Divide and Conquer)'은 가장 강력하고 널리 사용되는 패러다임 중 하나입니다. 이 이름은 로마 제국의 율리우스 카이사르가 사용했던 '분할하여 통치하라(Divide et Impera)'라는 정치 전략에서 유래했습니다. 거대하고 강력한 적을 한 번에 상대하기보다, 여러 개의 작은 그룹으로 분열시켜 각개격파하는 것이 훨씬 효과적이라는 이 아이디어는 컴퓨터 과학의 문제 해결에도 그대로 적용됩니다. 분할 정복 전략의 핵심 철학은 \"크고 복잡한 문제는 해결하기 어렵지만, 작고 단순한 문제는 해결하기 쉽다\"는 것입니다. 따라서 해결 불가능해 보이는 거대한 문제를 해결 가능한 작은 조각들로 나눈 뒤, 각 조각의 답을 구하고, 그 답들을 다시 현명하게 조합하여 원래 문제의 답을 찾아내는 방식입니다. 이 장에서는 분할 정복 패러다임의 기본 구조를 시작으로, 이 전략이 어떻게 최댓값/최솟값 찾기, 정렬(합병 정렬, 빠른 정렬), k번째 작은 원소 찾기(선택 문제)와 같은 고전적이고 중요한 문제들을 효율적으로 해결하는지 심도 있게 탐구할 것입니다. 또한, 분할 정복 전략이 항상 최선은 아니며, 특정 조건(부분 문제의 중복)에서는 오히려 비효율을 초래할 수 있다는 점을 피보나치 수열 예제를 통해 살펴보고, 이러한 경우 동적 계획법(Dynamic Programming)과 같은 다른 패러다임이 더 적합한 이유에 대해서도 논의할 것입니다. 분할 정복 설계 전략의 3단계 분할 정복 알고리즘은 일반적으로 다음과 같은 세 가지 명확한 단계로 구성됩니다. 이 3단계는 분할 정복의 심장과도 같으며, 거의 모든 분할 정복 알고리즘이 이 구조를 따릅니다. 분할 (Divide) 단계 첫 번째 단계는 주어진 문제를 해결 가능한 가장 작은 단위(base case)에 도달할 때까지 더 작은 부분 문제(subproblem)들로 나누는 것입니다. 핵심은 원래 문제와 동일한 유형의 더 작은 문제로 나누는 것입니다. 예를 들어, n개의 숫자를 정렬하는 문제라면, 이를 n/2개의 숫자를 정렬하는 두 개의 부분 문제로 나누는 식입니다. 핵심 원칙: 부분 문제들은 서로 독립적(independent)이어야 이상적입니다. 즉, 한 부분 문제의 해가 다른 부분 문제의 해에 영향을 주지 않아야 합니다. 분할 방법: 문제를 어떻게 나눌 것인가는 문제의 성격에 따라 다릅니다. 배열을 정확히 반으로 나누는 것이 일반적이지만(예: 합병 정렬), 특정 기준값(pivot)을 중심으로 나누기도 합니다(예: 빠른 정렬). 이 분할 과정 자체가 알고리즘의 효율성을 결정하는 중요한 요소가 될 수 있습니다. 정복 (Conquer) 단계 두 번째 단계는 분할된 각 부분 문제의 해를 구하는 것입니다. 이 과정은 주로 재귀(Recursion)를 통해 자연스럽게 구현됩니다. 부분 문제가 더 이상 분할할 수 없을 만큼 작아지면(이를 '기저 사례' 또는 'Base Case'라고 합니다), 바로 해를 구합니다. 예를 들어, 배열에 요소가 하나만 남았다면 그 자체가 최댓값이자 최솟값이며, 이미 정렬된 상태라고 볼 수 있습니다. 재귀의 역할: solve(problem) 함수가 있다면, 이 함수는 내부에서 solve(subproblem1) , solve(subproblem2) 등을 호출하는 구조를 가집니다. 이 재귀 호출의 연속이 바로 '정복' 과정입니다. 재귀는 문제를 점점 작은 단위로 쪼개 내려가다가, 기저 사례에 도달하면 해를 반환하며 다시 거슬러 올라오기 시작합니다. 합병 (Merge 또는 Combine) 단계 마지막 단계는 정복 단계에서 얻은 부분 문제들의 해를 적절히 조합하여 원래 문제의 전체 해를 구하는 것입니다. 이 합병 과정의 복잡도는 알고리즘의 전체 성능에 결정적인 영향을 미칩니다. 합병의 중요성: 합병 단계가 간단한 알고리즘(예: 빠른 정렬)도 있고, 합병 단계 자체가 핵심적인 작업을 수행하는 알고리즘(예: 합병 정렬)도 있습니다. 예를 들어, 두 개의 정렬된 부분 배열을 하나의 정렬된 배열로 합치는 작업은 합병 정렬의 핵심 연산입니다. 마찬가지로, 두 부분 배열의 최댓값/최솟값들을 비교하여 전체 배열의 최댓값/최솟값을 찾는 것도 합병의 한 예입니다. 이 세 가지 단계를 거치면서, 분할 정복은 복잡한 문제를 체계적으로 해결하는 강력한 프레임워크를 제공합니다. 1 최댓값과 최솟값 찾기 배열에서 가장 큰 값(최댓값)과 가장 작은 값(최솟값)을 찾는 문제는 매우 기본적인 문제이지만, 이 문제를 어떻게 접근하느냐에 따라 연산의 효율성이 달라질 수 있습니다. 분할 정복이 어떻게 비교 횟수를 줄여 효율성을 높이는지 살펴보겠습니다. 쉬운 전략 (Naive Approach) 가장 직관적이고 간단한 방법은 최댓값을 먼저 찾고, 그 다음에 최솟값을 찾는 것입니다. 최댓값 찾기: 배열의 첫 번째 요소를 현재 최댓값(max)으로 가정합니다. 이후 배열의 두 번째 요소부터 마지막 요소까지 순회하면서, 현재 요소가 현재 최댓값보다 크면 최댓값을 갱신합니다. n개의 요소가 있다면, 첫 요소를 제외한 (n-1)개의 요소와 비교해야 하므로, 비교 횟수는 (n-1)회 입니다. 최솟값 찾기: 최댓값을 찾는 과정과 동일합니다. 첫 번째 요소를 현재 최솟값(min)으로 가정하고, 나머지 (n-1)개의 요소와 비교하며 최솟값을 갱신합니다. 비교 횟수는 (n-1)회 입니다. 하지만, 슬라이드에서는 최댓값을 찾은 뒤 \"남은 배열\"에서 최솟값을 찾는다고 설명하여 약간의 오해를 줄 수 있습니다. 일반적인 순차 탐색에서는 최댓값을 찾는 과정과 최솟값을 찾는 과정이 별개로 진행됩니다. 따라서, 최댓값을 찾기 위한 비교: n-1 회 최솟값을 찾기 위한 비교: n-1 회 총 비교 횟수: (n-1) + (n-1) = 2n - 2 회 슬라이드에서 제시한 (n-1) + (n-2) = 2n-3 은 최댓값을 찾은 후, 그 최댓값은 최솟값이 될 수 없다는 가정 하에 남은 n-1 개의 요소 중에서 최솟값을 찾는 경우를 상정한 것으로 보입니다. 이 경우에도 시간 복잡도는 O(n) 으로 동일하며, 상수 배의 차이만 존재합니다. 이 방법은 매우 간단하지만, 과연 이것이 최선일까요? 분할 정복 전략 분할 정복은 이 문제를 다른 각도에서 접근합니다. 비교 횟수를 줄이는 것이 목표입니다. 분할(Divide): 주어진 배열 A 를 거의 같은 크기의 두 부분 배열 A_left 와 A_right 로 나눕니다. 예를 들어, n 개의 요소가 있다면, 앞쪽 n/2 개와 뒤쪽 n/2 개로 나눕니다. 정복(Conquer): 두 개의 부분 배열에 대해 재귀적으로 최댓값과 최솟값을 찾습니다. A_left 에서 최댓값( max1 )과 최솟값( min1 )을 찾습니다. A_right 에서 최댓값( max2 )과 최솟값( min2 )을 찾습니다. 이 과정은 배열에 요소가 하나 또는 두 개만 남을 때까지 계속됩니다 (기저 사례). 합병(Merge): 이제 네 개의 값( max1 , min1 , max2 , min2 )을 가지고 원래 배열 A 의 전체 최댓값과 최솟값을 결정합니다. 전체 최댓값 max(A) = max(max1, max2) (비교 1회) 전체 최솟값 min(A) = min(min1, min2) (비교 1회) 즉, 합병 단계에서는 단 2번의 비교만 필요합니다. 최댓값/최솟값 찾기 알고리즘 (findMaxMin) 상세 분석 슬라이드 8의 findMaxMin 알고리즘을 한 줄씩 자세히 분석해 보겠습니다. findMaxMin(A[], i, j, min, max) // A[i..j]의 최댓값과 최솟값을 찾는다 // 입력: 배열 A[i..j] // 출력: min(최솟값), max(최댓값) A[] : 전체 배열 i , j : 현재 탐색할 부분 배열의 시작 인덱스와 끝 인덱스 min , max : 결과(최솟값, 최댓값)를 저장할 변수 (참조에 의한 전달 방식) 기저 사례 (Base Cases): 1 if i = j { min = A[i]; max = A[i] } // 요소가 1개인 경우 Line 1: 부분 배열에 요소가 하나뿐일 때 ( i == j ). 이 경우 그 요소 자체가 최솟값이자 최댓값입니다. 더 이상 분할할 필요가 없으므로 재귀가 멈춥니다. 비교 연산은 없습니다. 2 else if i = j − 1 { // 요소가 2개인 경우 3 if A[i] < A[j] { min = A[i]; max = A[j] } 4 else { min = A[j]; max = A[i] } 5 } Line 2-5: 부분 배열에 요소가 두 개일 때 ( i == j-1 ). 이 경우 단 한 번의 비교( A[i] < A[j] )를 통해 둘 중 어느 것이 크고 작은지 바로 결정할 수 있습니다. 이 또한 재귀를 멈추는 기저 사례입니다. 비교 횟수는 1회입니다. 재귀 단계 (Recursive Step): 6 else { // 요소가 2개보다 많은 경우 7 mid = ⌊(i + j) / 2⌋ 8 findMaxMin(A, i, mid, min1, max1) 9 findMaxMin(A, mid + 1, j, min2, max2) Line 7: 분할 단계입니다. 현재 부분 배열의 중간 지점 mid 를 계산하여 두 개의 작은 부분 배열 A[i..mid] 와 A[mid+1..j] 로 나눕니다. Line 8, 9: 정복 단계입니다. 두 개의 부분 배열에 대해 findMaxMin 함수를 재귀적으로 호출하여 각각의 최솟값/최댓값( min1 , max1 과 min2 , max2 )을 구합니다. 10 if min1 < min2 { min = min1 } 11 else { min = min2 } 12 if max1 > max2 { max = max1 } // 슬라이드에서는 < 로 되어있으나, > 가 직관적 13 else { max = max2 } 14 } Line 10-14: 합병 단계입니다. 왼쪽 부분 배열의 최솟값( min1 )과 오른쪽 부분 배열의 최솟값( min2 )을 비교하여 전체의 최솟값을 결정합니다 (비교 1회). 왼쪽 부분 배열의 최댓값( max1 )과 오른쪽 부분 배열의 최댓값( max2 )을 비교하여 전체의 최댓값을 결정합니다 (비교 1회). 따라서, 이 합병 단계에서는 총 2회의 비교가 수행됩니다. 실행 예제와 비교 횟수 분석 예제 배열 A = {22, 13, -5, -8, 15, 60, 17, 31} (n=8)을 사용하여 알고리즘의 진행 과정을 따라가 보겠습니다. findMaxMin(A, 0, 7) 호출 mid = 3 . findMaxMin(A, 0, 3) (왼쪽)과 findMaxMin(A, 4, 7) (오른쪽)을 재귀 호출. findMaxMin(A, 0, 3) 처리 ( {22, 13, -5, -8} ) mid = 1 . findMaxMin(A, 0, 1) 과 findMaxMin(A, 2, 3) 을 재귀 호출. findMaxMin(A, 0, 1) ( {22, 13} ): 요소 2개. 13 < 22 . 비교 1회. min=-5, max=22 반환. ( min1=13, max1=22 ) findMaxMin(A, 2, 3) ( {-5, -8} ): 요소 2개. -8 < -5 . 비교 1회. min=-8, max=-5 반환. ( min2=-8, max2=-5 ) 합병: min(13, -8) -> -8 (비교 1회), max(22, -5) -> 22 (비교 1회). 결과: {22, 13, -5, -8} 의 min=-8, max=22. 총 비교 횟수 = 1+1+2 = 4회. findMaxMin(A, 4, 7) 처리 ( {15, 60, 17, 31} ) mid = 5 . findMaxMin(A, 4, 5) 와 findMaxMin(A, 6, 7) 을 재귀 호출. findMaxMin(A, 4, 5) ( {15, 60} ): 비교 1회. min=15, max=60 반환. ( min1=15, max1=60 ) findMaxMin(A, 6, 7) ( {17, 31} ): 비교 1회. min=17, max=31 반환. ( min2=17, max2=31 ) 합병: min(15, 17) -> 15 (비교 1회), max(60, 31) -> 60 (비교 1회). 결과: {15, 60, 17, 31} 의 min=15, max=60. 총 비교 횟수 = 1+1+2 = 4회. 최종 합병 (1단계의 결과) min1 = -8 , max1 = 22 (왼쪽 전체 결과) min2 = 15 , max2 = 60 (오른쪽 전체 결과) min(-8, 15) -> -8 (비교 1회) max(22, 60) -> 60 (비교 1회) 최종 결과: 전체 배열의 min=-8, max=60. 총 비교 횟수 계산: 4회 (왼쪽) + 4회 (오른쪽) + 2회 (최종 합병) = 10회. 점화식을 이용한 분석: T(n) 을 크기 n인 배열에 대한 비교 횟수라고 정의합시다. T(1) = 0 (요소가 하나면 비교 없음) T(2) = 1 (요소가 두 개면 비교 1회) T(n) = 2 * T(n/2) + 2 (n > 2) 2 * T(n/2) : 크기 n/2인 두 개의 부분 문제 해결을 위한 비교 횟수 + 2 : 두 부분 문제의 결과를 합병하기 위한 비교 횟수 (min 비교 1회, max 비교 1회) 이 점화식을 풀어보면 (n이 2의 거듭제곱이라고 가정): T(n) = 2 * T(n/2) + 2 = 2 * (2 * T(n/4) + 2) + 2 = 4 * T(n/4) + 4 + 2 = 4 * (2 * T(n/8) + 2) + 6 = 8 * T(n/8) + 8 + 6 ... = 2^k * T(n/2^k) + 2*(2^k - 1) n/2^k = 2 가 될 때까지, 즉 n = 2^(k+1) 일 때, T(n) = (n/2) * T(2) + 2 * (n/2 - 1) = (n/2) * 1 + n - 2 = n/2 + n - 2 = (3n/2) - 2 결론: 분할 정복을 사용한 최댓값/최솟값 찾기의 비교 횟수는 약 (3n/2) - 2 입니다. n=8일 때: (3*8/2) - 2 = 12 - 2 = 10 회. 위 예제와 일치합니다. 쉬운 전략: 2n - 2 = 2*8 - 2 = 14 회. 효율성 비교: (3n/2) - 2 는 2n - 2 보다 약 25% 더 효율적입니다. 데이터가 수십억 개에 달하는 경우, 이 25%의 차이는 상당한 성능 향상으로 이어집니다. 2 합병 정렬 (Merge Sort) 정렬은 컴퓨터 과학에서 가장 기본적이고 중요한 문제 중 하나입니다. 합병 정렬은 분할 정복 패러다임의 가장 대표적이고 교과서적인 예시입니다. 쉬운 전략 (선택 정렬)과 그 한계 슬라이드 10에서 언급된 '쉬운 전략'은 사실상 선택 정렬(Selection Sort)과 유사한 아이디어입니다. 전체 배열에서 최솟값을 찾습니다. (비교 n-1 회) 해당 최솟값을 배열의 첫 번째 위치에 둡니다. 남은 n-1 개의 요소에 대해 1~2번 과정을 반복합니다. 두 번째 최솟값을 찾아 두 번째 위치에 둡니다. (비교 n-2 회) ... 마지막 두 요소 중 작은 것을 n-1번째 위치에 둡니다. (비교 1회) 분석: 총 비교 횟수 = (n-1) + (n-2) + ... + 2 + 1 = n(n-1)/2 시간 복잡도: O(n^2) 슬라이드에서는 이 전략이 '균형 취하기 발견법'을 사용하지 않았다고 지적합니다. 여기서 '균형 취하기(balancing)'란 문제를 비슷한 크기의 부분 문제들로 나누는 것을 의미합니다. 선택 정렬은 크기 n 의 문제를 크기 1 (찾아낸 최솟값)과 크기 n-1 (나머지)의 부분 문제로 나눕니다. 이렇게 극도로 불균형하게 문제를 분할하는 것은 분할 정복의 이점을 전혀 살리지 못하며, 결국 O(n^2) 의 비효율적인 결과를 낳습니다. 합병 정렬의 분할 정복 전략 합병 정렬은 이 불균형 문제를 정면으로 해결합니다. 분할(Divide): 정렬되지 않은 배열을 크기가 거의 같은 두 개의 부분 배열로 나눕니다. 이 과정은 부분 배열에 요소가 하나만 남을 때까지 재귀적으로 계속됩니다. 요소가 하나인 배열은 그 자체로 정렬된 상태입니다. 정복(Conquer): 각 부분 배열을 재귀적으로 정렬합니다. 합병(Merge): 이 단계가 합병 정렬의 핵심입니다. 정복 단계에서 정렬된 두 개의 부분 배열을 하나의 전체 정렬된 배열로 합칩니다. 합병(Merge) 과정 상세 설명 두 개의 이미 정렬된 부분 배열 L 과 R 을 합병하는 과정을 생각해 봅시다. 새로운 임시 배열 temp 를 준비합니다. L 의 첫 번째 요소를 가리키는 포인터 i 와 R 의 첫 번째 요소를 가리키는 포인터 j 를 둡니다. L[i] 와 R[j] 를 비교합니다. 더 작은 값을 temp 배열에 넣고, 해당 포인터를 1 증가시킵니다. L 이나 R 중 어느 한쪽의 모든 요소가 temp 에 들어갈 때까지 이 비교-복사 과정을 반복합니다. 남아있는 다른 쪽 배열의 모든 요소들을 temp 배열의 뒤에 순서대로 복사합니다. 마지막으로, temp 배열의 내용을 원래 배열의 해당 위치에 다시 복사합니다. 이 합병 과정은 두 부분 배열의 길이를 합한 만큼의 시간, 즉 선형 시간 O(n) 이 걸립니다. 합병 정렬 알고리즘 의사코드(Pseudocode) mergeSort(A[], p, r) // 배열 A[p..r]을 정렬한다 1 if p < r { 2 q = ⌊(p + r) / 2⌋ // 분할: 중간 지점 계산 3 mergeSort(A, p, q) // 정복: 왼쪽 부분 배열 정렬 4 mergeSort(A, q + 1, r) // 정복: 오른쪽 부분 배열 정렬 5 merge(A, p, q, r) // 합병: 두 정렬된 부분 배열을 합병 6 } merge(A[], p, q, r) // 정렬된 두 부분 배열 A[p..q]와 A[q+1..r]을 합병한다 1 // 임시 배열 temp에 A[p..r]을 복사 2 i = p; j = q + 1; t = 0; 3 while i ≤ q and j ≤ r { 4 if A[i] ≤ A[j] { temp[t++] = A[i++] } 5 else { temp[t++] = A[j++] } 6 } 7 // 남아있는 요소들을 복사 8 while i ≤ q { temp[t++] = A[i++] } 9 while j ≤ r { temp[t++] = A[j++] } 10 // temp의 내용을 A에 다시 복사 11 for i = p, t = 0; i ≤ r; i++, t++ { A[i] = temp[t] } 합병 정렬 시간 복잡도 분석 T(n) 을 크기 n 인 배열을 합병 정렬하는 데 걸리는 시간이라고 합시다. 분할: mid 를 계산하는 것은 상수 시간 O(1) 입니다. 정복: 크기 n/2 인 두 개의 부분 문제를 재귀적으로 해결하므로 2 * T(n/2) 의 시간이 걸립니다. 합병: merge 함수는 n 개의 모든 요소를 한 번씩 다루므로 O(n) 의 시간이 걸립니다. 따라서 점화식은 다음과 같습니다. T(n) = 2 * T(n/2) + O(n) 이 점화식은 마스터 정리(Master Theorem)에 의해 또는 직접 풀어서 O(n log n) 임을 알 수 있습니다. 이는 최악, 평균, 최선 모든 경우에 동일하게 적용됩니다. 합병 정렬은 입력 데이터의 상태와 관계없이 항상 O(n log n) 의 성능을 보장하는 안정적인(stable) 정렬 알고리즘입니다. 공간 복잡도: 합병 과정에서 n 개의 요소를 담을 추가적인 임시 배열이 필요하므로 O(n) 의 공간 복잡도를 가집니다. 3 빠른 정렬 (Quick Sort) 빠른 정렬은 합병 정렬과 함께 분할 정복을 사용하는 가장 유명한 정렬 알고리즘 중 하나입니다. 이름에서 알 수 있듯이, 평균적으로 매우 빠른 성능을 자랑합니다. 빠른 정렬의 분할 정복 전략 빠른 정렬은 합병 정렬과 분할/합병 단계의 복잡도가 반대입니다. 분할(Divide): 이 단계가 빠른 정렬의 핵심입니다. 배열 내의 한 요소를 기준(pivot)으로 선택합니다. 그리고 기준보다 작은 모든 요소는 기준의 왼쪽으로, 큰 모든 요소는 기준의 오른쪽으로 옮기는 분할(Partition) 작업을 수행합니다. 이 작업이 끝나면 기준 요소는 최종적으로 정렬될 위치에 놓이게 됩니다. 합병 정렬과 달리, 이 분할 과정이 O(n) 의 작업을 수행합니다. 정복(Conquer): 기준의 왼쪽 부분 배열과 오른쪽 부분 배열에 대해 재귀적으로 빠른 정렬을 수행합니다. 합병(Merge): 아무것도 할 필요가 없습니다. 분할 단계에서 이미 기준을 중심으로 모든 요소가 정렬될 위치의 큰 틀(왼쪽/오른쪽)을 잡았기 때문에, 두 부분 배열이 각각 정렬되면 전체 배열이 자동으로 정렬됩니다. 이 합병 단계가 O(1) 로 매우 간단하다는 것이 빠른 정렬의 특징입니다. 빠른 정렬 시간 복잡도 분석 빠른 정렬의 성능은 기준(pivot)을 얼마나 잘 선택하느냐에 따라 극적으로 달라집니다. 최선의 경우 (Best Case): 매번 분할(Partition) 과정에서 기준이 배열을 정확히 절반으로 나눌 때 발생합니다. 점화식: T(n) = 2 * T(n/2) + O(n) (분할에 O(n) 소요) 시간 복잡도: O(n log n) . 합병 정렬과 동일합니다. 최악의 경우 (Worst Case): 매번 기준이 가장 작거나 가장 큰 요소로 선택될 때 발생합니다. 예를 들어, 이미 정렬된 배열에서 항상 첫 번째 요소를 기준으로 선택하는 경우입니다. 이 경우, 배열은 크기 0 과 n-1 의 극도로 불균형한 두 부분 배열로 나뉩니다. 점화식: T(n) = T(n-1) + O(n) 시간 복잡도: O(n^2) . 선택 정렬만큼 비효율적이 됩니다. 평균적인 경우 (Average Case): 기준이 무작위로 선택된다면, 분할이 어느 정도 균형을 이룰 확률이 높습니다. 수학적으로 분석하면, 평균 시간 복잡도는 O(n log n) 으로 최선의 경우와 같습니다. 이러한 이유로, 실제 구현에서는 기준을 무작위로 선택하거나, 세 개의 요소(처음, 중간, 끝) 중 중앙값을 선택하는 등의 기법을 사용하여 최악의 경우를 피하려 노력합니다. 빠른 정렬은 합병 정렬과 달리 추가적인 배열이 필요 없는 제자리 정렬(in-place sort)이 가능하여 공간 효율성이 높고, 평균 성능이 매우 뛰어나 널리 사용됩니다. 4 선택 (Selection) 선택 문제는 정렬되지 않은 배열에서 k번째로 작은 요소를 찾는 문제입니다. k=1이면 최솟값 찾기 k=n이면 최댓값 찾기 k=⌊(n+1)/2⌋이면 중앙값(median) 찾기 쉬운 전략 가장 간단한 방법은 배열 전체를 정렬한 뒤, k-1 인덱스에 있는 요소를 반환하는 것입니다. 합병 정렬이나 빠른 정렬(평균)을 사용하면 O(n log n) 의 시간이 걸립니다. 하지만 우리는 전체를 정렬할 필요 없이 단지 k번째 요소만 찾으면 됩니다. 과연 더 빠르게 할 수 있을까요? 분할 정복을 이용한 선택 알고리즘 (Quickselect) 이 알고리즘은 빠른 정렬(Quick Sort)의 아이디어를 차용하여 매우 효율적으로 선택 문제를 해결합니다. 그래서 종종 퀵셀렉트(Quickselect)라고도 불립니다. 분할(Divide): 빠른 정렬과 동일하게, 배열에서 기준(pivot)을 하나 선택하고 분할(Partition) 작업을 수행합니다. 이 작업이 끝나면 기준은 p 인덱스에 위치하게 되고, A[p] 는 배열 내에서 (p - first + 1) 번째로 작은 요소가 됩니다. 정복(Conquer) & 선택: 분할 작업 후 기준의 위치 p 를 k와 비교합니다. Small 그룹 (기준 왼쪽)의 크기를 s = p - first 라고 합시다. (슬라이드에서는 s = p - 1 - first + 1 로 표현, 즉 p - first 와 동일) Case 1: k == s + 1 : 우리가 찾던 k번째 요소가 바로 기준(pivot) 자신입니다! 탐색을 종료하고 A[p] 를 반환합니다. Case 2: k <= s : k번째 요소는 기준보다 작으므로, Small 그룹 안에 있습니다. Small 그룹(왼쪽 부분 배열)에 대해 재귀적으로 k번째 요소를 찾습니다. Case 3: k > s + 1 : k번째 요소는 기준보다 크므로, Large 그룹 안에 있습니다. Large 그룹(오른쪽 부분 배열)에서 찾아야 합니다. 단, 이때는 그냥 k 번째를 찾는 것이 아니라, Small 그룹과 기준을 제외한 나머지 중에서 찾아야 하므로 (k - s - 1) 번째 요소를 재귀적으로 찾습니다. 이 알고리즘의 핵심은 빠른 정렬처럼 양쪽을 모두 재귀 호출하는 것이 아니라, 한쪽 부분 배열에 대해서만 재귀 호출을 한다는 점입니다. 이로 인해 시간 복잡도가 크게 향상됩니다. 선택 알고리즘 예제 상세 분석 (슬라이드 42) 문제: < 48 12 70 38 75 67 96 52 81> (n=9) 에서 중앙값, 즉 5번째(k=5) 작은 수를 찾아라. 최초 호출: selection(A, 0, 8, k=5) 분할: 첫 번째 요소인 48 을 기준으로 선택합니다. 분할 후 배열 상태: < 38 12 | 48 | 70 75 67 96 52 81 > (실제 분할 구현에 따라 순서는 다를 수 있음). 기준 48 은 인덱스 2에 위치 ( p=2 ). Small 그룹( A[0.] )의 크기 s = p - first = 2 - 0 = 2 . 선택: 우리가 찾는 k=5 는 s+1=3 보다 큽니다 ( k > s+1 ). 따라서 답은 Large 그룹에 있습니다. Large 그룹 ( A[3.] )에서 k' = k - s - 1 = 5 - 2 - 1 = 2 번째 작은 요소를 찾습니다. 재귀 호출: selection(A, 3, 8, k=2) 두 번째 호출: selection(A, 3, 8, k=2) ( < 70 75 67 96 52 81 > 부분) 분할: 현재 부분 배열의 첫 요소인 70 을 기준으로 선택합니다. 분할 후 부분 배열 상태: < 67 52 | 70 | 96 75 81 > . 기준 70 은 원래 배열 인덱스 5에 위치 ( p=5 ). 현재 부분 배열 범위는 first=3, last=8 입니다. Small 그룹( A[3.] )의 크기 s = p - first = 5 - 3 = 2 . 선택: 우리가 찾는 k=2 는 s=2 와 같습니다 ( k <= s ). 따라서 답은 Small 그룹에 있습니다. Small 그룹 ( A[3.] )에서 여전히 k=2 번째 작은 요소를 찾습니다. 재귀 호출: selection(A, 3, 4, k=2) 세 번째 호출: selection(A, 3, 4, k=2) ( < 67 52 > 부분) 분할: 첫 요소 67 을 기준으로 선택합니다. 분할 후 부분 배열 상태: < 52 | 67 > . 기준 67 은 원래 배열 인덱스 4에 위치 ( p=4 ). 현재 부분 배열 범위는 first=3, last=4 입니다. Small 그룹 ( A[3.] )의 크기 s = p - first = 4 - 3 = 1 . 선택: 우리가 찾는 k=2 는 s+1=2 와 같습니다 ( k == s+1 ). 찾았습니다! 기준 요소인 A[p] = A[4] = 67 이 바로 우리가 찾던 5번째 작은 수입니다. 67 을 반환하고 모든 재귀 호출이 종료됩니다. (참고: 정렬된 배열: 12, 38, 48, 52, 67, 70, 75, 81, 96 . 5번째는 67이 맞습니다.) 선택 알고리즘 시간 복잡도 최선/평균의 경우: 매번 기준이 배열을 적절한 비율로 나눈다면 (예: 3:7), 탐색해야 할 배열의 크기는 매 단계마다 cn (단, c < 1 )으로 줄어듭니다. 점화식: T(n) = T(cn) + O(n) (분할에 O(n) 소요) 이 점화식의 해는 O(n) 입니다. n + cn + c^2n + ... = n(1+c+c^2+...) 는 등비수열의 합으로 n/(1-c) 이므로 O(n) 입니다. 정렬( O(n log n) )보다 훨씬 빠른 선형 시간에 k번째 요소를 찾을 수 있습니다. 최악의 경우: 빠른 정렬과 마찬가지로, 매번 기준이 가장 크거나 작은 값으로 선택되어 탐색 범위가 하나씩만 줄어드는 경우입니다. 점화식: T(n) = T(n-1) + O(n) 시간 복잡도: O(n^2) 5 분할 정복이 부적절한 경우 분할 정복은 매우 강력하지만 만병통치약은 아닙니다. 분할 정복이 비효율적이거나 부적절한 경우가 있는데, 가장 대표적인 사례는 부분 문제들이 서로 중복될 때입니다. 슬라이드 44에서 언급된 두 가지 경우는 이 문제를 잘 설명합니다. 경우 1: 크기 n 의 문제가 거의 n 에 가까운 크기의 두 개 이상의 부분 문제로 분할될 때. 경우 2: 크기 n 의 문제가 n/c 크기의 거의 n 개에 가까운 부분 문제로 분할될 때. 이 두 경우의 공통적인 문제는, 분할된 부분 문제들의 입력 크기의 합이 원래 문제의 입력 크기보다 훨씬 커진다는 점입니다. 이는 같은 부분 문제를 여러 번 반복해서 풀게 될 가능성이 높다는 것을 시사합니다. 대표적인 예: 피보나치 수열 피보나치 수열은 다음과 같이 정의됩니다. f(0) = 0 , f(1) = 1 (슬라이드에서는 1-based index로 f0=1, f1=1 로 정의) f(n) = f(n-1) + f(n-2) for n >= 2 이 정의는 그 자체로 분할 정복의 재귀적 구조를 가지고 있습니다. f(n) 이라는 문제를 f(n-1) 과 f(n-2) 라는 두 개의 작은 문제로 나누어 해결할 수 있기 때문입니다. 재귀를 이용한 분할 정복 알고리즘 F(n) 1 if n ≤ 1 return 1 2 else return F(n - 1) + F(n - 2) 이 코드는 피보나치 수열의 정의를 그대로 코드로 옮긴 것입니다. 매우 간결하고 직관적입니다. 하지만 치명적인 비효율성을 가지고 있습니다. 실행 트리와 중복 계산의 문제 F(5) 를 계산하는 과정을 생각해 봅시다 (슬라이드 48). F(5) 는 F(4) 와 F(3) 을 호출합니다. F(4) 는 F(3) 과 F(2) 를 호출합니다. F(3) 은 F(2) 와 F(1) 을 호출합니다. 실행 트리를 그려보면, F(3) 은 2번, F(2) 는 3번, F(1) 은 5번, F(0) 은 3번 호출됩니다 (0-based 기준). n 이 커질수록 이 중복 호출은 기하급수적으로 늘어납니다. F(n-1) 과 F(n-2) 는 매우 큰 부분 문제를 공유하고 있으며, 이것이 바로 분할된 부분 문제들이 독립적이지 않은(not independent) 경우입니다. 이 알고리즘의 호출 횟수는 피보나치 수와 비례하며, 시간 복잡도는 약 O(1.618^n) (황금비의 n제곱)이라는 지수 시간(Exponential Time) 복잡도를 가집니다. n=40만 되어도 계산에 엄청난 시간이 걸립니다. 더 나은 해결책: 동적 계획법 (Dynamic Programming) 이러한 중복 계산 문제를 해결하기 위한 패러다임이 바로 동적 계획법(Dynamic Programming)입니다. 동적 계획법의 핵심은 한 번 계산한 부분 문제의 결과는 저장해두고(Memoization), 다시 필요할 때 재계산 없이 가져다 쓰는 것입니다. 동적 계획법 기반 알고리즘 슬라이드 49의 알고리즘은 상향식(Bottom-up) 접근법을 사용합니다. 가장 작은 문제인 F(0) 과 F(1) 부터 계산을 시작합니다. 이 결과를 배열(또는 변수)에 저장합니다. 저장된 값들을 이용하여 F(2) , F(3) , ..., F(n) 을 차례대로 계산해 나갑니다. F_DP(n) 1 F[0] = 1 2 F[1] = 1 3 for i from 2 to n 4 F[i] = F[i-1] + F[i-2] 5 return F[n] 이 알고리즘은 for 루프를 n-1 번 반복하며, 각 반복마다 덧셈 한 번만 수행합니다. 따라서 시간 복잡도는 O(n) 으로, 재귀적인 분할 정복 방식의 O(1.618^n) 과 비교할 수 없을 정도로 효율적입니다. 결론 및 요약 이 장에서는 알고리즘 설계의 핵심 패러다임인 분할 정복에 대해 깊이 있게 탐구했습니다. 분할 정복의 힘: 분할 정복은 복잡한 문제를 균형 잡힌(balanced) 작은 부분 문제들로 나누어 해결함으로써 효율성을 극대화하는 전략입니다. '균형'은 알고리즘의 성능에 매우 중요합니다. 비슷한 크기로 나눌 때 O(n log n) 과 같은 효율적인 시간 복잡도를 달성할 수 있습니다. 최댓값/최솟값 찾기, 합병 정렬, 빠른 정렬, 선택 문제 등 다양한 문제에서 그 강력함을 확인할 수 있었습니다. 이들은 모두 부분 문제들이 서로 독립적이라는 공통점을 가집니다. 분할과 합병의 상호작용: 합병 정렬: 분할은 간단하고( O(1) ), 합병이 핵심적인 작업( O(n) )을 수행합니다. 빠른 정렬: 분할이 핵심적인 작업( O(n) )을 수행하고, 합병은 사실상 필요 없습니다( O(1) ). 문제의 특성에 따라 분할과 합병 단계 중 어느 쪽에 무게를 둘 것인지가 결정됩니다. 분할 정복의 한계와 대안: 피보나치 수열 예제에서 보았듯이, 부분 문제들이 서로 중복되어 재계산이 빈번하게 발생하는 경우, 분할 정복은 오히려 지수 시간의 비효율을 초래합니다. 이러한 중복되는 부분 문제(overlapping subproblems) 구조를 가진 문제에는 동적 계획법이 훨씬 효과적인 해결책을 제공합니다. 분할 정복은 단순히 알고리즘 목록에 있는 하나의 기법이 아니라, 문제를 바라보고 구조화하는 강력한 사고방식입니다. 복잡한 현실 세계의 문제를 만났을 때, 그것을 더 작고 관리 가능한 조각으로 나누어 해결하고 다시 합치는 이 접근법은 컴퓨터 과학을 넘어 다양한 분야에서 유용하게 적용될 수 있는 지혜를 제공합니다. 네, 제공해주신 강의 자료를 바탕으로 \"제6장: 동적 계획\"에 대한 매우 상세하고 포괄적인 설명을 6만자 이상으로 작성해 드리겠습니다. 각 슬라이드의 내용을 심층적으로 확장하고, 동적 계획의 핵심 원리, 문제 해결 과정, 구체적인 예제(막대 자르기, 연속 행렬 곱셈, 모든 쌍 최단 경로, 배낭 채우기)의 단계별 분석을 포함하여 구성하겠습니다. 제6장: 동적 계획 (Dynamic Programming) 네, 알겠습니다. 분할 정복(Divide and Conquer)의 한계를 살펴보고, 이를 극복하는 동적 계획법(Dynamic Programming)의 개념을 막대 자르기(Rod Cutting) 문제를 통해 매우 상세하고 이야기 형식으로 설명해 드리겠습니다. 분할 정복의 한계: 중복되는 계산의 비효율성 우리는 알고리즘 설계 기법 중 하나인 분할 정복(Divide and Conquer)에 대해 이미 알고 있습니다. 분할 정복은 주어진 문제를 더 이상 나눌 수 없을 때까지 작은 부분 문제(subproblem)들로 나누고(Divide), 각각의 부분 문제를 해결한(Conquer) 후, 그 해들을 다시 합쳐(Combine) 원래 문제의 해를 구하는 강력한 방식입니다. 대표적인 예로 병합 정렬(Merge Sort)이나 퀵 정렬(Quick Sort)이 있습니다. 하지만 분할 정복이 모든 문제에 효율적인 해결책을 제시하는 것은 아닙니다. 분할 정복의 핵심은 '서로 독립적인' 부분 문제로 나누는 것입니다. 예를 들어 병합 정렬에서 배열의 왼쪽 절반을 정렬하는 문제와 오른쪽 절반을 정렬하는 문제는 서로 전혀 영향을 주지 않습니다. 그러나 어떤 문제들은 나누어진 부분 문제들이 서로 독립적이지 않고, 동일한 부분 문제를 여러 번 반복해서 풀어야 하는 경우가 발생합니다. 가장 고전적인 예가 피보나치 수열입니다. fib(n) = fib(n-1) + fib(n-2) 이 점화식을 그대로 재귀 함수로 구현하면 fib(5) 를 계산하기 위해 fib(4) 와 fib(3) 을 호출합니다. 그런데 fib(4) 를 계산하는 과정에서 또다시 fib(3) 과 fib(2) 를 호출하게 됩니다. 결국 fib(3) 이라는 동일한 부분 문제가 두 번 이상 계산되는 것을 볼 수 있습니다. 문제의 크기가 커질수록 이런 중복 계산은 기하급수적으로 늘어나고, 알고리즘의 효율성은 최악으로 치닫게 됩니다. 이처럼 중복되는 부분 문제(Overlapping Subproblems) 구조를 가진 문제에 단순한 분할 정복을 적용하면 심각한 비효율을 초래합니다. 이제 이러한 특성을 가진 또 다른 문제, 바로 '막대 자르기 문제'를 통해 이 문제를 해결하는 동적 계획법(Dynamic Programming)의 위력을 살펴보겠습니다. 막대 자르기 문제(Rod Cutting Problem) 문제 정의 길이가 n 인 막대와 각 길이별 판매 가격표가 주어졌을 때, 막대를 어떻게 잘라야 전체 판매 금액을 최대로 할 수 있는지 찾는 문제입니다. 막대는 여러 조각으로 자를 수 있으며, 자르지 않고 통째로 파는 것도 하나의 방법입니다. 예를 들어, 다음과 같은 가격표가 있다고 가정해 보겠습니다. 길이 (i) 1 2 3 4 가격 (p[i]) 2 5 9 10 만약 우리에게 길이가 4인 막대가 주어진다면, 어떤 방법들이 있을까요? 자르지 않음: 길이 4짜리 한 개 -> 가격 10 한 번 자름: 1 + 3으로 자름 -> p[1] + p[3] = 2 + 9 = 11 2 + 2로 자름 -> p[2] + p[2] = 5 + 5 = 10 두 번 자름: 1 + 1 + 2로 자름 -> p[1] + p[1] + p[2] = 2 + 2 + 5 = 9 세 번 자름: 1 + 1 + 1 + 1로 자름 -> p[1] * 4 = 2 * 4 = 8 이 모든 경우를 비교해보니, 1과 3으로 잘랐을 때의 판매 금액 11이 최대 이익임을 알 수 있습니다. 우리의 목표는 어떤 길이 n 에 대해서도 이 최대 이익을 찾는 알고리즘을 설계하는 것입니다. 첫 번째 시도: 분할 정복을 이용한 재귀적 접근 이 문제를 처음 접했을 때, 가장 직관적으로 떠올릴 수 있는 방법은 분할 정복 기반의 재귀적 접근입니다. 길이 i 의 막대에서 얻을 수 있는 최대 판매 금액을 R(i) 라고 정의해 봅시다. 길이 i 의 막대를 얻기 위한 방법은, 첫 번째 조각을 길이 k (단, 1 ≤ k ≤ i )로 자르는 것입니다. 그러면 우리는 길이 k 짜리 조각 하나와 길이 i-k 짜리 막대 하나를 얻게 됩니다. 길이 k 짜리 조각은 가격표에 따라 p[k] 의 가격으로 판매합니다. 나머지 길이 i-k 짜리 막대는? 이 막대 또한 최적의 방법으로 잘라 팔아야 최대 이익을 얻을 수 있습니다. 즉, R(i-k) 의 이익을 얻게 됩니다. 따라서, 첫 조각을 길이 k 로 잘랐을 때의 총 이익은 p[k] + R(i-k) 가 됩니다. 우리는 가능한 모든 k (1부터 i 까지)에 대해 이 값을 계산하여 그중 최댓값을 찾으면 됩니다. 이를 점화식으로 표현하면 다음과 같습니다. R(i) = max( p[k] + R(i-k) ) for 1 ≤ k ≤ i (단, R(0) = 0 ) 이 점화식을 코드로 구현하면 다음과 같은 재귀 함수( cutRod_DC )가 됩니다. // 분할 정복(재귀) 방식의 막대 자르기 알고리즘 cutRod_DC(p[], i) { if (i == 0) return 0; // 길이가 0이면 가격도 0 maxSell = 0; for (j = 1; j <= i; j++) { // 첫 조각을 j로 잘랐을 때의 가격 p[j]와 // 나머지 부분(i-j)의 최대 이익을 더한 값 중 최댓값을 찾는다. maxSell = MAX(maxSell, p[j] + cutRod_DC(p, i - j)); } return maxSell; } 분할 정복 접근법의 치명적인 문제 이 코드는 정확한 답을 찾아줍니다. 하지만 피보나치 수열의 예처럼, 심각한 비효율을 내포하고 있습니다. cutRod_DC(p, 4) 를 호출했을 때의 실행 트리를 살펴봅시다. Recursion Tree for R(4) 161x81 위 그림은 R(4) 를 계산하기 위한 재귀 호출의 구조를 보여줍니다. R(4) 를 계산하기 위해 R(3), R(2), R(1), R(0) 을 호출합니다. R(3) 을 계산하기 위해 R(2), R(1), R(0) 을 호출합니다. R(2) 를 계산하기 위해 R(1), R(0) 을 호출합니다. 보시다시피, R(2) 는 R(4) 를 계산하는 과정과 R(3) 을 계산하는 과정에서 총 2번 호출됩니다. R(1) 은 R(4), R(3), R(2) 를 계산하는 과정에서 총 4번이나 호출됩니다. 이렇게 동일한 부분 문제에 대한 계산이 반복적으로 일어나는 '중복되는 부분 문제' 현상이 여기서도 명확하게 나타납니다. 이 알고리즘의 시간 복잡도를 분석해보면 T(n) = Σ T(n-k) + 1 (k=1 to n) 형태가 되며, 이는 T(n) = 2^n 에 가까운 지수 시간 복잡도(Exponential Time Complexity)를 가집니다. n 이 조금만 커져도 계산 시간이 폭발적으로 증가하여 현실적으로 사용할 수 없는 알고리즘이 됩니다. 두 번째 시도: 동적 계획법(Dynamic Programming) 동적 계획법은 바로 이 '중복되는 부분 문제'를 해결하기 위해 탄생한 알고리즘 설계 기법입니다. 아이디어는 매우 간단합니다. \"어떤 문제든 딱 한 번만 풀고, 그 결과를 저장해두었다가 나중에 필요할 때 다시 계산하지 말고 가져다 쓰자.\" 이 기법을 적용하기 위해서는 두 가지 속성이 문제에 존재해야 합니다. 중복되는 부분 문제 (Overlapping Subproblems): 위에서 확인했듯이, 막대 자르기 문제는 이 속성을 만족합니다. 최적 부분 구조 (Optimal Substructure): 문제의 최적의 해가 부분 문제들의 최적의 해로부터 구성될 수 있다는 것입니다. 막대 자르기 문제에서 길이 i 의 최적해는, 첫 조각 k 를 자른 후 남은 i-k 길이 막대의 최적해에 p[k] 를 더한 값들 중 최대값으로 구성되므로 이 속성 또한 만족합니다. 이러한 문제의 구조는 분할 정복의 트리 구조와 달리, 공유되는 부분 문제를 가진 DAG(Directed Acyclic Graph) 구조로 시각화할 수 있습니다. (a) 분할 정복은 각 부분 문제가 독립적인 트리 구조를 가집니다. (b) 동적 계획은 E 와 같은 부분 문제가 여러 상위 문제( B , C )에 의해 공유되는 구조를 가지며, 이 E 를 한 번만 계산하는 것이 핵심입니다. 동적 계획 알고리즘 설계 (Bottom-up 방식) 동적 계획법은 보통 '상향식(Bottom-up)'으로 문제를 해결합니다. 즉, 가장 작은 부분 문제부터 차례대로 풀어나가면서 그 결과를 테이블에 저장하고, 더 큰 문제를 풀 때 이 테이블의 값을 활용하는 방식입니다. 막대 자르기 문제에서 가장 작은 문제는 무엇일까요? 바로 길이가 0인 막대입니다. 그 다음은 길이 1, 길이 2, ... 순서로 n 까지 문제를 풀어 나가면 됩니다. 결과를 저장할 배열(테이블)을 만듭니다. maxSell[0...n] 이라는 배열을 선언하고, maxSell[j] 에는 길이 j 막대의 최대 판매 금액을 저장할 것입니다. 가장 작은 문제부터 해결합니다. maxSell[0] = 0 (길이 0인 막대의 가격은 0) 반복문을 통해 점진적으로 큰 문제를 해결합니다. j 를 1부터 n 까지 증가시키면서 maxSell[j] 를 계산합니다. maxSell[j] 를 계산하기 위해, 우리는 이전에 이미 계산해 둔 maxSell[0], maxSell[1], ..., maxSell[j-1] 값들을 사용할 수 있습니다. R(j) = max( p[k] + R(j-k) ) 점화식을 그대로 적용하되, R(j-k) 대신 이미 계산된 값인 maxSell[j-k] 를 사용합니다. 이를 코드로 구현하면 다음과 같습니다. // 동적 계획법 방식의 막대 자르기 알고리즘 cutRod_DP(p[], n) { // 1. 결과를 저장할 배열 선언 배열 maxSell[0...n]을 선언한다. // 2. 가장 작은 문제의 해 초기화 maxSell[0] = 0; // 3. 점진적으로 큰 문제 해결 (j = 1부터 n까지) for (j = 1; j <= n; j++) { maxVal = 0; // maxSell[j]를 계산하기 위해 모든 가능한 첫 조각 k를 고려 for (k = 1; k <= j; k++) { // p[k] + maxSell[j-k] : 첫 조각을 k로 잘랐을 때의 이익 // maxSell[j-k]는 이전에 이미 계산된 최적의 값이다. maxVal = MAX(maxVal, p[k] + maxSell[j - k]); } // j에 대한 최댓값을 테이블에 저장 maxSell[j] = maxVal; } // 최종적으로 우리가 원하는 길이 n에 대한 해를 반환 return maxSell[n]; } 동적 계획 알고리즘 실행 과정 추적 위 알고리즘이 어떻게 동작하는지 가격표 (p[1]=2, p[2]=5, p[3]=9, p[4]=10)와 n=4를 예로 들어 단계별로 따라가 보겠습니다. 초기 상태: maxSell 배열이 생성되고 maxSell[0] 은 0으로 초기화됩니다. p = [?, 2, 5, 9, 10] maxSell = [0, ?, ?, ?, ?] j = 1 (길이 1 막대의 최대 이익 계산) maxVal 를 0으로 초기화. k = 1: maxVal = MAX(0, p[1] + maxSell[1-1]) = MAX(0, 2 + maxSell[0]) = MAX(0, 2 + 0) = 2 maxSell[1] = 2 현재 상태: maxSell = [0, 2, ?, ?, ?] j = 2 (길이 2 막대의 최대 이익 계산) maxVal 를 0으로 초기화. k = 1: maxVal = MAX(0, p[1] + maxSell[2-1]) = MAX(0, 2 + maxSell[1]) = MAX(0, 2 + 2) = 4 k = 2: maxVal = MAX(4, p[2] + maxSell[2-2]) = MAX(4, 5 + maxSell[0]) = MAX(4, 5 + 0) = 5 maxSell[2] = 5 현재 상태: maxSell = [0, 2, 5, ?, ?] j = 3 (길이 3 막대의 최대 이익 계산) maxVal 를 0으로 초기화. k = 1: maxVal = MAX(0, p[1] + maxSell[3-1]) = MAX(0, 2 + maxSell[2]) = MAX(0, 2 + 5) = 7 k = 2: maxVal = MAX(7, p[2] + maxSell[3-2]) = MAX(7, 5 + maxSell[1]) = MAX(7, 5 + 2) = 7 k = 3: maxVal = MAX(7, p[3] + maxSell[3-3]) = MAX(7, 9 + maxSell[0]) = MAX(7, 9 + 0) = 9 maxSell[3] = 9 현재 상태: maxSell = [0, 2, 5, 9, ?] j = 4 (길이 4 막대의 최대 이익 계산) maxVal 를 0으로 초기화. k = 1: maxVal = MAX(0, p[1] + maxSell[4-1]) = MAX(0, 2 + maxSell[3]) = MAX(0, 2 + 9) = 11 k = 2: maxVal = MAX(11, p[2] + maxSell[4-2]) = MAX(11, 5 + maxSell[2]) = MAX(11, 5 + 5) = 11 k = 3: maxVal = MAX(11, p[3] + maxSell[4-3]) = MAX(11, 9 + maxSell[1]) = MAX(11, 9 + 2) = 11 k = 4: maxVal = MAX(11, p[4] + maxSell[4-4]) = MAX(11, 10 + maxSell[0]) = MAX(11, 10 + 0) = 11 maxSell[4] = 11 최종 상태: maxSell = [0, 2, 5, 9, 11] 모든 반복이 끝나면 maxSell[4] 에는 11이 저장되어 있습니다. 알고리즘은 이 값을 반환하고, 우리는 길이 4의 막대로 얻을 수 있는 최대 이익이 11이라는 것을 알게 됩니다. 이 과정을 슬라이드의 표와 비교해보면 정확히 일치하는 것을 볼 수 있습니다. 동적 계획법의 효율성 분석 시간 복잡도: 알고리즘은 2개의 중첩된 반복문으로 구성됩니다. 바깥쪽 j 루프는 1부터 n 까지 n 번 실행됩니다. 안쪽 k 루프는 j 번 실행됩니다. 따라서 기본 연산(MAX 계산 및 덧셈)의 총 실행 횟수는 1 + 2 + 3 + ... + n 이 됩니다. 이는 n(n+1)/2 와 같으므로, 시간 복잡도는 Θ(n²) 입니다. 공간 복잡도: 해를 저장하기 위해 크기가 n+1 인 maxSell 배열을 사용하므로, 공간 복잡도는 Θ(n) 입니다. 분할 정복의 지수 시간 복잡도 O(2^n) 과 비교해볼 때, 동적 계획법의 다항 시간 복잡도 Θ(n²) 는 엄청나게 효율적입니다. n 이 30일 때, 2^30 은 10억이 넘는 수지만 30^2 은 고작 900입니다. 이 차이가 바로 동적 계획법의 힘입니다. 결론: 현명한 기억의 힘 막대 자르기 문제는 분할 정복의 순진한 재귀적 접근이 왜 비효율적인지, 그리고 동적 계획법이 어떻게 그 문제를 해결하는지를 보여주는 훌륭한 예시입니다. 핵심은 '문제를 한 번만 푸는 것'입니다. 분할 정복은 동일한 질문을 계속해서 반복하지만, 동적 계획법은 한 번 얻은 답을 테이블에 기록해두고 필요할 때마다 꺼내 씁니다. 이 '기억(memoization)'이라는 단순한 아이디어 하나가 지수 시간을 다항 시간으로 바꾸는 극적인 성능 향상을 가져옵니다. 따라서, 어떤 문제를 해결해야 할 때 다음 두 가지 특징이 보인다면 동적 계획법을 가장 먼저 떠올려야 합니다. 중복되는 부분 문제: 작은 문제들의 해가 여러 번 필요하다. 최적 부분 구조: 전체 문제의 최적해가 부분 문제들의 최적해로 구성된다. 동적 계획법은 알고리즘의 효율성을 극대화하는 매우 중요하고 강력한 도구이며, 수많은 최적화 문제 해결의 기반이 됩니다. 행렬들의 곱셈 순서를 정하는 문제 행렬 곱셈의 특성과 새로운 문제의 발견 우리는 이전에 분할 정복의 한계와 이를 극복하는 동적 계획법의 원리를 막대 자르기 문제를 통해 살펴보았습니다. 핵심은 '중복되는 부분 문제'의 해를 한 번만 계산하고 저장하여 재사용하는 것이었죠. 이제 우리는 훨씬 더 복잡해 보이지만, 동일한 원리가 화려하게 적용되는 새로운 최적화 문제를 만나보겠습니다. 바로 연속된 행렬들의 곱셈 순서를 정하는 문제입니다. 먼저 행렬 곱셈의 기본적인 성질을 짚고 넘어가야 합니다. 두 행렬 A 와 B 를 곱하여 행렬 C 를 얻는 연산 C = A × B 는, 행렬 A 의 열의 개수와 행렬 B 의 행의 개수가 같을 때만 정의됩니다. 만약 A 가 m × p 행렬이고 B 가 p × n 행렬이라면, 결과 C 는 m × n 행렬이 됩니다. 이때, C 의 각 원소 c_ij 를 계산하기 위해서는 A 의 i 번째 행과 B 의 j 번째 열에 있는 원소들을 각각 곱해서 더해야 합니다. 이 과정에는 총 p 번의 스칼라 곱셈이 필요합니다. 결과 행렬 C 는 총 m × n 개의 원소를 가지므로, 두 행렬 A 와 B 를 곱하는 데 필요한 총 스칼라 곱셈 횟수는 m × p × n 이 됩니다. 이 곱셈 횟수가 바로 우리가 앞으로 최소화하려는 '비용(cost)'이 됩니다. 자, 이제 행렬이 세 개 이상, 예를 들어 M₁ × M₂ × M₃ 과 같이 연속으로 주어졌다고 생각해 봅시다. 행렬 곱셈은 결합 법칙(associative law)이 성립하기 때문에, 곱하는 순서를 바꿔도 최종 결과는 같습니다. 즉, (M₁ × M₂) × M₃ 와 M₁ × (M₂ × M₃) 의 결과 행렬은 동일합니다. 하지만, 계산 과정의 '비용'은 어떨까요? 예를 들어 세 행렬의 크기가 다음과 같다고 가정해 봅시다. M₁ : 10 × 20 M₂ : 20 × 5 M₃ : 5 × 50 경우 1: (M₁ × M₂) × M₃ M₁ × M₂ 를 먼저 계산합니다. 비용: 10 × 20 × 5 = 1,000 번의 곱셈. 결과 행렬 (M₁₂) 의 크기: 10 × 5 결과 행렬 (M₁₂) 와 M₃ 를 곱합니다. 비용: 10 × 5 × 50 = 2,500 번의 곱셈. 최종 결과 행렬의 크기: 10 × 50 총 비용: 1,000 + 2,500 = 3,500 경우 2: M₁ × (M₂ × M₃) M₂ × M₃ 를 먼저 계산합니다. 비용: 20 × 5 × 50 = 5,000 번의 곱셈. 결과 행렬 (M₂₃) 의 크기: 20 × 50 M₁ 과 결과 행렬 (M₂₃) 을 곱합니다. 비용: 10 × 20 × 50 = 10,000 번의 곱셈. 최종 결과 행렬의 크기: 10 × 50 총 비용: 5,000 + 10,000 = 15,000 놀랍게도 곱셈 순서에 따라 총 계산 비용이 3,500과 15,000으로 엄청난 차이를 보입니다. 행렬의 개수가 늘어날수록 가능한 곱셈 순서의 조합은 기하급수적으로 증가(카탈란 수, Catalan number를 따름)하며, 최적의 순서를 찾는 것은 매우 중요한 문제가 됩니다. 이것이 바로 연속 행렬 곱셈 문제(Matrix Chain Multiplication Problem)입니다. 문제 정의: n 개의 행렬 체인 M₁ × M₂ × ... × Mₙ 이 주어졌을 때, 총 스칼라 곱셈 횟수를 최소화하는 곱셈 순서(괄호 치는 방법)를 찾는 것. 컴퓨터가 두 행렬을 곱할 때 실제로 무슨 일을 하는지 들여다봅시다. A (2x3 크기) 와 B (3x2 크기) 두 행렬이 있다고 상상해 보세요. 결과 행렬 C의 딱 한 칸, C\\[1\\]\\[1\\] (1행 1열)을 채우려면 어떻게 해야 할까요? A의 1행 (\\[1, 2, 3\\])과 B의 1열 (\\[7, 9, 11\\])을 가져옵니다. 각각의 위치에 있는 숫자끼리 곱하고, 그 결과들을 전부 더합니다. (1 × 7) -> 곱셈 1번 (2 × 9) -> 곱셈 1번 (3 × 11) -> 곱셈 1번 결과들을 더하기: 7 + 18 + 33 = 58 C의 딱 한 칸을 채우는 데 곱셈을 3번 했습니다. 그럼 C의 모든 칸(총 2x2 = 4칸)을 채우려면 곱셈을 몇 번 해야 할까요? 모든 칸에 대해 똑같이 곱셈을 3번씩 해야 합니다. 총 곱셈 횟수 = (한 칸당 곱셈 횟수) × (총 칸의 개수) 총 곱셈 횟수 = 3 × (2 × 2) = 12번 이것을 일반화한 것이 바로 그 공식입니다. m × p 행렬과 p × n 행렬을 곱하면, 결과 행렬은 m × n 크기 (총 m × n 개의 칸) 한 칸을 채우는 데 곱셈을 p번 함 총 곱셈 횟수(계산량) = m × p × n 알고리즘에서 말하는 \"비용(Cost)\" 또는 \"계산량\"은 바로 이 총 곱셈 횟수를 의미합니다. 우리는 이 숫자를 가장 작게 만들고 싶은 것입니다. 첫 번째 시도: 분할 정복의 함정 이 문제 역시 분할 정복의 관점에서 접근해 볼 수 있습니다. Mᵢ × Mᵢ₊₁ × ... × Mⱼ 의 최소 곱셈 횟수를 구하는 문제를 생각해 봅시다. 이 행렬 체인의 마지막 곱셈은 반드시 어떤 k (단, i ≤ k < j )를 기준으로 두 부분 체인의 곱으로 이루어집니다. (Mᵢ × ... × Mₖ) × (Mₖ₊₁ × ... × Mⱼ) 즉, i 부터 j 까지의 행렬 곱셈 문제의 최적해를 구하려면, 가능한 모든 분할 지점 k 에 대해 문제를 나누어보고 그중 가장 비용이 적게 드는 경우를 선택하면 됩니다. Mᵢ 행렬의 크기를 rᵢ₋₁ × rᵢ 라고 표준화합시다. (즉, r 배열은 행렬들의 차원을 저장합니다.) Mᵢ × ... × Mₖ 를 계산한 결과 행렬의 크기는 rᵢ₋₁ × rₖ 가 됩니다. Mₖ₊₁ × ... × Mⱼ 를 계산한 결과 행렬의 크기는 rₖ × rⱼ 가 됩니다. 따라서, 분할 지점이 k 일 때의 총비용은 다음과 같이 세 부분으로 나뉩니다. Mᵢ × ... × Mₖ 를 계산하는 최소 비용. Mₖ₊₁ × ... × Mⱼ 를 계산하는 최소 비용. 두 결과 행렬을 마지막으로 곱하는 비용: rᵢ₋₁ × rₖ × rⱼ matMult(i, j) 를 Mᵢ 부터 Mⱼ 까지 곱하는 데 필요한 최소 곱셈 횟수라고 정의하면, 다음과 같은 점화식을 세울 수 있습니다. matMult(i, j) = min { matMult(i, k) + matMult(k+1, j) + rᵢ₋₁rₖrⱼ } (단, i ≤ k matMult(i, i) = 0 (행렬이 하나일 때는 곱셈이 필요 없으므로 비용은 0) 이 점화식은 문제의 구조를 완벽하게 표현합니다. 이를 그대로 재귀 함수로 구현하면 matMult_DC 와 같은 코드가 됩니다. // 분할 정복(재귀) 방식의 연속 행렬 곱셈 알고리즘 matMult_DC(r[], i, j) { if (i == j) return 0; // 곱할 행렬이 하나뿐인 경우 minVal = ∞; // 가능한 모든 분할 지점 k에 대해 탐색 for (k = i; k < j; k++) { // (i...k) 부분 문제 + (k+1...j) 부분 문제 + 마지막 곱셈 비용 cost = matMult_DC(r, i, k) + matMult_DC(r, k+1, j) + r[i-1]*r[k]*r[j]; minVal = MIN(minVal, cost); } return minVal; } 하지만 이 재귀 함수는 막대 자르기 문제에서 겪었던 것과 똑같은, 아니 훨씬 심각한 비효율을 가지고 있습니다. matMult(1, 4) 를 계산하려면 matMult(1, 1) , matMult(2, 4) ... 등등을 호출하고, matMult(1, 3) 을 계산하기 위해서도 matMult(2, 3) 등을 호출하는데, matMult(2, 4) 를 계산하는 과정에서도 matMult(2, 3) 이 또 호출됩니다. 즉, '중복되는 부분 문제'가 엄청나게 많이 발생하며, 시간 복잡도는 지수적으로 증가하여 n 이 20만 되어도 계산이 거의 불가능해집니다. 두 번째 시도: 동적 계획법의 설계 우리는 이미 해법을 알고 있습니다. 이 문제는 '최적 부분 구조'와 '중복되는 부분 문제'라는 동적 계획법의 전제 조건을 완벽하게 만족합니다. 따라서 재귀 대신, 가장 작은 부분 문제부터 차례로 해결하여 그 결과를 테이블에 저장하는 상향식(Bottom-up) 동적 계획법을 적용할 수 있습니다. 결과를 저장할 2차원 배열(테이블)을 만듭니다. m[i][j] 는 Mᵢ 부터 Mⱼ 까지 곱하는 데 필요한 최소 곱셈 횟수, 즉 matMult(i, j) 의 값을 저장할 공간입니다. 가장 작은 문제부터 해결합니다. 가장 작은 문제는 곱셈 체인의 길이가 1인 경우, 즉 m[i][i] 입니다. 이 경우 곱셈이 없으므로 비용은 0입니다. m[i][i] = 0 으로 테이블의 대각선을 모두 채웁니다. 점진적으로 문제의 크기를 키워나갑니다. 그 다음으로 작은 문제는 체인 길이가 2인 경우( m[i][i+1] ), 그 다음은 3인 경우( m[i][i+2] ), ... 마지막으로 우리가 원하는 체인 길이 n 인 경우( m[1][n] )까지 순서대로 계산합니다. 이 '체인의 길이'를 제어하는 것이 이 알고리즘의 핵심입니다. 변수 l 을 체인의 길이라고 합시다. l 은 2부터 n 까지 증가합니다. (슬라이드에서는 l 을 j-i 로 정의하여 1부터 n-1 까지 증가시켰는데, 이는 동일한 개념입니다.) l = 1 일 때: 길이가 2인 체인( m[1,2] , m[2,3] , ...)을 모두 계산합니다. l = 2 일 때: 길이가 3인 체인( m[1,3] , m[2,4] , ...)을 모두 계산합니다. 이때 m[1,3] 을 계산하려면 이미 계산된 m[1,1] , m[2,3] , m[1,2] , m[3,3] 등의 값이 필요하며, 우리는 이미 이 값들을 테이블에 저장해 두었습니다. ... l = n-1 일 때: 길이가 n 인 유일한 체인, 즉 m[1,n] 을 계산합니다. 이것이 최종 답입니다. 이 로직을 코드로 구현하면 다음과 같습니다. // 동적 계획법 방식의 연속 행렬 곱셈 알고리즘 matMult_DP(r[], n) { // 1. 결과 저장용 2차원 배열 m 선언 배열 m[1..n, 1..n]을 선언한다. // 2. 가장 작은 문제(체인 길이 1) 해결 for (i = 1; i <= n; i++) { m[i, i] = 0; } // 3. 점진적으로 문제 크기(체인 길이 l)를 키움 // l은 체인의 길이 - 1 (j-i) for (l = 1; l <= n - 1; l++) { // i는 체인의 시작 인덱스 for (i = 1; i <= n - l; i++) { j = i + l; // j는 체인의 끝 인덱스 m[i, j] = ∞; // 최솟값을 찾기 위해 무한대로 초기화 // k는 분할 지점 for (k = i; k < j; k++) { // 점화식을 DP 테이블을 이용하여 계산 cost = m[i, k] + m[k+1, j] + r[i - 1]*r[k]*r[j]; if (cost < m[i, j]) { m[i, j] = cost; } } } } // 최종적으로 우리가 원하는 해(m[1,n])를 반환 return m[1, n]; } 동적 계획 알고리즘 실행 과정 추적 (예제 풀이) 이제 슬라이드의 예제를 통해 이 알고리즘이 마법처럼 동작하는 과정을 단계별로 따라가 보겠습니다. 문제: M₁ × M₂ × M₃ × M₄ 의 최소 곱셈 횟수를 구하라. M₁ : 10 × 20 M₂ : 20 × 50 M₃ : 50 × 1 M₄ : 1 × 100 차원 배열 r : 행렬 Mᵢ 가 rᵢ₋₁ × rᵢ 크기이므로, r 배열은 다음과 같습니다. r = [r₀, r₁, r₂, r₃, r₄] = [10, 20, 50, 1, 100] 테이블 m : 4x4 크기의 테이블을 준비합니다. 1단계: 초기화 (체인 길이 1, l=0) 가장 작은 문제인 m[i,i] 를 0으로 채웁니다. m[1,1] = 0 m[2,2] = 0 m[3,3] = 0 m[4,4] = 0 i\\j 1 2 3 4 1 0 2 0 3 0 4 0 2단계: l = 1 (체인 길이 2) 길이가 2인 체인들( m[1,2] , m[2,3] , m[3,4] )의 최소 비용을 계산합니다. 이 경우 분할 지점 k 는 하나뿐입니다. m[1,2] 계산 ( M₁ × M₂ ) i=1, j=2 , 분할 지점 k=1 비용 = m[1,1] + m[2,2] + r₀r₁r₂ 비용 = 0 + 0 + 10 × 20 × 50 = 10,000 m[1,2] = 10000 m[2,3] 계산 ( M₂ × M₃ ) i=2, j=3 , 분할 지점 k=2 비용 = m[2,2] + m[3,3] + r₁r₂r₃ 비용 = 0 + 0 + 20 × 50 × 1 = 1,000 m[2,3] = 1000 m[3,4] 계산 ( M₃ × M₄ ) i=3, j=4 , 분할 지점 k=3 비용 = m[3,3] + m[4,4] + r₂r₃r₄ 비용 = 0 + 0 + 50 × 1 × 100 = 5,000 m[3,4] = 5000 현재 테이블 상태: i\\j 1 2 3 4 1 0 10000 2 0 1000 3 0 5000 4 0 3단계: l = 2 (체인 길이 3) 길이가 3인 체인들( m[1,3] , m[2,4] )의 최소 비용을 계산합니다. 이제 분할 지점이 두 개씩 생깁니다. m[1,3] 계산 ( M₁ × M₂ × M₃ ) i=1, j=3 . 가능한 분할 지점 k=1, 2 . k=1 일 때: (M₁) × (M₂ × M₃) 비용 = m[1,1] + m[2,3] + r₀r₁r₃ 비용 = 0 + 1000 + 10 × 20 × 1 = 1,200 k=2 일 때: (M₁ × M₂) × (M₃) 비용 = m[1,2] + m[3,3] + r₀r₂r₃ 비용 = 10000 + 0 + 10 × 50 × 1 = 10,500 min(1200, 10500) = 1200 . 따라서 m[1,3] = 1200 m[2,4] 계산 ( M₂ × M₃ × M₄ ) i=2, j=4 . 가능한 분할 지점 k=2, 3 . k=2 일 때: (M₂) × (M₃ × M₄) 비용 = m[2,2] + m[3,4] + r₁r₂r₄ 비용 = 0 + 5000 + 20 × 50 × 100 = 105,000 k=3 일 때: (M₂ × M₃) × (M₄) 비용 = m[2,3] + m[4,4] + r₁r₃r₄ 비용 = 1000 + 0 + 20 × 1 × 100 = 3,000 min(105000, 3000) = 3000 . 따라서 m[2,4] = 3000 현재 테이블 상태: i\\j 1 2 3 4 1 0 10000 1200 2 0 1000 3000 3 0 5000 4 0 4단계: l = 3 (체인 길이 4) 마지막으로, 우리가 구하고자 하는 전체 체인( m[1,4] )의 최소 비용을 계산합니다. m[1,4] 계산 ( M₁ × M₂ × M₃ × M₄ ) i=1, j=4 . 가능한 분할 지점 k=1, 2, 3 . k=1 일 때: (M₁) × (M₂ × M₃ × M₄) 비용 = m[1,1] + m[2,4] + r₀r₁r₄ 비용 = 0 + 3000 + 10 × 20 × 100 = 23,000 k=2 일 때: (M₁ × M₂) × (M₃ × M₄) 비용 = m[1,2] + m[3,4] + r₀r₂r₄ 비용 = 10000 + 5000 + 10 × 50 × 100 = 65,000 k=3 일 때: (M₁ × M₂ × M₃) × (M₄) 비용 = m[1,3] + m[4,4] + r₀r₃r₄ 비용 = 1200 + 0 + 10 × 1 × 100 = 2,200 min(23000, 65000, 2200) = 2200 . 따라서 m[1,4] = 2200 최종 테이블 상태: i\\j 1 2 3 4 1 0 10000 1200 2200 2 0 1000 3000 3 0 5000 4 0 모든 계산이 끝났습니다. 테이블의 m[1,4] 에 저장된 값 2,200이 바로 M₁ × M₂ × M₃ × M₄ 를 계산하는 데 필요한 최소 스칼라 곱셈 횟수입니다. 그리고 이 최적의 비용은 k=3 일 때, 즉 (M₁ × M₂ × M₃) × M₄ 순서로 곱했을 때 얻어진다는 것도 알 수 있습니다. (물론 (M₁ × M₂ × M₃) 의 최적 순서는 m[1,3] 계산 과정에서 (M₁) × (M₂ × M₃) 로 이미 결정되었습니다.) 결론: 복잡성을 길들이는 체계적인 접근 효율성 분석: 동적 계획법 알고리즘은 3개의 중첩된 루프( l , i , k )로 구성되어 있습니다. 각 루프는 최대 n 번 정도 반복되므로, 전체 시간 복잡도는 Θ(n³)입니다. 이는 분할 정복 방식의 지수 시간 복잡도와는 비교할 수 없을 정도로 효율적입니다. n 이 100이라면, 100³ 은 100만이지만, 지수 시간은 천문학적인 숫자가 됩니다. 공간 복잡도: n × n 크기의 테이블 m 을 사용하므로 공간 복잡도는 Θ(n²) 입니다. 연속 행렬 곱셈 문제는 동적 계획법의 설계 과정을 가장 잘 보여주는 교과서적인 예제입니다. 최적화 문제의 구조를 파악하고 점화식을 세웁니다. 가장 작은 부분 문제부터 시작하여 상향식으로 해를 구축합니다. 테이블을 사용하여 계산된 해를 저장하고 재사용함으로써 중복 계산을 완벽하게 제거합니다. 겉보기에는 복잡하고 수많은 경우의 수를 따져야 할 것 같은 문제였지만, 동적 계획법이라는 체계적인 접근법을 통해 다항 시간 내에 효율적으로 해결할 수 있었습니다. 이는 복잡한 문제에 직면했을 때, 문제를 올바르게 분해하고 그 관계를 파악하는 것이 얼마나 중요한지를 보여주는 강력한 증거입니다. 네, 알겠습니다. 앞서 다룬 막대 자르기 문제에 이어, 동적 계획법의 힘을 더욱 극적으로 보여주는 연속 행렬 곱셈(Matrix Chain Multiplication) 문제에 대해 이야기 형식으로 매우 상세하게 설명해 드리겠습니다. 네, 알겠습니다. 제공해주신 우수한 자료를 바탕으로, 독자의 이해 흐름을 최적화하기 위해 순서를 재구성하고 내용을 다듬어 이야기 형식으로 상세하게 설명해 드리겠습니다. 모든 쌍 최단 경로 (All-Pairs Shortest Path) 서막: 궁극의 내비게이션을 향하여 우리가 매일 사용하는 내비게이션은 \"A에서 B까지 가는 가장 빠른 길\"이라는 하나의 질문에 답합니다. 이는 컴퓨터 과학에서 단일 출발점 최단 경로(Single-Source Shortest Path) 문제로, 다익스트라 알고리즘이 훌륭한 해답을 제공합니다. 하지만 만약 우리가 이 서비스를 운영하는 회사라면 어떨까요? 우리는 수많은 사용자의 무작위적인 출발-도착점 요청에 즉각적으로 응답해야 합니다. 더 나아가, 도시 전체의 교통 흐름을 분석하거나, 물류 회사가 수백 개의 지점 사이의 최적 운송 루트를 미리 계산하거나, 항공사가 모든 공항 간의 최적 환승 경로를 파악해야 하는 상황을 상상해 봅시다. 이런 경우, 우리는 도시의 모든 지점에서 다른 모든 지점까지의 최단 경로를 미리 계산해 두는 것이 훨씬 효율적입니다. 이것이 바로 모든 쌍 최단 경로(All-Pairs Shortest Path, APSP) 문제입니다. 이는 단순히 길 하나를 찾는 것을 넘어, 네트워크 전체의 구조적인 연결성을 완벽하게 이해하려는 궁극의 내비게이션 과제와 같습니다. 제1장: 문제의 공식화 - 그래프 언어로의 번역 이 거대한 과제를 컴퓨터 과학의 언어로 옮겨 보겠습니다. 그래프 G = (V, E) : 도시의 교차로들은 정점(Vertex) V 가 되고, 교차로를 잇는 도로들은 간선(Edge) E 이 됩니다. 가중치(Weight): 각 도로는 길이, 통행 시간 등의 비용을 가집니다. 도로는 일방통행일 수 있으므로, 방향 그래프(Directed Graph)를 가정합니다. 문제 정의: 가중치 방향 그래프 G 가 주어졌을 때, 모든 정점 쌍 (i, j) 에 대해, i 에서 j 로 가는 최단 경로의 거리를 찾는 것. 컴퓨터는 이 정보를 인접 행렬(Adjacency Matrix) W 로 표현합니다. W 는 n x n 크기의 행렬이며, wᵢⱼ 는 정점 i 에서 j 로 직접 가는 간선의 가중치입니다. wᵢⱼ = 0 (만약 i = j , 자기 자신으로 가는 비용은 0) wᵢⱼ = 간선의 가중치 (만약 i 에서 j 로 가는 간선이 존재) wᵢⱼ = ∞ (만약 i 에서 j 로 직접 가는 간선이 없음) 우리가 최종적으로 얻고 싶은 결과물은 거리 행렬(Distance Matrix) D 입니다. dᵢⱼ 는 정점 i 에서 j 로 가는, 중간에 다른 정점들을 거쳐갈 수 있는 모든 경로 중 가장 짧은 경로의 거리를 담고 있습니다. 제2장: 첫 번째 시도와 절망 - 억지 기법의 한계 가장 순진한 접근법은 모든 정점 쌍 (i, j) 에 대해, i 에서 j 로 갈 수 있는 모든 가능한 경로를 하나도 빠짐없이 찾아보는 것입니다. 하지만 이는 정점의 개수 n 이 조금만 커져도 경우의 수가 기하급수적으로 폭발합니다. i 에서 j 로 가는 경로에 나머지 n-2 개의 정점들을 어떤 순서로 방문하느냐에 따라 (n-2)! 에 비례하는 경로가 생성됩니다. 이는 천문학적인 숫자로, 현실적으로 불가능한 접근법입니다. 제3장: 발상의 전환 - 동적 계획법의 새로운 관점 이 막다른 길에서 로버트 플로이드(Robert Floyd)와 스티븐 워셜(Stephen Warshall)은 문제를 완전히 새로운 각도에서 바라보는 천재적인 아이디어를 제시합니다. \"최단 경로를 구성하는 '중간 정점'의 범위를 점진적으로 늘려가며 해를 구하자.\" 이것이 바로 플로이드-워셜 알고리즘의 핵심 사상입니다. 우리는 부분 문제를 다음과 같이 새롭게 정의합니다. dᵢⱼ⁽ᵏ⁾ : 정점 i 에서 j 로 가는 경로 중에서, 중간에 거쳐갈 수 있는 정점이 {1, 2, ..., k} 집합에 속하는 정점들로만 제한되었을 때의 최단 경로의 거리. 이 정의는 마치 우리가 세상을 알아가는 과정과 같습니다. dᵢⱼ⁽⁰⁾ : 중간에 어떤 정점도 거쳐갈 수 없을 때의 최단 경로. 이는 i 에서 j 로 직접 가는 길 뿐입니다. 즉, dᵢⱼ⁽⁰⁾ = wᵢⱼ . 이것이 우리의 출발점입니다. dᵢⱼ⁽¹⁾ : 이제 '정점 1'을 경유지로 사용할 수 있습니다. 기존의 직접 가는 길과, '정점 1'을 거쳐가는 새로운 길( i → 1 → j ) 중 더 짧은 길을 선택합니다. dᵢⱼ⁽²⁾ : 이제 '정점 1 또는 2'를 경유지로 사용할 수 있습니다. dᵢⱼ⁽¹⁾ 을 기반으로, 새롭게 허용된 '정점 2'를 거쳐가면 혹시 길이 더 짧아지지 않을까 검토합니다. ... dᵢⱼ⁽ⁿ⁾ : 마침내 모든 정점 {1, 2, ..., n} 을 경유지로 허용합니다. 이것이 바로 우리가 최종적으로 원하는 최단 거리 dᵢⱼ 입니다. 이처럼 '허용된 중간 정점'의 범위를 k=0 에서 n 까지 점진적으로 확장하며, 거리 행렬(우리의 지도)을 계속해서 갱신해 나가는 것이 이 알고리즘의 전략입니다. 제4장: 점화식의 발견 - 정점 k 를 둘러싼 선택 이 아이디어를 구체적인 점화식으로 만들어 봅시다. dᵢⱼ⁽ᵏ⁾ 를 계산해야 하는 시점에서, 우리는 이미 그 이전 단계의 완벽한 해답, 즉 dᵢⱼ⁽ᵏ⁻¹⁾ (중간 정점을 k-1 까지만 허용했을 때의 최단 경로)를 알고 있습니다. i 에서 j 로 가는 최단 경로(중간 정점 k 까지 허용)는 다음 두 가지 경우 중 하나입니다. 경우 1: 최단 경로가 정점 k 를 거쳐가지 않는다. 이 경로에 정점 k 가 필요 없다면, 이 경로는 오직 {1, ..., k-1} 집합의 정점들만 중간에 사용합니다. 그렇다면 이 경로의 최단 거리는 k 를 고려하기 이전의 최단 거리와 동일합니다. 즉, dᵢⱼ⁽ᵏ⁻¹⁾ 입니다. 경우 2: 최단 경로가 정점 k 를 거쳐간다. 만약 최단 경로가 k 를 거쳐간다면, 그 경로는 i → ... → k → ... → j 와 같은 형태입니다. 최적 부분 구조 원리에 따라, 이 경로의 i → k 부분과 k → j 부분 역시 각각 최단 경로여야 합니다. 이 부분 경로들은 중간에 k 를 또 거칠 수 없으므로, 오직 {1, ..., k-1} 집합의 정점들만 사용합니다. 따라서 k 를 거쳐가는 경로의 총거리는 dᵢₖ⁽ᵏ⁻¹⁾ + dₖⱼ⁽ᵏ⁻¹⁾ 가 됩니다. 우리는 이 두 가지 경우 중 더 짧은 쪽, 즉 더 작은 값을 선택해야 합니다. 이로써 플로이드-워셜 알고리즘의 핵심 점화식이 완성됩니다. dᵢⱼ⁽ᵏ⁾ = min( dᵢⱼ⁽ᵏ⁻¹⁾, dᵢₖ⁽ᵏ⁻¹⁾ + dₖⱼ⁽ᵏ⁻¹⁾ ) 이 점화식은 우리에게 k-1 단계의 해답 행렬만 있으면 k 단계의 해답 행렬을 만들 수 있음을 알려줍니다. 제5장: 플로이드-워셜 알고리즘과 분석 알고리즘 구현: 단순함의 미학 이 점화식은 놀랍도록 간결한 코드로 구현됩니다. k-1 단계와 k 단계의 행렬을 따로 저장할 필요 없이, 하나의 행렬을 계속 갱신하는 방식으로 최적화할 수 있습니다. // D는 W로 초기화되었다고 가정 for (k = 1; k <= n; k++) { for (i = 1; i <= n; i++) { for (j = 1; j <= n; j++) { // D[i][j]는 'dᵢⱼ⁽ᵏ⁻¹⁾'의 역할을, 갱신 후에는 'dᵢⱼ⁽ᵏ⁾'가 됨 D[i][j] = min(D[i][j], D[i][k] + D[k][j]); } } } 알고리즘 분석 시간 복잡도: k , i , j 세 개의 중첩 반복문이 각각 n 번씩 실행되므로, 총 시간 복잡도는 매우 직관적으로 Θ(n³) 입니다. 공간 복잡도: n x n 크기의 거리 행렬 하나만 저장하면 되므로, 공간 복잡도는 Θ(n²) 입니다. 제6장: 알고리즘 실행 과정 추적 알고리즘이 어떻게 한 단계씩 최단 경로를 '발견'해 나가는지 예제로 따라가 보겠습니다. 초기 인접 행렬 W = D⁽⁰⁾ : D⁽⁰⁾ 1 2 3 4 1 0 1 ∞ 2 2 6 0 4 ∞ 3 ∞ ∞ 0 3 4 ∞ 8 ∞ 0 1단계: k = 1 (정점 1을 경유지로 허용) dᵢⱼ⁽¹⁾ = min( dᵢⱼ⁽⁰⁾, dᵢ₁⁽⁰⁾ + d₁ⱼ⁽⁰⁾ ) d₂₄⁽¹⁾ : min(∞, d₂₁⁽⁰⁾ + d₁₄⁽⁰⁾) = min(∞, 6 + 2) = 8 . ( 2 → 1 → 4 경로 발견) D⁽¹⁾ 1 2 3 4 1 0 1 ∞ 2 2 6 0 4 8 3 ∞ ∞ 0 3 4 ∞ 8 ∞ 0 2단계: k = 2 (정점 1, 2를 경유지로 허용) dᵢⱼ⁽²⁾ = min( dᵢⱼ⁽¹⁾, dᵢ₂⁽¹⁾ + d₂ⱼ⁽¹⁾ ) d₁₃⁽²⁾ : min(∞, d₁₂⁽¹⁾ + d₂₃⁽¹⁾) = min(∞, 1 + 4) = 5 . ( 1 → 2 → 3 경로 발견) d₄₁⁽²⁾ : min(∞, d₄₂⁽¹⁾ + d₂₁⁽¹⁾) = min(∞, 8 + 6) = 14 . ( 4 → 2 → 1 경로 발견) D⁽²⁾ 1 2 3 4 1 0 1 5 2 2 6 0 4 8 3 ∞ ∞ 0 3 4 14 8 ∞ 0 3단계: k = 3 (정점 1, 2, 3을 경유지로 허용) dᵢⱼ⁽³⁾ = min( dᵢⱼ⁽²⁾, dᵢ₃⁽²⁾ + d₃ⱼ⁽²⁾ ) d₂₄⁽³⁾ : min(8, d₂₃⁽²⁾ + d₃₄⁽²⁾) = min(8, 4 + 3) = 7 . (기존 2→1→4 보다 2→3→4 가 더 빠름) d₄₂⁽³⁾ : min(8, d₄₃⁽²⁾ + d₃₂⁽²⁾) = min(8, ∞ + ∞) = 8 . (갱신 없음) D⁽³⁾ 1 2 3 4 1 0 1 5 2 2 6 0 4 7 3 ∞ ∞ 0 3 4 14 8 ∞ 0 (주: 원본 자료의 예시 테이블과 달리, 정확한 계산에 따라 값을 갱신하였습니다.)* 4단계: k = 4 (모든 정점을 경유지로 허용) dᵢⱼ⁽⁴⁾ = min( dᵢⱼ⁽³⁾, dᵢ₄⁽³⁾ + d₄ⱼ⁽³⁾ ) d₁₃⁽⁴⁾ : min(5, d₁₄⁽³⁾ + d₄₃⁽³⁾) = min(5, 2 + ∞) = 5 . d₃₂⁽⁴⁾ : min(∞, d₃₄⁽³⁾ + d₄₂⁽³⁾) = min(∞, 3 + 8) = 11 . ( 3 → 4 → 2 경로 발견) d₃₁⁽⁴⁾ : min(∞, d₃₄⁽³⁾ + d₄₁⁽³⁾) = min(∞, 3 + 14) = 17 . ( 3 → 4 → 2 → 1 경로 발견) D⁽⁴⁾ - 최종 결과 1 2 3 4 1 0 1 5 2 2 6 0 4 7 3 17 11 0 3 4 14 8 ∞ 0 이것이 최종 거리 행렬 D 입니다. 이제 이 도시의 어떤 두 지점 사이의 최단 거리를 즉시 알 수 있습니다. 결론: 단순함 속에 담긴 깊이 모든 쌍 최단 경로 문제는 동적 계획법의 설계 패러다임을 보여주는 위대한 예시입니다. 이 문제는 우리에게 중요한 교훈을 줍니다. 때로는 문제의 차원을 다르게 바라보는 것, 즉 경로의 길이나 정점의 개수가 아닌 '허용된 중간 정점' 이라는 새로운 기준을 도입하는 창의적인 발상이 복잡성의 장벽을 무너뜨리는 열쇠가 될 수 있다는 것입니다. 플로이드-워셜 알고리즘의 우아함은 바로 이 지점에서 빛을 발하며, 복잡한 네트워크 문제에 대한 강력하고 체계적인 해법을 제시합니다. 0-1 배낭 채우기 문제(0-1 Knapsack Problem) 한 탐험가의 고민: 무엇을 챙겨야 할까? 전설 속 보물이 잠들어 있다는 미지의 유적을 탐사하러 떠나는 한 탐험가를 상상해 봅시다. 이 탐험가에게는 평생의 꿈을 이룰 단 한 번의 기회입니다. 오랜 탐사 끝에 마침내 보물 창고를 발견했지만, 기쁨도 잠시, 큰 난관에 부딪힙니다. 보물은 산더미처럼 쌓여 있지만, 탐험가가 가진 배낭의 용량은 한정되어 있습니다. 각 보물은 저마다 다른 무게를 가지고 있고, 그 가치 또한 천차만별입니다. 어떤 것은 작고 가볍지만 엄청난 가치를 지녔고, 어떤 것은 크고 무겁지만 상대적으로 가치가 낮습니다. 유적은 곧 무너질 위기에 처해 있고, 보물을 챙겨 나갈 기회는 단 한 번뿐입니다. 탐험가는 이제 인생일대의 선택을 해야 합니다. \"이 한정된 용량의 배낭에 어떤 보물들을 담아야 그 가치의 총합을 최대로 만들 수 있을까?\" 이것이 바로 배낭 채우기 문제입니다. 각 보물은 통째로 가져가거나(1), 아예 가져가지 않거나(0) 둘 중 하나만 선택할 수 있습니다. 보물을 쪼개서 일부만 가져갈 수는 없습니다. 그래서 이 문제를 특별히 '0-1 배낭 채우기 문제'라고 부릅니다. 문제의 공식화와 첫 번째 시도: 모든 가능성을 시험하다 (지 기법) 탐험가의 고민을 알고리즘 문제로 바꾸어 봅시다. n : 보물의 총개수 w[i] : i 번째 보물의 무게 v[i] : i 번째 보물의 가치 C : 배낭이 견딜 수 있는 최대 무게 (용량) 우리의 목표는, 선택한 보물들의 총무게가 C 를 넘지 않으면서, 총가치가 최대가 되는 보물들의 조합을 찾는 것입니다. 가장 단순하고 무식한 방법은 무엇일까요? 바로 가능한 모든 경우의 수를 다 시험해 보는 것입니다. 보물이 n 개 있다면, 각 보물에 대해 '가져간다'와 '가져가지 않는다' 두 가지 선택지가 있습니다. 따라서 총 경우의 수는 2 × 2 × ... × 2 (n번), 즉 2ⁿ 가지가 됩니다. 이 억지 기법(Brute-force) 알고리즘은 다음과 같이 동작합니다. n 개 보물들의 모든 부분 집합(조합)을 하나씩 만들어 봅니다. (총 2ⁿ 개) 각 조합에 대해, 포함된 보물들의 총무게를 계산합니다. 총무게가 배낭 용량 C 를 초과하면, 그 조합은 버립니다. 용량을 초과하지 않는 조합이라면, 포함된 보물들의 총가치를 계산합니다. 모든 유효한 조합들의 총가치 중 가장 큰 값을 찾아냅니다. 이 방법은 보물의 개수( n )가 10개만 되어도 2¹⁰ = 1024 개의 조합을, 20개면 2²⁰ 으로 백만 개가 넘는 조합을, 30개면 10억 개가 넘는 조합을 일일이 확인해야 합니다. 시간 복잡도가 Ω(2ⁿ) 으로, n 이 조금만 커져도 현실적으로 계산이 불가능한 '나쁜' 알고리즘입니다. 우리에겐 더 똑똑한 방법이 필요합니다. 더 나은 길을 찾아서: 동적 계획법의 통찰 이 문제, 어딘가 익숙하지 않나요? 하나의 큰 결정(n개의 보물 선택)이 여러 개의 작은 결정들의 연속으로 이루어집니다. 그리고 그 결정들은 서로 영향을 줍니다. 이는 동적 계획법이 활약할 수 있는 좋은 무대입니다. 동적 계획법을 적용하기 위한 두 가지 핵심 속성을 다시 확인해 봅시다. 최적 부분 구조 (Optimal Substructure): n 개의 보물과 용량 C 에 대한 최적의 해(최대 가치)는, 그보다 작은 부분 문제들의 최적의 해를 기반으로 구성될 수 있어야 합니다. 중복되는 부분 문제 (Overlapping Subproblems): 전체 문제를 해결하는 과정에서 동일한 부분 문제가 반복적으로 나타나야 합니다. 배낭 문제도 이 속성을 가질까요? 그렇습니다. n 개의 아이템에 대한 최적의 선택은, n-1 개의 아이템에 대한 최적의 선택과 관련이 있습니다. 이 관계를 명확히 정의하는 것이 동적 계획법 설계의 첫걸음입니다. 동적 계획법의 핵심: 부분 문제 정의하기 문제를 체계적으로 풀기 위해, 우리는 결과를 기록할 테이블을 만들고, 그 테이블의 각 칸이 무엇을 의미하는지 명확하게 정의해야 합니다. K[i][j] 라는 2차원 배열을 사용해 봅시다. K[i][j] : 첫 번째 보물부터 i 번째 보물까지만 고려했을 때, 용량이 j 인 배낭에 담을 수 있는 최대 가치 이 정의가 가장 중요합니다. i 는 우리가 고려할 보물의 범위를, j 는 우리가 가진 배낭의 용량을 나타내는 '상태(state)'입니다. 우리의 최종 목표는 테이블의 가장 마지막 칸, 즉 K[n][C] (모든 n 개의 보물을 고려했고, 배낭의 최대 용량이 C 일 때의 최대 가치)를 알아내는 것입니다. 점화식 세우기: 선택의 기로 자, 이제 테이블의 한 칸인 K[i][j] 를 어떻게 계산할 수 있을지 생각해 봅시다. 우리는 i 번째 보물을 앞에 두고 선택의 기로에 서 있습니다. \"i번째 보물을 배낭에 넣을 것인가, 말 것인가?\" 이 결정에 따라 K[i][j] 의 값이 정해집니다. 경우 1: i 번째 보물을 배낭에 넣지 않는다. 만약 i 번째 보물을 넣지 않기로 결정했다면, 문제는 간단해집니다. i 번째 보물은 그냥 무시하고, 나머지 i-1 개의 보물을 가지고 용량 j 의 배낭을 채우는 문제와 동일해집니다. 이 문제의 최적해는 우리가 이미 정의한 바에 따라 K[i-1][j] 입니다. 경우 2: i 번째 보물을 배낭에 넣는다. 이 선택은 i 번째 보물의 무게 w[i] 가 현재 배낭 용량 j 보다 작거나 같을 때만 가능합니다. 만약 w[i] > j 라면 이 경우는 아예 고려할 수조차 없습니다. 만약 넣는 것이 가능하다면, 우리는 i 번째 보물의 가치 v[i] 를 얻게 됩니다. 그리고 i 번째 보물을 넣었으니 배낭에는 j - w[i] 만큼의 용량이 남게 됩니다. 이 남은 공간은 어떻게 채워야 할까요? 당연히 최적의 방법으로 채워야 합니다. 즉, 남은 i-1 개의 보물을 가지고 용량 j - w[i] 의 배낭을 채워 얻을 수 있는 최대 가치를 더해야 합니다. 이 값은 바로 K[i-1][j - w[i]] 입니다. 따라서 이 경우의 총가치는 v[i] + K[i-1][j - w[i]] 가 됩니다. 우리는 항상 최대의 가치를 원하므로, 이 두 가지 경우 중 더 큰 값을 선택하면 됩니다. 이것이 바로 배낭 문제의 점화식입니다. if w[i] > j (i번째 물건이 배낭 용량 j보다 무거워 담을 수 없는 경우): K[i, j] = K[i-1, j] (못 담으니 이전 상태와 같음) if w[i] <= j (i번째 물건을 담을 수 있는 경우): K[i, j] = max( K[i-1, j], v[i] + K[i-1, j - w[i]] ) (안 담았을 때의 가치 vs 담았을 때의 가치) 이 점화식을 이용해 i=1 부터 n 까지, j=1 부터 C 까지 테이블을 순서대로 채워나가면 마침내 최종 답 K[n][C] 에 도달할 수 있습니다. 동적 계획 알고리즘 실행 과정 추적 이제 슬라이드의 예제를 통해 이 마법 같은 과정이 어떻게 이루어지는지 한 칸 한 칸 따라가 보겠습니다. 문제 상황: 배낭 용량 C = 7 물건 n = 4 개 물건 i 무게 w[i] 가치 v[i] 1 3 25 2 1 15 3 2 20 4 4 30 테이블 준비: K[0.][0.] 크기의 테이블을 만듭니다. 1단계: 초기화 i=0 인 행(0번째 행)은 '아무 물건도 고려하지 않았을 때'를 의미하므로, 어떤 용량의 배낭이든 최대 가치는 0입니다. K[0, j] = 0 . j=0 인 열(0번째 열)은 '배낭 용량이 0일 때'를 의미하므로, 어떤 물건도 담을 수 없어 최대 가치는 0입니다. K[i, 0] = 0 . i\\j 0 1 2 3 4 5 6 7 0 0 0 0 0 0 0 0 0 1 0 2 0 3 0 4 0 2단계: i = 1 행 계산 (물건 1: w=3, v=25 고려) 이제 물건 1 하나만 가지고 배낭을 채워봅니다. j w[1]>j ? 계산: max( K[0,j], v[1]+K[0,j-w[1]] ) K[1,j] 1 3 > 1 (Yes) K[0,1] = 0 0 2 3 > 2 (Yes) K[0,2] = 0 0 3 3 1 (Yes) K[2,1] = 15 15 2 2 1 (Yes) K[3,1] = 15 15 2 4 > 2 (Yes) K[3,2] = 20 20 3 4 > 3 (Yes) K[3,3] = 35 35 4 4 <= 4 (No) max(K[3,4], 30+K[3,0]) = max(40, 30+0) 40 5 4 <= 5 (No) max(K[3,5], 30+K[3,1]) = max(45, 30+15) 45 6 4 <= 6 (No) max(K[3,6], 30+K[3,2]) = max(60, 30+20) 60 7 4 <= 7 (No) max(K[3,7], 30+K[3,3]) = max(60, 30+35) 65 최종 테이블: i\\j 0 1 2 3 4 5 6 7 0 (w) 0 0 0 0 0 0 0 0 1 (3) 0 0 0 25 25 25 25 25 2 (1) 0 15 15 25 40 40 40 40 3 (2) 0 15 20 35 40 45 60 60 4 (4) 0 15 20 35 40 45 60 65 드디어 테이블이 완성되었습니다! 우리가 찾던 최종 답, K[4, 7] 은 65입니다. 탐험가는 최대 65의 가치를 배낭에 담아 유적을 탈출할 수 있습니다. 해답을 넘어: 어떤 물건을 담았는가? (역추적) 최대 가치가 65라는 것은 알았지만, 탐험가에게 정말 필요한 정보는 '그래서 어떤 보물을 챙겨야 하는가?' 입니다. 이 정보 또한 우리가 만든 테이블 안에 숨겨져 있습니다. 테이블을 거꾸로 거슬러 올라가며 우리가 내렸던 선택을 복기해 봅시다. 이 과정을 역추적(Backtracking)이라고 합니다. K[4, 7] = 65 에서 시작합니다. 이 값은 바로 윗칸 K[3, 7] = 60 과 같습니까? 다릅니다. 65 ≠ 60 . 이는 K[4, 7] 을 계산할 때 물건 4를 포함하는 선택이 최적이었다는 의미입니다. 선택: 물건 4 (w=4, v=30)를 배낭에 넣는다! 이제 우리는 물건 4를 넣었으므로, 남은 용량 7 - w[4] = 7 - 4 = 3 에 대해, 남은 물건 1, 2, 3 으로 채우는 문제로 이동해야 합니다. 즉, K[3, 3] 으로 이동합니다. 현재 위치는 K[3, 3] = 35 입니다. 이 값은 바로 윗칸 K[2, 3] = 25 와 같습니까? 다릅니다. 35 ≠ 25 . 이는 K[3, 3] 을 계산할 때 물건 3을 포함하는 선택이 최적이었다는 의미입니다. 선택: 물건 3 (w=2, v=20)를 배낭에 넣는다! 남은 용량 3 - w[3] = 3 - 2 = 1 에 대해, 남은 물건 1, 2 로 채우는 문제로 이동합니다. 즉, K[2, 1] 로 이동합니다. 현재 위치는 K[2, 1] = 15 입니다. 이 값은 바로 윗칸 K[1, 1] = 0 과 같습니까? 다릅니다. 15 ≠ 0 . 이는 K[2, 1] 을 계산할 때 물건 2를 포함하는 선택이 최적이었다는 의미입니다. 선택: 물건 2 (w=1, v=15)를 배낭에 넣는다! 남은 용량 1 - w[2] = 1 - 1 = 0 에 대해, 남은 물건 1 로 채우는 문제로 이동합니다. 즉, K[1, 0] 으로 이동합니다. 현재 위치는 K[1, 0] = 0 입니다. 배낭 용량이 0이 되었으므로 역추적을 종료합니다. 역추적 결과, 탐험가가 챙겨야 할 보물은 물건 2, 3, 4입니다. 확인해 볼까요? 총무게: w[2] + w[3] + w[4] = 1 + 2 + 4 = 7 (배낭 용량 7에 딱 맞습니다!) 총가치: v[2] + v[3] + v[4] = 15 + 20 + 30 = 65 (우리가 구한 최대 가치와 일치합니다!) 알고리즘 분석 및 결론 시간 복잡도: 알고리즘은 i 를 1부터 n 까지, j 를 1부터 C 까지 순회하는 2중 반복문으로 구성됩니다. 각 칸을 채우는 데 걸리는 시간은 상수 시간( O(1) )입니다. 따라서 총 시간 복잡도는 Θ(nC) 입니다. 의사 다항 시간 (Pseudo-polynomial time): 이 시간 복잡도는 한 가지 특이한 점이 있습니다. 문제의 입력 크기는 보통 물건의 개수 n 으로 생각하지만, 복잡도가 배낭의 용량 C 라는 숫자 값에 직접적으로 의존합니다. 만약 C 가 n 에 비해 터무니없이 큰 값이라면(예를 들어, C 가 2ⁿ 이라면) 이 알고리즘은 다시 지수 시간처럼 동작할 수 있습니다. 이처럼 입력으로 주어진 '숫자의 크기'에 따라 다항 시간이 되는 경우를 의사 다항 시간이라고 부릅니다. 하지만 대부분의 현실적인 문제에서 C 는 적절한 크기를 가지므로 매우 효율적인 알고리즘입니다. 공간 복잡도: (n+1) × (C+1) 크기의 테이블을 사용하므로 공간 복잡도는 Θ(nC) 입니다. 배낭 채우기 문제는 동적 계획법의 철학을 완벽하게 보여줍니다. 복잡하고 거대한 문제를 '고려할 물건의 개수'와 '배낭의 용량'이라는 두 가지 차원으로 잘게 나누어 가장 작은 문제부터 체계적으로 해결해 나갑니다. 각 단계의 해를 테이블에 꼼꼼히 기록함으로써, 한 번 푼 문제는 다시 풀지 않고 그 결과를 즉시 활용하여 폭발적인 경우의 수를 효율적으로 제어합니다. 탐험가가 모든 조합을 머릿속으로 떠올리며 혼란에 빠지는 대신, 이처럼 체계적인 표를 그려 나갔다면, 그는 틀림없이 최적의 보물 조합을 찾아 유적을 무사히 빠져나왔을 것입니다. 이것이 바로 복잡한 최적화 문제를 해결하는 동적 계획법의 힘입니다. 제 7장: 탐욕 기법 (Greedy)",
      "frontmatter": {
        "tags": [
          "ai-content",
          "university"
        ],
        "date": "2025-10-19T18:41:43+09:00",
        "lastmod": "2025-10-21T20:40:22+09:00"
      }
    },
    "university mobile programming": {
      "path": "/university-mobile-programming/",
      "filename": "university mobile programming",
      "content": "kotlin 기본 문법 ​kotlin keyword Hard Keywords (문법 구조에 필수적이며, 식별자로 절대 사용할 수 없음)* 키워드 설명 as (cast) 타입 캐스팅 ( val x = obj as String ) as (import) 임포트 시 별칭 지정 ( import foo.Bar as Baz ) as? 안전한 타입 캐스팅 (실패 시 null 반환) break 루프 또는 라벨 블록 탈출 class 클래스 정의 continue 현재 루프 반복 건너뛰기 do do-while 루프 시작 else if 조건의 대안 블록 false 불리언 상수: 거짓 for 컬렉션/범위 반복 ( for (i in list) ) fun 함수 정의 if 조건 분기 in 범위/컬렉션 멤버 확인 또는 for 루프에서 사용 !in in 의 부정 interface 인터페이스 정의 is 타입 확인 ( if (obj is String) ) !is is 의 부정 null 널 참조 값 object 싱글톤 객체 또는 익명 객체 정의 package 패키지 선언 return 함수 또는 라벨에서 반환 super 상위 클래스 참조 this 현재 객체 참조 throw 예외 발생 true 불리언 상수: 참 try 예외 처리 블록 시작 typealias 타입 별칭 정의 typeof Kotlin/JS 전용; 일반 Kotlin에서는 사용되지 않음 val 읽기 전용 변수 선언 var 변경 가능한 변수 선언 when 다중 조건 분기 ( switch 대체) while 조건이 참일 동안 반복 Soft Keywords (특정 문맥에서만 키워드로 동작하며, 일반 식별자로 사용 가능)* 키워드 설명 by 위임 구현 ( class C : B by b ) catch try-catch 에서 예외 처리 블록 constructor 주/부생성자 명시 ( class A constructor(...) ) delegate 속성 위임 내부 식별자 (컴파일러용) dynamic Kotlin/JS에서 동적 타입 선언 field 커스텀 getter/setter 내 백킹 필드 참조 file 파일 레벨 식별자 (메타프로그래밍/컴파일러용) finally try-finally 에서 항상 실행되는 블록 get 속성 getter 정의 ( val x: Int get() = ... ) import 패키지/클래스 임포트 init 초기화 블록 ( init { ... } ) param 어노테이션 파라미터 식별자 property 속성 관련 메타정보 식별자 receiver 확장 함수 수신 객체 식별자 set 속성 setter 정의 ( var x: Int set(value) { ... } ) setparam setter 내 파라미터 식별자 ( value ) where 제네릭 제약 조건 ( <T> where T : Comparable<T> ) Modifier Keywords (선언에 대한 속성 또는 동작을 지정)* 키워드 설명 actual 멀티플랫폼 프로젝트에서 실제 구현 지정 abstract 추상 클래스 또는 멤버 (구현 없음) annotation 어노테이션 클래스 정의 companion 컴패니언 객체 선언 ( companion object { } ) const 컴파일 타임 상수 ( const val MAX = 100 ) crossinline 람다 내 non-local return 금지 data 데이터 클래스 ( equals , hashCode , toString 자동 생성) enum 열거 클래스 정의 expect 멀티플랫폼에서 예상 선언 external 외부(Native/JS) 구현 선언 final 오버라이드 불가 (기본값, 명시적 사용 드묾) infix 중위 함수 ( a plus b 대신 a plus b ) inline 함수/클래스를 호출 위치에 인라인 확장 inner 내부 클래스 (외부 클래스 인스턴스 참조 가능) internal 같은 모듈 내에서만 접근 가능 lateinit 나중에 초기화되는 non-null var noinline 람다를 인라인하지 않음 open 오버라이드 가능 (기본은 final ) operator 연산자 오버로딩 함수 지정 out 공변성 제네릭 ( Producer<out T> ) override 상위 클래스/인터페이스 멤버 재정의 private 선언된 스코프 내에서만 접근 가능 protected 하위 클래스까지 접근 가능 public 모든 곳에서 접근 가능 (기본 접근 수준) reified 인라인 함수에서 제네릭 타입 실체화 sealed 제한된 하위 클래스 집합 (밀봉 클래스) suspend 코루틴에서 사용 가능한 함수 tailrec 꼬리 재귀 최적화 함수 vararg 가변 인자 ( fun f(vararg args: Int) ) Special Identifiers (특정 문맥에서만 특별한 의미를 가지는 식별자)* 식별자 설명 field 커스텀 getter/setter 내에서 백킹 필드 참조 ( get() = field ) it 람다 식에서 단일 파라미터의 암시적 이름 ( list.map { it * 2 } ) ✅ 참고: field 는 속성 접근자 내에서만 특별한 의미를 가지며, 그 외에서는 일반 식별자로 사용 가능합니다. it 은 람다 파라미터가 하나일 때 자동으로 사용되는 이름입니다. basic data type var a: Boolean = true // 논리 (true, false) var b: Byte = 123 // 8 트 정수 (-128 ~ 127) var c: Short = 123 // 16비트 정수 (-32768 ~ 32767) var d: Int = 123 // 32비트 정수 (-2 31 ~ 2 31 -1) var e: Long = 123L // 64비트 정수 (-2 63 ~ 2 63 -1) var f: Float = 12.3F // 32비트 부동 소수점 var g: Double = 12.3 // 64비트 부동 소수점 var h: Char = 'A' // 문자 (글자 하나) var i: String= \"ABC\" // 문자열 안드로이드 컴포넌트란 android component 안드로이드 앱은 네 가지 핵심 구성 요소, 즉 안드로이드 컴포넌트로 이루어져 있습니다. 이들은 각각 다른 역할을 수행하며, 앱이 정상적으로 동작하도록 지원합니다. 네 가지 컴포넌트는 다음과 같습니다: 액티비티 (Activity) 액티비티는 앱의 화면을 구성하는 컴포넌트입니다. 사용자가 보는 하나의 화면(예: 로그인 화면, 메인 화면 등)은 하나의 액티비티로 표현됩니다. 앱을 실행하면, 기본 액티비티가 시작되어 그 내용이 안드로이드 기기의 화면에 표시됩니다. 따라서 화면을 출력하려면 반드시 액티비티를 만들어야 합니다. 관련 폴더 및 파일: res/layout/ → 액티비티의 UI를 정의하는 XML 레이아웃 파일이 위치합니다. 예: activity_main.xml , login_screen.xml → setContentView(R.layout.activity_main) 에서 참조됨. src/ (Kotlin/Java 소스 코드) → 각 액티비티는 AppCompatActivity 를 상속하는 클래스로 구현됩니다. 예: MainActivity.kt AndroidManifest.xml → 모든 액티비티는 반드시 여기에 <activity> 태그로 등록되어야 합니다. → LAUNCHER 액티비티는 <intent-filter> 로 지정됩니다. res/values/strings.xml , colors.xml , dimens.xml 등 → 액티비티 UI에서 사용하는 텍스트, 색상, 크기 등의 값 정의. 액티비티는 res/layout/ 과 가장 밀접하며, AndroidManifest.xml 에 등록되고, src/ 내 클래스로 구현됩니다. 서비스 (Service) 서비스는 백그라운드에서 장시간 작업을 수행하는 컴포넌트입니다. 화면을 표시하지 않으며, 사용자와 직접 상호작용하지 않습니다. 예를 들어, 음악 재생, 파일 다운로드, 위치 추적과 같은 작업은 앱이 백그라운드에 있어도 계속 실행되어야 하므로 서비스를 사용합니다. 관련 폴더 및 파일: src/ (Kotlin/Java 소스 코드) → Service 또는 IntentService , JobIntentService 등을 상속하는 클래스로 구현. 예: MusicPlaybackService.kt AndroidManifest.xml → <service android:name=\".MusicPlaybackService\" /> 로 등록 필수. → 권한이 필요한 경우 <uses-permission> 과 함께 사용. res/ 폴더와의 관계: → 서비스는 UI가 없으므로 layout 이나 drawable 등과 직접적 연관 없음. → 단, 알림(Notification)을 표시할 경우 res/drawable/ 의 아이콘을 사용할 수 있음. 서비스는 주로 src/ 와 AndroidManifest.xml 에 의존, 리소스 폴더와는 간접적 연관만 있음. 콘텐츠 프로바이더 (Content Provider) 콘텐츠 프로바이더는 앱 간에 데이터를 공유하는 컴포넌트입니다. 안드로이드 기기에는 여러 앱이 설치되어 있고, 이들 사이에서 데이터를 안전하게 주고받을 필요가 있습니다. 예를 들어, 카카오톡에서 프로필 사진을 변경할 때 갤러리 앱의 사진을 선택할 수 있습니다. 이때 갤러리 앱은 자신의 이미지 데이터를 콘텐츠 프로바이더를 통해 공유하고, 카카오톡은 이를 통해 데이터에 접근합니다. 관련 폴더 및 파일: src/ → ContentProvider 를 상속하는 클래스로 구현. 예: ContactProvider.kt AndroidManifest.xml → <provider> 태그로 등록 필수. → android:authorities 속성으로 고유 URI 지정 (예: com.example.app.provider ). 데이터베이스 관련: → 일반적으로 Room , SQLiteOpenHelper 와 함께 사용되며, 이는 src/ 내부에 구현됨. res/ 폴더와의 관계: → 콘텐츠 프로바이더 자체는 리소스 폴더와 거의 무관. → 단, 공유되는 데이터가 이미지일 경우 res/drawable/ 또는 외부 저장소 경로를 반환할 수 있음. 콘텐츠 프로바이더는 src/ 와 AndroidManifest.xml 중심, 리소스 폴더와는 거의 독립적. 브로드캐스트 리시버 (Broadcast Receiver) 브로드캐스트 리시버는 시스템에서 발생하는 특정 이벤트에 반응하는 컴포넌트입니다. 여기서 말하는 이벤트는 사용자의 터치나 클릭과 같은 화면 상의 동작이 아니라, 시스템 수준의 이벤트를 의미합니다. 예를 들어, 기기 부팅 완료, 배터리 잔량 부족, 네트워크 연결 상태 변경 등이 있습니다. 이러한 이벤트가 발생하면, 등록된 브로드캐스트 리시버가 자동으로 실행되어 적절한 처리를 수행할 수 있습니다. 관련 폴더 및 파일: src/ → BroadcastReceiver 를 상속하는 클래스로 구현. 예: BootReceiver.kt AndroidManifest.xml → 정적 등록 시 <receiver> 태그 사용. 예: 기기 부팅 시 실행되려면 <intent-filter> 에 android.intent.action.BOOT_COMPLETED 지정. 동적 등록: → 액티비티나 서비스 내에서 registerReceiver() 로 코드 상에서 등록 가능 → 이 경우 Manifest 등록 불필요. res/ 폴더와의 관계: → 리시버는 UI 없이 이벤트만 처리하므로 리소스 폴더와 거의 무관. → 단, 이벤트 처리 결과로 알림을 띄울 경우 res/drawable/ 아이콘 사용 가능. 브로드캐스트 리시버는 src/ 와 AndroidManifest.xml 에 의존, 리소스 폴더와는 거의 관련 없음. 정리 컴포넌트 태그 주요 역할 액티비티 <activity> 화면 표시 및 사용자 인터페이스 제공 서비스 <service> 백그라운드에서 장시간 작업 실행 콘텐츠 프로바이더 <provider> 앱 간 데이터 공유 브로드캐스트 리시버 <receiver> 시스템 이벤트 수신 및 처리 resource folder 리소스 폴더 와 R.java drawable： 이미지 리소스 layout： UI 구성에필요한 XML 리소스 mipmap： 앱 아이콘 이미지 values： 문자열 등의 값으로 이용되는 리소스 R.java 는 모든 리소스를 컴포넌트 코드에서 참조할 수 있게 해주는 브릿지 역할 R.java 의 경우 AGP 3.6 부터 제거 되었고 현재 R.txt 와 R.jar 형태로만 존재한다 $ find app/build -name \"R.*\" app/build/intermediates/compile_and_runtime_not_namespaced_r_class_jar/debug/processDebugResources/R.jar app/build/intermediates/runtime_symbol_list/debug/processDebugResources/R.txt $ jar -tf app/build/intermediates/compile_and_runtime_not_namespaced_r_class_jar/debug/processDebugResources/R.jar ## 특정 리소스 파일 검색 $ grep -i \"person1\\|send\\|test\" app/build/intermediates/runtime_symbol_list/debug/processDebugResources/R.txt int drawable abc_vector_test 0x7f070077 int drawable person1 0x7f0700e4 int drawable send 0x7f0700e5 int drawable test_level_drawable 0x7f0700e6 int id immediateStop 0x7f0800dc int layout ime_base_split_test_activity 0x7f0b002d int layout ime_secondary_split_test_activity 0x7f0b002e GUI 기본 개념 관심사의 분리 (Separation of Concerns) 모든 GUI 프레임워크는 다음 세 가지 책임을 명확히 분리하려는 목표를 공유합니다: 역할 책임 핵심 질문 구조 (Structure) 화면에 무엇이 있는가? “어떤 컴포넌트들이 있고, 어떻게 배치되어 있는가?” 스타일 (Presentation) 화면이 어떻게 보이는가? “색상, 크기, 여백, 폰트, 정렬은 어떻게 되는가?” 동작 (Behavior) 화면이 어떻게 반응하는가? “버튼 클릭 시 무엇을 하며, 데이터는 어떻게 바뀌는가?” 각 역할의 구현 방식: 플랫폼별 비교 역할 Web (HTML/CSS/JS) Android (Jetpack Compose 제외) iOS (SwiftUI) Flutter 데스크톱 (예: Qt) 구조(무엇을 보여줄까?) HTML→ 태그 기반 계층 구조 XML 레이아웃 파일→ <LinearLayout> , <TextView> 등 Swift 코드→ 선언적 뷰 DSL ( VStack , Text , Button ) Dart 위젯 트리→ Column , Text , ElevatedButton QML (Qt Modeling Language)또는 C++ 위젯 트리 스타일(어떻게 보일까?) CSS→ 완전 분리된 스타일 시트→ 클래스, 선택자, 반응형 디자인 XML 속성 + 스타일 리소스→ android:padding , style=\"@style/MyButton\" → dimens.xml , colors.xml 메서드 체인→ .frame(width: 100) , .foregroundColor(.blue) → ViewModifier 로 재사용 위젯 내장 속성→ TextStyle , BoxDecoration , SizedBox → ThemeData 로 전역 스타일 통일 QSS (Qt Style Sheets)또는 속성 바인딩 동작(어떻게 반응할까?) JavaScript (또는 TypeScript)→ DOM 이벤트, 상태 관리 (React/Vue 등) Kotlin / Java→ setOnClickListener , ViewModel, LiveData Swift→ @State , @Binding , Button(action: { ... }) Dart→ 상태 관리 ( setState , Provider , Riverpod ) C++ / JavaScript→ 슬롯/시그널, 이벤트 핸들러 💡 공통점: 모두 트리 구조(Tree)로 UI를 표현 (DOM, View Hierarchy, Widget Tree 등) 선언적(Declarative) 방식이 주류 (특히 최신 프레임워크) 상태 → UI의 단방향 흐름을 지향 추상화된 개념 이름 (Cross-Platform Terminology) 개념 설명 대응 용어 예시 UI Tree / Element Tree 컴포넌트의 계층 구조 DOM Tree, View Hierarchy, Widget Tree Layout System 크기와 위치를 계산하는 방식 CSS Box Model, Android LayoutParams, SwiftUI Auto Layout, Flutter RenderObject Styling System 시각적 속성 관리 체계 CSS, Android Themes, SwiftUI Modifiers, Flutter ThemeData Reactivity / State Management 상태 변화 → UI 자동 갱신 React Hooks, Android ViewModel, SwiftUI @State , Flutter setState 진화 방향: 선언적 UI와 통합 과거에는 명령형(Imperative) 방식이 일반적이었으나 (예: button.setWidth(100) ), 최근에는 선언적(Declarative) 방식이 표준이 되었습니다: “어떻게 그리는가”가 아니라, “어떤 상태일 때 어떤 UI를 원하는가”를 기술 React → Virtual DOM + 선언적 JSX Jetpack Compose / SwiftUI / Flutter → 순수 함수형 UI, 상태 기반 재구성 Web Components / CSS Container Queries → 웹도 선언적/컴포넌트 기반으로 진화 중 요약: GUI 개발의 거시적 틀 ┌───────────────────────┐ │ 사용자 입력 │ │ (터치, 클릭, 키보드) │ └──────────┬────────────┘ ▼ ┌───────────────────────┐ │ 동작 **(Behavior) │ ← 프로그래밍 언어 │ (상태 변경, 로직 처리) │ └──────────┬────────────┘ ▼ ┌───────────────────────┐ │ 구조 **(Structure) │ ← 마크업 또는 선언적 코드 │ (어떤 컴포넌트가 있는가)│ └──────────┬────────────┘ ▼ ┌───────────────────────┐ │ 스타일 **(Style) │ ← 시각적 속성 │ (크기, 색상, 여백 등) │ └──────────┬────────────┘ ▼ ┌───────────────────────┐ │ 렌더링 엔진 │ │ (브라우저, OS, 프레임워크)│ └───────────────────────┘ General Sizing Strategies 종류 설명 대표 예시 절대 크기 (Absolute / Fixed Size) 고정된 수치로 크기를 지정 width: 200px , height: 100dp 상대 크기 (Relative Size) 부모, 형제, 화면 등 다른 요소에 비례해 크기 결정 width: 50% , flex: 1 , weight 콘텐츠 기반 크기 (Content-based / Intrinsic Size) 내부 콘텐츠에 따라 자동 조정 wrap_content , width: fit-content , intrinsicContentSize 부모 기반 크기 (Parent-filling Size) 부모 컨테이너를 꽉 채움 match_parent , width: 100% , flex: 1 (특정 상황) 결론 모든 현대 GUI 시스템은 “구조–스타일–동작”이라는 3층 구조를 따르며, 플랫폼에 따라 표현 방식은 다르지만 동일한 설계 철학을 공유합니다. 이 틀을 이해하면 어떤 UI 프레임워크든 빠르게 학습하고 전이할 수 있습니다. 이 구조는 단순한 기술 분류를 넘어, 효율적인 소프트웨어 설계 원칙 그 자체입니다. 현대 GUI(Graphical User Interface) 개발은 선언적(declarative) 접근 방식을 많이 사용하며, 이는 일반적으로 다음과 같은 3가지 핵심 역할 분담으로 구성됩니다: 레이아웃 구성 방법 레이아웃 xml 을 통해 선언적으로 구성 엑티비티 코드로 작성하는 방법 코드로 구성하는 방법 class MainActivity : AppCompatActivity() { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) // 이름 문자열을 출력할 TextView 생성 val name = TextView(this).apply { typeface = Typeface.DEFAULT_BOLD text = \"Lake Louise\" } // 이미지를 출력할 ImageView 생성 val image = ImageView(this).apply { setImageDrawable(ContextCompat.getDrawable(this, R.drawable.lake_l)) } // 주소 문자열을 출력할 TextView 생성 val address = TextView(this).apply { typeface = Typeface.DEFAULT_BOLD text = \"Lake Louise, AB, 캐나다\" } // 세로 방향 LinearLayout 생성 및 설정 val layout = LinearLayout(this).apply { orientation = LinearLayout.VERTICAL gravity = Gravity.CENTER // 뷰들을 LinearLayout에 추가 addView(name, LinearLayout.LayoutParams.WRAP_CONTENT, LinearLayout.LayoutParams.WRAP_CONTENT) addView(image, LinearLayout.LayoutParams.WRAP_CONTENT, LinearLayout.LayoutParams.WRAP_CONTENT) addView(address, LinearLayout.LayoutParams.WRAP_CONTENT, LinearLayout.LayoutParams.WRAP_CONTENT) } // 최종 레이아웃을 화면에 설정 setContentView(layout) } } Empty Views Activity 실행 흐름 AndroidManifest.xml <intent-filter> 내부에 <action android:name=\"android.intent.action.MAIN\" /> <category android:name=\"android.intent.category.LAUNCHER\" /> 가 있는 엑티비티를 열어야 한다 android:name=\".MainActivity\" 이므로 MainAvtivity 를 컴포넌트(액티비티)를 실행 --- MainAvtivity.ks setContentView(R.layout.activity_main) 에 따라 activity_main.xml 를 로드 --- 레이아웃이 하는 일 **화면 전체를 차지하는 `ConstraintLayout`** 을 만들고, 그 안에 **\"Hello World!\" 텍스트를 정중앙에 표시**하며, **코드에서 이 레이아웃을 `R.id.main`으로 참조**할 수 있도록 ID를 부여합니다. 안드로이드 리소스 식별자 종류 및 사용 사례 종합표 리소스 유형(Resource Type) 정의 위치(Resource Directory / File) 주요 목적 XML에서 사용 예 코드에서 참조 방식 실제 사용 사례 id 레이아웃 XML 내부 ( android:id ) 뷰를 코드에서 식별 android:id=\"@+id/submit_button\" R.id.submit_button findViewById(R.id.submit_button) 로 버튼 제어 string res/values/strings.xml 텍스트 내용 관리, 다국어 지원 android:text=\"@string/app_name\" R.string.app_name getString(R.string.welcome) 로 메시지 표시 color res/values/colors.xml 색상 값 정의 및 재사용 android:textColor=\"@color/primary\" R.color.primary ContextCompat.getColor(context, R.color.error) dimen res/values/dimens.xml 여백, 크기, 글자 크기 정의 android:padding=\"@dimen/activity_margin\" R.dimen.activity_margin resources.getDimension(R.dimen.text_size) style res/values/styles.xml 뷰 또는 앱 전체의 디자인 규칙 정의 style=\"@style/CustomButton\" R.style.CustomButton 액티비티 테마: android:theme=\"@style/Theme.MyApp\" layout res/layout/ 폴더 내 XML 파일 화면 UI 구조 정의 — (파일 자체가 리소스) R.layout.activity_main setContentView(R.layout.activity_main) drawable res/drawable/ 폴더 이미지, 아이콘, 벡터 그래픽 android:src=\"@drawable/ic_home\" R.drawable.ic_home ImageView.setImageResource(R.drawable.logo) mipmap res/mipmap-*/ 폴더 앱 아이콘 전용 이미지 android:icon=\"@mipmap/ic_launcher\" R.mipmap.ic_launcher AndroidManifest.xml 에서 앱 아이콘 지정 array res/values/arrays.xml 문자열/정수 배열 정의 — R.array.countries resources.getStringArray(R.array.countries) bool res/values/bools.xml 논리값 상수 정의 android:enabled=\"@bool/is_pro\" R.bool.is_pro resources.getBoolean(R.bool.is_tablet) integer res/values/integers.xml 정수 상수 정의 android:max=\"@integer/max_count\" R.integer.max_count resources.getInteger(R.integer.retry_limit) anim res/anim/ 폴더 뷰 애니메이션 정의 — R.anim.fade_in overridePendingTransition(R.anim.slide_in, R.anim.slide_out) animator res/animator/ 폴더 속성 애니메이션 (Property Animation) — R.animator.rotate AnimatorInflater.loadAnimator(context, R.animator.bounce) menu res/menu/ 폴더 액션바, 컨텍스트 메뉴 정의 — R.menu.main_menu menuInflater.inflate(R.menu.main_menu, menu) xml res/xml/ 폴더 설정, 검색, 키보드 등 구조화된 데이터 — R.xml.preferences PreferenceFragmentCompat 에서 설정 로드 font res/font/ 폴더 커스텀 글꼴 정의 android:fontFamily=\"@font/nanum_gothic\" R.font.nanum_gothic ResourcesCompat.getFont(context, R.font.my_font) raw res/raw/ 폴더 원시 데이터 파일 (오디오, JSON 등) — R.raw.sound_effect MediaPlayer.create(context, R.raw.bgm) plurals res/values/strings.xml 복수형 문자열 (수량에 따라 문장 변경) — R.plurals.item_count resources.getQuantityString(R.plurals.item_count, 3, 3) 📌 참고 설명 @+id/ vs @id/ : @+id/name → 새로 생성 (처음 사용 시) @id/name → 기존 ID 참조 (이미 정의된 경우) drawable vs mipmap : drawable : 일반 이미지 (버튼 아이콘, 배경 등) mipmap : 앱 런처 아이콘 전용 → 시스템이 다양한 해상도에서 안정적으로 표시 dimen 단위: dp → 화면 밀도 독립적 길이 (여백, 크기) sp → 사용자 글꼴 크기 설정 반영 (텍스트 크기) plurals 예시 ( strings.xml ): <plurals name=\"item_count\"> <item quantity=\"zero\">아이템 없음</item> <item quantity=\"one\">%d개의 아이템</item> <item quantity=\"other\">%d개의 아이템들</item> </plurals> 뷰 클래스 Android 레이아웃(Layout) 및 UI 뷰(View) 종류 Android SDK에서 제공하는 주요 레이아웃(ViewGroup) 및 UI 뷰(View) 컴포넌트를 아래 표로 정리했습니다. 레이아웃(Layout/ViewGroup): 다른 뷰를 담을 수 있는 컨테이너 역할 UI 뷰(View): 사용자와 직접 상호작용하거나 정보를 표시하는 위젯 분류 클래스 이름 설명 주요 특징 레이아웃 (ViewGroup) LinearLayout 자식 뷰를 수평( horizontal ) 또는 수직( vertical )으로 배열 layout_weight , orientation 사용 RelativeLayout 자식 뷰 간 또는 부모 기준 상대 위치로 배치 layout_alignParent* , layout_toRightOf 등 사용 ConstraintLayout 유연하고 성능 좋은 제약 기반 레이아웃 layout_constraint* 속성, 플랫 계층 구조 가능 FrameLayout 자식 뷰를 스택처럼 겹쳐 배치 (기본: 좌상단) 간단한 오버레이, 프래그먼트 컨테이너로 자주 사용 GridLayout 그리드(행/열) 형태로 자식 뷰 배치 row , column , span 지정 가능 CoordinatorLayout 자식 뷰 간 상호작용(예: 스크롤 동작) 지원 Behavior 기반, Material Design과 통합 ScrollView 세로 스크롤 가능한 단일 자식 컨테이너 자식은 하나만 허용 (보통 LinearLayout 등 포함) HorizontalScrollView 가로 스크롤 가능한 단일 자식 컨테이너 ScrollView 의 수평 버전 RecyclerView 대량 데이터 리스트/그리드 효율적 표시 ViewHolder , LayoutManager , Adapter 사용 ViewPager / ViewPager2 좌우 스와이프로 페이지 전환 Fragment 또는 뷰 페이징, ViewPager2 는 RecyclerView 기반 AppBarLayout Toolbar 등과 함께 스크롤 동작 통합 CoordinatorLayout 과 함께 사용 DrawerLayout 슬라이딩 드로어(네비게이션 메뉴) 제공 좌/우 사이드 메뉴 구현 SwipeRefreshLayout 내부 뷰를 당겨서 새로고침 onRefresh() 콜백 제공 UI 뷰 (View) TextView 텍스트 표시 text , textSize , textColor 등 EditText 사용자 텍스트 입력 inputType , hint , imeOptions 등 Button 클릭 가능한 버튼 text , onClick ImageButton 이미지 기반 버튼 src 로 이미지 지정 ImageView 이미지 표시 src , scaleType , adjustViewBounds CheckBox 체크 가능 토글 checked , onCheckedChanged RadioButton 라디오 버튼 (RadioButtonGroup 내 사용) 단일 선택 RadioGroup RadioButton 그룹 컨테이너 ViewGroup 이지만 UI 위젯 성격 강함 Switch / ToggleButton ON/OFF 토글 스위치 checked , textOn / textOff ProgressBar 진행 상태 표시 indeterminate , progress , max SeekBar 사용자가 값을 드래그로 선택 progress , thumb , onProgressChanged RatingBar 별점 평가 UI numStars , rating , stepSize Spinner 드롭다운 선택 목록 Adapter , onItemSelected AutoCompleteTextView 입력 시 자동 완성 제안 Adapter , threshold WebView 웹 콘텐츠 표시 HTML/JavaScript 렌더링 SurfaceView / TextureView 고성능 그래픽/비디오 렌더링 별도 렌더링 스레드 사용 CardView 그림자와 모서리 둥근 카드 UI cardElevation , cardCornerRadius FloatingActionButton (FAB) Material Design 플로팅 액션 버튼 src , size , onClick Toolbar 액션바 대체 커스텀 타이틀바 메뉴, 타이틀, 네비게이션 아이콘 포함 가능 참고: 모든 뷰는 android.view.View 를 상속하며, 레이아웃은 android.view.ViewGroup (View의 하위 클래스)를 상속합니다. Material Components (예: MaterialButton , TextInputLayout ) 등은 위 기본 위젯을 확장한 라이브러리 기반 뷰입니다. 커스텀 뷰를 직접 정의하여 사용할 수도 있습니다. Android 뷰 속성 분류: 레이아웃 속성 vs UI 뷰 속성 Android에서 뷰(View)는 두 가지 주요 속성 범주로 나뉩니다: 레이아웃 (Layout Attributes): 부모 레이아웃(ViewGroup)이 자식 뷰의 배치를 제어하기 위해 사용하는 속성입니다. → 예: android:layout_width , android:layout_height , app:layout_constraint... 등 → 이 속성들은 자식 뷰에서 선언되지만, 실제로는 부모 레이아웃이 해석합니다. UI 뷰 속성(View Attributes): 뷰 자체의 외형, 동작, 상태 등을 제어하는 속성입니다. → 예: android:text , android:background , android:onClick 등 → 이 속성들은 해당 뷰 클래스 내부에서 직접 처리됩니다. 아래 표는 대표적인 속성들을 분류한 것입니다. 속성 이름 범주 설명 사용 가능한 뷰/레이아웃 추가 android:layout_width 레이아웃 속성 뷰의 가로 크기 지정 ( match_parent , wrap_content , 고정값) 모든 ViewGroup의 자식 뷰 android:layout_height 레이아웃 속성 뷰의 세로 크기 지정 모든 ViewGroup의 자식 뷰 ex) android:layout_margin* ( top , bottom , start , end 등) 레이아웃 속성 뷰 외부 여백 지정 모든 ViewGroup의 자식 뷰 android:layout_gravity 레이아웃 속성 부모 내에서 뷰의 정렬 방식 (LinearLayout, FrameLayout 등에서 사용) LinearLayout, FrameLayout 자식 android:layout_weight 레이아웃 속성 LinearLayout에서 공간 배분 비율 지정 LinearLayout 자식 app:layout_constraint* 레이아웃 속성 ConstraintLayout에서 제약 조건 정의 ConstraintLayout 자식 android:layout_alignParent* 레이아웃 속성 RelativeLayout에서 부모 기준 정렬 RelativeLayout 자식 android:layout_toRightOf , android:layout_below 등 레이아웃 속성 RelativeLayout에서 다른 뷰 기준 배치 RelativeLayout 자식 android:text UI 뷰 속성 TextView 계열 뷰에 표시할 텍스트 TextView, Button, EditText 등 android:background UI 뷰 속성 뷰의 배경 지정 (색상, drawable 등) 모든 View android:padding* ( left , right , top , bottom , start , end ) UI 뷰 속성 뷰 내부 여백 지정 모든 View android:onClick UI 뷰 속성 클릭 이벤트 핸들러 지정 모든 View android:visibility UI 뷰 속성 뷰의 표시 여부 ( visible , invisible : 자리 차지, gone : 자리 사라짐) 모든 View 코드에서 사용 : View.VISIBLE, View.INVISIBLE android:enabled UI 뷰 속성 뷰의 활성화 상태 모든 View android:src UI 뷰 속성 ImageView에 표시할 이미지 리소스 ImageView android:hint UI 뷰 속성 EditText의 힌트 텍스트 EditText android:textColor UI 뷰 속성 텍스트 색상 TextView 계열 android:gravity UI 뷰 속성 뷰 내부 콘텐츠 정렬 방식 TextView, Button 등 콘텐츠 포함 뷰 android:id UI 뷰 속성 뷰의 고유 식별자 지정. 코드에서 참조하거나, 레이아웃 제약 조건에서 참조 가능 모든 View 및 ViewGroup( TextView , Button , LinearLayout , ConstraintLayout 등 전부) 참고: layout_* 접두사가 붙은 속성은 부모 레이아웃에 따라 다르게 해석되며, 해당 부모 레이아웃에서만 유효합니다. 일반 android:* 속성은 뷰 자체에 적용되며, 뷰 클래스가 지원해야 사용 가능합니다. app:* 네임스페이스는 커스텀 속성 또는 ConstraintLayout, Material Design 컴포넌트 등에서 사용됩니다. android:id=\"@+id/text\" , findViewByld() width height 의 경우 3가지 방식 사용가능 수치 : px, dp match_parent : 부모의 크기 전체 wrap_content : 자신의 적절한 크기 프로퍼티(property) == 필드 + getter + setter 내부적으로는 여전히 private 필드가 있고, public getter/setter 메서드가 생성 즉 문법적 설탕 inputType 속성값 속성값 설명 none 입력 유형을 지정하지 않은 상태. 모든 문자 입력 가능하며 줄바꿈 가능 text 문자열 한 줄 입력 textCapCharacters 대문자 입력 모드 (모든 문자가 자동으로 대문자) textCapWords 각 단어의 첫 글자를 자동으로 대문자로 입력 textCapSentences 각 문장의 첫 글자를 자동으로 대문자로 입력 textMultiLine 여러 줄 입력 가능 textNoSuggestions 단어 입력 시 키보드의 추천 단어(자동 완성)를 표시하지 않음 textUri URL 입력 모드 (URL에 적합한 키보드 레이아웃 제공) textEmailAddress 이메일 주소 입력 모드 (이메일 형식에 맞는 키보드 제공) textPassword 비밀번호 입력 모드. 입력한 문자는 점(●)으로 표시되며, 키보드는 영문자, 숫자, 특수 문자만 표시 textVisiblePassword textPassword 와 동일하나, 입력한 문자가 그대로 보임 (가시적 비밀번호) number 숫자만 입력 가능한 모드 numberSigned number 와 동일하나, 부호(마이너스 - ) 입력 가능 numberDecimal number 와 동일하나, 소수점( . ) 입력 가능 numberPassword 숫자 키만 사용 가능하며, 입력 내용은 점(●)으로 표시 (숫자 기반 비밀번호) phone 전화번호 입력 모드 (전화기 스타일 키패드 제공) 뷰 바인딩 : findViewByld() 대신 뷰를 찾는 방법 build.gradle 파일을 열고 android 영역에 viewBinding.isEnabled = true를 설정 레이아웃 XML 파일에 등록된 뷰 객체를 포함하는 클래스가 자동으로 만들어진다 자동으로 만들어 지는 클래스의 이름은 레이아웃 XML 파일명을 따릅니다. 첫 글자를 대문자 로 하고 밑줄 (_) 은 빼고 뒤에 오는 단어를 대문자로 만든 후 ‘Binding’을 추가합니다. 예를 들 어 다음과 같습니다. • activity_main.xml -> ActivityMainBinding • item_main.xml -> ItemMainBinding class MainActivity : AppCompatActivity() { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) // 바인딩 객체 획득 val binding = ActivityMainBinding.inflate(layoutInflater) // 액티비티 화면 출력 setContentView(binding.root) // 뷰 객체 이용 (visibleBtn 클릭 시 targetView 보이기) binding.visibleBtn.setOnClickListener { binding.targetView.visibility = View.VISIBLE } // 뷰 객체 이용 (invisibleBtn 클릭 시 targetView 숨기기) binding.invisibleBtn.setOnClickListener { binding.targetView.visibility = View.INVISIBLE } } } 뷰 바인딩 의도적 비 활성화 tools:viewBindingIgnore=\"true\" 를 레이아웃 안에 넣는다 Linear Layout 직선 배치 android:orientation=\"vertical\": 자식 뷰를 세로 방향으로, 위에서 아래로 쌓습니다. (열, Column) android:orientation=\"horizontal\": 자식 뷰를 가로 방향으로, 왼쪽에서 오른쪽으로 쌓습니다. (행, Row) layout_weight 속성 핵심 원리: layout_weight 는 \"남은 공간\"에 작동한다 먼저, layout_width 또는 layout_height 가 고정값이나 wrap_content 인 자식 뷰들의 크기를 계산합니다. 그 후, 부모 레이아웃의 전체 크기에서 이미 사용된 공간을 뺀 \"남은 공간\"을 계산합니다. 이 남은 공간을 layout_weight 비율에 따라 분배합니다. 🔑 따라서, \"남은 공간이 존재해야 layout_weight 가 효과를 발휘\" 합니다. 세로 방향 LinearLayout ( orientation=\"vertical\" ) 기준 상위 레이아웃의 height 자식 뷰의 layout_height layout_weight 효과 match_parent 0dp ✅ 효과 있음 → 남은 공간이 있음 wrap_content 0dp ❌ 거의 효과 없음 → 남은 공간 ≈ 0 match_parent wrap_content + weight ⚠️ 비추천 → 예측 불가능한 동작 wrap_content wrap_content + weight ❌ 의미 없음 → 공간이 없음 Gravity 속성 핵심 원리: gravity 는 \"내부 정렬\", layout_gravity 는 \"외부 정렬\" 두 속성은 정렬 대상이 완전히 다릅니다. android:gravity : 뷰 자신 내부의 콘텐츠(예: 텍스트, 아이콘)를 정렬합니다. “내 안에 있는 글자를 어디에 둘까?”* android:layout_gravity : 부모 레이아웃 안에서 뷰 자체를 정렬합니다. “부모 공간 안에서 나를 어디에 둘까?”* 🔑 LinearLayout에서는 layout_gravity 가 레이아웃의 주축(orientation)과 수직인 방향으로만 작동합니다. 주축 방향으로는 정렬할 수 없습니다. LinearLayout 방향 기준 LinearLayout 방향 layout_gravity 가 유효한 축 설명 vertical (세로) 수평 축 ( left , center_horizontal , right ) ✅ 자식 뷰를 좌/우/가운데로 정렬 가능 vertical (세로) 수직 축 ( top , bottom 등) ❌ 무시됨 (이미 위에서 아래로 쌓고 있음) horizontal (가로) 수직 축 ( top , center_vertical , bottom ) ✅ 자식 뷰를 상/하/가운데로 정렬 가능 horizontal (가로) 수평 축 ( left , right 등) ❌ 무시됨 (이미 왼쪽에서 오른쪽으로 쌓고 있음) layout_weight 예시 경우 1: 부모가 match_parent → layout_weight 작동 <LinearLayout android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" <!-- 전체 화면 --> android:orientation=\"vertical\"> <Button android:layout_width=\"match_parent\" android:layout_height=\"0dp\" android:layout_weight=\"1\" /> <!-- 남은 공간 전부 차지 --> </LinearLayout> → 화면 전체 중 남은 공간 = 전체 높이 → weight 가 잘 작동. 경우 2: 부모가 wrap_content → layout_weight 무시됨 <LinearLayout android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" <!-- 콘텐츠 크기만 --> android:orientation=\"vertical\"> <Button android:layout_width=\"match_parent\" android:layout_height=\"0dp\" android:layout_weight=\"1\" /> </LinearLayout> → 부모는 \"자식 크기만큼만\" 높이를 갖습니다. → 자식은 0dp 이므로 처음에 높이 = 0 → 남은 공간 = 0 → weight 가 0을 나누는 꼴 → 버튼이 보이지 않음. 예시 3: 혼합된 height 설정 + weight (세로 LinearLayout) <LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:orientation=\"vertical\"> <!-- 고정 높이 헤더 --> <TextView android:layout_width=\"match_parent\" android:layout_height=\"60dp\" android:text=\"헤더\" android:background=\"#FFCDD2\" /> <!-- wrap_content인 콘텐츠 --> <TextView android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:text=\"이건 짧은 설명입니다.\" android:background=\"#BBDEFB\" /> <!-- weight로 남은 공간 차지 --> <Button android:layout_width=\"match_parent\" android:layout_height=\"0dp\" android:layout_weight=\"1\" android:text=\"남은 공간 전체 (weight=1)\" android:background=\"#C8E6C9\" /> <!-- 또 다른 weight 뷰 --> <Button android:layout_width=\"match_parent\" android:layout_height=\"0dp\" android:layout_weight=\"2\" android:text=\"남은 공간의 2/3 (weight=2)\" android:background=\"#FFCC80\" /> </LinearLayout> 부모: height=\"match_parent\" → 전체 화면 먼저 60dp 헤더 + wrap_content 텍스트 뷰가 공간을 차지 남은 공간 = 전체 높이 − (60dp + 텍스트 높이) 이 남은 공간을 weight 비율 1:2로 나눔: 첫 번째 버튼: 1/3 두 번째 버튼: 2/3 ✅ layout_weight 가 잘 작동 — 부모가 match_parent 이기 때문. 예시 4: 부모가 wrap_content 인 중첩 LinearLayout → weight 무시됨 <LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:orientation=\"vertical\" android:padding=\"16dp\"> <!-- ⚠️ 이 LinearLayout은 wrap_content! --> <LinearLayout android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:orientation=\"vertical\" android:background=\"#E0E0E0\"> <Button android:layout_width=\"match_parent\" android:layout_height=\"0dp\" android:layout_weight=\"1\" android:text=\"버튼 A (weight=1)\" /> <Button android:layout_width=\"match_parent\" android:layout_height=\"0dp\" android:layout_weight=\"1\" android:text=\"버튼 B (weight=1)\" /> <TextView android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:text=\"이 텍스트는 항상 보임\" android:background=\"#FFF9C4\" /> </LinearLayout> <!-- 아래는 정상적으로 작동하는 예 --> <Button android:layout_width=\"match_parent\" android:layout_height=\"0dp\" android:layout_weight=\"1\" android:layout_marginTop=\"16dp\" android:text=\"이 버튼은 전체 화면의 남은 공간 차지\" android:background=\"#C8E6C9\" /> </LinearLayout> 중첩된 내부 LinearLayout: height=\"wrap_content\" → 자식 크기만 계산 자식 버튼들은 height=\"0dp\" → 초기 높이 = 0 TextView 만 wrap_content → 실제 높이 있음 남은 공간 = 0 → 두 weight 버튼은 높이 0 → 화면에 보이지 않음! 오직 TextView 만 회색 배경 영역에 표시됨. 외부 LinearLayout의 마지막 버튼: 부모는 match_parent → 남은 공간 있음 layout_weight=\"1\" → 화면 하단에 정상 표시 ✅ 이 예시는 중첩 레이아웃에서 부모 크기 설정이 weight 동작을 완전히 바꾼다는 점을 보여줍니다. 💡 핵심 교훈 상황 layout_weight 동작 부모가 match_parent 또는 고정 높이 ✅ 정상 작동 — 남은 공간 분배 부모가 wrap_content ❌ 거의 항상 무시 — 남은 공간 없음 wrap_content + layout_weight 혼합 ⚠️ 비추천 — 성능 저하 + 예측 불가 중첩 레이아웃 🔍 각 레이어별로 부모 크기 확인 필수 Gravity 속성 예시 gravity 와 layout_gravity 의 핵심 차이 android:gravity : 뷰 내부 콘텐츠의 정렬 방식을 지정합니다. → “내 안에 있는 텍스트나 자식 요소를 어디에 놓을까?” android:layout_gravity : 부모 레이아웃 내에서 해당 뷰 자체의 위치를 지정합니다. → “부모 안에서 나를 어디에 놓을까?” 🔑 두 속성은 완전히 다른 대상에 작용하므로 혼동하지 말 것. LinearLayout 기준 동작 원리 LinearLayout은 자식 뷰를 한 방향(수평 또는 수직)으로만 배치하므로, layout_gravity 는 그 반대 방향에서만 효과를 발휘합니다. LinearLayout 방향 layout_gravity 가 유효한 축 예시 horizontal 수직 축 ( top , center_vertical , bottom ) 버튼을 위/가운데/아래로 정렬 vertical 수평 축 ( left , center_horizontal , right ) 버튼을 왼쪽/가운데/오른쪽으로 정렬 ⚠️ 같은 축(예: vertical 에서 layout_gravity=\"top\" )은 무시됩니다. → LinearLayout은 수직 방향으로 자식을 쌓기 때문에, 수직 위치는 이미 결정됨. 예시 1: 수직 LinearLayout에서 layout_gravity 사용 <LinearLayout android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:orientation=\"vertical\" android:background=\"#E0E0E0\"> <Button android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_gravity=\"center_horizontal\" android:text=\"가운데 정렬\" /> <Button android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_gravity=\"right\" android:text=\"오른쪽 정렬\" /> <TextView android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:gravity=\"center\" android:text=\"이 텍스트는 뷰 안에서 가운데\" android:background=\"#BBDEFB\" /> </LinearLayout> 수직 방향 → layout_gravity 는 수평 정렬만 제어. 첫 번째 버튼: 부모 내에서 가로 중앙. 두 번째 버튼: 부모 내에서 오른쪽. TextView: gravity=\"center\" → 자신의 내부 텍스트를 가로·세로 중앙 정렬 (단, 높이가 wrap_content 라 세로 중앙은 시각적으로 미약). ✅ layout_gravity 는 부모 기준, gravity 는 자기 내부 기준. 예시 2: 수평 LinearLayout에서 layout_gravity 사용 <LinearLayout android:layout_width=\"match_parent\" android:layout_height=\"200dp\" android:orientation=\"horizontal\" android:background=\"#E0E0E0\"> <Button android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_gravity=\"top\" android:text=\"위\" /> <Button android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_gravity=\"center_vertical\" android:text=\"세로 중앙\" /> <Button android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_gravity=\"bottom\" android:text=\"아래\" /> </LinearLayout> 수평 방향 → layout_gravity 는 수직 정렬만 제어. 각 버튼이 부모 높이( 200dp ) 내에서 상단, 중앙, 하단에 위치. 💡 부모 높이가 wrap_content 라면 모든 버튼이 붙어 있어 layout_gravity 효과가 시각적으로 보이지 않음. → 부모에 충분한 여유 공간이 있어야 layout_gravity 가 드러남. 예시 3: gravity vs layout_gravity 혼합 <LinearLayout android:layout_width=\"300dp\" android:layout_height=\"150dp\" android:orientation=\"vertical\" android:background=\"#FFEBEE\"> <TextView android:layout_width=\"match_parent\" android:layout_height=\"0dp\" android:layout_weight=\"1\" android:gravity=\"center\" android:layout_gravity=\"center_horizontal\" android:text=\"gravity=center\\n(layout_gravity는 무시됨)\" android:background=\"#B2DFDB\" /> </LinearLayout> TextView 는 수직 LinearLayout의 자식 → layout_gravity=\"center_horizontal\" 는 유효. 하지만 layout_width=\"match_parent\" 이므로 가로로 이미 꽉 찬 상태 → layout_gravity 효과 없음. gravity=\"center\" → 내부 텍스트가 가로·세로 중앙 정렬. ✅ layout_width=\"match_parent\" 일 때는 layout_gravity 가 무의미할 수 있음. → 정렬을 원하면 wrap_content + layout_gravity 조합 사용. 💡 핵심 교훈 속성 대상 주의 사항 android:gravity 뷰 내부 콘텐츠 (텍스트, 아이콘 등) 뷰 크기가 콘텐츠보다 커야 효과 시각화 android:layout_gravity 부모 내에서의 뷰 위치 LinearLayout 방향의 반대 축에서만 작동 조합 사용 시 두 속성은 독립적 match_parent 와 layout_gravity 는 충돌 가능 부모 크기 wrap_content 면 정렬 효과 미약 여유 공간 확보 필수 ✅ 명심: gravity → “내 속 내용 정렬” layout_gravity → “나를 부모 안에서 정렬” LinearLayout에서는 정렬 가능한 축이 제한됨. 네, 요청하신 형식과 스타일에 맞춰 RelativeLayout 에 대한 설명을 작성해 드리겠습니다. RelativeLayout 상대 레이아웃 핵심 원리: 모든 뷰는 ID를 통해 서로, 또는 부모와의 '관계'를 통해 위치가 결정된다. RelativeLayout은 자식 뷰들을 직선으로 쌓는 LinearLayout과 달리, 다른 뷰(Sibling)나 부모 레이아웃(Parent)을 기준으로 상대적인 위치를 지정하는 매우 유연한 레이아웃입니다. \"A의 아래에 B를 놓아라\", \"C를 부모의 오른쪽에 붙여라\" 와 같은 규칙들의 조합으로 화면을 구성합니다. 🔑 ID 지정이 필수적입니다. 관계를 설정하려면 기준이 될 뷰에 android:id 가 반드시 있어야 합니다. RelativeLayout의 주요 배치 규칙 RelativeLayout의 속성은 크게 3가지 그룹으로 나눌 수 있습니다. 다른 뷰(Sibling)를 기준으로 위치 지정 “버튼 B를 버튼 A의 오른쪽에 두겠다”* 속성 설명 android:layout_above=\"@+id/기준뷰\" 지정한 ID를 가진 뷰의 위쪽에 배치합니다. android:layout_below=\"@+id/기준뷰\" 지정한 ID를 가진 뷰의 아래쪽에 배치합니다. android:layout_toLeftOf=\"@+id/기준뷰\" 지정한 ID를 가진 뷰의 왼쪽에 배치합니다. android:layout_toRightOf=\"@+id/기준뷰\" 지정한 ID를 가진 뷰의 오른쪽에 배치합니다. 다른 뷰(Sibling)를 기준으로 정렬 “버튼 B의 상단 라인을 이미지 A의 상단 라인에 맞추겠다”* 속성 설명 android:layout_alignTop=\"@+id/기준뷰\" 지정한 뷰와 위쪽 가장자리를 맞춥니다. android:layout_alignBottom=\"@+id/기준뷰\" 지정한 뷰와 아래쪽 가장자리를 맞춥니다. android:layout_alignLeft=\"@+id/기준뷰\" 지정한 뷰와 왼쪽 가장자리를 맞춥니다. android:layout_alignRight=\"@+id/기준뷰\" 지정한 뷰와 오른쪽 가장자리를 맞춥니다. android:layout_alignBaseline=\"@+id/기준뷰\" 텍스트 뷰의 경우, 텍스트 기준선(Baseline)을 맞춥니다. 부모(Parent) 레이아웃을 기준으로 정렬 “이 버튼을 화면 정중앙에 놓겠다”* 속성 설명 android:layout_alignParentTop=\"true\" 부모의 위쪽 가장자리에 맞춥니다. android:layout_alignParentBottom=\"true\" 부모의 아래쪽 가장자리에 맞춥니다. android:layout_alignParentLeft=\"true\" 부모의 왼쪽 가장자리에 맞춥니다. android:layout_alignParentRight=\"true\" 부모의 오른쪽 가장자리에 맞춥니다. android:layout_centerHorizontal=\"true\" 부모의 수평 중앙에 배치합니다. android:layout_centerVertical=\"true\" 부모의 수직 중앙에 배치합니다. android:layout_centerInParent=\"true\" 부모의 수평 및 수직 중앙에 배치합니다. RelativeLayout 예시 예시 1: 기본적인 상대 위치 지정 <RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\"> <!-- 기준점이 될 버튼 A --> <Button android:id=\"@+id/buttonA\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"버튼 A (기준)\" /> <!-- 버튼 A의 아래, 오른쪽에 위치 --> <Button android:id=\"@+id/buttonB\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_below=\"@id/buttonA\" android:layout_toRightOf=\"@id/buttonA\" android:text=\"버튼 B\" /> </RelativeLayout> buttonA : 아무런 규칙이 없으므로 부모의 좌측 상단(0,0)에 위치합니다. buttonB : buttonA 의 아래( layout_below )이면서 동시에 오른쪽( layout_toRightOf )에 위치하게 됩니다. 예시 2: 부모 기준 정렬 (화면 우측 하단에 버튼 배치) <RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\"> <TextView android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"화면 콘텐츠\" android:textSize=\"24sp\" /> <!-- 부모의 오른쪽, 아래쪽에 붙이기 --> <Button android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_alignParentRight=\"true\" android:layout_alignParentBottom=\"true\" android:layout_margin=\"16dp\" android:text=\"플로팅 버튼\" /> </RelativeLayout> Floating Action Button과 유사한 UI를 만들 때 유용합니다. layout_alignParentRight=\"true\" 와 layout_alignParentBottom=\"true\" 두 규칙을 통해 부모의 우측 하단 코너에 뷰를 고정시킵니다. 예시 3: 복합적인 실제 레이아웃 (프로필 UI) <RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:padding=\"16dp\"> <!-- 1. 프로필 이미지 (기준점 1) --> <ImageView android:id=\"@+id/profile_image\" android:layout_width=\"60dp\" android:layout_height=\"60dp\" android:layout_alignParentLeft=\"true\" android:layout_centerVertical=\"true\" android:src=\"@mipmap/ic_launcher\" /> <!-- 2. 시간 텍스트 (기준점 2) --> <TextView android:id=\"@+id/timestamp\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_alignParentRight=\"true\" android:text=\"2시간 전\" /> <!-- 3. 사용자 이름 --> <TextView android:id=\"@+id/user_name\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_toRightOf=\"@id/profile_image\" android:layout_toLeftOf=\"@id/timestamp\" android:layout_marginLeft=\"16dp\" android:text=\"Android User\" android:textSize=\"18sp\" android:textStyle=\"bold\" /> <!-- 4. 설명 --> <TextView android:id=\"@+id/description\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_below=\"@id/user_name\" android:layout_alignLeft=\"@id/user_name\" android:layout_marginTop=\"4dp\" android:text=\"RelativeLayout은 유연합니다.\" /> </RelativeLayout> profile_image : 부모의 왼쪽에 붙고 수직 중앙 정렬. timestamp : 부모의 오른쪽에 붙음. user_name : profile_image 의 오른쪽이면서 timestamp 의 왼쪽에 위치하여, 두 뷰 사이에 공간을 채웁니다. description : user_name 의 아래에 위치하며, 왼쪽 라인을 user_name 과 동일하게 맞춥니다( layout_alignLeft ). ✅ 이처럼 여러 뷰의 관계를 사슬처럼 엮어 복잡한 UI를 효과적으로 만들 수 있습니다. ⚠️ RelativeLayout 의 주의사항 순환 종속성 (Circular Dependency) A 뷰가 B 뷰의 오른쪽에( toRightOf=\"B\" ) 있고, B 뷰가 A 뷰의 오른쪽에( toRightOf=\"A\" ) 있도록 서로를 참조하면 앱이 비정상 종료될 수 있습니다. 뷰의 관계는 한 방향으로만 흐르도록 설계해야 합니다. 성능 문제 뷰의 위치를 결정하기 위해 레이아웃 계산을 두 번(Two measurement passes) 수행하는 경우가 많아, 뷰가 매우 많은 복잡한 구조에서는 성능 저하를 유발할 수 있습니다. 중첩된 RelativeLayout 은 성능에 더욱 좋지 않습니다. 기준점 부재 뷰에 아무런 위치 규칙을 주지 않으면, 기본값으로 부모의 좌측 상단 (0, 0)에 겹쳐서 표시됩니다. 의도치 않게 뷰들이 겹쳐 보인다면 위치 규칙이 누락되었는지 확인해야 합니다. 💡 핵심 교훈 항목 설명 핵심 개념 ID를 통한 관계 설정. 뷰들이 서로 또는 부모를 기준으로 위치를 잡습니다. 장점 중첩을 줄이면서 복잡하고 유연한 UI를 만들 수 있습니다. 단점 뷰가 많아지면 관계가 복잡해지고, XML 가독성이 떨어지며 성능 저하가 발생할 수 있습니다. 주의사항 순환 종속성 오류를 피하고, 기준점 ID를 명확히 해야 합니다. 현대적 대안 ConstraintLayout . RelativeLayout 의 모든 기능과 그 이상을 제공하면서 성능이 더 뛰어나고, 강력한 편집기(Layout Editor)를 지원합니다. 새로운 레이아웃을 작성할 때는 ConstraintLayout 사용이 적극 권장됩니다. FrameLayout 프레임 레이아웃 핵심 원리: 뷰를 겹겹이 쌓는 가장 단순한 레이아웃 마치 투명한 종이를 포개듯, 나중에 추가된 뷰가 가장 위에 올라오는 구조입니다. 자식 뷰들은 기본적으로 부모의 좌측 상단에 쌓이기 시작합니다. 🔑 위치를 제어하려면 layout_gravity 를, 화면에 보이거나 숨기려면 visibility 속성을 함께 사용합니다. FrameLayout의 주요 속성 LinearLayout이나 RelativeLayout처럼 복잡한 위치 지정 규칙은 없습니다. 대신 다음 두 속성이 핵심적인 역할을 합니다. android:layout_gravity : 부모 FrameLayout 안에서 자식 뷰를 정렬합니다. “나를 부모 프레임 안에서 어디에 둘까?”* center , top , bottom , left , right 값을 조합하여 사용합니다. (예: top|right ) android:visibility : 뷰를 화면에 보여줄지 여부를 결정합니다. visible : 뷰를 화면에 보여줍니다. (기본값) invisible : 뷰를 화면에서 숨기지만, 차지하던 공간은 그대로 유지됩니다. gone : 뷰를 화면에서 숨기고, 차지하던 공간도 함께 사라집니다. FrameLayout 예시 예시 1: 기본적인 뷰 겹치기와 정렬 <FrameLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:layout_height=\"200dp\" android:background=\"#E0E0E0\"> <!-- 배경이 될 이미지 --> <ImageView android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:scaleType=\"centerCrop\" android:src=\"@drawable/background_image\" /> <!-- 이미지 위에 겹쳐질 텍스트 --> <TextView android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_gravity=\"bottom|right\" android:layout_margin=\"16dp\" android:text=\"Sample Text\" android:textColor=\"@android:color/white\" android:background=\"#80000000\" android:padding=\"8dp\"/> </FrameLayout> ImageView 가 먼저 추가되어 배경처럼 맨 아래에 깔립니다. 그 위에 TextView 가 겹쳐집니다. TextView 는 layout_gravity=\"bottom|right\" 속성으로 인해 프레임의 우측 하단에 위치하게 됩니다. 예시 2: visibility 를 이용한 뷰 전환 (로딩 화면) <FrameLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\"> <!-- 로딩이 끝나면 보여줄 콘텐츠 --> <ImageView android:id=\"@+id/content_image\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:src=\"@drawable/my_content\" android:visibility=\"invisible\" /> <!-- 처음엔 숨김 --> <!-- 로딩 중에 보여줄 프로그레스 바 --> <ProgressBar android:id=\"@+id/loading_indicator\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_gravity=\"center\" android:visibility=\"visible\" /> <!-- 처음엔 보임 --> </FrameLayout> 처음 화면에서는 ProgressBar 만 중앙에 보입니다. 코드(Activity/Fragment)에서 데이터 로딩이 완료되면 visibility 속성을 동적으로 변경합니다. // 로딩 완료 후 binding.loadingIndicator.visibility = View.GONE // 프로그레스 바 숨기기 (공간도 제거) binding.contentImage.visibility = View.VISIBLE // 이미지 보여주기 ✅ FrameLayout 은 이렇게 같은 공간에서 서로 다른 뷰를 번갈아 보여줘야 할 때 매우 유용합니다. 💡 핵심 교훈 항목 설명 핵심 기능 뷰 겹치기 (Stacking). 나중에 선언된 뷰가 위에 온다. 위치 지정 android:layout_gravity 를 사용하여 부모 내에서 정렬한다. 동적 제어 android:visibility 속성 ( visible , invisible , gone )을 이용해 뷰를 제어하는 것이 핵심 용도. 주요 용도 - 화면 전환: 탭(Tab) 화면처럼 한 번에 하나의 화면만 보여줄 때.- UI 오버레이: 비디오 위의 재생 버튼, 이미지 위의 프로그레스 바 등. 단순함 단 하나의 자식 뷰를 담는 가장 간단한 컨테이너로도 사용된다. (예: Fragment 를 담는 컨테이너) GridLayout 표 (격자) 레이아웃 핵심 원리: 뷰를 보이지 않는 표(spreadsheet)의 셀에 순서대로 배치한다. GridLayout은 자식 뷰들을 행(row)과 열(column)으로 구성된 격자무늬에 배치하는 레이아웃입니다. LinearLayout처럼 한 방향으로 뷰를 추가하지만, 지정된 행/열의 개수를 채우면 자동으로 다음 줄로 넘어가는 특징이 있습니다. 🔑 orientation 으로 주된 흐름 방향을 정하고, columnCount 또는 rowCount 로 한 줄에 몇 개의 뷰를 넣을지 결정합니다. GridLayout의 기본 배치 규칙 GridLayout 자체에 설정하는 속성들입니다. 속성 설명 android:orientation 뷰를 배치할 주된 방향을 결정합니다.- horizontal (기본값): 왼쪽 → 오른쪽으로 채우고, 꽉 차면 아래로 줄바꿈.- vertical : 위 → 아래로 채우고, 꽉 차면 오른쪽으로 줄바꿈. android:columnCount 가로 방향으로 배치할 열의 개수를 지정합니다. orientation=\"horizontal\" 일 때 핵심 속성입니다. android:rowCount 세로 방향으로 배치할 행의 개수를 지정합니다. orientation=\"vertical\" 일 때 핵심 속성입니다. 자식 뷰의 핵심 속성 각각의 자식 뷰에 설정하여 위치, 크기, 병합을 제어합니다. 속성 설명 android:layout_row 뷰가 위치할 행의 인덱스(0부터 시작)를 직접 지정합니다. android:layout_column 뷰가 위치할 열의 인덱스(0부터 시작)를 직접 지정합니다. android:layout_rowSpan 현재 뷰가 세로로 몇 개의 셀을 병합(차지)할지 지정합니다. (기본값: 1) android:layout_columnSpan 현재 뷰가 가로로 몇 개의 셀을 병합(차지)할지 지정합니다. (기본값: 1) android:layout_gravity 셀 안에서 뷰를 정렬하거나, 뷰를 늘려 셀을 채우도록 합니다.- center , top , bottom , left , right : 셀 내부 정렬- fill , fill_horizontal , fill_vertical : 뷰를 확장하여 셀 공간 채우기 GridLayout 예시 예시 1: 기본적인 그리드 배치 (계산기 키패드) <GridLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:columnCount=\"3\" android:orientation=\"horizontal\"> <Button android:text=\"1\" /> <Button android:text=\"2\" /> <Button android:text=\"3\" /> <Button android:text=\"4\" /> <Button android:text=\"5\" /> <Button android:text=\"6\" /> </GridLayout> columnCount=\"3\" 이므로 버튼이 가로로 3개씩 배치됩니다. 4번째 버튼( text=\"4\" )은 자동으로 다음 줄의 첫 번째 칸으로 넘어갑니다. 예시 2: 셀 병합( columnSpan )과 채우기( gravity=\"fill\" ) <GridLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:columnCount=\"4\"> <!-- 이 뷰는 가로 4칸을 모두 차지 --> <TextView android:layout_columnSpan=\"4\" android:layout_gravity=\"fill_horizontal\" android:text=\"0\" android:textSize=\"48sp\" android:gravity=\"right\"/> <Button android:text=\"7\" /> <Button android:text=\"8\" /> <Button android:text=\"9\" /> <Button android:text=\"/\" /> <!-- 이 뷰는 세로 2칸을 차지 --> <Button android:layout_rowSpan=\"2\" android:layout_gravity=\"fill_vertical\" android:text=\"=\"/> </GridLayout> TextView: layout_columnSpan=\"4\" 로 4개의 열을 병합하여 한 줄을 모두 차지합니다. layout_gravity=\"fill_horizontal\" 로 너비를 꽉 채웁니다. '=' 버튼: layout_rowSpan=\"2\" 로 2개의 행을 병합하여 세로로 긴 버튼이 됩니다. layout_gravity=\"fill_vertical\" 로 높이를 꽉 채웁니다. ✅ Span 속성은 셀을 병합할 뿐, 뷰의 크기를 늘리지는 않습니다. 뷰를 실제로 확장하려면 layout_gravity 의 fill 계열 값을 함께 사용해야 합니다. 예시 3: 특정 위치 지정 ( row , column ) <GridLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:columnCount=\"3\" android:rowCount=\"3\"> <Button android:text=\"A (0,0)\" /> <!-- (0,1)은 비워둠 --> <Button android:text=\"B (0,2)\" android:layout_column=\"2\"/> <!-- C 버튼을 (2,1) 위치에 강제 배치 --> <Button android:text=\"C (2,1)\" android:layout_row=\"2\" android:layout_column=\"1\" /> </GridLayout> 뷰를 순서대로 배치하지 않고, layout_row 와 layout_column 을 이용해 원하는 셀에 직접 배치할 수 있습니다. 지정되지 않은 셀은 빈 공간으로 남게 됩니다. ⚠️ GridLayout 사용 시 주의점 및 TableLayout과의 비교 TableLayout 과의 차이: TableLayout 은 <TableRow> 태그로 각 행을 명시적으로 선언해야 해서 구조가 더 경직됩니다. GridLayout 은 columnCount 나 rowCount 만 지정하면 자동으로 줄바꿈을 처리해 훨씬 유연합니다. layout_gravity 의 중요성: 셀 병합( Span )이나 공간 채우기를 할 때 layout_gravity 속성을 올바르게 사용하는 것이 매우 중요합니다. 현대적 대안: ConstraintLayout 은 가이드라인(Guideline)과 체인(Chain) 기능을 통해 GridLayout 보다 더 복잡하고 유연한 그리드 시스템을 평평한 구조(flat hierarchy)로 구현할 수 있어, 복잡한 화면에서는 성능상 더 유리할 수 있습니다. 💡 핵심 교훈 항목 설명 핵심 개념 자동 줄바꿈이 되는 격자무늬(Grid) 레이아웃. 주요 속성 orientation , columnCount / rowCount 로 전체 구조를 잡는다. 자식 뷰 제어 layout_span 으로 셀을 병합하고, layout_gravity 로 셀을 채우거나 정렬한다. 장점 TableLayout 보다 유연하고 적은 코드로 격자 UI를 만들 수 있다. 주요 용도 계산기, 갤러리 썸네일, 대시보드 메뉴 등 표 형태의 UI 구성. ConstraintLayout 제약 레이아웃 핵심 원리: 모든 뷰는 다른 뷰 또는 부모와의 '제약(Constraint)' 관계로 위치가 결정된다. ConstraintLayout은 RelativeLayout의 유연함과 LinearLayout의 비율 배치를 합친 것보다 더 강력한 레이아웃입니다. 뷰의 각 가장자리(상, 하, 좌, 우)에 있는 앵커 포인트(Anchor Point)를 다른 뷰의 앵커 포인트나 부모 레이아웃에 '연결'하여 위치를 정의합니다. 이를 통해 중첩 없이도 매우 복잡하고 반응형인 UI를 만들 수 있습니다. 🔑 모든 뷰는 최소 하나 이상의 수평 제약과 수직 제약이 있어야 위치가 확정됩니다. 제약이 없으면 뷰는 (0,0) 좌표(좌측 상단)에 표시됩니다. ConstraintLayout의 주요 제약 규칙 속성들은 누구의(Source) 어느 쪽(Anchor) 을 누구의(Target) 어느 쪽(Anchor) 에 연결할지 정의합니다. app:layout_constraint[SourceAnchor]_to[TargetAnchor]Of=\"[TargetID]\" 형식입니다. 상대 위치 지정 (다른 뷰 또는 부모 기준) “이 뷰의 왼쪽(Start)을 저 뷰의 오른쪽(End)에 연결하겠다”* “이 뷰의 상단(Top)을 부모(parent)의 상단(Top)에 연결하겠다”* 속성 설명 app:layout_constraintStart_toStartOf 왼쪽 가장자리를 대상의 왼쪽 가장자리에 맞춥니다. app:layout_constraintStart_toEndOf 왼쪽 가장자리를 대상의 오른쪽 가장자리에 맞춥니다. app:layout_constraintEnd_toStartOf 오른쪽 가장자리를 대상의 왼쪽 가장자리에 맞춥니다. app:layout_constraintEnd_toEndOf 오른쪽 가장자리를 대상의 오른쪽 가장자리에 맞춥니다. app:layout_constraintTop_toTopOf 위쪽 가장자리를 대상의 위쪽 가장자리에 맞춥니다. app:layout_constraintTop_toBottomOf 위쪽 가장자리를 대상의 아래쪽 가장자리에 맞춥니다. app:layout_constraintBottom_toTopOf 아래쪽 가장자리를 대상의 위쪽 가장자리에 맞춥니다. app:layout_constraintBottom_toBottomOf 아래쪽 가장자리를 대상의 아래쪽 가장자리에 맞춥니다. 💡 Start/End vs Left/Right: Start/End 는 RTL(Right-to-Left) 언어 환경을 지원하므로 Left/Right 보다 사용이 권장됩니다. 크기 조정 (Dimension Constraints) 뷰의 너비나 높이를 제약 조건에 따라 동적으로 결정합니다. layout_width / layout_height 값 설명 [고정값] (예: 50dp ) 뷰의 크기를 고정된 값으로 지정합니다. wrap_content 뷰 내부의 콘텐츠에 맞게 크기를 조절합니다. 0dp (match_constraint) 가장 중요. 제약 조건이 허용하는 한 최대한의 공간을 차지하도록 크기를 확장합니다. LinearLayout의 layout_weight 와 유사한 역할을 합니다. 중앙 정렬과 편향 (Bias) 양쪽(수평 또는 수직)으로 제약이 걸렸을 때, 뷰를 해당 공간 내에서 정렬합니다. 속성 설명 app:layout_constraintHorizontal_bias 수평 제약 공간 내에서 뷰의 위치를 조절합니다. (0.0=왼쪽, 0.5=중앙, 1.0=오른쪽) app:layout_constraintVertical_bias 수직 제약 공간 내에서 뷰의 위치를 조절합니다. (0.0=위쪽, 0.5=중앙, 1.0=아래쪽) ConstraintLayout 예시 예시 1: 기본적인 제약 관계 설정 (프로필 UI) 제공된 텍스트의 카카오톡 UI 예시를 XML 코드로 분석한 것입니다. <androidx.constraintlayout.widget.ConstraintLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:app=\"http://schemas.android.com/apk/res-auto\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:padding=\"16dp\"> <!-- 1. 프로필 이미지: 부모의 왼쪽, 위쪽에 고정 --> <ImageView android:id=\"@+id/imageView\" android:layout_width=\"50dp\" android:layout_height=\"50dp\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" app:srcCompat=\"@drawable/ic_launcher_background\" /> <!-- 2. 시간 텍스트: 부모의 오른쪽, 위쪽에 고정 --> <TextView android:id=\"@+id/dateView\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"9월 7일\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" /> <!-- 3. 제목 텍스트: 이미지의 오른쪽, 이미지의 위쪽 라인에 정렬 --> <TextView android:id=\"@+id/titleView\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_marginStart=\"16dp\" android:text=\"카카오톡\" app:layout_constraintStart_toEndOf=\"@+id/imageView\" app:layout_constraintTop_toTopOf=\"@+id/imageView\" /> <!-- 4. 메시지 텍스트: 이미지의 오른쪽, 이미지의 아래쪽 라인에 정렬 --> <TextView android:id=\"@+id/messageView\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_marginStart=\"16dp\" android:text=\"[기기 로그인 알림]\" app:layout_constraintBottom_toBottomOf=\"@+id/imageView\" app:layout_constraintStart_toEndOf=\"@+id/imageView\" /> </androidx.constraintlayout.widget.ConstraintLayout> imageView : 왼쪽은 부모의 왼쪽에, 위쪽은 부모의 위쪽에 연결. dateView : 오른쪽은 부모의 오른쪽에, 위쪽은 부모의 위쪽에 연결. titleView : 왼쪽은 imageView 의 오른쪽에 연결, 위쪽은 imageView 의 위쪽과 정렬. messageView : 왼쪽은 imageView 의 오른쪽에 연결, 아래쪽은 imageView 의 아래쪽과 정렬. ✅ 각 뷰가 다른 뷰나 부모와 어떻게 관계를 맺는지 명확하게 보여줍니다. 예시 2: 0dp(match_constraint) 와 Bias를 이용한 동적 크기 및 중앙 정렬 <androidx.constraintlayout.widget.ConstraintLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:app=\"http://schemas.android.com/apk/res-auto\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\"> <!-- 1. 부모의 중앙에 위치시키기 --> <Button android:id=\"@+id/center_button\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"정중앙 버튼\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" app:layout_constraintBottom_toBottomOf=\"parent\" /> <!-- 상하좌우를 모두 부모에 연결하면 자동으로 중앙 정렬됨 --> <!-- 2. 남은 공간을 모두 차지하는 텍스트뷰 (0dp) --> <TextView android:id=\"@+id/header_text\" android:layout_width=\"0dp\" <!-- 제약에 따라 너비가 결정됨 --> android:layout_height=\"wrap_content\" android:layout_marginHorizontal=\"16dp\" android:text=\"이 텍스트는 양 옆의 공간을 채웁니다\" android:gravity=\"center\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintBottom_toTopOf=\"@+id/center_button\" android:layout_marginBottom=\"16dp\"/> </androidx.constraintlayout.widget.ConstraintLayout> center_button : 뷰의 상/하/좌/우 앵커를 모두 부모에 연결하면, 뷰는 부모의 정중앙에 위치하게 됩니다. bias 값을 조정하여 중앙이 아닌 다른 위치로 옮길 수도 있습니다. header_text : layout_width=\"0dp\" 로 설정하고 왼쪽과 오른쪽을 부모에 연결하여, 부모의 너비에서 마진을 뺀 나머지 공간을 모두 차지하게 됩니다. 이는 LinearLayout의 weight 와 비슷한 효과를 냅니다. ⚠️ ConstraintLayout 의 주의사항 제약 누락 (Missing Constraints) 수평 또는 수직 제약 중 하나라도 없으면, 해당 축의 위치가 확정되지 않아 뷰가 레이아웃의 (0,0) 위치로 이동합니다. 레이아웃 편집기는 이런 경우 경고를 표시해 줍니다. XML의 복잡성 뷰가 많아지고 제약이 복잡해지면 XML 코드가 길고 가독성이 떨어질 수 있습니다. 따라서 안드로이드 스튜디오의 레이아웃 편집기(Layout Editor)를 사용하는 것이 거의 필수적입니다. 성능 ConstraintLayout 은 평평한 뷰 계층(Flat View Hierarchy)을 만들도록 설계되었습니다. LinearLayout 이나 RelativeLayout 을 여러 번 중첩하는 것보다 단일 ConstraintLayout 을 사용하는 것이 일반적으로 렌더링 성능에 훨씬 유리합니다. 💡 핵심 교훈 항목 설명 핵심 개념 ID와 앵커 포인트(Anchor Point)를 연결하는 '제약'으로 뷰의 위치와 크기를 정의합니다. 장점 평평한 계층 구조(Flat Hierarchy)로 중첩 없이 복잡한 UI 구현이 가능하며, 이는 성능에 유리합니다. 매우 유연하고 강력합니다. 단점 XML 코드가 길고 복잡해질 수 있어, 레이아웃 편집기 사용이 거의 필수적입니다. 주의사항 모든 뷰는 수평/수직 제약이 모두 필요합니다. 제약 누락 시 뷰가 의도치 않은 위치(0,0)로 이동할 수 있습니다. 현대적 위치 현재 안드로이드 UI 개발의 표준 레이아웃입니다. 새로운 화면을 구성할 때 가장 먼저 고려해야 하며, RelativeLayout 을 완벽히 대체합니다. 사용자 이벤트 처리 터치 이벤트 터치 이벤트 종류 터치 이벤트 발생 좌표 키 이벤트 사용자 이벤트 처리 안드로이드 앱은 사용자의 다양한 상호작용(터치, 키 입력 등)에 반응해야 합니다. 이는 이벤트 기반 프로그래밍(Event-driven Programming) 모델을 통해 이루어집니다. 사용자가 행동을 취하면 시스템이 '이벤트' 객체를 생성하여 앱에 전달하고, 앱은 이 이벤트를 처리할 '리스너(Listener)' 또는 '콜백(Callback)' 함수를 통해 특정 로직을 수행합니다. 터치 이벤트 핵심 원리: 터치 이벤트는 Activity 에서 시작하여 계층 구조를 따라 가장 깊은 View 까지 전달되며, 이 과정에서 특정 뷰가 이벤트를 '소비(consume)'하면 전파가 중단된다. 터치 이벤트는 onTouchEvent() 콜백 함수를 통해 처리되며, MotionEvent 객체에 모든 관련 정보가 담겨 있습니다. 터치 이벤트의 종류와 생명주기 터치 이벤트는 단발성이 아니라 하나의 연속된 흐름을 가집니다. 이벤트 종류 (in MotionEvent ) 설명 ACTION_DOWN 사용자가 화면을 처음 눌렀을 때 단 한 번 발생합니다. 모든 터치 상호작용의 시작점입니다. ACTION_MOVE 손가락을 누른 상태로 움직일 때 좌표가 바뀔 때마다 연속적으로 발생합니다. 드래그(Drag) 동작을 구현할 때 핵심적인 역할을 합니다. ACTION_UP 사용자가 화면에서 손가락을 뗄 때 단 한 번 발생합니다. 터치 상호작용의 종료점입니다. ACTION_CANCEL 터치 이벤트가 비정상적으로 종료될 때 발생합니다. 예를 들어, 뷰를 누른 상태에서 스크롤 가능한 상위 뷰(e.g., ScrollView )가 스크롤을 시작하면, 하위 뷰는 이 이벤트를 받게 됩니다. 터치 이벤트 발생 좌표 (심화) 속성 설명 사용 사례 event.x / event.y 이벤트가 발생한 뷰(View)의 좌표계에서의 상대 좌표. 즉, 해당 뷰의 좌측 상단이 (0,0)입니다. 뷰 자체의 로직 처리: 커스텀 버튼 내에서 특정 영역이 눌렸는지 확인하거나, 그림판 앱에서 캔버스 뷰 내부에 그림을 그릴 때 사용합니다. event.rawX / event.rawY 화면(Screen)의 좌표계에서의 절대 좌표. 즉, 스마트폰 화면의 좌측 상단이 (0,0)입니다. 뷰 외부와의 상호작용: 뷰를 화면 전체에서 드래그하여 위치를 옮기거나, 화면 특정 위치에 팝업을 띄울 때 기준점으로 사용합니다. 🔑 시각적 비유: rawX/rawY 가 '벽에 걸린 TV 스크린 전체'의 좌표라면, x/y 는 'TV 화면 속에서 재생되는 영화' 내부의 좌표와 같습니다. 이벤트 소비(Consumption)와 전파(Propagation) onTouchEvent() 함수의 반환값은 이벤트 처리 흐름에서 매우 중요합니다. return true : \"이 이벤트를 내가 처리했으니(소비했으니), 더 이상 다른 곳으로 전파하지 마세요.\" ACTION_DOWN 이벤트에 대해 true 를 반환하면, 이후의 ACTION_MOVE , ACTION_UP 이벤트가 모두 이 뷰로 전달됩니다. 드래그와 같은 연속적인 동작을 구현하려면 필수입니다. return false (또는 super.onTouchEvent(event) ): \"나는 이 이벤트를 처리하지 않았으니, 상위 뷰나 기본 동작이 처리하도록 전달하세요.\" ACTION_DOWN 에 대해 false 를 반환하면, 이후의 ACTION_MOVE , ACTION_UP 이벤트는 이 뷰로 전달되지 않습니다. // CustomView.kt override fun onTouchEvent(event: MotionEvent?): Boolean { when (event?.action) { MotionEvent.ACTION_DOWN -> { Log.d(\"TouchEvent\", \"DOWN 이벤트 발생. 이 뷰가 처리 시작!\") return true // 이 뷰가 모든 터치 이벤트를 받겠다고 선언 } MotionEvent.ACTION_MOVE -> { Log.d(\"TouchEvent\", \"MOVE: (${event.x}, ${event.y})\") } MotionEvent.ACTION_UP -> { Log.d(\"TouchEvent\", \"UP 이벤트 발생. 처리 종료.\") } } return super.onTouchEvent(event) // ACTION_DOWN에서 true를 반환했으므로 사실상 이 코드는 거의 호출되지 않음 } 키 이벤트 핵심 원리: 시스템 레벨의 물리적/가상 버튼 입력을 감지하는 메커니즘으로, 일반적인 텍스트 입력(소프트 키보드)과는 완전히 분리되어 있다. 키 이벤트 콜백 함수 콜백 함수 설명 onKeyDown(keyCode, event) 키를 누르는 순간에 호출됩니다. event.repeatCount 를 통해 길게 눌렀을 때 반복적으로 호출되는 것을 감지할 수 있습니다. onKeyUp(keyCode, event) 눌렀던 키에서 손을 떼는 순간에 호출됩니다. onKeyLongPress(keyCode, event) 사용자가 키를 길게 누르고 있을 때, onKeyDown 이 처음 호출된 후 일정 시간이 지나면 호출됩니다. 왜 소프트 키보드는 키 이벤트를 발생시키지 않는가? 소프트 키보드는 그 자체가 하나의 독립적인 애플리케이션(IME: Input Method Editor)입니다. 사용자가 키를 누르면 IME는 '키 입력' 이벤트를 시스템에 보내는 것이 아니라, '글자' 자체를 현재 포커스된 EditText 와 같은 뷰에 직접 전달합니다. 따라서 텍스트 변경을 감지하려면 EditText 의 addTextChangedListener 와 같은 고수준의 리스너를 사용해야 합니다. 앱에서 처리 가능한 시스템 버튼 (심화) 앱이 전원, 홈 버튼 등을 가로챌 수 없는 이유는 사용자 경험의 일관성과 보안 때문입니다. 이 버튼들은 어떤 앱을 사용하든 항상 동일하게 동작해야 하는 시스템의 핵심 제어 기능입니다. 만약 악의적인 앱이 홈 버튼을 막는다면 사용자는 앱을 빠져나갈 수 없게 됩니다. 구분 버튼 종류 설명 ✅ 처리 가능 뒤로 가기, 볼륨 조절 앱의 컨텍스트 내에서 사용자 경험을 향상시키기 위해 커스터마이징이 허용됩니다. (예: 게임 중 볼륨 키를 다른 기능으로 매핑) ❌ 처리 불가 전원, 홈, 오버뷰 시스템의 최상위 제어권을 가지며, 앱이 개입할 수 없도록 OS 레벨에서 차단됩니다. 뒤로 가기 버튼 처리: onBackPressed vs. OnBackPressedCallback onBackPressed() 가 Deprecated된 이유는 현대 안드로이드 아키텍처, 특히 Fragment와의 통합이 어렵기 때문입니다. Activity 에만 존재하는 이 메서드는 여러 Fragment가 화면을 구성할 때 \"현재 활성화된 Fragment가 뒤로 가기 이벤트를 먼저 처리해야 한다\"는 로직을 구현하기 복잡하게 만듭니다. OnBackPressedCallback 의 장점: 생명주기 인식(Lifecycle-Aware): 콜백을 Activity 나 Fragment 의 생명주기에 바인딩할 수 있어, 화면이 보일 때만 콜백이 활성화되도록 쉽게 관리할 수 있습니다. 모듈성 및 유연성: 각 Fragment나 컴포넌트가 자신만의 뒤로 가기 로직을 독립적으로 등록하고 관리할 수 있습니다. 동적 활성화/비활성화: callback.isEnabled = false 와 같이 콜백을 실시간으로 켜고 끌 수 있어, 특정 조건(예: 양식 작성 중)에서만 뒤로 가기 동작을 막는 것이 매우 편리합니다. // Activity의 onCreate 등에서 콜백 등록 // '뒤로 가기'를 두 번 눌러야 종료되는 기능 구현 예시 private var backPressedTime: Long = 0 val callback = object : OnBackPressedCallback(true) { // true: 콜백을 초기에 활성화 override fun handleOnBackPressed() { // 2초 이내에 다시 누르면 액티비티 종료 if (System.currentTimeMillis() - backPressedTime < 2000) { finish() } else { // 처음 눌렀거나, 누른 지 2초가 지났으면 토스트 메시지 표시 Toast.makeText(this@MainActivity, \"한 번 더 누르면 종료됩니다.\", Toast.LENGTH_SHORT).show() backPressedTime = System.currentTimeMillis() } } } this.onBackPressedDispatcher.addCallback(this, callback) 💡 핵심 교훈 항목 설명 저수준(Low-level) vs. 고수준(High-level) 이벤트 onTouchEvent , onKeyDown 은 시스템에서 오는 날 것의 저수준 이벤트입니다. 반면 setOnClickListener , addTextChangedListener 등은 이러한 저수준 이벤트를 조합하고 추상화하여 특정 목적에 맞게 만든 고수준 이벤트 리스너입니다. 대부분의 경우, 고수준 리스너를 사용하는 것이 더 간단하고 안정적입니다. 이벤트 처리의 책임 return true 를 통해 이벤트를 '소비'하는 것은 해당 이벤트에 대한 모든 책임을 지겠다는 의미입니다. 연속적인 터치 동작(드래그, 핀치 줌 등)을 구현할 때 이 개념을 반드시 이해해야 합니다. 시스템 동작과의 조화 키 이벤트를 커스터마이징할 때는 사용자의 일반적인 기대를 해치지 않는 선에서 신중하게 구현해야 합니다. 특히 뒤로 가기 버튼의 기본 동작을 막을 때는 사용자에게 명확한 피드백을 제공해야 합니다. 아키텍처의 변화 수용 onBackPressed() 에서 OnBackPressedCallback 으로의 전환은 안드로이드가 생명주기와 컴포넌트 기반 아키텍처를 얼마나 중요하게 생각하는지를 보여주는 좋은 예입니다. 항상 최신 권장 방식을 따르는 것이 좋습니다. 뷰 이벤트 처리 안드로이드에서 사용자 상호작용의 가장 기본이 되는 단위는 뷰(View)에서 발생하는 이벤트입니다. 이벤트 처리는 명확하게 역할이 나뉜 세 가지 구성 요소의 협력을 통해 이루어집니다. 이 구조를 이해하면 어떤 뷰의 이벤트라도 일관된 방식으로 처리할 수 있습니다. 뷰 이벤트 처리의 기본 구조 뷰 이벤트 처리는 '이벤트가 발생한 객체'와 '이벤트를 처리할 로직'을 '연결'하는 과정입니다. 이벤트 소스 (Event Source): 이벤트가 발생한 뷰 객체 그 자체입니다. (예: 사용자가 터치한 버튼, 체크 상태를 바꾼 체크박스) 이벤트 핸들러 (Event Handler): 이벤트가 발생했을 때 실제로 실행될 코드를 담고 있는 객체입니다. 특정 인터페이스를 구현하여 만들어집니다. 리스너 (Listener): 이벤트 소스에 이벤트 핸들러를 등록(연결)하는 함수입니다. 보통 set...Listener() 형태의 이름을 가집니다. 그림 8-6 뷰 이벤트 처리 구조 즉, 이벤트 소스에 리스너 함수를 사용하여 이벤트 핸들러를 등록해두면, 해당 이벤트가 발생했을 때 시스템이 등록된 핸들러의 로직을 실행시켜주는 구조입니다. 아래 코드는 체크박스의 체크 상태가 변경될 때의 이벤트를 처리하는 예시입니다. 이벤트 소스: binding.checkbox 객체 리스너: setOnCheckedChangeListener() 함수 이벤트 핸들러: CompoundButton.OnCheckedChangeListener 인터페이스를 구현한 object 익명 객체 // 체크박스 이벤트 처리 // 이벤트 소스 리스너(이벤트 핸들러 등록) 이벤트 핸들러 binding.checkbox.setOnCheckedChangeListener(object : CompoundButton.OnCheckedChangeListener { override fun onCheckedChanged(buttonView: CompoundButton?, isChecked: Boolean) { Log.d(\"kkang\", \"체크박스 클릭, 현재 상태: $isChecked\") } }) 그림 8-7 CheckEvent 구조 대부분의 이벤트 핸들러는 On...Listener 형태의 인터페이스를 구현하여 만듭니다. 안드로이드 프레임워크는 OnClickListener , OnLongClickListener , OnItemClickListener 등 다양한 상황에 맞는 인터페이스를 제공합니다. 이벤트 핸들러 구현의 3가지 방법 지정된 인터페이스를 구현한 객체를 이벤트 핸들러로 등록한다는 핵심 원칙은 동일하지만, 구현 방식은 코드 구조와 스타일에 따라 여러 가지가 가능합니다. 액티비티에서 인터페이스 구현 Activity 클래스 자체가 리스너 인터페이스를 직접 구현하는 방식입니다. class MainActivity3 : AppCompatActivity(), CompoundButton.OnCheckedChangeListener { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) val binding = ActivityMain3Binding.inflate(layoutInflater) setContentView(binding.root) // 리스너의 인자로 Activity 자신(this)을 전달 binding.checkbox.setOnCheckedChangeListener(this) } // 인터페이스의 추상 메서드 구현 override fun onCheckedChanged(buttonView: CompoundButton?, isChecked: Boolean) { Log.d(\"kkang\", \"체크박스 클릭\") } } 특징 및 장단점: 장점: 관련된 코드가 Activity 클래스 내에 모여있어 간단한 경우 파악하기 쉽습니다. 단점: Activity가 여러 종류의 리스너를 구현하면 코드가 비대해지고 역할이 불분명해집니다. (단일 책임 원칙 위배 가능성) 별도의 클래스로 구현 이벤트 핸들러 로직을 완전히 분리된 클래스로 작성하는 방식입니다. // 이벤트 핸들러를 별도의 클래스로 분리 class MyEventHandler : CompoundButton.OnCheckedChangeListener { override fun onCheckedChanged(buttonView: CompoundButton?, isChecked: Boolean) { Log.d(\"kkang\", \"체크박스 클릭\") } } class MainActivity3 : AppCompatActivity() { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) val binding = ActivityMain3Binding.inflate(layoutInflater) setContentView(binding.root) // 분리된 핸들러 클래스의 인스턴스를 생성하여 등록 binding.checkbox.setOnCheckedChangeListener(MyEventHandler()) } } 특징 및 장단점: 장점: 로직이 복잡하거나 여러 곳에서 재사용될 때 유용합니다. 코드의 역할 분리가 명확해져 가독성과 유지보수성이 향상됩니다. 단점: 간단한 일회성 이벤트 처리를 위해 매번 클래스를 만드는 것은 번거로울 수 있습니다. SAM 기법 (람다식)으로 구현 코틀린에서 가장 보편적이고 권장되는 방식입니다. 자바의 함수형 인터페이스를 간결한 람다(Lambda) 표현식으로 대체할 수 있는 SAM(Single Abstract Method) 변환을 활용합니다. class MainActivity3 : AppCompatActivity() { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) val binding = ActivityMain3Binding.inflate(layoutInflater) setContentView(binding.root) // 인터페이스 구현 객체 대신 람다식을 바로 전달 binding.checkbox.setOnCheckedChangeListener { compoundButton, b -> Log.d(\"kkang\", \"체크박스 클릭\") } } } 특징 및 장단점: 장점: 코드가 매우 간결하고 직관적입니다. 불필요한 보일러플레이트 코드를 제거하여 생산성이 높습니다. 단점: SAM 변환이 가능한 인터페이스(추상 메서드가 하나인 인터페이스)에만 사용할 수 있습니다. 주요 뷰 이벤트: 클릭과 롱클릭 뷰가 아무리 많아도 이벤트 처리 구조는 동일합니다. 여기서는 모든 뷰의 기반이 되는 View 클래스에 정의된 가장 대표적인 두 이벤트를 알아봅니다. ClickEvent: 뷰를 짧게 클릭했을 때 발생 LongClickEvent: 뷰를 길게 눌렀을 때 발생 두 이벤트의 핸들러를 등록하는 리스너 함수는 다음과 같습니다. open fun setOnClickListener(l: View.OnClickListener?): Unit open fun setOnLongClickListener(l: View.OnLongClickListener?): Unit SAM 기법을 활용하여 버튼의 클릭, 롱클릭 이벤트를 처리하는 코드는 다음과 같습니다. binding.button.setOnClickListener { Log.d(\"kkang\", \"클릭 이벤트\") } binding.button.setOnLongClickListener { Log.d(\"kkang\", \"롱클릭 이벤트\") true // 이벤트 처리가 완료되었음을 알림 } SAM (Single Abstract Method) 변환 심층 분석 binding.button.setOnClickListener { ... } 와 같은 람다식 코드가 어떻게 동작하는지 이해하는 것은 매우 중요합니다. Java 방식의 이벤트 핸들러 자바에서는 보통 아래와 같이 익명 내부 클래스(anonymous inner class)를 사용하여 이벤트 핸들러를 작성합니다. // 자바로 작성한 이벤트 핸들러 binding.btn.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View view) { // ... 클릭 시 실행될 로직 ... } }); 코틀린의 정식 변환 (object 사용) 위 자바 코드를 코틀린으로 그대로 변환하면 object 키워드를 사용한 익명 객체 표현식이 됩니다. // 코틀린으로 작성한 이벤트 핸들러 binding.btn.setOnClickListener(object: View.OnClickListener { override fun onClick(p0: View?) { // ... 클릭 시 실행될 로직 ... } }) 코틀린 SAM 기법을 통한 간소화 코틀린은 추상 메서드가 단 하나뿐인 자바 인터페이스를 인자로 받는 자바 함수를 호출할 때, 위와 같은 object 표현식 대신 람다식으로 코드를 자동 변환해주는 SAM 변환 기능을 제공합니다. View.OnClickListener 인터페이스는 onClick() 이라는 추상 메서드를 단 하나만 가지고 있습니다. 따라서 코틀린 컴파일러는 아래와 같은 람다식을, binding.btn.setOnClickListener { ... } 내부적으로는 View.OnClickListener 를 구현한 객체로 변환하여 setOnClickListener 함수에 전달해 줍니다. 이 덕분에 개발자는 매우 간결하고 읽기 쉬운 코드를 작성할 수 있습니다. 💡 SAM 변환의 조건 SAM 변환은 코틀린 코드에서 자바로 작성된 메서드를 호출할 때, 그 메서드가 추상 메서드가 하나인 자바 인터페이스를 매개변수로 요구하는 경우에만 적용됩니다. ❓ 질문 있어요! (Q&A) Q. 롱클릭 이벤트 핸들러에서 마지막 줄의 true 를 생략하면 오류가 발생합니다. 어떤 의미인가요? A. 이는 함수의 반환값을 의미합니다. setOnLongClickListener 가 요구하는 OnLongClickListener 인터페이스의 추상 메서드 onLongClick 은 다음과 같이 정의되어 있습니다. abstract fun onLongClick(v: View!): Boolean 메서드의 반환 타입이 Boolean 이므로, 이벤트를 처리한 후 반드시 true 또는 false 를 반환해야 합니다. 코틀린에서 여러 줄로 구성된 람다식은 마지막 줄의 실행 결과를 자동으로 반환값으로 취급합니다. 따라서 return 키워드 없이 true 만 적어도 onLongClick 메서드가 true 를 반환하는 것으로 처리됩니다. true 반환: \"이 롱클릭 이벤트를 내가 완전히 처리(소비)했음. 이후 다른 이벤트(예: 클릭 이벤트)를 발생시키지 말 것.\" false 반환: \"롱클릭 이벤트에 반응은 했지만, 완전히 처리한 것은 아님. 시스템이 이어서 다른 이벤트(예: 클릭 이벤트)를 발생시켜도 됨.\" 추가 Empty Activity 초기 main.kt 설명 package com.example.myapplication import android.os.Bundle import androidx.activity.ComponentActivity import androidx.activity.compose.setContent import androidx.activity.enableEdgeToEdge import androidx.compose.foundation.layout.fillMaxSize import androidx.compose.foundation.layout.padding import androidx.compose.material3.Scaffold import androidx.compose.material3.Text import androidx.compose.runtime.Composable import androidx.compose.ui.Modifier import androidx.compose.ui.tooling.preview.Preview import com.example.myapplication.ui.theme.MyApplicationTheme class MainActivity : ComponentActivity() { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) enableEdgeToEdge() setContent { MyApplicationTheme { Scaffold(modifier = Modifier.fillMaxSize()) { innerPadding -> Greeting( name = \"Android\", modifier = Modifier.padding(innerPadding) ) } } } } } @Composable fun Greeting(name: String, modifier: Modifier = Modifier) { Text( text = \"Hello $name!\", modifier = modifier ) } @Preview(showBackground = true) @Composable fun GreetingPreview() { MyApplicationTheme { Greeting(\"Android\") } } 네, 좋습니다. 전문가를 대상으로 Jetpack Compose의 내부 아키텍처와 디자인 철학에 초점을 맞춰 해당 코드를 심도 있게 분석해 보겠습니다. 이 코드는 단순해 보이지만, Compose의 핵심 원리들을 압축적으로 보여주는 훌륭한 예시입니다. 분석 개요 이 코드는 크게 세 가지 핵심 레이어를 보여줍니다. Framework Integration Layer ( MainActivity ): 전통적인 Android View 시스템과 새로운 Compose 런타임 간의 브릿지 역할. Composition & State Layer ( MyApplicationTheme , Scaffold ): UI 트리를 구성하고, 암시적 데이터를 전파하며, 상태에 따라 UI를 재구성(Recomposition)하는 메커니즘. UI Declaration & Modification Layer ( Greeting , Text , Modifier ): 실제 UI 노드를 선언하고, 이들의 속성을 데코레이터 패턴으로 확장하는 방식. 이제 각 부분을 내부 구조 중심으로 상세히 살펴보겠습니다. Framework Integration Layer: MainActivity & setContent class MainActivity : ComponentActivity() { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) enableEdgeToEdge() setContent { // ... Composable Content ... } } } ComponentActivity 와 setContent 의 역할 ComponentActivity : 단순히 Compose를 쓰기 위한 액티비티가 아닙니다. 이는 LifecycleOwner , ViewModelStoreOwner , SavedStateRegistryOwner 등 현대적인 Android 아키텍처 컴포넌트들의 생명주기와 상태를 관리하는 핵심적인 역할을 합니다. setContent : 이 함수가 바로 Compose World로 진입하는 관문입니다. 내부적으로 다음과 같은 중요한 작업을 수행합니다. 기존 액티비티의 contentView 를 제거합니다. ComposeView 라는 특수한 View 를 생성하여 액티비티의 루트 뷰로 설정합니다. 이 ComposeView 내에서 새로운 Composition 을 생성합니다. Composition 은 UI 트리의 상태를 추적하고 관리하는 객체입니다. Recomposer 를 액티비티의 생명주기에 연결(attach)합니다. Recomposer 는 State 변화를 감지하고 Recomposition을 스케줄링하는 백그라운드 코루틴 기반의 스케줄러입니다. Lifecycle.State.CREATED 에서 시작하여 Lifecycle.State.DESTROYED 에서 중단됩니다. enableEdgeToEdge() 의 내부 동작 이 함수는 단순히 UI를 확장하는 것 이상입니다. 내부적으로 WindowCompat.setDecorFitsSystemWindows(window, false) 를 호출합니다. 이는 Window Decor View가 시스템 인셋(insets)을 더 이상 고려하지 않도록 하여, 앱의 콘텐츠 영역이 상태 표시줄과 내비게이션 바 아래까지 확장되도록 합니다. 이후 ViewCompat.setOnApplyWindowInsetsListener 를 통해 시스템 인셋 정보를 수신하고, 이를 Compose 레이아웃 시스템으로 전달할 준비를 합니다. Scaffold 와 같은 컴포저블은 이 정보를 활용하여 콘텐츠에 적절한 패딩을 적용하게 됩니다. Composition & State Layer: MyApplicationTheme & Scaffold MyApplicationTheme { Scaffold(modifier = Modifier.fillMaxSize()) { innerPadding -> Greeting( name = \"Android\", modifier = Modifier.padding(innerPadding) ) } } MyApplicationTheme 와 CompositionLocal MyApplicationTheme 은 단순한 스타일링 래퍼(wrapper)가 아닙니다. 이는 CompositionLocal 이라는 강력한 메커니즘의 실제 사용 사례입니다. 내부적으로 CompositionLocalProvider 를 사용하여 LocalColorScheme , LocalTextStyle 등의 CompositionLocal 객체에 특정 값(e.g., MaterialTheme.colorScheme )을 제공합니다. CompositionLocal 은 하위 컴포저블 트리 전체에 데이터를 암시적으로(implicitly) 전파하는 방법입니다. 모든 컴포저블에 theme 객체를 명시적으로 전달하는 'Prop Drilling'을 피할 수 있게 해줍니다. 하위 컴포저블(예: Text , Button )은 MaterialTheme.colorScheme 과 같은 코드를 통해 현재 스코프의 CompositionLocal 값을 읽어와 자신을 렌더링합니다. Scaffold 와 Slot-based API Scaffold 는 머티리얼 디자인의 구조를 구현한 고수준 컴포저블이며, Slot-based API의 대표적인 예시입니다. topBar , bottomBar , floatingActionButton , content 등 특정 목적을 가진 람다 파라미터를 \"슬롯\"으로 제공합니다. 가장 중요한 부분은 content 람다에 전달되는 innerPadding: PaddingValues 입니다. 이는 부모 컴포저블( Scaffold )이 자식( Greeting )의 레이아웃에 필요한 정보를 제공하는 Compose의 협력적인 레이아웃 모델을 보여줍니다. Scaffold 는 topBar 등의 슬롯에 채워진 컴포저블들의 크기를 측정하고, enableEdgeToEdge 로 인해 발생한 시스템 인셋을 고려하여, 메인 콘텐츠가 그려져야 할 안전한 영역을 계산합니다. 이 계산 결과가 바로 innerPadding 이며, 자식은 이 값을 Modifier.padding 에 적용하여 UI가 가려지는 것을 방지합니다. UI Declaration & Modification Layer @Composable 함수의 변환(Transformation) @Composable fun Greeting(name: String, modifier: Modifier = Modifier) { ... } @Composable 어노테이션은 단순한 마커가 아닙니다. 이는 Compose 컴파일러 플러그인을 활성화하는 트리거입니다. 컴파일 시점에 컴파일러는 이 함수를 변환하여 두 개의 숨겨진 파라미터를 추가합니다. composer: Composer : 현재 Composition의 \"지휘자\" 역할을 합니다. composer.startNode() , composer.emit() 등의 호출을 통해 UI 트리를 Slot Table이라는 내부 데이터 구조에 기록합니다. changed: Int : 파라미터들의 변경 여부를 추적하는 비트마스크(bitmask)입니다. 이를 통해 Recomposition 시, 변경되지 않은 컴포저블의 실행을 건너뛰는 Smart Recomposition 최적화가 가능해집니다. name 파라미터가 안정적(stable)이고 이전과 같은 값이라면, Greeting 함수의 재실행을 건너뛸 수 있습니다. Text 컴포저블과 UI 노드 Text 와 같은 기본 컴포저블은 UI 트리의 \"잎(leaf)\"에 해당합니다. 이 함수가 실행되면, 내부적으로 composer.emitNode 를 호출하여 Slot Table에 LayoutNode 를 생성합니다. LayoutNode 는 Compose UI 트리의 기본 단위이며, 측정(measure), 배치(layout), 그리기(draw) 로직을 포함하고 있습니다. text , color 등의 파라미터는 이 LayoutNode 의 속성으로 설정됩니다. Modifier 의 아키텍처: Decorator Pattern modifier = Modifier.padding(innerPadding) Modifier 는 순서가 중요한, 불변(immutable) Modifier.Element 들의 체인입니다. 이는 데코레이터 패턴과 매우 유사하게 동작합니다. Modifier.padding(innerPadding) 은 PaddingModifier 라는 Modifier.Element 를 생성합니다. MainActivity 에서 Modifier.fillMaxSize() 는 SizeModifier 를 생성합니다. Modifier 체인은 바깥쪽부터 안쪽으로 적용됩니다. Scaffold 의 Modifier.fillMaxSize() 가 먼저 적용되어 크기를 결정하고, 그 후 Greeting 의 Modifier.padding(innerPadding) 이 적용되어 내부 콘텐츠의 위치를 조정합니다. 각 Modifier.Element 는 레이아웃의 특정 단계(measurement, layout, drawing, semantics, input)에 영향을 줄 수 있습니다. 이 아키텍처 덕분에 단일 책임 원칙을 지키면서도 UI 요소의 기능을 유연하게 확장할 수 있습니다. @Preview 와 Tooling의 분리 @Preview(showBackground = true) @Composable fun GreetingPreview() { ... } @Preview 는 런타임이 아닌 Tool-time 어노테이션입니다. Android Studio는 이 어노테이션을 감지하고, 별도의 프로세스에서 LayoutLib을 사용하여 해당 컴포저블을 렌더링합니다. 이는 앱의 런타임 로직과 UI 렌더링 로직의 완벽한 분리를 보여줍니다. 이를 통해 개발자는 전체 앱을 빌드하고 실행할 필요 없이, 각 UI 컴포넌트를 독립적으로 빠르게 개발하고 테스트할 수 있습니다. 이는 컴포넌트 기반 아키텍처(Component-Based Architecture)를 장려하는 Compose의 핵심 철학입니다. 결론 setContent 를 통해 View 시스템에서 Compose 런타임으로 제어권을 이전하고, CompositionLocal 로 테마 같은 전역적 데이터를 효율적으로 전파하며, Scaffold 와 같은 고수준 컴포저블의 Slot API를 통해 구조를 정의하고, 컴파일러 플러그인에 의해 변환된 @Composable 함수들이 Slot Table에 UI 트리를 선언적으로 기술하며, Modifier 체인을 통해 UI 노드의 속성을 계층적으로 꾸미는, 매우 정교하고 잘 설계된 아키텍처를 보여주고 있습니다. 이는 기존의 명령형(imperative) UI 툴킷과 근본적으로 다른, 선언형(declarative) 패러다임이 어떻게 구현되었는지를 명확히 보여주는 교과서적인 예제입니다.",
      "frontmatter": {
        "tags": [
          "android",
          "university"
        ],
        "date": "2025-09-24T07:16:39+09:00",
        "lastmod": "2025-10-16T16:04:40+09:00"
      }
    },
    "university movie": {
      "path": "/university-movie/",
      "filename": "university movie",
      "content": "I. 영화를 구성하는 기본적인 시각 요소 영화는 종합 예술로서 마술적 리얼리즘을 통해 다양한 삶을 간접적으로 경험하고 성찰하며 변화시킬 수 있는 힘을 지닙니다. 이러한 영화의 고유한 문법 중 핵심은 숏, 앵글, 미장센, 조명 등 감각적 효과(오프시스, opsis)를 창출하는 시각적 구성요소입니다. 숏 (Shot)의 단위와 종류 숏(shot)은 영화의 기본 단위로, 카메라가 작동해서 끝날 때까지의 시간 동안 빛에 노출되어 촬영된 필름의 길이를 의미하며 커트(cut)와 동일시되기도 합니다. 보통 10~15초 정도의 길이를 가집니다. 다양한 숏의 종류는 피사체와의 거리에 따라 나뉘며, 이는 감독이 전달하고자 하는 의미를 강화합니다. 숏의 분류 및 기능: 익스트림 롱 숏 (Extreme Long Shot, ELS): 망원 렌즈를 사용하여 인물보다 배경을 강조하고, 움직임을 많이 전달하는 데 활용됩니다 (예: ). 롱 숏 (Long Shot, LS): 연극에서 관객과 무대 사이의 거리 정도를 보여주는 숏입니다. 딥 포커스 숏 (Deep-Focus Shot): 롱 숏의 변형으로, 광각 렌즈에 의해 전경, 중경, 배경에 있는 피사체를 동시에 선명하게 초점을 맞추어 심도(depth)를 구성합니다. 이는 시각의 연속성을 강화하며, 편집 대신 여러 행위를 하나의 숏에서 일어나게 함으로써 관객 시선에 선택의 자유를 부여합니다 (예: 오슨 웰스, , 1941). 딥 포커스 숏과 롱 테이크는 바쟁의 사실주의적 편집에서 몽타주에 의한 인위적인 조작에 반대하며 ‘현실의 내적인 애매성’을 존중하기 때문에 핵심적으로 선호됩니다. 풀 숏 (Full Shot): 인물의 몸 전체를 화면에 담습니다. 미디엄 숏 (Medium Shot, MS): 무릎 위부터 인물을 담는 숏입니다. 오버 더 숄더 숏 (Over-the-Shoulder Shot, OTS): 카메라가 시선을 돌리지 않고 두 피사체를 배치해서 연결하는 데 사용됩니다 (예: , ). 클로즈업 (Close-up, CU): 피사체를 강조하거나 상징적인 의미 작용을 유도합니다. 서사의 압박에서 벗어난 ‘부분’이자 ‘전체’로서 인물의 정신성, 인격성을 드러내는 데 효과적입니다 (예: 베르히만, ). 익스트림 클로즈업 (Extreme-Close-up, ECU): 피사체를 상세히 보여주지만, 이로 인해 문맥과 단절되어 의미와 스토리 생성을 지연시키기도 합니다. 앵글 (Angle)의 역할과 종류 앵글(Angle)은 선택된 소재에 대한 감독의 논평(주석) 역할을 하며, 카메라가 놓인 장소에 의해 결정되어 스토리에 대한 해설이나 주석의 역할을 합니다. 앵글의 분류 및 기능: 버즈 아이 뷰 (Bird's-eye view): 새의 눈으로 보는 시각으로, 영화 공간을 소개하는 데 사용됩니다. 하이 앵글 (High Angle): 위쪽에서 피사체를 바라보는 앵글로, 인물의 상실감, 절망, 패배, 조롱, 왜소함을 전달하는 데 사용됩니다. 아이 레벨 (Eye Level): 우리 눈높이로 바라보는 일반적인 시각이며, 특정한 목적 없이 관객 스스로 판단할 수 있게 안정감을 제공합니다. 로우 앵글 (Low Angle): 아래에서 피사체를 올려다보는 앵글로, 인물의 강인함, 위압감, 우월감, 카리스마, 공포, 경외심을 묘사하는 데 효과적입니다. 또한 움직임의 속도를 더 빠르게 보여주어 폭력적이거나 혼란스러운 느낌 포착에도 용이합니다. 사각 앵글 (Dutch Angle/Canted Angle): 카메라를 기울여 촬영하며, 긴장, 변화, 불안감, 폭동 등 긴박한 움직임을 암시하고 폭력과 불안감 포착에 효과적입니다. 미장센 (Mise en scène)과 디스포지티프 미장센은 문자 그대로 '장면에 배치한다'는 의미로, 시각적인 재료가 무대화, 프레임화, 촬영되는 방법 전체를 포괄합니다. 미장센의 핵심 요소: • 프레임(Frame): 현실로부터 영화 세계를 구분하는 기본 단위입니다. • 구도와 디자인, 배우의 이동, 카메라의 이동, 조명 등이 모두 미장센에 포함됩니다. • 미장센은 인위적인 표현주의적 미장센과 자연스러운 사실주의적 미장센으로 구분될 수 있으며, 감독은 이 중 어떤 방식으로 영화의 매체적 특징을 드러낼지 선택합니다. 카메라의 이동 (움직임): 카메라의 움직임은 미장센의 중요한 부분이며, 스토리 해설이나 정서 전달에 기여합니다. 팬 쇼트 (Pans): 삼각대 위에서 수평으로 카메라를 이동하며, 등장인물(사물들) 간의 동등하고 수평적인 관계나 의미를 묘사합니다. 틸트 쇼트 (Tilts): 정지된 수평 축을 중심으로 수직으로 움직이며, 동시적 인과성이나 인물들의 내면적 심리 변화를 암시합니다. 달리 쇼트 (Dolly/Tracking/Crane): 촬영하는 동안 움직이는 피사체와 함께 이동하거나 스토리의 흐름에 따라 카메라 자체가 움직입니다. 크레인 쇼트는 공중에서 사용하는 달리 쇼트의 한 형태입니다. 줌 렌즈 쇼트 (Zoom): 렌즈 교환 없이 화면 크기를 빠르게 설정할 수 있습니다. 핸드헬드 쇼트 (Handheld): 폭동, 액션 등 현장의 빠르고 다양한 즉흥적이고 우연한 움직임 묘사에 적합합니다. 스테디캠 촬영은 핸드헬드보다 안정적이면서 부드러운 움직임을 표현합니다. 기계적 움직임: 애니메이션, 슬로모션, 역동작, 프리즈 프레임 (움직이는 화면 속 정지된 화면을 통해 강렬하고 극적인 인상과 여운을 남기는 효과) 등이 있습니다. 디스포지티프 (Dispositif)로의 확장: 현대에 들어 미장센 비평은 그 정의상의 한계를 갖는다는 주장이 제기되었습니다. 특히 프리프로덕션과 포스트프로덕션이 강조되는 ‘확장된 개념의 영화’(expanded cinema) 국면에서, 마틴(Adrian Martin)은 촬영이 이루어지는 순간만을 지나치게 강조하는 미장센 대신 디스포지티프 개념을 옹호합니다. 디스포지티프는 “최초의 구상부터 최종 믹싱과 색보정에 이르는 모든 수준에서 형식과 내용의 통합적 배열을 고려하는” 것으로, 영화에 대한 전체론적인 접근을 가능하게 합니다. 조명 (Lighting)과 분위기 조명은 주광(정면), 보조광(측면), 역광(후면)을 통해 영화의 주제, 분위기, 의미를 표현합니다. 조명 스타일은 보통 키(key)라고 칭합니다. 조명 스타일의 분류: 로우 키 (Low Key): 어두운 분위기를 강조하며, 미스터리, 스릴러, 갱스터처럼 느와르 장르의 폭력, 공포에 적합합니다 (예: 빌리 와일더, , 1994). 하이 콘트라스트 (High Contrast): 빛과 그림자의 극적인 대비를 통해 통념을 뒤집는 효과적 분위기를 산출합니다. 하이 키 라이팅 (High Key Lighting): 밝고 즐거운 분위기를 연출하며, 코미디, 로맨스, 뮤지컬 등 장르에 적합합니다. 영화 의 시각 구성요소 분석을 통한 주제의식 강화 보고서 I. 서론: 영화 매체의 특징과 주제의식의 접점 영화 는 한정된 공간인 방송국 라디오 부스 내에서 벌어지는 긴박한 테러 생중계 상황을 통해 현대사회의 주요한 쟁점들, 특히 언론의 타락과 매체 윤리 부재, 사회적 약자의 목소리, 그리고 통제 불가능한 현대사회의 부조리함을 강도 높게 비판하는 주제의식을 전달합니다 [주제의식]. 영화는 단순한 스릴러 장르를 넘어, 종합 예술로서 마술적 리얼리즘을 통해 관객에게 세상을 간접적으로 경험하고 성찰하며 변화시킬 수 있는 힘을 지니고 있으며, 이는 감각적 효과(오프시스, opsis)를 창출하는 시각적 구성요소들을 통해 구체화됩니다. 본 보고서는 영화의 대본과 시각 구성 요소(숏, 앵글, 미장센, 몽타주)에 대한 이론적 자료를 바탕으로, 감독이 어떻게 이러한 영화적 문법을 활용하여 주인공 윤영화(하정우)를 중심으로 전개되는 극한의 상황과 그 이면에 깔린 사회 비판적 주제의식을 효과적으로 강화했는지 심층적으로 분석하고자 합니다. 특히, 영화의 시각적 요소들이 어떻게 진실(에이콘, eikon)보다는 환영(판타스마, phantasma)을 추구하며 시청률에만 매몰된 현대 언론의 민낯을 폭로하고, 거대한 시스템 속에서 무력한 개인의 파국을 그려내는지 살펴봅니다. II. 시각 구성 요소의 구조적 분석 영화의 시각적 구성 요소는 단순히 내용을 전달하는 기능을 넘어, 그 자체로 감독의 논평이자 주석의 역할을 수행하며 주제를 표현합니다. 는 '실시간 생방송'이라는 형식적 제약 속에서 숏, 앵글, 미장센, 그리고 몽타주를 통해 긴장감을 극대화하고 다층적인 의미를 생성합니다. A. 미장센(Mise en scène) 및 공간 구성 미장센은 시각적인 재료가 프레임화, 촬영되는 방법 전체를 포괄하며, 영화의 기본적인 시각 요소 중 하나입니다. 의 미장센은 '폐쇄된 내부 공간'과 '붕괴하는 외부 세계'의 극적인 대비를 통해 주제의식을 강화합니다. 폐쇄적인 스튜디오와 통제 불가능성: 영화의 주요 배경은 여의도 SNC 방송국 라디오국, 구체적으로는 진행자 부스와 조종실로 나뉘는 방음 유리창 경계의 좁은 공간입니다. 이러한 좁은 공간은 외부의 거대한 사건(마포대교 폭발)과 대조되며, 테러 상황이 이 한정된 공간 안에서 통제 불가능하게 폭발적으로 커지는 아이러니를 시각적으로 구현합니다 [주제의식]. 프레임의 역할: 스튜디오는 외부 현실로부터 영화 세계를 구분하는 기본 단위인 프레임 역할을 하지만, 창밖으로는 한강과 마포대교가 한눈에 내려다보이는 '열린 창문'의 기능도 동시에 수행합니다. 이 창문 너머로 시커먼 연기와 불길이 보이며, 붕괴된 다리 상판이 한강에 처박히는 모습이 직접적으로 노출됩니다. 이는 방송국이라는 매체가 현실과 완전히 분리될 수 없으며, 사건의 '볼거리화'가 이 프레임을 통해 이루어짐을 보여줍니다. 기술적 미장센의 변화: 라디오 생방송이 TV 속보로 변환되면서, 스튜디오 내부에 다양한 각도의 멀티 카메라와 타 방송국 중계화면을 모니터링할 수 있는 멀티 모니터가 설치됩니다. 이러한 기계적 장치들은 공간을 복잡하게 만들고, 윤영화의 테러 중계가 '기술적 복제에 의한 전시가치'를 가지도록 합니다. 조명기 설치를 위해 창문 블라인드를 내리는 행위는 외부의 현실(마포대교)을 가리고 내부에서 만들어지는 '조작된 현실'을 위한 무대화 과정임을 암시합니다. 공간 분리와 인물의 이중성: 진행자석이 있는 부스와 제작진이 있는 조종실 사이의 방음 유리창은 인물 간의 관계와 심리를 드러냅니다. 윤영화는 헤드폰(인이어)과 토크 백 마이크를 통해서만 외부와 소통이 가능하며, 이는 그가 거대한 시스템에 갇힌 채 [주제의식] 통제당하거나 또는 스스로 고립을 자초하는 상황을 시각적으로 나타냅니다. 특히 차국장(보도국)이 윤영화를 이용해 시청률을 올리려 할 때, 부스 유리를 사이에 둔 이들의 대화는 긴장과 불신을 증폭시키며, 이들이 진실을 추구하기보다 개인의 욕망을 위해 움직이는 악인들이라는 주제의식을 강화합니다. B. 숏(Shot) 및 앵글(Angle)의 활용 숏과 앵글은 피사체와의 거리 및 카메라 위치를 통해 감독의 논평을 전달하며, 인물의 심리 상태, 권력 관계, 그리고 상황의 긴박함을 극적으로 표현합니다. 클로즈업(Close-up)과 윤영화의 내면: 영화는 윤영화의 심리 변화와 감정선을 면밀히 따라가기 위해 클로즈업(CU) 또는 익스트림 클로즈업(ECU)을 효과적으로 사용합니다. 테러범과의 통화 직후, 윤영화가 자신의 복귀를 위해 테러범의 녹음 파일을 챙기고, 전기 면도기를 꺼내 수염을 깎고, 넥타이를 매는 능숙한 손놀림 등의 클로즈업은 그가 개인의 성공과 복귀 [주제의식]라는 야망을 위해 국가적 재난을 이용하려는 언론인의 이중성을 드러냅니다 [주제의식]. 반면, 인이어에 폭탄이 장착되었음을 깨닫는 순간, 윤영화가 인이어를 빼려다가 멈추고 빨간색 LED 램프가 깜빡이는 장면은 ECU로 처리되어, 윤영화의 극한의 공포와 함께 의미와 스토리의 생성을 지연시키며, 통제 불가능한 상황에 놓인 개인의 무력감을 강조합니다 [주제의식]. 주진철 청장의 폭사 직후, 윤영화가 피로 얼룩진 종이에 메모를 하려다 펜이 희미해지는 장면이나, 주진철의 피를 보고 헛구역질하는 장면 등은 윤영화가 겪는 도덕적 혐오와 극심한 스트레스를 CU로 포착하며, 그의 갈등과 최소한의 양심을 드러냅니다. 하이 앵글과 로우 앵글의 대비: 앵글은 인물의 힘과 지위를 시각적으로 나타내는 중요한 도구입니다. 하이 앵글 (High Angle) 또는 버즈 아이 뷰 (Bird's-eye view)는 마포대교 폭발 현장이나 인질이 된 시민들을 비추는 장면에 사용됩니다. 이는 테러범의 요구가 무시당하고 인질들이 처한 위태로운 상황을 객관적으로 보여주며, 인물들의 절망, 패배, 왜소함을 전달하고, 거대한 사회 시스템 앞에서 약자의 목소리가 쉽게 묻히는 [주제의식] 현실을 시각화합니다. 반대로 로우 앵글 (Low Angle)은 권력이나 폭력이 강조되는 순간에 사용될 수 있습니다. 비록 대본에서 직접적인 앵글 지시는 없으나, 경찰청장 주진철이 박노규의 신상 정보를 공개하며 테러범을 몰아붙이는 위압적인 순간이나, 시청률 70%를 목표로 협상 불가 방침을 강요하는 차국장의 모습은 로우 앵글을 통해 그들의 강인함(혹은 악함)과 우월감을 묘사하는 데 효과적으로 이용될 수 있습니다. 핸드헬드 숏과 불안정성: 핸드헬드 숏(Handheld Shot)은 현장의 빠르고 즉흥적인 움직임이나 폭동, 액션 묘사에 적합하며, 불안감과 긴장감을 증폭시킵니다. 이지수 기자가 마포대교 현장에서 휴대전화를 통해 생중계하는 장면은 화면이 고르지 못하고 흔들리는 핸드헬드의 특성을 가집니다. 이는 가장 올곧은 인물인 이지수가 처한 극심한 불안과 위태로운 상황을 관객에게 촉각적으로 전달합니다. 이지수가 결국 교각 붕괴에 휘말려 비참한 최후를 맞는 결말은 핸드헬드 숏의 불안정한 시각 효과를 통해 약한 것은 죄악이라는 냉혹한 주제의식을 극적으로 강화합니다. C. 몽타주(Montage)와 편집의 주제적 기능 몽타주는 촬영된 필름들을 주제나 스토리에 따라 연결하는 편집(Editing)을 의미하며, 이미지들의 충돌, 병치, 반복을 통해 두 개의 숏에 없던 숨겨진 의미를 산출하여 주제의식을 명확하게 드러냅니다. 교차 편집(Cross Montage)과 긴장감 극대화: 는 테러범과의 전화 통화(내부)와 마포대교 상황(외부), 그리고 경찰의 추적 작전을 쉼 없이 교차 편집합니다. 이 교차 몽타주는 동시에 전개되는 두 가지 이상의 사건을 뛰어넘어 통제 불가능한 긴장과 감정을 고조시킵니다. 특히 윤영화가 방송국 내에서 자신의 생명(인이어 폭탄)을 위협받는 동시에, 그의 전처(이지수)가 마포대교 현장에서 위험에 처하는 상황을 교차할 때, 관객은 윤영화의 개인적 고통과 공적인 책임감 사이의 딜레마를 더욱 강하게 체감하게 됩니다. 경찰청장 주진철이 폭사한 후, 윤영화가 차국장에게 \"인질이 죽어야 테러가 끝난다?\"는 메모를 작성하는 순간과 차국장의 냉혹한 딜(본부장 승진)이 교차될 때, 이는 언론과 권력이 인명을 도구화하는 사회적 부조리함을 주제적으로 명확히 드러내는 기능을 합니다. 평행 몽타주(Parallel Montage)와 언론의 타락: 영화는 여러 대의 모니터를 통해 SNC를 포함한 KTN, APN, EATV 등 타 방송국 화면을 끊임없이 보여줍니다. 이 모니터들은 평행 몽타주의 역할을 수행하며, 이질적인 시공간을 한 장면에 편집하여 이야기의 다층적 구성을 가능하게 합니다. 테러범이 SNC뿐만 아니라 KTN에도 연락을 시도하고, KTN이 윤영화의 뇌물 수수 및 특종 가로채기 스캔들을 폭로하는 순간은, 재난 상황에서도 시청률 경쟁에 눈먼 언론의 타락 [주제의식]을 보여줍니다. KTN이 아반떼 추락 장면이나 주진철 폭사 장면을 슬로우모션으로 반복 리플레이하는 자막과 함께 테러범의 요구에 대한 찬/반 여론조사를 보여주는 것은, 진실 규명보다 자극적인 전시 [주제의식]와 여론 조작에 치중하는 미디어의 모습을 비판합니다. 이는 영화 매체의 시각적 효과(opsis)가 진리를 가리는 환영술(phantastike)로 변질될 수 있음을 시사합니다. 은유 몽타주(Metaphorical Montage)와 결말의 의미: 은유 몽타주는 서로 다른 사물 사이의 비슷한 특징을 부각시켜 관객의 연상을 불러일으켜 감독의 의도를 전달하는 방식입니다. 영화의 클라이맥스 직전, 윤영화는 인이어 폭탄이 \"가짜\"라는 테러범의 말에 속아 안도합니다. 그러나 직후, 실제 테러범(박노규의 아들, 박신우)과의 격투 끝에 윤영화는 총상을 입고 콘크리트 벽에 기대앉습니다. 이때, 그의 시선이 바닥에 떨어진 메모 [\"인질이 죽어야 테러가 끝난다?\"]에 머무르고, 그의 손에는 박신우에게서 미끄러진 폭파 스위치가 남겨집니다. 이때의 시각적 병치(메모, 폭파 스위치, 경찰 무전)는 은유 몽타주의 기능을 합니다. 테러범의 말대로 정부는 인질이 죽어야 테러범을 사살하고 여론을 잠재우려는 의도가 있었다는 것이 사실로 입증된 상황에서, 윤영화는 이 폭파 스위치를 이용해 최종적으로 자신과 자신을 둘러싼 부조리한 시스템 전체를 파국으로 몰아넣습니다. 윤영화의 손에 남겨진 스위치는 시스템의 부조리함에 대한 약자(테러범)의 분노이자 무력했던 언론인(윤영화)의 마지막 저항을 상징합니다. III. 시각 구성요소에 의한 주제의식 강화 전략 영화는 위에서 분석한 시각적 요소들을 주제의식의 각 층위와 긴밀하게 결합하여 메시지를 전달합니다. A. 언론의 타락과 매체 윤리 부재 강화 영화는 매체가 진실보다는 자극적이고 폭력적인 이미지(판타스마)에 매몰되는 과정을 시각적으로 집요하게 포착합니다. 진실보다 자극적인 이미지의 우위: 보도국장 차대은은 진실 규명이나 인명 구조보다 '생중계'라는 형식을 통해 더 자극적인 방송을 만들려 합니다 [주제의식]. 그는 테러 상황을 '야마를 휴먼코드로 가야 돼'라고 표현하며 '마지막엔 자수하는 걸로' 그림을 그리거나, 심지어 인질이 죽는 장면이 나와 줘야 테러범을 개새끼로 만들 수 있다고 주장합니다. 이러한 과정은 카메라의 움직임과 숏 선택에 반영됩니다. 고립된 차량의 인질들을 망원 렌즈로 포착하고, 차량 안에서 울고 있는 아이들을 줌인하여 위태로운 상황을 과장하는 연출은, 인명 구조보다 시청률을 위한 감각적 충격을 우선시하는 언론의 행태를 고발합니다. 윤영화 역시 프롬프터에 뜬 '충격적/테러/생방송' 단어를 사용하여 멘트를 진행하도록 강요받습니다. 언론인의 위선과 이중성의 시각화: 윤영화는 처음에 숙취 해소 음료를 마시는 흐트러진 모습에서, 테러 정보를 입수하자마자 면도와 넥타이 착용으로 '국민 앵커'의 외양을 되찾습니다. 이러한 시각적 변신은 윤영화가 자신의 복귀를 위해 테러범과 거래하는 언론인의 이중성을 상징적으로 보여줍니다 [주제의식]. 그러나 이러한 이미지(에이콘)는 결국 차국장이 건네준 넥타이와 차국장이 흘린 정보를 통해 만들어진 환영(판타스마)에 불과하며, 후원금 수수와 특종 가로채기 논란이 폭로되면서 그의 이미지는 완전히 붕괴됩니다. B. 사회적 약자의 목소리 시각화 테러범 박신우는 2년 전 마포대교 보수 공사 중 희생된 아버지(박노규)의 죽음에 대한 사과를 요구합니다. 영화는 시각적 요소를 통해 억압받는 하층 계급의 절규가 어떻게 무시되고 분노로 폭발하는지 보여줍니다 [주제의식]. 무시당하는 목소리와 분노의 폭발: 테러범 박신우(박노규)는 라디오 생방송 중 자신의 억울함을 호소하지만, 윤영화는 이를 '전기세 문제'라며 서둘러 마무리하고 전화를 끊어버립니다. 윤영화는 심지어 통화 종료 후 테러범에게 \"씨발새끼가 진짜…\"라고 욕설을 하는데, 이 욕설은 녹음되어 생방송 중 여과 없이 노출됩니다. 이처럼 청각적 요소(욕설)가 시각적 매체(TV 생방송)를 통해 확산되는 사건은, 약자의 억압된 목소리가 언론인 개인의 오만과 기득권층의 부조리(차국장이 윤영화의 욕설 파일을 KTN에 넘긴 정황) 속에서 무시당하다가 결국 폭발을 통해 거대한 시각적 사건으로 치환되는 과정을 상징합니다. 권력에 의한 약자의 왜곡된 재현: 주진철 경찰청장이 박노규의 신상 정보(사진)를 카메라 앞에 비추는 장면은 시각 매체(TV)를 통해 테러범의 얼굴을 공개하고 그를 '흥청망청 술하고 노름으로 탕진하다가 국가를 원망하는 쓰레기'로 재현하려는 권력의 의도를 시각화합니다. 이러한 모사(에이콘)는 합리적인 재현 규칙에 근거해야 하지만, 실제로는 진실과 비진리의 경계를 허무는 환영(판타스마)이 됩니다. 나중에 박노규가 사실은 2년 전 사망한 인부이며, 현재의 테러범은 그의 아들 박신우임이 밝혀집니다. 권력(경찰/정부)이 약자(박노규)의 신상 정보를 악의적으로 왜곡하여 여론을 조작하려 했음이 시각적으로 증명되며, 불평등한 구조와 기득권층의 비열함이라는 주제가 강화됩니다. C. 통제 불가능한 현대사회와 개인의 무력함 영화는 예측 불가능한 폭발과 통제 상실의 순간들을 강렬한 시각 효과로 연출하여, 미디어와 사회 시스템의 불안정성을 보여줍니다 [주제의식]. 도미노 현상과 파국: 테러범은 여의도 최고층 빌딩 JR 타워를 폭파하여 SNC 방송국 건물에 충돌하게 만드는 도미노 현상을 일으킵니다. 이 거대한 건축물의 붕괴 장면은 통제 불가능한 현대사회의 무력한 개인 [주제의식]에게 가해지는 파국을 시각적으로 극대화합니다. 경찰의 진압 작전 중 빌딩이 급격히 쓰러지는 모습은 폭력적이고 혼란스러운 느낌을 포착하는 데 용이한 로우 앵글이나 사각 앵글(Canted Angle)을 통해 연출되어, 긴박함과 불안감을 암시할 것입니다. 윤영화가 부서진 스튜디오 바닥에 쓰러졌을 때, 카메라가 쓰러지고 시그널 아웃되는 연출은 단순한 방송 사고를 넘어, 언론인 윤영화가 기댈 수 있던 모든 시스템과 매체적 안정성이 완전히 붕괴되었음을 시각적으로 선언합니다. 프리즈 프레임(Freeze Frame)과 비참한 최후: 영화의 결말은 윤영화가 권력과 언론에 이용당하다가 결국 거대한 시스템에 맞서 폭파 스위치를 누르는 순간으로 끝납니다. 이때, 폭파 스위치를 누르는 윤영화의 모습이 프리즈 프레임(Freeze Frame)과 유사하게 처리된다면, 움직이는 화면 속 정지된 화면을 통해 강렬하고 극적인 인상과 여운을 남기는 효과를 창출할 수 있습니다. 윤영화는 테러범에게 폭탄이 가짜임을 알았지만, 결국 자신을 궁지로 몰아넣은 사회와 정부 시스템(검찰 소환, 뇌물수수 혐의 조작)에 대한 개인의 마지막이자 가장 극렬한 복수를 감행합니다. 이 파국적인 선택은 통제 불가능한 사회 속에서 무력했던 개인의 성찰과 변화의 결과이며, 가장 비극적인 방식으로 주제의식을 완성합니다. IV. 결론: 감각적 효과(Opsis)와 비판적 성찰 영화 는 '생중계'라는 고유의 매체적 형식을 활용하여, 영화의 고유한 문법인 감각적 효과(opsis)를 극대화하고 주제의식을 관객에게 묵직하게 전달합니다. 미장센을 통해 폐쇄된 공간과 외부의 파국을 대비시키고, 클로즈업을 통해 윤영화의 이중성과 고뇌를 포착하며, 몽타주를 통해 언론의 타락과 권력의 비열함을 폭로함으로써, 약한 것은 죄악이며 언론은 이미 진실을 잃었다는 비판적 메시지를 강화했습니다. 특히, 카메라의 움직임, 앵글, 그리고 편집의 모든 선택은 '기술과 의미, 사유와 감각'의 결합 속에서 이루어졌으며, 이는 관객에게 공감각적 소통 방식과 쇼크 효과를 제공하여, 단순한 여흥을 넘어 현대 사회의 문제에 대한 비판적 성찰을 요구합니다. 최종적으로 윤영화가 사회 시스템에 대한 복수를 감행하는 비극적인 결말은, 관객에게 우리는 스크린의 틀 안에서 무엇을 보고 싶은가?라는 근본적인 질문을 던지며, 영화가 가진 세상을 변화시킬 수 있는 힘을 증명하는 것입니다.",
      "frontmatter": {
        "date": "2025-10-23T06:26:47+09:00",
        "lastmod": "2025-10-23T06:48:34+09:00"
      }
    },
    "university 딥러닝(기술 동향)": {
      "path": "/university-딥러닝기술-동향/",
      "filename": "university 딥러닝(기술 동향)",
      "content": "제1주 강의 소개, 인공지능 개요(정의, 유형, 역사, 동향 분석) 제2주 머신러닝 시스템 정의, 머신러닝 학습 과정, 학습 유형(지도학습, 비지도학습, 강화학습 등) 제3주 머신러닝 데이터 엔지니어링 – 데이터 소스, 포맷, 데이터 스토리지와 처리방법, 데이터 플로우(flow)에 대한 이해, 데이터 불균형 문제와 해결방법), 머신러닝 모델 배포 제4주 딥러닝의 기본 개념(딥러닝의 정의, 딥러닝 구조, 딥러닝 라이브러리) 제5주 다층 퍼셉트론, 다양한 신경망 아키텍처 제6주 CNN(합성곱 신경망) 기본 개념, CNN의 기술적 구조와 작동 방식 제7주 자연어 처리(NLP) 기초 - 최적의 자연어 처리 성능을 위한 텍스트 전처리 기법, 머신러닝 기법을 활용한 텍스트 분류, 대규모 언어 모델과 인공지능이 주도하는 과거, 현재, 미래 트렌드 분석 제8주 중간고사 제9주 LLM 정의 및 종류, 언어표현 방법, Transformer 모델과 대형 언어 모델의 기본 구조 이해 제10주 RAG & LangChain의 기초 개념과 활용 제11주 AI Agent와 MCP 제12주 프롬프트 엔지니어링의 개념과 중요성, LLM 기반 AI 시스템에서 최적의 응답을 이끌어내기 위한 프롬프트 설계 기법 생성형 AI 기술과 그 원리 이해 (GAN, DALL-E, ChatGPT) 제13주 클라우드 컴퓨팅에 대한 이해, 클라우드 기반 AI의 특징과 장점, 주요 클라우드 플랫폼 제14주 AI 기술을 비즈니스에 도입할 때의 리스크 관리 및 책임 문제, AI 기술의 윤리적 과제와 법적 고려 사항 이해, AI 시스템의 신뢰성 개념과 핵심 구성요소 제15주 기말고사 1주",
      "frontmatter": {
        "date": "2025-10-23T06:07:26+09:00",
        "lastmod": "2025-10-23T06:08:38+09:00"
      }
    },
    "vscode makefile 자동화": {
      "path": "/vscode-makefile-자동화/",
      "filename": "vscode makefile 자동화",
      "content": "",
      "frontmatter": {
        "date": "2025-10-17T21:57:11+09:00",
        "lastmod": "2025-10-17T21:57:27+09:00"
      }
    },
    "window 10 에서 11 로 이전 과정": {
      "path": "/window-10-에서-11-로-이전-과정/",
      "filename": "window 10 에서 11 로 이전 과정",
      "content": "window 측 chrome firefox brave 개발 환경 설정 visual studio vscode git bash jetbrain docker desktop mysql workbench pgadmin (web 앱) postman jetbrain toolbox cursor? draw io everything 게임 steam epic games 기타 word powerpoint excel 카카오톡 스티커 메모 google drive window to do notion obsidian 및 gitbash 설정 typora typedown telegram desktop vlc zoom power toy window 터미널 한컴 myserver.bat : ssh shinnk@shinnk.iptime.org -p 10293 wslexit.bat @echo off echo WSL poweroff.... wsl --shutdown # Settings apply across all Linux distros running on WSL 2 [wsl2] # Limits VM memory to use no more than 4 GB, this can be set as whole numbers using GB or MB memory = 10GB # Sets the VM to use two virtual processors processors = 12 # Sets amount of swap storage space to 8GB, default is 25% of available RAM swap=8GB wsl 측 wslconfig 설정 python alias 설정 uv install node fnm bash bashrc.d 기타 pueue — 백그라운드 작업 큐 매니저",
      "frontmatter": {
        "date": "2025-10-16T18:36:49+09:00",
        "lastmod": "2025-10-28T04:45:40+09:00"
      }
    },
    "개인 블로그 사이트 후기": {
      "path": "/개인-블로그-사이트-후기/",
      "filename": "개인 블로그 사이트 후기",
      "content": "메모 관리 나는 평소 공부나 여러가지 들을 기록하는 습관이 있다 노트들을 엄청 이동한 경험이 있다 현재 나는 obsidian 으로 순수한 markdown 을 통해 내가 혼자 공부했던 것들이나 메모할 것들을 적어두고 있다 원래는 onenote, notion 같은 것들을 사용했지만 데이터의 소유권을 온전히 가질 수 없다는 느낌이 들었다 특히 export 할 때 pdf 로 받거나 html 로 받거나 할 때 노트를 이동하거나 할 때 깨지거나 하는 문제가 발생했다 이러한 느낌이 들고 난 후 그냥 순수한 마크다운을 편집하는 것에 익숙해지는 것이 장기적으로 일관성 있도록 이러한 메모를 정리할 수 있다는 생각이 들었다 노트 관리 obsidian 으로 관리하는 markdown 은 [[]] 같은 특수 문법을 사용하지 않고 frontmatter 만 도입해서 최대한 markdown systex 에 적혀 있는 사양대로 사용중이다 이렇게 사용하다 보니 언젠가 부터 원하는 파일을 적절히 다른 사람들에게 보여주고 싶다는 생각이 들었다 branch, 티스토리, 네이버 등등 여러 방식으로 할 수 있지만 나만의 사이트로 처리하고 싶다는 생각이 들었다(광고도 적당히 붙히고) 백엔드를 두고 싶지 않아 사이트를 만들되 초기에는 단순히 공유 목적으로 사용하려고 했다 그래서 백엔드를 두는 것이 관리 측면에서 비효율 적이라고 느꼈다 그래서 생각해낸 방식이 이것이다 블로그 사이트 이지만 실제 모든 처리는 브라우저 내(사용자)에서 처리된다. markdown => html 코드 하이라이팅, latex 렌더링 등등 모든 것을 사용자 브라우저에서 해결한다 단 현재 블로그에서 어떤 파일이 존재하는지는 미리 알려주어야 한다 이를 위해 index.html 에 진입시 사전에 생성된 JSON 파일을 데이터 소스로 활용하여 동작하도록 설계하였다 데이터 소스는 1. file_index.json ---------> .src/js/ui/file-tree.js (파일 트리) 2. search_index.json ---------> .src/js/core/search.js (검색 엔진) 3. graph_index.json ---------> (그래프 뷰 기능) 4. recent_index.json ---------> (최근 문서 탭 기능) 5. tag_index.json ---------> (태그 클라우드/목록 기능) 이런식으로 미리 python 스크립트를 만들어 전체 문서를 적절히 인덱싱해서 보여주는 방식이다 이러한 방식으로 사이트를 구성하면서 웹 표준을 모두 지키려고 했지만 결국 아쉬운 문제가 발생했다 블로그 사이트의 경우 그 특성상 모든 트래픽이 구글검색을 통해 발생하므로 정말 seo 가 가장 중요하다 하지만 spa 방식으로 만들어진 이 사이트는 seo 에서 별로 좋지 않은 성능을 가져온다 마크다운 파일을 서버측에 요청해 실시간으로 렌더링 한 후 보여주므로 시간 차가 발생할 수 밖에 없고 javascript 해석 전에는 htnl 에는 의미를 알 수 없는 코드 조각만 존재한다 기능이 늘어날 수록 비동기 처리가 많아진다 본문이 렌더링이 된다는 가정하에 붙일 기능들이 점점 많아진다. promise 를 통해 비동기 우선순위를 정해 적절히 처리할 수 있고 그렇게 해보았지만 너무나 많은 기능은 점점 개발 난이도를 향상시켰다 HTML5 History API 등등 여러가지 방식으로 spa 에서도 seo 를 향상 시키는 여러 방법들이 있지만 코드를 계속 건드리다 보니 이럴거면 그냥 이라는 생각이 들었다 추가적으로 유튜브 영상 임베딩, excalidraw, 등등의 기능을 붙일때도 혼자 개발하다 보니 모든 규칙을 내가 세우고 정의하고 개발해서 점점 힘이 부친다는 생각이 들었다 모든 것을 포스팅 거이 대부분 즉 완결성 있도록 문서가 적혀 있지 않더라도 공유 대상 즉 포스팅 대상으로 삼았지만 그냥 완결성 있도록 적힌 것들만 공유하자는 생각이 들었다 Hugo가 특별하게 처리하는 Front Matter 변수들 Hugo는 콘텐츠 파일의 메타데이터를 정의하는 Front Matter에 특정 변수들을 사용하여 웹사이트를 생성하고 관리하는 데 활용합니다. 다음은 사용자가 제공한 목록에 추가적인 변수들을 통합하여 재구성한, Hugo가 특별하게 처리하는 주요 Front Matter 변수들과 그 기능입니다. 핵심 메타데이터 title : 콘텐츠의 제목을 정의합니다. 이 값은 페이지의 제목 태그( <title> )와 콘텐츠 목록 등 다양한 곳에서 사용됩니다. description : 콘텐츠에 대한 간략한 설명입니다. 이 값은 주로 검색 엔진 최적화(SEO)를 위해 HTML의 메타 설명 태그에 사용됩니다. draft : true 로 설정하면 해당 콘텐츠는 hugo 명령어로 사이트를 빌드할 때 생성되지 않습니다. 아직 완성되지 않은 초안 상태의 글을 관리하는 데 유용합니다. 없어도 빌드됨 summary : 콘텐츠의 요약 내용을 정의합니다. 목록 페이지 등에서 미리보기 텍스트로 사용될 수 있습니다. 날짜 관련 변수 date : 콘텐츠의 발행 날짜를 지정합니다. 이 값을 기준으로 콘텐츠가 정렬될 수 있습니다. lastmod : 콘텐츠의 마지막 수정 날짜를 의미합니다. 이 값을 명시적으로 설정하거나, Git의 커밋 기록을 통해 자동으로 설정할 수도 있습니다. publishDate : 콘텐츠가 공개될 날짜를 예약합니다. 이 날짜 이전에는 hugo 명령어를 실행해도 해당 콘텐츠가 빌드되지 않습니다. expiryDate : 콘텐츠의 유효 기간을 설정하여, 지정된 날짜 이후에는 사이트에서 보이지 않게 됩니다. 콘텐츠 분류 (Taxonomies) tags : 콘텐츠에 하나 이상의 태그를 지정하여 분류합니다. categories : 콘텐츠가 속할 카테고리를 정의합니다. 이는 기본적으로 분류되는 값들이면 사용자가 추가적으로 정의할 수 있음 기본값은 아니지만 사람들이 많이 사용하는 분류 기준 series : 연재물과 같이 여러 콘텐츠가 하나의 시리즈에 속할 경우 사용됩니다. 콘텐츠 제어 및 렌더링 layout : 특정 콘텐츠에 적용할 템플릿(레이아웃) 파일을 지정할 수 있습니다. 이를 통해 기본 템플릿 규칙을 무시하고 원하는 디자인을 적용할 수 있습니다. type : 콘텐츠의 유형을 지정합니다. 지정하지 않으면 콘텐츠 파일이 위치한 디렉토리(섹션) 이름으로 자동 설정됩니다. weight : 콘텐츠 목록의 정렬 순서를 지정하는 데 사용되는 숫자 값입니다. 값이 작을수록 목록의 앞쪽에 위치합니다. slug : URL의 일부로 사용될 문자열을 직접 지정합니다. 설정하지 않으면 일반적으로 파일 이름에서 자동으로 생성됩니다. url : 페이지의 전체 URL 경로를 덮어씁니다. slug 보다 더 포괄적인 제어가 필요할 때 사용됩니다. aliases : 이전 URL 주소를 이 콘텐츠로 리디렉션할 때 사용됩니다. 콘텐츠의 URL이 변경되었을 때 유용합니다. outputs : 해당 콘텐츠에 대해 생성할 출력 포맷을 지정합니다. 예를 들어, HTML과 함께 JSON이나 AMP 버전을 생성할 수 있습니다. headless : true 로 설정하면 해당 페이지는 자체 URL 없이 다른 페이지에서만 참조할 수 있는 \"헤드리스 번들\"이 됩니다. cascade : 특정 섹션의 모든 하위 콘텐츠에 Front Matter 값을 한 번에 적용(상속)할 때 사용됩니다. SEO 및 메뉴 관련 keywords : SEO를 위한 메타 키워드를 배열 형태로 지정합니다. linkTitle : 메뉴 등에서 표시될 짧은 제목을 지정합니다. title 이 너무 길 경우에 유용합니다. menu : 페이지를 특정 메뉴에 추가합니다. 메뉴 이름, 가중치(weight), 부모 메뉴 등을 지정할 수 있습니다. 기타 유용한 변수들 isCJKLanguage (또는 hasCJKLanguage ): 콘텐츠에 한중일 문자가 포함되어 있는지 여부를 수동으로 설정합니다. .WordCount 나 .Summary 생성에 영향을 줍니다. params : 위에서 언급된 예약된 변수 외에 사용자 정의 변수를 추가할 때 사용됩니다. 예를 들어, author 나 show_comments 와 같은 임의의 필드를 추가하고 템플릿에서 .Params.author 와 같이 접근할 수 있습니다. build : 빌드 옵션을 설정하는 맵(map)입니다. 예를 들어, 특정 페이지를 빌드에서 제외하거나 다른 처리를 지시할 수 있습니다. 이러한 Front Matter 변수들은 TOML, YAML, JSON 형식 중 하나로 작성할 수 있으며, 콘텐츠 파일의 맨 위에 위치하여 해당 파일의 속성을 정의하는 중요한 역할을 합니다. 휴고 테마 https://themes.gohugo.io/themes/hextra/ 휴고가 생성하는 종류 (.KIND) home : 초기 진입 페이지 page : 컴파일된 마크다운 => 일반 콘텐츠 section : 폴더 taxonomy : 분류 : 폴더 기반 이외의 접근 ( ex) tag, author.. ) 내가 설정할 수 있음 term : 특정 분류에 속한 파일 content/ └── books/ ├── book-1/ <-- kind = section │ └── index.md <-- kind = page └── book-2.md <-- kind = page 빌드 후 public/ ├── books/ │ ├── book-1/ <-- kind = section │ │ └── index. <-- kind = page │ ├── book-2.md <-- kind = page │ └── _index.md <-- kind = section ├── tags/ │ ├── fiction/ │ │ └── _index.md <-- kind = term │ └── _index.md <-- kind = taxonomy └── _index.md <-- kind = home {{ .Kind }} page 객체는 위에서 설명한 page 와는 다른 개념이며 page 는 Page object (페이지 객체) Hugo에서 페이지 객체(Page object)는 모든 출력 가능한 콘텐츠 단위를 나타내는 핵심 개념입니다. Hugo는 정적 사이트를 생성할 때, 모든 URL 경로를 하나의 Page 객체로 표현하며, 이 객체는 템플릿에서 다양한 정보와 메서드를 제공합니다. Page 는 Hugo가 렌더링하는 모든 콘텐츠의 추상화된 표현입니다. 단일 게시물, 섹션 목록, 홈, 태그 목록 등 모든 것이 Page 입니다. 속성 설명 모든 출력은 Page Hugo는 Page 가 아닌 콘텐츠는 렌더링하지 않음 .Kind 로 유형 구분 home , section , page , taxonomy , term 등 템플릿에서 직접 사용 {{ .Title }} , {{ .Content }} , {{ .Pages }} 등 메타데이터 포함 front matter, URL, 날짜, 카테고리, 태그 등 계층 구조 지원 .Parent , .Ancestors , .Children 등으로 탐색 가능 페이지 객체가 가지는 대표 변수/메서드 변수/메서드 설명 .Title 제목 ( title front matter 또는 파일명) .Content 마크다운 렌더링된 HTML .Summary 자동 또는 수동 요약 .RelPermalink 상대 URL .Params 사용자 정의 front matter 필드 .Pages 직속 자식 페이지 목록 (섹션에서만 의미 있음) .Sections 직속 자식 섹션 목록 .Parent 부모 페이지 (섹션 또는 홈) .Ancestors 조상 페이지 배열 ( [benefits, product-1, products, home] ) .IsPage , .IsSection 불리언 플래그 ✅ 모든 항목은 템플릿 내에서 {{ . }} 기준으로 사용 가능 예: {{ .Title }} , {{ range .Pages }}...{{ end }} 문자열(String) 속성 설명 예시 .Title title front matter 값. 없으면 파일명(확장자 제외) \"제품 소개\" .LinkTitle .Title 과 동일하나, URL 생성용으로 정제됨 \"제품 소개\" .Description description front matter 값 \"Hugo로 만든 정적 사이트\" .RelPermalink 상대 경로 URL (사이트 루트 기준) \"/products/\" .Permalink 절대 URL ( baseURL 포함) \"https://example.com/products/\" .URL .RelPermalink 와 동일 (호환성용) \"/products/\" .Type 콘텐츠 타입. front matter의 type 또는 최상위 섹션 이름 \"products\" , \"page\" .Kind 페이지 종류 \"home\" , \"section\" , \"page\" , \"taxonomy\" , \"term\" .Layout 사용 중인 레이아웃 이름 \"single\" , \"section\" .Lang 언어 코드 ( languages 설정 기반) \"ko\" , \"en\" 날짜(Time) 속성 설명 우선순위 .Date 게시일 date → publishDate → 파일 수정일 .PublishDate 명시적 게시일 publishDate front matter .Lastmod 최종 수정일 lastmod → Git log → 파일 수정일 .ExpiryDate 만료일 expiryDate front matter 💡 모두 Go의 time.Time 타입 → .Format \"2006-01-02\" 등 포맷 가능 콘텐츠 관련 속성 설명 .Content 마크다운 → 완전히 렌더링된 HTML .Plain .Content 에서 HTML 태그 제거한 순수 텍스트 .Summary 자동 또는 수동 요약 (마크다운에 <!--more--> 있으면 그 앞부분) .TableOfContents 자동 생성된 목차 (설정 필요) .WordCount 단어 수 .ReadingTime 예상 읽는 시간 (분 단위, 영문 기준) 계층 구조 관련 속성/메서드 반환값 설명 .Parent *Page 직속 부모 페이지 (없으면 nil ) .Ancestors []*Page 조상 페이지 배열. 자식 → 부모 → 조부모 → 홈 순 .Children Pages 직속 자식 페이지 ( .Pages 와 유사, 단 .Kind 무관) .Pages Pages 직속 자식 중 일반 페이지만 (섹션 제외) .RegularPages Pages .Pages 와 동일 (호환성) .Sections Pages 직속 자식 중 섹션만 .AllPages Pages 모든 하위 페이지 (재귀 포함) .RegularPagesRecursive Pages 모든 하위 일반 페이지 (재귀 포함) 💡 Pages 는 Hugo의 페이지 컬렉션 타입으로, range 가능하고 메서드 제공 (예: .ByDate , .ByWeight ) 분류(Taxonomy) 속성 설명 .Site.Taxonomies 전체 택사노미 맵 (전역) .Params.tags 현재 페이지의 태그 목록 [\"hugo\", \"web\"] .Params.categories 현재 페이지의 카테고리 목록 [\"tutorial\"] .GetTerms \"tags\" 태그에 해당하는 Term 페이지 객체 목록 반환 논리/상태 플래그 속성 설명 .IsHome 홈 페이지 여부 .IsPage 일반 콘텐츠 페이지 여부 ( Kind == \"page\" ) .IsSection 섹션 페이지 여부 .IsNode 리스트 페이지 여부 ( home , section , taxonomy , term ) .IsRenderable 렌더링 가능한지 여부 .Draft draft: true 여부 .Expired 현재 시간 > .ExpiryDate 여부 .FuzzyWordCount 대략적인 단어 수 (100, 200, 500 등 구간) 기타 유용한 메서드/속성 속성 설명 .File 파일 메타데이터 ( .File.Path , .File.Dir 등) .Site 전역 사이트 객체 ( {{ .Site.Title }} 등) .Resources 연결된 미디어 리소스 (Page Bundle 사용 시) .Weight 정렬 우선순위 ( weight front matter) .Params 사용자 정의 front matter 필드 전체 (맵) .Aliases 별칭 URL 목록 예시 브레드크럼 (Ancestors) {{ range .Ancestors.Reverse }} <a href=\"{{ .RelPermalink }}\">{{ .Title }}</a> » {{ end }} <span>{{ .Title }}</span> 섹션 하위 모든 글 목록 {{ range .RegularPagesRecursive }} <li><a href=\"{{ .RelPermalink }}\">{{ .Title }}</a></li> {{ end }} 태그 목록 출력 {{ range .Params.tags }} <span class=\"tag\">{{ . }}</span> {{ end }} 예시 2 (kind 별로 다르게 반환되는 예시들) home (홈 페이지) .Kind : \"home\" .Type : \"home\" (또는 사이트 설정에 따라 다름) .Sections : 전체 최상위 섹션 목록 반환 .Pages : 일반적으로 빈 목록 또는 최근 게시물 등 ( .Site.RegularPages 기반으로 설정 가능) .InSection : 항상 false .IsHome : true page (일반 마크다운 콘텐츠) .Kind : \"page\" .Type : 파일이 속한 폴더 이름 (예: posts/my-post.md → \"posts\" ) .Sections : 사용 불가 (빈 목록) .Pages : 사용 불가 (빈 목록) .InSection : true (해당 섹션 내에 있음) .Parent : 해당 섹션 페이지 객체 .IsNode : false section (섹션 페이지, 폴더 기반) .Kind : \"section\" .Type : 섹션 폴더 이름 (예: posts/ → \"posts\" ) .Sections : 하위 섹션 목록 (존재할 경우) .Pages : 해당 섹션에 속한 모든 일반 페이지 목록 .IsNode : true .IsHome : false .Parent : 상위 섹션이나 홈 페이지 taxonomy (분류 목록 페이지, 예: /tags/ ) .Kind : \"taxonomy\" .Type : 택사노미 이름 (예: \"tags\" ) .Pages : 모든 터미 (terms) 목록 (예: [\"hugo\", \"web\", ...] ) .Data.Singular : 터미 단수 이름 (예: \"tag\" ) .Data.Plural : 터미 복수 이름 (예: \"tags\" ) .IsNode : true term (특정 터미 페이지, 예: /tags/hugo/ ) .Kind : \"term\" .Type : 택사노미 이름 (예: \"tags\" ) .Pages : 해당 터미(예: hugo )에 속한 모든 일반 페이지 목록 .Data.Term : 현재 터미 값 (예: \"hugo\" ) .Data.Singular : \"tag\" .Data.Plural : \"tags\" .IsNode : true 공통 메서드/변수 .RelPermalink , .Permalink : 모두 유효 .Title , .Date , .Params : 모두 사용 가능 (단, home , section , taxonomy , term 은 front matter가 없으면 기본값 사용) .Content : 일반 page 에서만 실제 마크다운 콘텐츠가 있음. 나머지는 layout에서 직접 정의하거나 비어 있음. 활용 팁 템플릿에서 종류별 동작을 구분하려면 다음과 같이 사용할 수 있습니다: {{ if eq .Kind \"page\" }} <!-- 일반 페이지용 로직 --> {{ else if eq .Kind \"section\" }} <!-- 섹션 페이지용 로직 --> {{ else if .IsHome }} <!-- 홈 페이지용 로직 --> {{ end }} 혹은: {{ if .IsNode }} <!-- 섹션, 택사노미, 터미, 홈은 모두 IsNode=true --> {{ end }} layout 템플릿 종류 기본 템플릿 (Base Templates): 모든 페이지의 공통적인 뼈대를 제공합니다. 페이지 종류 템플릿 (Page Kind Templates): 홈페이지, 목록 페이지, 상세 페이지 등 페이지의 '종류'에 따라 다른 템플릿을 적용합니다. 콘텐츠 타입별 템플릿 (Content Type-Specific Templates): '책', '영화' 등 특정 콘텐츠 타입에만 적용되는 전용 템플릿입니다. 콘텐츠 뷰 (Content Views): 한 페이지를 여러 방식으로 (카드 형태, 리스트 형태 등) 렌더링할 때 사용됩니다. 부분 템플릿 (Partials): 헤더, 푸터처럼 여러 템플릿에서 재사용되는 코드 조각입니다. 숏코드 (Shortcodes): 마크다운 콘텐츠 파일 안에서 직접 호출하여 사용하는 템플릿 조각입니다. 렌더링 훅 (Render Hooks): 마크다운이 HTML로 변환될 때 기본 동작(예: 이미지, 링크 렌더링)을 재정의합니다. 핵심 개념: 파일명 규칙과 우선순위 [KIND|TYPE]/[LAYOUT|SINGLE|LIST].[OUTPUTFORMAT].[LANG].[SUFFIX] 가장 중요한 규칙: 더 구체적인 경로와 파일명이 일반적인 것보다 우선합니다. layouts/docs/page.html는 layouts/page.html보다 우선순위가 높습니다. layouts/term.mylayout.html는 layouts/term.html보다 우선순위가 높습니다. template 문법 지금부터 총 3개의 문서를 만들꺼야 document markdown 이름을 정해줘 all_workflow.md : 현재 나는 obsidian 에서 편집하고 obsidian valut 전체를 github 로 관리중임, 대부분의 resource(image, pdf, audio, video, asciinema(cast 확장자), excalidraw) 를 08.media 에 관리하고 있어 특수하게 excalidraw 는 04.Excalidraw 라는 폴더에 담아서 관리중, hugo 프로젝트를 동시에 따로 진행중이며 obsidian valut 전체를 content 폴더에 담아서 렌더링해서 정적 사이트를 만들고 있어, obsidian valut 는 push 될때 마다 .github/workflow/... 에 정의된 action 을 통해 github 가 제공하는 가상머신에서 hugo 프로젝트를 clone 하여 내부에 content 폴더에 obsidian valut 전체를 넣고 전처리를 진행하고, 빌드해서 public 폴더를 github page 에 배포하고 있어 resourcemanagementstrategy.md : 리소스는 2가지 종류로 나뉜다 사이트와 관련된 리소스, 문서와 관련된 리소스 (하단을 참조) linking_conventions.md : 문서에 적는 모든 링크에 관한 문서 markdown 문법 활용 리소스 : 2가지 종류의 링크가 있어 []() 링크와 ![]() 링크가 있고 2번째는 본래 이미지를 위한 문법이지만 추가적으로 여러 리소스를 적용하고 싶어 순수 html 활용 : <img src=\"../08.media/20251020040744-1760900864840-image.png\"> 이렇게 적은 local 상대경로 이미지도 올바르게 보이도록 해야해 local 경로와 외부 경로 : local 경로의 경우 모두 상대경로로 적혀 있으며 ../../08.media/sample.jpg 리소스 관리 나는 resourcemanagementstrategy.md 라는 문서를 작성할거야 리소스는 2가지 종류가 있어 문서와 관련된 리소스 -> 문서에서 보이는 이미지, 비디오 등 (image, pdf, audio, video, asciinema(cast 확장자), excalidraw) 를 08.media 에 관리하고 있어 특수하게 excalidraw 는 04.Excalidraw 라는 폴더에 담아서 관리중 사이트와 관련된 리소스 (static or asstes) -> 사이트 아이콘, 사이트 배너 이미지, 사이트 시연 동영상 문서에서 사용하는 리소스 대부분의 local 리소스가 content/08.media 경로에 있지만 없어도 올바르게 동작하도록 하고 싶어 local 리소스의 경우 2가지 방식으로 경로가 적혀 있을 수 있어 절대 경로 시작 : /08.media/sample.jpg 절대 경로로 적혀있는 경우 실제 절대경로가 아닌 content/08.media/sample.jpg 라는 의미야 remote 리소스의 경우 경로를 그대로 사용하면서 올바르게 리소스를 사용하면되 본래 markdown 문서에서는 ![]() 라는 문법은 오직 이미지를 위한 문법이지만 obsidian 에서 지원하므로 나는 video, audio 와 같은 리소스들도 이것으로 관리하고 있음 즉 이것으로도 올바르게 html 로 변환해서 렌더링 할 수 있어야함 local url break markdown 에 적힌 모든 경로는 상대경로로 적혀있어 이때 내부 올바르게 렌더링 되던것이 hugo 가 렌더를 진행하면서 아래와 같은 구조로 폴더 구조가 바뀌어 content ├── A/ │ ├── A-1/ │ │ └── a-1.md │ └── a.md ├── B/ │ └── b.md ├── excalidraw/ │ └── sample.excalidraw ├── media/ │ ├── sample.jpg │ └── sample.mp4 ├── c.md └── d.md public ├── A/ │ ├── A-1/ │ │ └── a-1/ │ │ └── index.html │ └── a/ │ └── index.html ├── B/ │ └── b/ │ └── index.html ├── excalidraw/ │ └── sample.excalidraw ├── media/ │ ├── sample.jpg │ └── sample.mp4 ├── c/ | └── index.html └── d/ └── index.html 일반적인 빌드된 접근은 {{baseURL}}/A/a/index.html 이 아닌 {{baseURL}}/A/a/ 으로 접근하며 hugo methon 를 통해 반환되는 값들오 후자와 동일한 방식이다 이렇게 되면서 경로 구조가 깨지는 문제가 있어 추가적인 문제를 해결하기 위해서는 상대경로를 해석해 무조건 절대경로로 만들어서 변환해야해 그럼에도 문제가 발생해 이를 render-image.html, render-link.html 을 올바르게 만들어야해 a.md 파일에서 c.md 파일 즉 일반 파일을 링크했을때 ../c.md 가 url 접속시 검색엔진 최적화를 위한 의미론적 태그 구조 의미론적(semantic) HTML 태그는 콘텐츠의 구조와 의미를 명확히 전달하는 데 사용됩니다. 이는 접근성(스크린 리더 호환), SEO, 유지보수성, 코드 가독성 모두에 큰 도움이 됩니다. 블로그 사이트에서 자주 쓰이는 의미론적 태그들을 용도별로 분류해 자세히 설명합니다. 🧱 1. 문서 전체 구조 (Document Outline) <header> 용도: 페이지나 섹션의 머리말(introductory content) 블로그 예시: 사이트 로고, 네비게이션, 검색창 특징: 문서 전체에 하나만 있을 필요 없음 → 각 섹션마다 <header> 가능 ✅ 비의미론적 대체: <div class=\"header\"> ❌ <header> <h1><a href=\"/\">내 블로그</a></h1> <nav>...</nav> </header> <main> 용도: 문서의 주요 콘텐츠(unique content) 블로그 예시: 글 본문, 포스트 목록 규칙: 페이지당 오직 하나만 사용해야 함 ✅ 비의미론적 대체: <div id=\"content\"> ❌ <main> <article>...</article> </main> <footer> 용도: 페이지나 섹션의 꼬리말(closing content) 블로그 예시: 저작권, 연락처, 관련 링크 특징: <header> 처럼 여러 번 사용 가능 ✅ 비의미론적 대체: <div class=\"footer\"> ❌ <footer> <p>© 2025 신년기. All rights reserved.</p> </footer> 📝 2. 콘텐츠 단위 <article> 용도: 독립적으로 배포/재사용 가능한 콘텐츠 단위 블로그 예시: 블로그 포스트, 댓글, 뉴스 기사 핵심: <article> 하나만 떼서 RSS나 피드에 넣어도 의미 있음 ✅ 비의미론적 대체: <div class=\"post\"> ❌ <article> <h2>마크다운 문법 정리</h2> <p>...</p> </article> <section> 용도: 테마별로 묶인 콘텐츠 그룹 블로그 예시: 소개 섹션, 관련 글 섹션, 댓글 섹션 주의: <section> 은 항상 제목(heading)과 함께 사용해야 함 ✅ 비의미론적 대체: <div class=\"section\"> ❌ <section> <h2>관련 글</h2> <ul>...</ul> </section> 💡 <article> 안에 <section> 이 들어가거나, 그 반대도 가능 예: 블로그 포스트( <article> ) 안에 \"요약\", \"코드 예제\" 섹션( <section> ) 🔗 3. 탐색 및 보조 콘텐츠 <nav> 용도: 주요 탐색 링크 모음 블로그 예시: 상단 메뉴, 사이드바 목록, 페이지네이션 주의: 모든 링크에 <nav> 쓰지 말 것 → 주요 이동 경로만 ✅ 비의미론적 대체: <div class=\"menu\"> ❌ <nav> <ul> <li><a href=\"/posts\">글 목록</a></li> <li><a href=\"/tags\">태그</a></li> </ul> </nav> <aside> 용도: 주 콘텐츠와 간접적으로 관련된 보조 정보 블로그 예시: 사이드바(태그 클라우드, 프로필, 최근 글), 인용문 핵심: <aside> 를 제거해도 본문 의미 손실 없음 ✅ 비의미론적 대체: <div class=\"sidebar\"> ❌ <aside> <h3>태그</h3> <ul>...</ul> </aside> 💡 <article> 안에 <aside> 를 넣어 포스트 관련 참고자료도 표현 가능 📚 4. 텍스트 의미 강조 <figure> + <figcaption> 용도: 이미지, 코드 블록, 수식 등에 캡션 추가 블로그 예시: 다이어그램 + 설명, 코드 예제 + 설명 장점: 콘텐츠와 캡션이 논리적으로 묶임 <figure> <pre><code>console.log(\"Hello\");</code></pre> <figcaption>JavaScript 출력 예제</figcaption> </figure> <blockquote> 용도: 다른 출처에서 인용한 긴 문장 블로그 예시: 명언, 다른 글 인용 권장: <cite> 로 출처 명시 <blockquote> <p>삶이 있는 한 희망은 있다.</p> <cite>— 키케로</cite> </blockquote> <time> 용도: 날짜/시간을 기계가 읽을 수 있게 표시 블로그 예시: 포스트 작성일, 이벤트 일정 속성: datetime 에 ISO 형식 사용 <time datetime=\"2025-10-20\">2025년 10월 20일</time> <mark> 용도: 참고 목적으로 강조된 텍스트 블로그 예시: 검색 키워드 하이라이트, 중요한 문장 강조 주의: <em> 이나 <strong> 과 다름 → 의미 강조가 아니라 시각적/참고 강조 <p>이 함수는 <mark>매우 중요</mark>합니다.</p> <address> 용도: 문서 또는 article의 연락처 정보 블로그 예시: 저자 이메일, 프로필 링크 범위: <article> 안에 있으면 글 저자, 문서 루트에 있으면 사이트 운영자 <address> 작성자: <a href=\"mailto:shinnk@example.com\">신년기</a> </address> 📌 5. 기타 유용한 의미론적 태그 태그 용도 블로그 활용 예 <details> + <summary> 접고 펼치는 콘텐츠 FAQ, 코드 설명 토글 <dialog> 모달/팝업 대화상자 구독 팝업, 알림 <output> 계산 결과 표시 폼 입력 결과 (드물게 사용) <progress> 진행률 읽기 진행률 바 <meter> 스코어/비율 만족도, 점수 ❌ 피해야 할 일반적인 오해 잘못된 사용 올바른 대안 <section> 을 스타일용 div처럼 남발 제목이 없다면 <div> 사용 <article> 없이 <section> 만 사용 독립 콘텐츠면 <article> 우선 <nav> 에 SNS 아이콘만 넣음 주요 이동 경로가 아니면 <div> ✅ 블로그 구조 예시 (의미론적 태그만 사용) <body> <header> ... </header> <main> <article> <header> <h1>의미론적 HTML이란?</h1> <time datetime=\"2025-10-20\">2025-10-20</time> </header> <p>...</p> <figure> <img src=\"...\" alt=\"...\"> <figcaption>의미론적 태그 구조</figcaption> </figure> </article> </main> <aside> <h2>프로필</h2> <address>...</address> </aside> <footer> <p>© 2025</p> </footer> </body> 물론입니다! 아래는 의미론적 HTML 태그를 동일한 포맷으로 추가로 소개한 내용입니다. 기존 내용과 일관된 스타일을 유지하며, 블로그나 기술 문서에서 유용하게 쓰이는 태그들을 중심으로 정리했습니다. 📚 6. 인용 및 출처 관련 태그 <cite> 용도: 작품, 문서, 사람 이름 등 출처를 명시할 때 사용 블로그 예시: 책 제목, 논문, 영화, 인용된 저자 이름 주의: 인용 문구 전체가 아니라 출처 자체에만 적용 ✅ 비의미론적 대체: <span class=\"cite\"> ❌ <p>이 아이디어는 <cite>클린 코드</cite>에서 영감을 받았습니다.</p> 💡 <blockquote> 안에서 <cite> 를 사용해 출처를 명확히 할 수 있음 <q> 용도: 짧은 인용문(inline quotation)을 표현 블로그 예시: 문장 중간에 다른 사람 말 인용 자동 처리: 브라우저가 자동으로 따옴표( “” )를 추가 ✅ 비의미론적 대체: \"...\" 또는 <span> ❌ <p>그는 이렇게 말했다: <q>성공은 실패를 거듭한 뒤 오는 것이다.</q></p> 💡 긴 인용문은 <blockquote> 사용, 짧은 인용문은 <q> 사용 📝 7. 코드 및 기술 콘텐츠 태그 <code> 용도: 짧은 코드 조각, 함수 이름, 명령어 등을 인라인으로 표시 블로그 예시: console.log() , git commit , 변수명 ✅ 비의미론적 대체: <span class=\"code\"> ❌ <p><code>Array.prototype.map()</code>은 새로운 배열을 반환합니다.</p> 💡 블록 전체 코드는 <pre><code>...</code></pre> 조합 사용 <kbd> 용도: 키보드 입력 또는 사용자 조작을 표현 블로그 예시: 단축키, 명령 입력 안내 ✅ 비의미론적 대체: <span class=\"key\">Ctrl+C</span> ❌ <p>복사하려면 <kbd>Ctrl</kbd> + <kbd>C</kbd>를 누르세요.</p> <samp> 용도: 프로그램의 출력 결과를 표시 블로그 예시: 터미널 출력, 오류 메시지 ✅ 비의미론적 대체: <code> 는 입력, <samp> 는 출력 <p>명령어 실행 결과: <samp>File not found</samp></p> <var> 용도: 변수, 수학 기호, 플레이스홀더 이름을 표현 블로그 예시: 알고리즘 설명, 수식 변수 ✅ 비의미론적 대체: <i> 나 <em> 은 의미 없음 <p>피타고라스 정리: <var>a</var><sup>2</sup> + <var>b</var><sup>2</sup> = <var>c</var><sup>2</sup></p> 🗂️ 8. 목록 및 정의 관련 태그 <dl> , <dt> , <dd> 용도: 정의 목록(definition list) — 용어와 설명 쌍 블로그 예시: 용어 사전, 설정 설명, FAQ 형식 ✅ 비의미론적 대체: <ul><li><strong>용어</strong>: 설명</li></ul> ❌ <dl> <dt>SEO</dt> <dd>검색 엔진 최적화(Search Engine Optimization)의 약자입니다.</dd> <dt>Hugo</dt> <dd>빠르고 확장 가능한 정적 사이트 생성기입니다.</dd> </dl> 💡 Hugo의 Goldmark는 기본적으로 정의 목록 확장 문법 지원 ( definitionList: true ) 📅 9. 시간 및 이벤트 관련 태그 <time> (추가 활용) 용도 확장: 이벤트 일정, 업데이트 시간, 읽는 데 걸리는 시간 등 SEO 효과: 구조화된 데이터로 검색 결과에 리치 스니펫 가능 <article> <header> <h1>새로운 기능 출시</h1> <p>게시일: <time datetime=\"2025-10-20\">2025년 10월 20일</time></p> <p>업데이트: <time datetime=\"2025-10-21T14:30:00+09:00\">오늘 오후 2시 30분</time></p> </header> </article> 🧩 10. 상호작용 및 상태 표현 태그 <details> + <summary> 용도: 접고 펼칠 수 있는 콘텐츠 제공 블로그 예시: 해설 코드, 정답 보기, 추가 정보 장점: JavaScript 없이 순수 HTML로 구현 가능 <details> <summary>정답 보기</summary> <p>정답은 42입니다.</p> </details> <dialog> 용도: 대화 상자(modal, popup)를 의미론적으로 표현 블로그 예시: 구독 유도 팝업, 쿠키 동의 창 (JavaScript와 함께 사용) 주의: 아직 모든 브라우저에서 완벽 지원은 아님 <dialog open> <p>이 글이 도움이 되셨나요?</p> <button>네</button> <button>아니요</button> </dialog> 📊 11. 진행 상태 및 측정 태그 <progress> 용도: 작업 진행률 시각화 블로그 예시: 강의 완료율, 읽기 진행률 (JavaScript로 동적 제어) <p>현재 글 읽기 진행률: <progress value=\"70\" max=\"100\">70%</progress></p> <meter> 용도: 범위 내 값(score, disk usage 등) 표시 블로그 예시: 만족도 평가, 성능 점수 <p>CPU 사용률: <meter min=\"0\" max=\"100\" value=\"65\">65%</meter></p> 💡 <progress> 는 \"진행 중\", <meter> 는 \"정적인 측정값\" ✅ 요약: 의미론적 태그 선택 가이드 콘텐츠 유형 추천 태그 독립 포스트 <article> 섹션/챕터 <section> + <h2~h6> 사이드바 <aside> 네비게이션 <nav> 코드 조각 <code> , <pre> , <kbd> , <samp> , <var> 인용 <blockquote> , <q> , <cite> 정의 <dl> , <dt> , <dd> 시간 <time> 강조(참고) <mark> 접기 콘텐츠 <details> , <summary> 의미론적 HTML은 검색엔진이 콘텐츠를 더 정확히 이해하도록 도와주며, 접근성 기준(WCAG)을 충족하는 데도 필수적입니다. Hugo 템플릿( layouts/ )을 작성할 때 위 태그들을 적극 활용하시면, SEO 친화적이고 유지보수 쉬운 블로그를 만들 수 있습니다. 필요하시면, Hugo shortcodes로 의미론적 태그를 자동 생성하는 방법도 알려드릴 수 있어요!",
      "frontmatter": {
        "date": "2025-10-17T05:04:15+09:00",
        "lastmod": "2025-10-30T23:39:32+09:00"
      }
    },
    "명지대 공지": {
      "path": "/명지대-공지/",
      "filename": "명지대 공지",
      "content": "명지대 공지사항 mcp 요느낌 gonggi_list = [] # 스케줄러 설정 scheduler = BackgroundScheduler() scheduler.add_job(scheduled_job, \"interval\", seconds=30) # 30초마다 폴링 @endpoint(/api/all_mju_gonggi): 홈페이지에서 공지 다가져와 (scraping으로 하던가, 홈페이지 엔드포인드 조지던가) gonggi_list = 응답.데이터 @endpoint(/api/갱신_시작): scheduler.start() @endpoint(/api/갱신_종료): scheduler.shutdown() 스케줄 안에서 @secadule old; map 자료구조 new = 가져와() if old == new { 넘어가 } else { old = new 실행() }",
      "frontmatter": {
        "date": "2025-10-29T15:09:00+09:00",
        "lastmod": "2025-10-29T15:09:07+09:00"
      }
    },
    "무제 1": {
      "path": "/무제-1/",
      "filename": "무제 1",
      "content": "pub/sub 아키텍쳐와 promise 기능 javascript 이벤트 위임의 효과와 그 기능 Drawing 2025-10-11 13.12.38.excalidraw",
      "frontmatter": {
        "date": "2025-09-29T04:58:08+09:00",
        "lastmod": "2025-10-06T23:21:26+09:00"
      }
    },
    "무제 10": {
      "path": "/무제-10/",
      "filename": "무제 10",
      "content": "분열 1: ┌─────────────────────────────────────────┐ │ 56 30 46 64 69 37 87 41 19 43 │ └─────────────────────────────────────────┘ ↓ 분열 2: ┌─────────────────────┐ ┌─────────────────────┐ │ 56 30 46 64 69 │ │ 37 87 41 19 43 │ └─────────────────────┘ └─────────────────────┘ ↓ ↓ 분열 3: ┌───────────┐ ┌───────┐ ┌───────────┐ ┌───────┐ │ 56 30 46 │ │ 64 69 │ │ 37 87 41 │ │ 19 43 │ └───────────┘ └───────┘ └───────────┘ └───────┘ ↓ ↓ 분열 4: ┌────┐ ┌──────┐ ┌──┐ ┌──┐ ┌──┐ ┌──────┐ ┌──┐ ┌──┐ │ 56 │ │ 30 46│ │64│ │69│ │37│ │87 41 │ │19│ │43│ └────┘ └──────┘ └──┘ └──┘ └──┘ └──────┘ └──┘ └──┘ ↓ ↓ 분열 5: ┌──┐ ┌──┐ ┌──┐ ┌──┐ │30│ │46│ │87│ │41│ └──┘ └──┘ └──┘ └──┘ ─────────────────────────────────────────────────────── ↑ (이제 합병 시작) ─────────────────────────────────────────────────────── 합병 6: ┌────────┐ ┌────────┐ │30 46 │ │41 87 │ └────────┘ └────────┘ 합병 7: ┌───────────┐ ┌────────┐ ┌───────────┐ ┌────────┐ │30 46 56 │ │64 69 │ │37 41 87 │ │19 43 │ └───────────┘ └────────┘ └───────────┘ └────────┘ 합병 9: ┌───────────────────┐ ┌────────────────────┐ │30 46 56 64 69 │ │19 37 41 43 87 │ └───────────────────┘ └────────────────────┘ 합병14: ┌───────────────────────────────────────┐ │19 30 37 41 43 46 56 64 69 87 │ └───────────────────────────────────────┘ 초기 : 56 30 46 64 69 37 87 41 19 43 분할1 : 56 30 46 64 69 | 37 87 41 19 43 분할2 : 56 30 | 46 64 69 | 37 87 | 41 19 43 분할3 : 56 | 30 | 46 | 64 69 | 37 | 87 | 41 | 19 43 분할4 : 56 | 30 | 46 | 64 | 69 | 37 | 87 | 41 | 19 | 43 병합1 : 30 56 | 46 | 64 69 | 37 | 87 | 41 | 19 43 병합2 : 30 56 | 46 64 69 | 37 87 | 41 | 19 43 병합3 : 30 56 | 46 64 69 | 37 87 | 19 41 43 병합4 : 30 46 56 64 69 | 19 37 41 43 87 병합5 : 19 30 37 41 43 46 56 64 69 87 dct, cvt도 자동변속기로 분류되지만, 작동원리와 승차감은 좀 다릅니다. dct는 수동변속기의 클러치를 두개 넣어서 자동으로 변속하게 한건데, 사용법이 자동변속기랑 같을뿐, 승차감은 수동에 가까운 변속기 입니다. cvt는 원뿔모양의 도르레 두개를 체인으로 연결한건데, 체인이 연속된 원뿔을 왔다갔다하므로, 유체변속기처럼 끊김없이 부드러운 승차감을 주는 동시에, 물리적으로 연결되어있어서 정속주행시 연비도 뛰어납니다. 다만, cvt는 이 체인이 갑작스런 변속시 미끄러질수 있어서, 내구성의 문제가 있죠. 같은 자동변속기 범주에 있지만, 물리적으로 완전히 다른 변속기들입니다.",
      "frontmatter": {
        "date": "2025-10-13T16:08:20+09:00",
        "lastmod": "2025-10-17T19:53:55+09:00"
      }
    },
    "무제 11": {
      "path": "/무제-11/",
      "filename": "무제 11",
      "content": "├── build // 빌드 결과 파일 프로젝트 경로와 완전히 일치 │ ├── src │ │ ├── binary_tree │ │ │ ├── binary_tree.d │ │ │ ├── binary_tree.o │ │ | ├── test │ │ | │ └── test.h │ │ │ ├── test.d │ │ │ ├── test.o │ │ │ └── test // /src/binary_tree/test.cpp 를 보고 있을때 f5 를 눌러서 생기는 파일 │ │ └── data_structure │ │ ├── data_structure.d │ │ ├── data_structure.o │ │ ├── test │ │ │ └── test.h │ │ ├── test.d │ │ ├── test.o │ │ └── test // /src/data_structure/test.cpp 를 보고 있을때 f5 를 눌러서 생기는 파일 │ ├── test // /test.cpp 를 보고 있을때 f5 를 눌러서 생기는 파일 │ ├── test.d │ └── test.o ├── Makefile ├── src │ ├── binary_tree │ │ ├── binary_tree.cpp │ │ ├── binary_tree.h │ │ ├── test │ │ │ └── test.h │ │ ├── test.cpp │ │ └── test2.h │ ├── data_structure │ │ ├── data_structure.cpp │ │ ├── data_structure.h │ │ ├── test │ │ │ └── test.h │ │ ├── test.cpp │ │ └── test2.h │ └── graph └── test.cpp { \"version\": \"2.0.0\", \"tasks\": [ { \"label\": \"(내가 만든) Build active file with Makefile\", \"type\": \"shell\", \"command\": \"make\", \"args\": [ \"MAIN_SRC=${relativeFile}\" ], \"group\": { \"kind\": \"build\", \"isDefault\": true }, \"problemMatcher\": [ \"$gcc\" ], \"detail\": \"Makefile을 사용하여 현재 활성화된 C++ 파일을 빌드합니다.\" } ] } CC=<컴파일러> CFLAGS=<컴파일 옵션> LDFLAGS=<링크 옵션> LDLIBS=<링크 라이브러리 목록> OBJS=<Object 파일 목록> TARGET=<빌드 대상 이름> all: $(TARGET) clean: rm -f *.o rm -f $(TARGET) $(TARGET): $(OBJS) $(CC) -o $@ $(OBJS) 이것은 큰 틀로 해서 확장 { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"(내가 만든) Launch C++ file with Makefile\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"${workspaceFolder}/build/${relativeFileDirname}/${fileBasenameNoExtension}\", \"args\": [], \"stopAtEntry\": false, \"cwd\": \"${workspaceFolder}\", \"environment\": [], \"externalConsole\": false, \"MIMode\": \"lldb\", \"setupCommands\": [ { \"description\": \"gdb 또는 lldb에 자동 서식 지정 사용\", \"text\": \"-enable-pretty-printing\", \"ignoreFailures\": true } ], \"preLaunchTask\": \"Build active file with Makefile\" } ] } manual 이 존재하는 이유 => MAIN_SRC 를 넘겨서 어떤 파일을 기준으로 컴파일 하는지 알려주어야해 그리고 그것을 기준으로 컴파일 프로젝트 전체에 여러개의 main 이 존재할 수 있으므로 컴파일러 자체의 의존성 분석 기능(g++ -MM)을 활용해서 적절한 파일만 적절히 링크 @.src/js/core/router.js 현재 file 파라미터를 HTML5 History API 를 사용하여 path 기반으로 처리하고 싶은데 계획을 세워줘 최상위 경로는 실제 raw 파일을 접근할때 필요하므로 다른 url 구조를 원해 먼저 어떻게 url 구조를 설계할지 부터 말해줘 아직 코드는 수정하지마 ✦ 안녕하세요! 현재 ?file= 쿼리 파라미터를 사용하는 URL 구조를 HTML5 History API를 활용한 경로 기반 방식으로 변경하는 계획에 대해 문의주셨군요. 최상위 경로를 다른 용도로 사용하고 싶어하시는 점까지 고려하여 다음과 같은 URL 구조를 제안합니다. 제안하는 URL 구조 기존 URL 구조를 보다 의미 있고 현대적인 방식으로 개선하기 위해, 애플리케이션의 페이지를 표시하는 모든 URL에 /view/ 라는 접두사를 사용하는 것을 제안합니다. 기본 경로(Base Path): /view/ 노트 내용을 보여주는 모든 페이지는 이 경로 아래에 위치합니다. 이렇게 하면 말씀하신 대로, 웹사이트의 최상위 경로 (/)는 원본 파일(raw file) 접근 등 다른 목적으로 자유롭게 사용할 수 있습니다. 파일 경로: /view/ 접두사 뒤에 표시할 파일의 경로를 그대로 사용합니다. 예시: notes/project-a.md 파일을 보려면 URL은 /view/notes/project-a.md가 됩니다. 부가 정보 (해시, 하이라이트 등): 제목 링크를 위한 해시(#)나 검색어 하이라이트를 위한 쿼리 파라미터는 기존 구조와 동일하게 유지합니다. 예시: /view/notes/project-a.md?highlightTerm=important#section-2 홈 화면 (초기 페이지): 파일이 선택되지 않은 초기 화면은 /view/ URL로 접근합니다. 기존 구조와 새 구조 비교 ┌────────────────────┬──────────────────────────────────────┬─────────────────────────────────────┐ │ 상황 │ 현재 URL 구조 (?file=...) │ 제안하는 URL 구조 (/view/...) │ ├────────────────────┼──────────────────────────────────────┼─────────────────────────────────────┤ │ 노트 보기 │ /?file=notes/project-a.md │ /view/notes/project-a.md │ │ 특정 제목으로 이동 │ /?file=notes/a.md#heading-1 │ /view/notes/a.md#heading-1 │ │ 검색어 하이라이트 │ /?file=notes/a.md&highlightTerm=test │ /view/notes/a.md?highlightTerm=test │ │ 초기 화면 │ / 또는 /?file= │ /view/ │ └────────────────────┴──────────────────────────────────────┴─────────────────────────────────────┘ 이 구조의 장점 직관적인 URL: URL만 봐도 어떤 콘텐츠를 보고 있는지 명확하게 알 수 있습니다. 경로 충돌 방지: 최상위 경로를 다른 목적으로 사용할 수 있도록 하여 유연성을 확보합니다. 표준적인 방식: 많은 최신 단일 페이지 애플리케이션(SPA)에서 사용하는 검증된 방식입니다. 이 URL 구조 설계에 동의하시면, 다음 단계로 이 구조를 실제로 코드에 적용하기 위한 구체적인 계획을 세워드리겠습니다. 참고: 이 변경을 적용하려면, 개발 및 실제 운영 환경에서 /view/로 시작하는 모든 요청을 index.html 파일로 전달하도록 서버 설정 변경이 필요할 수 있습니다. 이 부분은 구현 단계에서 다시 한번 자세히 안내해 드리겠습니다. 하이라이트의 경우는 프래그먼트 식별자로 처리하는 것이 더 웹에 어울려 https://example.com#:~:text=[prefix-,]textStart[,textEnd][,-suffix] /view/notes 로 처리하는 이유가 있어? 그냥 실제 없을 거 같은 이름의 경로로 하면 될거 같은데 어떻게 생각해 github page 에서도 동작하게 하고 싶어 따로 웹서버 설정을 만지지 않고 처리할려면 어떻게 해야 하는 거야 seo 친화적이고 싶은데 임시로 경로의 markdown 파일의 내용을 index.html 에 넣고 검색엔진이 인덱싱 처리를 할 수 있도록 하는 것은 어때? 전반적인 계획을 다시 제시해줘 ✦ 네, 좋은 질문들입니다! 웹 표준, GitHub Pages 배포, SEO까지 고려한 덕분에 훨씬 더 완성도 높은 계획을 세울 수 있겠습니다. 주신 의견들을 모두 반영하여 전반적인 계획을 다시 제시해 드립니다. 새로운 계획: URL 구조, GitHub Pages 배포, SEO URL 구조: 경로 기반 및 웹 표준 하이라이트 핵심: GitHub Pages에서 서버 설정 없이도 동작하고, 웹 표준을 최대한 활용하는 '깨끗한 URL(Clean URL)' 구조를 채택합니다. 라우팅 방식: 기존의 ?file= 이나 제안했던 /view/ 대신, 파일 경로 자체를 URL 경로로 사용합니다. 예시: notes/project-a.md 파일을 보기 위한 URL은 https://.github.io//notes/project-a.md가 됩니다. 하이라이트 방식: 쿼리 파라미터(?highlightTerm=...) 대신, 웹 표준인 텍스트 프래그먼트(Text Fragments)를 사용합니다. 이는 브라우저가 URL의 일부로 특정 텍스트를 찾아 하이라이트하는 기능입니다. 구조: #:~:text=[접두사-,]시작텍스트[,끝텍스트][,-접미사] 예시: .../notes/project-a.md#:~:text=중요한 내용 최종 URL 구조 비교 ┌───────────────────┬─────────────────────────────┬────────────────────────────────────────────────┐ │ 상황 │ 기존 │ 새로운 제안 │ ├───────────────────┼─────────────────────────────┼────────────────────────────────────────────────┤ │ 노트 보기 │ /?file=notes/a.md │ /notes/a.md │ │ 특정 제목 이동 │ /?file=notes/a.md#heading-1 │ /notes/a.md#heading-1 (변경 없음, 가장 이상적) │ │ 텍스트 하이라이트 │ /?file=notes/a.md&h=test │ /notes/a.md#:~:text=test (웹 표준) │ └───────────────────┴─────────────────────────────┴────────────────────────────────────────────────┘ GitHub Pages 배포 전략: '404.html' 트릭 말씀하신 대로 GitHub Pages는 Apache의 .htaccess나 Nginx의 rewrite 같은 서버 설정을 지원하지 않습니다. 이 문제를 해결하기 위해 SPA(단일 페이지 애플리케이션)에서 널리 사용되는 영리한 방법인 '404.html 트릭'을 사용합니다. 동작 원리: 사용자가 https://.../notes/project-a.md에 접속하면, GitHub Pages 서버에는 실제로 해당 경로에 파일이 없으므로 404 Not Found 오류가 발생합니다. 이때, 서버는 우리가 미리 만들어 둔 맞춤 404.html 파일을 사용자에게 보여줍니다. 우리는 index.html의 모든 내용을 그대로 복사해서 404.html을 만듭니다. html 안의 스크립트(router.js)는 현재 URL(.../notes/project-a.md)을 읽어서, 이것이 404 페이지가 아니라 특정 노트를 보여달라는 요청임을 파악합니다. 라우터는 해당 경로의 마크다운 파일을 동적으로 불러와 화면에 렌더링합니다. 결과: 사용자는 404 페이지를 전혀 보지 못하고, 마치 서버가 해당 URL을 직접 처리한 것처럼 자연스럽게 노트 내용을 보게 됩니다. 이 방법으로 서버 설정 없이 '깨끗한 URL'을 사용할 수 있습니다. SEO 전략: 정적 사이트 생성(SSG)을 통한 사전 렌더링 검색 엔진은 JavaScript를 실행하지 않고 HTML 내용만 보는 경우가 많습니다. 이 문제를 해결하고 SEO를 극대화하기 위해, 웹사이트를 게시하기 전에 모든 마크다운 파일을 각각의 HTML 파일로 미리 만들어두는 '정적 사이트 생성(Static Site Generation)' 방식을 도입합니다. 구현 계획: 빌드 스크립트 생성: Python이나 Shell 스크립트로 build.py 같은 빌드 자동화 스크립트를 만듭니다. 스크립트의 역할: 프로젝트 내의 모든 마크다운 파일 (.md)을 찾습니다. 각 마크다운 파일을 HTML로 변환합니다. 미리 정의된 HTML 템플릿(헤더, 푸터, 그리고 변환된 HTML 내용이 들어갈 공간)에 변환된 내용을 삽입하여 완전한 HTML 페이지를 생성합니다. 생성된 HTML 파일들을 dist와 같은 배포용 폴더에 원본과 동일한 경로 구조로 저장합니다. (예: notes/a.md → dist/notes/a.md/index.html 또는 dist/notes/a.md.html) 배포: GitHub Pages의 배포 소스를 dist 폴더로 지정합니다. 기대 효과: 검색 엔진은 각 URL에 대해 내용이 모두 채워진 HTML 파일을 크롤링하므로 SEO가 극대화됩니다. 사용자가 처음 페이지에 접속할 때는 이 정적 HTML을 먼저 받으므로 초기 로딩 속도가 매우 빠릅니다. 페이지가 로드된 후에는 기존의 SPA 스크립트가 동작하여, 페이지 이동이나 검색 등 모든 상호작용을 동적으로 부드럽게 처리합니다. 종합 실행 계획 1단계: 라우터 리팩토링 ( router.js ) URL 파싱 로직을 location.search(?file=...)에서 location.pathname을 읽도록 변경합니다. URL 생성 로직(navigate, updateURLOnly)이 새로운 '깨끗한 URL' 구조를 사용하도록 수정합니다. 하이라이트 관련 쿼리 파라미터 로직을 제거합니다. (텍스트 프래그먼트는 브라우저가 처리) 2단계: GitHub Pages 배포 설정 index.html을 복사하여 404.html 파일을 생성하고 프로젝트 루트에 둡니다. 3단계 (선택적이지만 강력 추천): SEO를 위한 빌드 스크립트 작성 모든 마크다운 파일을 HTML로 사전 렌더링하여 dist 폴더에 저장하는 빌드 스크립트를 작성합니다. GitHub Actions를 설정하여, 코드가 변경될 때마다 이 빌드 스크립트가 자동으로 실행되고 dist 폴더의 내용이 GitHub Pages에 배포되도록 자동화할 수 있습니다. 이 계획은 제안해주신 아이디어들을 모두 통합하여, 현대적이고 효율적이며 검색 엔진에도 친화적인 솔루션을 제공합니다. 이 계획대로 진행할까요?",
      "frontmatter": {
        "date": "2025-10-08T05:03:05+09:00",
        "lastmod": "2025-10-19T18:30:08+09:00"
      }
    },
    "무제 12": {
      "path": "/무제-12/",
      "filename": "무제 12",
      "content": "",
      "frontmatter": {
        "date": "2025-10-31T21:25:09+09:00",
        "lastmod": "2025-11-01T22:31:31+09:00"
      }
    },
    "무제 2": {
      "path": "/무제-2/",
      "filename": "무제 2",
      "content": "다음은 요청하신 문서의 마크다운 버전입니다. 검사 결과 보고서 1111",
      "frontmatter": {
        "date": "2025-08-27T22:04:34+09:00",
        "lastmod": "2025-08-29T01:25:55+09:00"
      }
    },
    "무제 3": {
      "path": "/무제-3/",
      "filename": "무제 3",
      "content": "┌───────────────────────────┐ │ CPU │ │ (Intel SoC / Host Bridge) │ │ [00:00.0] │ └───────┬─────┬─────────────┘ │ │ │ │ ┌───────────────────┴─┐ ┌─┴────────────────────────┐ │ Integrated Memory │ │ Integrated Devices (On-chip) │ │ Controller (IMC) │ └─┬────────────────────────┘ └─────────┬───────────┘ │ │ ├─ Integrated Graphics (iGPU) [00:02.0] ↓ │ ┌───┐ ├─ RAM Memory (Management) [00:14.2] │RAM│ │ └───┘ └─ PCIe Root Complex (CPU의 PCIe 레인 관리자) │ │ ┌─────────────────────┴────────────────────────────────┐ │ DMI/OPI Link (CPU와 칩셋을 연결하는 전용 고속도로) │ └─────────────────────┬────────────────────────────────┘ │ ↓ ┌───────────────────────────────────┐ │ PCH (Platform Controller Hub) │ │ (메인보드 칩셋) │ └─────────────────┬─────────────────┘ │ ┌─────────────────────────┴──────────────────────────┐ │ PCH 내장 기능 (Relatively Slower I/O) │ ├─ SATA Controller (HDD/SSD) [00:17.0] │ ├─ USB Controller [00:14.0] │ ├─ Audio Device (HD Audio) [00:1f] │ ├─ SMBus Controller [00:1f] │ ├─ Communication Controller (Intel ME) [00:16.0] │ ├─ ISA Bridge (Legacy) [00:1f] │ ├─ Serial Bus Controllers [00:19.0, 00:19.1, 00:1f] │ │ │ │ PCH 자체의 PCIe 레인을 통한 확장 │ └─┬──────────────────────────────────────────────────┘ │ ├─ PCI Bridge [00:1c] ───▶ Bus 01 ─▶ Ethernet Controller #1 [01:00.0] (Realtek RTL8111) │ (첫 번째 유선 랜카드) │ ├─ PCI Bridge [00:1d] ───▶ Bus 02 ─▶ Network Controller [02:00.0] (Realtek RTL8821CE Wi-Fi) │ (무선 랜카드) │ └─ PCI Bridge [00:1d] ───▶ Bus 03 ─▶ Ethernet Controller #2 [03:00.0] (Realtek RTL8111) (두 번째 유선 랜카드) graph TD subgraph CPU 와 직접 연결 CPU[\"CPU<br/>(Intel SoC / Host Bridge)<br/>[00:00.0]\"] IMC[\"Integrated Memory Controller\"] RAM[\"RAM\"] iGPU[\"Integrated Graphics (iGPU)<br/>[00:02.0]\"] RAM_Mgmt[\"RAM Memory (Management)<br/>[00:14.2]\"] RootComplex[\"PCIe Root Complex\"] CPU --> IMC --> RAM CPU --> iGPU CPU --> RAM_Mgmt CPU --> RootComplex end DMI[\"DMI/OPI Link<br/>(CPU-칩셋 연결 버스)\"] PCH[\"PCH<br/>(Platform Controller Hub)<br/>메인보드 칩셋\"] RootComplex --> DMI --> PCH subgraph PCH 내장 기능 direction LR SATA[\"SATA Controller<br/>[00:17.0]\"] USB[\"USB Controller<br/>[00:14.0]\"] Audio[\"Audio Device<br/>[00:1f]\"] SMBus[\"SMBus Controller<br/>[00:1f]\"] Comm[\"Communication Controller<br/>[00:16.0]\"] ISA[\"ISA Bridge<br/>[00:1f]\"] Serial[\"Serial Bus Controllers<br/>[00:19.0, 00:19.1, 00:1f]\"] end subgraph PCH PCIe 확장 Bridge1[\"PCI Bridge<br/>[00:1c]\"] LAN1[\"Ethernet Controller #1<br/>Realtek RTL8111<br/>[01:00.0]\"] Bridge2[\"PCI Bridge<br/>[00:1d]\"] WiFi[\"Network Controller (Wi-Fi)<br/>Realtek RTL8821CE<br/>[02:00.0]\"] Bridge3[\"PCI Bridge<br/>[00:1d]\"] LAN2[\"Ethernet Controller #2<br/>Realtek RTL8111<br/>[03:00.0]\"] Bridge1 --> LAN1 Bridge2 --> WiFi Bridge3 --> LAN2 end %% PCH에서 각 하위 장치로의 연결을 명확하게 정의 PCH --> SATA PCH --> USB PCH --> Audio PCH --> SMBus PCH --> Comm PCH --> ISA PCH --> Serial PCH --> Bridge1 PCH --> Bridge2 PCH --> Bridge3 %% 스타일링 classDef cpu fill:#cde4ff,stroke:#555,stroke-width:2px; classDef pch fill:#d5e8d4,stroke:#555,stroke-width:2px; classDef device fill:#f8cecc,stroke:#555,stroke-width:1px; classDef bridge fill:#fff2cc,stroke:#555,stroke-width:1px; class CPU cpu; class PCH pch; class LAN1,WiFi,LAN2,SATA,USB,Audio,SMBus,Comm,ISA,Serial,iGPU,RAM,RAM_Mgmt device; class Bridge1,Bridge2,Bridge3 bridge;",
      "frontmatter": {
        "date": "2025-08-29T01:25:55+09:00",
        "lastmod": "2025-08-30T05:37:10+09:00"
      }
    },
    "무제 4": {
      "path": "/무제-4/",
      "filename": "무제 4",
      "content": "# 서버 -> 로컬로 동기화 (서버의 최신 변경사항을 로컬로 가져옴) rsync -avz --delete user@your_server_ip:/path/to/remote/dir/ /path/to/local/dir/ # 로컬 -> 서버로 동기화 (로컬의 최신 변경사항을 서버로 보냄) rsync -avz --delete /path/to/local/dir/ user@your_server_ip:/path/to/remote/dir/ ./rstnc.sh push pull 2개의 기능",
      "frontmatter": {
        "date": "2025-09-03T08:04:21+09:00",
        "lastmod": "2025-09-03T15:48:39+09:00"
      }
    },
    "무제 5": {
      "path": "/무제-5/",
      "filename": "무제 5",
      "content": "shinnyeonki: 파일에서 사용자가 마우스로 클릭을 했을때 기존 타이머를 정지시키는 코드가 뭐야? GitHub Copilot: 사용자가 마우스로 클릭했을 때 기존 타이머를 정지시키는 코드를 찾기 위해 관련 파일들을 살펴보겠습니다. 이제 content.js 파일에서 클릭 이벤트 처리 코드를 더 자세히 살펴보겠습니다: 찾았습니다! content.js 파일에서 사용자가 마우스로 클릭했을 때 기존 타이머를 정지시키는 코드는 다음과 같습니다: $(document).click((function(e){ chrome.storage.local.get({data:[]},(function(e){ var t=e.data; for(let[e,r]of Object.entries(t)) null!=t[e]&&(null!=t[e]&&!0===t[e].stop_refresh_check||\"true\"===t[e].stop_refresh_check)&& chrome.runtime.sendMessage({ tabId:\"\", action:\"pause_refresh_active_tab\", tab_url:t[e].tab_url, from:\"content_script\" },(function(e){ null!=e&&\"YES\"==e.status&&$(\"#show_visual_timer\").remove() })) })) })); 이 코드의 동작 방식: $(document).click : 문서 전체에서 클릭 이벤트를 감지합니다. 스토리지 확인: chrome.storage.local.get 을 통해 저장된 데이터를 가져옵니다. 조건 확인: 저장된 데이터에서 stop_refresh_check 옵션이 true 로 설정된 항목들을 찾습니다. 메시지 전송: 조건에 맞는 경우 백그라운드 스크립트(worker.js)에 pause_refresh_active_tab 액션을 보냅니다. 타이머 제거: 응답이 \"YES\"인 경우 화면에서 시각적 타이머( #show_visual_timer )를 제거합니다. 즉, 이 코드는 사용자가 웹페이지의 아무 곳이나 클릭했을 때, 만약 \"클릭 시 새로고침 정지\" 옵션이 활성화되어 있다면 자동 새로고침 타이머를 일시정지시키는 기능을 담당합니다. shinnyeonki: 타이머 변수는 어디에 저장되어 있는지 알려줘 일반 javascirpt 변수 인지 extension strage 에 저장되어 있는지 GitHub Copilot: 코드를 분석해보니 타이머와 관련된 데이터는 여러 곳에 저장되어 있습니다: Extension Storage (chrome.storage.local) 주요 타이머 설정 데이터가 여기에 저장됩니다: 이제 worker.js에서 실제 타이머 인터벌을 찾아보겠습니다: 이제 sessionStorage 관련 코드도 확인해보겠습니다: 이제 전체적인 타이머 변수 저장 위치를 정리해드리겠습니다: 타이머 변수 저장 위치 분석 Extension Storage (chrome.storage.local) 주요 타이머 설정 데이터가 여기에 영구적으로 저장됩니다 저장되는 데이터: mili_seconds : 새로고침 간격 (밀리초) btn_start : 타이머 시작/정지 상태 custom_refresh : 커스텀 새로고침 여부 dynamic_limit_input : 동적 제한 입력값 show_visual_timer_check : 시각적 타이머 표시 여부 stop_refresh_check : 클릭 시 정지 옵션 hard_refresh_check : 하드 새로고침 여부 tab_url , tabId 등 JavaScript 변수 (worker.js) var htvg = []; // 실제 setInterval 타이머들을 저장하는 배열 var vgb = []; // 타이머 설정 데이터를 임시 저장하는 배열 htvg 배열: 실제 setInterval 객체들이 저장됨 var v = setInterval((function(){ chrome.tabs.sendMessage(e, {action:\"refresh\", ...}) }), s); // s는 밀리초 간격 t.push({ tabId: e, intervals: v, // 실제 타이머 ID mili_seconds: s, // ... 기타 설정들 }); Session Storage 카운터 정보만 임시 저장: sessionStorage.setItem(\"counterObj\", JSON.stringify({ counter: ++n, // 새로고침 횟수 tabId: e.tabId })); DOM 요소 시각적 타이머 UI: $(\"#timer_iframe_arte\").contents().find(\"body\").append( '<span id=\"timer\" data-seconds-left=' + e + '></span>' ); 요약 실제 타이머 변수는 여러 곳에 분산 저장됩니다: 실제 타이머 인터벌: worker.js의 htvg 배열 (JavaScript 변수) 타이머 설정: chrome.storage.local (Extension Storage) 새로고침 카운터: sessionStorage (브라우저 세션 스토리지) UI 타이머: DOM 요소 클릭 시 타이머를 정지시키는 코드는 Extension Storage에서 설정을 확인한 후, JavaScript 변수에 저장된 실제 setInterval ID를 사용하여 clearInterval() 로 타이머를 정지시킵니다.",
      "frontmatter": {
        "date": "2025-07-12T13:15:36+09:00",
        "lastmod": "2025-07-12T13:15:48+09:00"
      }
    },
    "무제 6": {
      "path": "/무제-6/",
      "filename": "무제 6",
      "content": "네, 직접 매우 단순한 WSGI 애플리케이션과 이를 실행할 수 있는 매우 단순한 WSGI 서버를 만들어 보겠습니다. 이를 통해 WSGI 인터페이스가 어떻게 동작하는지 직관적으로 이해할 수 있을 겁니다. WSGI의 핵심 원리: Callable(호출 가능한 객체) WSGI의 가장 중요한 부분은 \"애플리케이션\"이 application(environ, start_response) 이라는 시그니처를 가진 호출 가능한 객체(callable)여야 한다는 것입니다. environ : 웹 서버로부터 받은 요청 정보(HTTP 헤더, 경로, 쿼리 파라미터 등)를 담은 딕셔너리입니다. start_response : 애플리케이션이 웹 서버에게 HTTP 응답 상태 코드(예: \"200 OK\")와 응답 헤더를 보내기 위해 호출해야 하는 함수입니다. 웹 서버가 이 함수를 제공합니다. 애플리케이션은 이 두 인자를 받아 처리하고, 응답 본문(body)을 포함하는 이터러블(iterable) 객체를 반환해야 합니다. 매우 단순한 WSGI 애플리케이션 ( my_simple_app.py ) 이것은 우리가 만들 웹 \"애플리케이션\"입니다. 웹 프레임워크 없이 순수 WSGI 규약에 맞춰 작성됩니다. # my_simple_app.py import sys def my_simple_app(environ, start_response): \"\"\" 이것은 WSGI 규약을 따르는 매우 단순한 웹 애플리케이션입니다. Args: environ (dict): 웹 서버로부터 받은 요청 정보(환경 변수). 예: 'REQUEST_METHOD', 'PATH_INFO', 'QUERY_STRING' 등 'wsgi.input', 'wsgi.errors' 등 WSGI 관련 정보도 포함. start_response (callable): 서버가 제공하는 함수로, 애플리케이션이 HTTP 응답 상태 코드와 헤더를 서버에 전달할 때 사용합니다. 시그니처: start_response(status, headers, exc_info=None) Returns: iterable: 응답 본문(body)을 포함하는 바이트 문자열의 이터러블. \"\"\" status = '200 OK' # HTTP 상태 코드 headers = [ ('Content-type', 'text/plain; charset=utf-8'), ('X-Powered-By', 'MySimpleWSGIApp') # 커스텀 헤더도 가능 ] # start_response 함수를 호출하여 상태 코드와 헤더를 서버에게 전달합니다. start_response(status, headers) # 요청 경로를 확인하여 다른 응답을 줄 수 있습니다. path = environ.get('PATH_INFO', '/') if path == '/': response_body = b\"Hello from My Simple WSGI App! (Homepage)\\n\" elif path == '/hello': response_body = b\"Hello, specific page!\\n\" elif path == '/info': response_body = b\"Environment info:\\n\" # environ 딕셔너리의 내용을 응답 본문에 추가 for key, value in sorted(environ.items()): response_body += f\"{key}: {value}\\n\".encode('utf-8') else: status = '404 Not Found' headers = [('Content-type', 'text/plain; charset=utf-8')] start_response(status, headers) # 404 상태로 다시 호출 response_body = b\"404 Not Found: This page does not exist.\\n\" # 응답 본문은 바이트 문자열의 이터러블이어야 합니다. # 단순화를 위해 여기서는 하나의 바이트 문자열을 리스트에 담아 반환합니다. return [response_body] # 이 파일이 직접 실행될 경우 (예: 개발 서버용) if __name__ == '__main__': # 이 부분은 실제 WSGI 서버(Gunicorn, uWSGI)가 하는 일을 매우 간소화한 것입니다. # 실제 서버가 애플리케이션을 로드하여 실행합니다. print(\"This is a WSGI application module. It needs a WSGI server to run.\") print(\"Try running `python simple_wsgi_server.py` and access http://localhost:8000/\") 매우 단순한 WSGI 서버 ( simple_wsgi_server.py ) 이것은 클라이언트의 HTTP 요청을 받아 우리 my_simple_app WSGI 애플리케이션을 호출하고, 그 결과를 클라이언트에게 다시 HTTP 응답으로 보내는 역할을 하는 서버입니다. 실제 Gunicorn이나 uWSGI와 비교할 수 없을 정도로 단순하며, 한 번에 하나의 요청만 처리합니다. # simple_wsgi_server.py import socket import sys import os # PATH_INFO에 필요한 모듈 # 우리가 만든 WSGI 애플리케이션을 임포트합니다. from my_simple_app import my_simple_app class SimpleWSGIServer: def __init__(self, host, port, application): self.host = host self.port = port self.application = application self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 개발 시 재시작할 때 주소가 이미 사용 중인 문제를 방지 self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) def serve_forever(self): self.socket.bind((self.host, self.port)) self.socket.listen(1) # 한 번에 하나의 연결만 대기 print(f\"Serving WSGI app on http://{self.host}:{self.port}/...\") print(\"Try accessing /hello, /info, or /nonexistent_page\") while True: client_connection, client_address = self.socket.accept() print(f\"Accepted connection from {client_address}\") self.handle_request(client_connection) def handle_request(self, client_connection): # 1. 클라이언트 요청 데이터 수신 및 파싱 request_data = client_connection.recv(1024).decode('utf-8') request_lines = request_data.split('\\r\\n') if not request_lines: client_connection.close() return request_line = request_lines[0] try: method, path, version = request_line.split(' ') except ValueError: # 유효하지 않은 요청 라인 처리 client_connection.sendall(b\"HTTP/1.1 400 Bad Request\\r\\n\\r\\nBad Request\") client_connection.close() return # 2. `environ` 딕셔너리 생성 (매우 간소화된 버전) # 실제 WSGI 서버는 이 딕셔너리를 훨씬 더 풍부하게 채웁니다. environ = { 'REQUEST_METHOD': method, 'PATH_INFO': path.split('?')[0], # 쿼리 스트링 분리 'QUERY_STRING': path.split('?')[1] if '?' in path else '', 'SERVER_NAME': self.host, 'SERVER_PORT': str(self.port), 'SERVER_PROTOCOL': version, 'wsgi.version': (1, 0), 'wsgi.url_scheme': 'http', 'wsgi.input': client_connection.makefile('rb', -1), # 요청 본문 스트림 (여기에선 사용 안함) 'wsgi.errors': sys.stderr, # 에러 로그 스트림 'wsgi.multithread': False, # 이 서버는 단일 쓰레드 'wsgi.multiprocess': False, # 이 서버는 단일 프로세스 'wsgi.run_once': False, # 계속 실행됨 } # 3. `start_response` 콜러블 정의 # 이 함수는 애플리케이션이 호출하여 응답 상태 및 헤더를 서버에게 알려줍니다. headers_set = [] # 애플리케이션이 설정할 상태 및 헤더를 저장할 리스트 response_started = False def start_response(status, headers, exc_info=None): nonlocal response_started if response_started: raise RuntimeError(\"start_response() already called!\") response_started = True headers_set.append(status) headers_set.extend(headers) # exc_info는 에러 처리 시 사용되지만, 여기서는 생략합니다. # print(f\"[Server] App called start_response: Status={status}, Headers={headers}\") return None # WSGI 스펙에 따라 None 반환 # 4. WSGI 애플리케이션 호출 try: app_body_iterable = self.application(environ, start_response) # 5. HTTP 응답 구성 및 전송 response_status = headers_set[0] response_headers = headers_set[1:] response_line = f\"HTTP/1.1 {response_status}\\r\\n\" response_headers_str = \"\" for header_name, header_value in response_headers: response_headers_str += f\"{header_name}: {header_value}\\r\\n\" # 최종 헤더 full_response_headers = (response_line + response_headers_str + \"\\r\\n\").encode('utf-8') client_connection.sendall(full_response_headers) # 애플리케이션이 반환한 이터러블로부터 본문을 읽어 전송 for data in app_body_iterable: client_connection.sendall(data) # data는 이미 바이트 문자열이어야 합니다. except Exception as e: # 애플리케이션 내부에서 발생한 예상치 못한 오류 처리 print(f\"Error handling request: {e}\", file=sys.stderr) error_response = b\"HTTP/1.1 500 Internal Server Error\\r\\nContent-Type: text/plain\\r\\n\\r\\nInternal Server Error\" client_connection.sendall(error_response) finally: # 요청 처리 완료 후 클라이언트 연결 닫기 client_connection.close() print(f\"Connection from {client_address} closed.\") # 서버 실행 if __name__ == '__main__': HOST = '127.0.0.1' # localhost PORT = 8000 server = SimpleWSGIServer(HOST, PORT, my_simple_app) server.serve_forever() 실행 방법 위 코드들을 각각 my_simple_app.py 와 simple_wsgi_server.py 파일로 저장합니다. 터미널을 열고 simple_wsgi_server.py 파일이 있는 디렉토리로 이동합니다. 다음 명령어를 실행합니다: python simple_wsgi_server.py 콘솔에 \"Serving WSGI app on http://127.0.0.1:8000/...\" 메시지가 나타나면, 웹 브라우저를 열고 다음 주소들을 방문해 보세요: http://127.0.0.1:8000/ http://127.0.0.1:8000/hello http://127.0.0.1:8000/info (environ 딕셔너리 내용을 볼 수 있습니다) http://127.0.0.1:8000/nonexistent_page (404 에러를 확인) 코드 설명 및 WSGI 이해 이 예제를 통해 WSGI의 핵심을 이해할 수 있습니다: 애플리케이션(App)과 서버(Server)의 분리: my_simple_app.py 는 오직 WSGI 규약에 맞춰 application 이라는 함수를 구현합니다. 이 함수는 HTTP 요청/응답에 대한 지식은 거의 없고, 오직 environ 딕셔너리와 start_response 함수를 다루는 방법만 압니다. simple_wsgi_server.py 는 HTTP 프로토콜(소켓 통신, 요청 파싱, 응답 전송)을 처리하고, my_simple_app 을 호출하기 위한 environ 과 start_response 를 준비합니다. 이러한 분리 덕분에, 개발자는 my_simple_app 과 같은 WSGI 애플리케이션을 한 번 작성하면, 어떤 WSGI 호환 서버(Gunicorn, uWSGI 등)에서도 실행할 수 있습니다. 반대로, 서버 개발자는 어떤 WSGI 호환 애플리케이션이라도 실행할 수 있습니다. environ 딕셔너리: 서버는 클라이언트의 HTTP 요청을 파싱하여 다양한 정보를 environ 딕셔너리에 담아 애플리케이션에 전달합니다. PATH_INFO , REQUEST_METHOD , QUERY_STRING 등이 대표적입니다. 또한 wsgi.version , wsgi.input (요청 본문을 읽기 위한 파일 객체), wsgi.errors (에러 로그를 위한 파일 객체)와 같은 WSGI 자체의 환경 변수도 포함됩니다. start_response 콜러블: 애플리케이션은 응답 본문을 반환하기 전에 반드시 이 start_response 함수를 호출해야 합니다. 이 호출을 통해 서버는 HTTP 상태 코드(예: \"200 OK\")와 응답 헤더(예: Content-Type )를 알게 되고, 실제 응답의 첫 부분을 클라이언트에게 보낼 준비를 합니다. 반환 값: 이터러블 본문: 애플리케이션은 응답 본문을 바이트 문자열의 이터러블로 반환합니다. 이 예제에서는 [b\"Hello...\"] 와 같이 하나의 리스트에 담았지만, 큰 파일 전송의 경우 제너레이터를 사용하여 메모리 효율적으로 데이터를 스트리밍할 수도 있습니다. 이 간단한 예제를 통해 WSGI가 웹 서버와 Python 웹 애플리케이션 사이에 어떻게 표준화된 인터페이스 역할을 하는지 이해할 수 있을 것입니다. 실제 Gunicorn 같은 WSGI 서버는 이 기본적인 원리 위에 멀티프로세싱/멀티스레딩, 로깅, 설정 관리 등 훨씬 더 복잡한 기능들을 추가하여 프로덕션 환경에 적합하도록 만듭니다.",
      "frontmatter": {
        "date": "2025-07-17T21:27:20+09:00",
        "lastmod": "2025-07-17T23:50:34+09:00"
      }
    },
    "무제 7": {
      "path": "/무제-7/",
      "filename": "무제 7",
      "content": "rsync(1) 사용자 명령어 rsync(1) NAME rsync - 빠르고, 다용도이며, 원격(및 로컬) 파일 복사 도구 SYNOPSIS 로컬: rsync [OPTION...] SRC... [DEST] 원격 셸을 통한 접근: 가져오기 (Pull): rsync [OPTION...] [USER@]HOST:SRC... [DEST] 보내기 (Push): rsync [OPTION...] SRC... [USER@]HOST:DEST rsync 데몬을 통한 접근: 가져오기 (Pull): rsync [OPTION...] [USER@]HOST::SRC... [DEST] rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST] 보내기 (Push): rsync [OPTION...] SRC... [USER@]HOST::DEST rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST) 단일 SRC 인자만 있고 DEST 인자가 없는 사용법은 파일을 복사하는 대신 소스 파일을 나열합니다. 이 맨페이지의 온라인 버전(주제 간 상호 링크 포함)은 https://download.samba.org/pub/rsync/rsync.1 에서 확인할 수 있습니다. DESCRIPTION Rsync는 빠르고 매우 다용도의 파일 복사 도구입니다. 로컬에서, 모든 원격 셸을 통해 다른 호스트로/로부터, 또는 원격 rsync 데몬으로/로부터 파일을 복사할 수 있습니다. 이 도구는 동작의 모든 측면을 제어하고 복사할 파일 집합을 매우 유연하게 지정할 수 있는 수많은 옵션을 제공합니다. Rsync는 소스 파일과 대상에 있는 기존 파일 간의 차이점만 전송하여 네트워크를 통해 전송되는 데이터 양을 줄이는 델타-전송 알고리즘으로 유명합니다. Rsync는 백업 및 미러링에 널리 사용되며, 일상적인 사용을 위한 개선된 복사 명령어로도 활용됩니다. Rsync는 \"빠른 검사(quick check)\" 알고리즘(기본값)을 사용하여 전송해야 할 파일을 찾는데, 이 알고리즘은 크기 또는 최종 수정 시간이 변경된 파일을 찾습니다. (옵션으로 요청된) 다른 보존된 속성의 변경사항은 빠른 검사가 파일 데이터 업데이트가 필요 없다고 판단할 때 대상 파일에 직접 적용됩니다. rsync의 추가 기능 중 일부는 다음과 같습니다: o 링크, 장치, 소유자, 그룹 및 권한 복사 지원 o GNU tar와 유사한 exclude 및 exclude-from 옵션 o CVS가 무시하는 파일과 동일한 파일을 무시하는 CVS exclude 모드 o ssh 또는 rsh를 포함한 모든 투명한 원격 셸 사용 가능 o 슈퍼유저 권한 불필요 o 지연 시간 비용을 최소화하기 위한 파일 전송 파이프라이닝 o 익명 또는 인증된 rsync 데몬 지원 (미러링에 이상적) GENERAL Rsync는 원격 호스트로/로부터, 또는 현재 호스트의 로컬에서 파일을 복사합니다 (두 개의 원격 호스트 간 파일 복사는 지원하지 않습니다). rsync가 원격 시스템에 접속하는 두 가지 방법이 있습니다: 원격-셸 프로그램(예: ssh 또는 rsh)을 전송 수단으로 사용하거나, TCP를 통해 rsync 데몬에 직접 접속하는 것입니다. 원격-셸 전송은 소스 또는 대상 경로에 호스트 사양 뒤에 단일 콜론(:) 구분자가 포함될 때 사용됩니다. rsync 데몬에 직접 접속하는 경우는 소스 또는 대상 경로에 호스트 사양 뒤에 이중 콜론(::) 구분자가 포함되거나, OR rsync:// URL이 지정될 때 발생합니다 (후자 규칙의 예외는 USING RSYNC-DAEMON FEATURES VIA A RE‐ MOTE-SHELL CONNECTION 섹션 참조). 특별한 경우로, 대상 없이 단일 소스 인자만 지정되면, 파일은 \"ls -l\"과 유사한 출력 형식으로 나열됩니다. 예상대로, 소스 또는 대상 경로 모두 원격 호스트를 지정하지 않으면 로컬 복사가 발생합니다 (--list-only 옵션 참조). Rsync는 로컬 측을 클라이언트(client)로, 원격 측을 서버(server)로 지칭합니다. 서버와 rsync 데몬을 혼동하지 마십시오. 데몬은 항상 서버이지만, 서버는 데몬 또는 원격-셸에서 실행된 프로세스일 수 있습니다. SETUP 설치 지침은 README.md 파일을 참조하십시오. 설치 후, 원격 셸을 통해 접근할 수 있는 모든 머신(및 rsync 데몬 모드 프로토콜을 사용하여 접근할 수 있는 일부 머신)에 rsync를 사용할 수 있습니다. 원격 전송의 경우, 최신 rsync는 통신을 위해 ssh를 사용하지만, 기본적으로 rsh 또는 remsh와 같은 다른 원격 셸을 사용하도록 구성되었을 수도 있습니다. -e 명령줄 옵션을 사용하거나 RSYNC_RSH 환경 변수를 설정하여 원하는 원격 셸을 지정할 수도 있습니다. rsync는 소스 및 대상 머신 모두에 설치되어야 합니다. USAGE rsync는 rcp를 사용하는 것과 동일한 방식으로 사용합니다. 소스와 대상을 지정해야 하며, 그 중 하나는 원격일 수 있습니다. 구문을 설명하는 가장 좋은 방법은 몇 가지 예시를 드는 것입니다: rsync -t *.c foo:src/ 이것은 현재 디렉토리에서 패턴 *.c와 일치하는 모든 파일을 머신 foo의 src 디렉토리로 전송합니다. 만약 파일 중 일부가 원격 시스템에 이미 존재한다면, rsync 원격-업데이트 프로토콜은 데이터의 차이점만을 전송하여 파일을 업데이트하는 데 사용됩니다. 명령줄 와일드카드(*.c)가 파일 목록으로 확장되는 것은 rsync 자체가 아닌 셸에 의해 처리됩니다 (다른 모든 Posix-스타일 프로그램과 정확히 동일합니다). rsync -avz foo:src/bar /data/tmp 이것은 머신 foo의 src/bar 디렉토리에서 로컬 머신의 /data/tmp/bar 디렉토리로 모든 파일을 재귀적으로 전송합니다. 파일은 아카이브 모드로 전송되어 심볼릭 링크, 장치, 속성, 권한, 소유권 등이 전송 시 보존됩니다. 또한, 압축이 사용되어 전송되는 데이터 부분의 크기를 줄입니다. rsync -avz foo:src/bar/ /data/tmp 소스에 있는 후행 슬래시(trailing slash)는 대상에 추가 디렉토리 수준이 생성되는 것을 방지하도록 이 동작을 변경합니다. 소스의 후행 /를 \"이 디렉토리의 내용을 복사하라\"는 의미로 생각할 수 있으며, 이는 \"디렉토리 이름으로 복사하라\"는 것과 대조됩니다. 하지만 두 경우 모두 포함하는 디렉토리의 속성은 대상의 포함하는 디렉토리로 전송됩니다. 다시 말해, 다음 명령들은 /dest/foo의 속성 설정을 포함하여 동일한 방식으로 파일을 복사합니다: rsync -av /src/foo /dest rsync -av /src/foo/ /dest/foo 또한 호스트 및 모듈 참조는 기본 디렉토리의 내용을 복사하기 위해 후행 슬래시를 필요로 하지 않습니다. 예를 들어, 다음 두 가지 모두 원격 디렉토리의 내용을 \"/dest\"로 복사합니다: rsync -av host: /dest rsync -av host::module /dest 소스 또는 대상 경로에 ':'가 없는 로컬 전용 모드로 rsync를 사용할 수도 있습니다. 이 경우, rsync는 개선된 복사 명령처럼 동작합니다. 마지막으로, 모듈 이름을 생략하면 특정 rsync 데몬에서 사용 가능한 모든 (나열 가능한) 모듈을 나열할 수 있습니다: rsync somehost.mydomain.com:: COPYING TO A DIFFERENT NAME 디렉토리를 다른 이름으로 복사하려면, 소스 디렉토리에 후행 슬래시를 사용하여 디렉토리의 내용을 원하는 대상 디렉토리에 넣으십시오: rsync -ai foo/ bar/ Rsync는 단일 항목을 복사할 때 대상 파일의 이름을 사용자 정의하는 기능도 가지고 있습니다. 이에 대한 규칙은 다음과 같습니다: o 전송 목록은 단일 항목(파일 또는 빈 디렉토리)으로 구성되어야 합니다. o 대상 경로의 마지막 요소가 디렉토리로 존재해서는 안 됩니다. o 대상 경로가 후행 슬래시로 지정되지 않아야 합니다. 이러한 상황에서 rsync는 대상의 단일 항목 이름을 대상 경로의 마지막 요소로 설정합니다. 이러한 구문은 파일을 복사할 때만 사용하고, 디렉토리를 복사할 때는 위에 언급된 후행 슬래시 구문을 사용하는 것이 가장 좋습니다. 다음 예시는 foo.c 파일을 save 디렉토리 내에서 bar.c로 복사합니다 (bar.c가 디렉토리가 아니라고 가정): rsync -ai src/foo.c save/bar.c 단일 항목 복사 규칙은 사용자가 알지 못하는 사이에 단일 항목을 복사하고 존재하지 않는 대상 디렉토리를 지정할 때 (후행 슬래시 없이) 실수로 문제를 일으킬 수 있습니다. 예를 들어, src/*.c가 하나의 파일과 일치하고 save/dir이 존재하지 않으면, 대상 파일의 이름을 save/dir로 지정하여 혼란을 줄 수 있습니다: rsync -ai src/*.c save/dir 이러한 사고를 방지하려면 대상 디렉토리가 존재하는지 확인하거나 후행 슬래시를 사용하여 대상 경로를 지정하십시오: rsync -ai src/*.c save/dir/ SORTED TRANSFER ORDER Rsync는 항상 지정된 파일 이름을 내부 전송 목록으로 정렬합니다. 이것은 동일한 이름의 디렉토리 내용을 병합하고, 중복 파일 이름을 쉽게 제거할 수 있도록 합니다. 그러나 파일이 명령줄에 주어진 순서와 다른 순서로 전송될 때 혼란을 줄 수 있습니다. 특정 파일을 다른 파일보다 먼저 전송해야 하는 경우, 파일을 별도의 rsync 호출로 분리하거나, --delay-updates (정렬된 전송 순서에는 영향을 주지 않지만, 최종 파일 업데이트 단계를 훨씬 빠르게 만듭니다)를 사용하는 것을 고려하십시오. MULTI-HOST SECURITY Rsync는 전송에서 공유되는 파일 요청이 다양한 보안 문제로부터 보호되도록 조치를 취합니다. 잠재적인 문제의 대부분은 수신 측에서 발생하며, rsync는 전송되는 파일 목록이 요청된 범위 내에 있도록 조치를 취합니다. 이를 위해 rsync 3.1.2 이상 버전은 파일 목록에 전송 상단을 벗어나려는 절대 또는 상대 경로가 포함될 경우 중단됩니다. 또한, 버전 3.2.5부터 rsync는 파일 목록에 대한 두 가지 안전 점검을 추가로 수행합니다. (1) 클라이언트가 요청한 것 외에 추가 소스 인자가 전송에 추가되지 않았는지 확인하고, (2) 파일 목록이 전송자에게 보낸 제외 규칙을 준수하는지 확인합니다. 아직 3.2.5 클라이언트 rsync를 가지고 있지 않거나(또는 추가로 조심하고 싶은 사람들을 위해), 원격 호스트를 신뢰하지 않을 때는 원격 파일을 위한 전용 대상 디렉토리로 복사하는 것이 가장 안전합니다. 예를 들어, 홈 디렉토리로 rsync 복사를 수행하는 대신: rsync -aiv host1:dir1 ~ 원격 콘텐츠를 위한 \"host1-files\" 디렉토리를 전용으로 사용하십시오: rsync -aiv host1:dir1 ~/host1-files 자세한 내용은 --trust-sender 옵션을 참조하십시오. 주의: rsync를 사용하여 대소문자 구별 파일 시스템에서 대소문자 무시 파일 시스템으로 파일을 복사하는 것은 특별히 안전하지 않습니다. 이러한 복사를 수행해야 하는 경우, --no-links를 통해 심볼릭 링크를 비활성화하거나 --munge-links를 통해 심볼릭 링크의 변형을 활성화해야 합니다 (그리고 올바른 로컬 또는 원격 옵션을 사용해야 합니다). 이는 심볼릭 링크 이름이 파일 또는 디렉토리와 겹칠 경우 rsync가 잠재적으로 위험한 작업을 수행하는 것을 방지합니다. 그러나 모든 파일의 완전한 복사를 보장하지는 않습니다 (이름이 겹치는 경우 불가능할 수 있기 때문입니다). 더 나은 해결책은 모든 소스 파일을 나열하고 --files-from 옵션에 전달할 안전한 파일 이름 목록을 만드는 것입니다. 이름이 충돌하는 파일은 여러 번의 복사를 사용하여 다른 대상 디렉토리로 복사해야 합니다. 대소문자 무시 파일 시스템에서 대소문자 무시 파일 시스템으로의 복사는 상당히 잘 작동할 수 있지만, --delete-during 또는 --delete-before 옵션이 활성화되어 있지 않으면, rsync는 수신 측의 기존 파일을 업데이트하면서 파일 이름의 대소문자를 전송자와 일치하도록 변경해야 한다는 것을 인지하지 못할 수 있습니다. ADVANCED USAGE 원격 호스트에서 여러 파일을 요청하는 구문은 첫 번째와 동일한 스타일로 추가 원격-호스트 인자를 지정하거나 호스트 이름을 생략하여 수행됩니다. 예를 들어, 다음 모두 작동합니다: rsync -aiv host:file1 :file2 host:file{3,4} /dest/ rsync -aiv host::modname/file{1,2} host::modname/extra /dest/ rsync -aiv host::modname/first ::extra-file{1,2} /dest/ 데몬 연결은 한 복사 명령당 하나의 모듈만 액세스할 수 있으므로, 후속 경로의 시작이 첫 번째 경로의 모듈 이름으로 시작하지 않으면 해당 모듈 내의 경로로 간주됩니다 (예: 위에 가져온 extra-file1 및 extra-file2). 아주 오래된 rsync 버전(2.6.9 이하)은 단일 원격-소스 인자만 허용했으므로, 일부 사람들은 대신 원격 셸이 공백 분할을 수행하여 인자를 여러 경로로 나눌 것에 의존했습니다. 그러한 직관적이지 않은 동작은 더 이상 기본적으로 지원되지 않습니다 (하지만 아래 설명된 대로 요청할 수 있습니다). 3.2.4부터 파일 이름은 원격 셸에 전달될 때 사용자가 제공한 문자를 보존하는 방식으로 전달됩니다. 따라서, 이름에 공백이 있는 파일을 요청하면 원격 rsync는 해당 파일을 찾습니다: rsync -aiv host:'a simple file.pdf' /dest/ 원격 rsync 인자에 수동으로 추가 인용(quoting)을 적용하거나 원격 인자 분할을 요구하도록 작성된 스크립트를 사용하는 경우, rsync에게 스크립트가 추가 이스케이핑을 처리하도록 요청할 수 있습니다. 이는 스크립트의 rsync 실행에 --old-args 옵션을 추가하거나 (새로운 rsync 필요) RSYNC_OLD_ARGS=1 및 RSYNC_PROTECT_ARGS=0을 내보내면 됩니다 (이것은 오래된 또는 새로운 rsync 버전 모두에서 작동합니다). CONNECTING TO AN RSYNC DAEMON 원격 셸을 전송 수단으로 사용하지 않고 rsync를 사용하는 것도 가능합니다. 이 경우 일반적으로 TCP 포트 873을 사용하여 원격 rsync 데몬에 직접 연결합니다. (이는 당연히 데몬이 원격 시스템에서 실행 중이어야 하므로, 이에 대한 정보는 아래의 STARTING AN RSYNC DAEMON TO ACCEPT CONNECTIONS 섹션을 참조하십시오.) 이러한 방식으로 rsync를 사용하는 것은 원격 셸과 함께 사용하는 것과 동일하지만 다음을 제외합니다: o 단일 콜론(원격 셸) 구문 대신 이중 콜론 구문 또는 rsync:// URL 구문을 사용하십시오. o \"경로\"의 첫 번째 요소는 실제 모듈 이름입니다. o 추가 원격 소스 인자는 ADVANCED USAGE에서 논의된 바와 같이 호스트 이름 및/또는 모듈 이름을 생략하는 축약된 구문을 사용할 수 있습니다. o 원격 데몬은 연결 시 \"오늘의 메시지(message of the day)\"를 출력할 수 있습니다. o 호스트만 지정하고 (모듈이나 경로 없이) 지정하면 데몬에서 접근 가능한 모듈 목록이 출력됩니다. o 원격 소스 경로를 지정했지만 대상을 지정하지 않으면 원격 데몬에서 일치하는 파일 목록이 출력됩니다. o --rsh (-e) 옵션은 소켓 연결 방식에서 USING RSYNC-DAEMON FEATURES VIA A REMOTE-SHELL CONNECTION 방식으로 연결 스타일이 변경되는 것을 방지하기 위해 생략해야 합니다. \"src\"라는 이름의 원격 모듈의 모든 파일을 복사하는 예: rsync -av host::src /dest 원격 데몬의 일부 모듈은 인증을 요구할 수 있습니다. 이 경우 연결 시 암호 프롬프트가 나타납니다. 환경 변수 RSYNC_PASSWORD를 사용하려는 암호로 설정하거나 --password-file 옵션을 사용하여 암호 프롬프트를 피할 수 있습니다. 이는 rsync를 스크립트화할 때 유용할 수 있습니다. 경고: 일부 시스템에서는 환경 변수가 모든 사용자에게 보일 수 있습니다. 해당 시스템에서는 --password-file 사용을 권장합니다. 환경 변수 RSYNC_PROXY를 웹 프록시를 가리키는 hostname:port 쌍으로 설정하여 웹 프록시를 통해 연결을 설정할 수 있습니다. 웹 프록시의 구성은 포트 873으로의 프록시 연결을 지원해야 합니다. 또한 환경 변수 RSYNC_CONNECT_PROG를 직접 소켓 연결을 수행하는 대신 실행할 명령어로 설정하여 프로그램을 프록시로 사용하여 데몬 연결을 설정할 수 있습니다. 이 문자열에는 rsync 명령에 지정된 호스트 이름을 나타내는 이스케이프 \"%H\"가 포함될 수 있습니다 (따라서 문자열에 단일 \"%\"가 필요한 경우 \"%%\"를 사용하십시오). 예: export RSYNC_CONNECT_PROG='ssh proxyhost nc %H 873' rsync -av targethost1::module/src/ /dest/ rsync -av rsync://targethost2/module/src/ /dest/ 위에 지정된 명령은 ssh를 사용하여 프록시 호스트에서 nc (netcat)를 실행하고, 모든 데이터를 대상 호스트(%H)의 포트 873 (rsync 데몬)으로 전달합니다. 또한 RSYNC_SHELL 환경 변수가 설정되면, 해당 프로그램이 system() 호출의 기본 셸을 사용하는 대신 RSYNC_CONNECT_PROG 명령을 실행하는 데 사용됩니다. USING RSYNC-DAEMON FEATURES VIA A REMOTE-SHELL CONNECTION 실제로 시스템에 새로운 소켓 연결을 허용하지 않고(원격 셸 액세스에 이미 필요한 것 외에) rsync 데몬의 다양한 기능(예: 명명된 모듈)을 사용하는 것이 유용할 때가 있습니다. Rsync는 원격 셸을 사용하여 호스트에 연결한 다음, 원격 사용자의 홈 디렉토리에서 구성 파일을 읽을 것으로 예상되는 단일 사용 \"데몬\" 서버를 스폰(spawn)하는 것을 지원합니다. 이는 데몬 스타일 전송의 데이터를 암호화하려는 경우 유용할 수 있지만, 데몬이 원격 사용자에 의해 새로 시작되므로 chroot와 같은 기능을 사용하거나 데몬이 사용하는 uid를 변경하지 못할 수 있습니다. (데몬 전송을 암호화하는 또 다른 방법으로는 ssh를 사용하여 로컬 포트를 원격 머신으로 터널링하고, 해당 원격 호스트의 일반 rsync 데몬이 \"localhost\"에서만 연결을 허용하도록 구성하는 것을 고려하십시오.) 사용자 관점에서, 원격-셸 연결을 통한 데몬 전송은 일반 rsync-데몬 전송과 거의 동일한 명령줄 구문을 사용하며, 유일한 예외는 --rsh=COMMAND 옵션을 사용하여 명령줄에서 원격 셸 프로그램을 명시적으로 설정해야 한다는 것입니다. (환경에서 RSYNC_RSH을 설정하는 것은 이 기능을 켜지 않습니다.) 예를 들어: rsync -av --rsh=ssh host::module /dest 다른 원격-셸 사용자를 지정해야 하는 경우, 호스트 앞의 user@ 접두사는 rsync-user 값(사용자 기반 인증이 필요한 모듈의 경우)을 지정한다는 점을 명심하십시오. 이는 원격-셸을 지정할 때 ssh에 '-l user' 옵션을 제공해야 함을 의미하며, 예를 들어 --rsh 옵션의 짧은 버전을 사용하는 다음과 같습니다: rsync -av -e \"ssh -l ssh-user\" rsync-user@host::module /dest \"ssh-user\"는 ssh 수준에서 사용되고, \"rsync-user\"는 \"module\"에 로그인하는 데 사용됩니다. 이 설정에서는 시스템에 접속하는 ssh 명령에 의해 데몬이 시작됩니다 (원하는 경우 ~/.ssh/authorized_keys 파일을 통해 강제할 수 있습니다). 그러나 데몬에 직접 접속할 때는 미리 시작되어야 합니다. STARTING AN RSYNC DAEMON TO ACCEPT CONNECTIONS rsync 데몬에 연결하려면 원격 시스템에 데몬이 이미 실행 중이거나 (또는 특정 포트에서 들어오는 연결에 대해 rsync 데몬을 스폰(spawn)하도록 inetd와 같은 것을 구성해야 합니다). 들어오는 소켓 연결을 처리할 데몬을 시작하는 방법에 대한 전체 정보는 rsyncd.conf(5) 맨페이지를 참조하십시오. 이 파일은 데몬의 구성 파일이며, 데몬을 실행하는 방법에 대한 전체 세부 정보(독립형 및 inetd 구성 포함)를 포함합니다. 전송에 원격 셸 전송을 사용하는 경우, 수동으로 rsync 데몬을 시작할 필요가 없습니다. EXAMPLES 다음은 rsync가 어떻게 사용될 수 있는지 보여주는 몇 가지 예시입니다. 대용량 MS Word 파일 및 메일 폴더로 구성된 홈 디렉토리를 백업하려면, 각 사용자별 cron 작업을 사용하여 매일 다음을 실행할 수 있습니다: rsync -aiz . bkhost:backup/joe/ 원격 호스트에서 로컬 호스트로 일부 파일을 이동하려면 다음을 실행할 수 있습니다: rsync -aiv --remove-source-files rhost:/tmp/{file1,file2}.c ~/src/ OPTION SUMMARY 다음은 rsync에서 사용 가능한 옵션에 대한 간략한 요약입니다. 각 옵션에는 이 맨페이지에서 자세한 설명이 있습니다. --verbose, -v 상세 정보 출력 증가 --info=FLAGS 세분화된 정보 출력 상세도 --debug=FLAGS 세분화된 디버그 출력 상세도 --stderr=e|a|c 표준 오류 출력 모드 변경 (기본값: errors) --quiet, -q 오류가 아닌 메시지 억제 --no-motd 데몬 모드 MOTD 억제 --checksum, -c 수정 시간 및 크기 대신 체크섬 기반으로 건너뛰기 --archive, -a 아카이브 모드는 -rlptgoD (--no-A,-X,-U,-N,-H) --no-OPTION 암시된 OPTION 끄기 (예: --no-D) --recursive, -r 디렉토리 재귀적으로 복사 --relative, -R 상대 경로 이름 사용 --no-implied-dirs --relative 사용 시 암시된 디렉토리 전송 안 함 --backup, -b 백업 생성 (--suffix & --backup-dir 참조) --backup-dir=DIR DIR 내에 계층 구조 기반으로 백업 생성 --suffix=SUFFIX 백업 접미사 (기본값: ~ --backup-dir 없음) --update, -u 수신자 측이 더 새로운 파일 건너뛰기 --inplace 대상 파일 제자리에서 업데이트 --append 짧은 파일에 데이터 추가 --append-verify --append에 기존 데이터 체크섬 확인 포함 --dirs, -d 재귀 없이 디렉토리 전송 --old-dirs, --old-d 오래된 rsync와 통신할 때 --dirs처럼 작동 --mkpath 대상에 없는 경로 구성 요소 생성 --links, -l 심볼릭 링크를 심볼릭 링크로 복사 --copy-links, -L 심볼릭 링크를 참조 파일/디렉토리로 변환 --copy-unsafe-links \"안전하지 않은\" 심볼릭 링크만 변환 --safe-links 트리 외부를 가리키는 심볼릭 링크 무시 --munge-links 심볼릭 링크를 안전하고 사용할 수 없게 변형 --copy-dirlinks, -k 디렉토리로의 심볼릭 링크를 참조 디렉토리로 변환 --keep-dirlinks, -K 수신자 측의 심볼릭 링크된 디렉토리를 실제 디렉토리로 처리 --hard-links, -H 하드 링크 보존 --perms, -p 권한 보존 --executability, -E 실행 권한 보존 --chmod=CHMOD 파일 및/또는 디렉토리 권한에 영향 --acls, -A ACL 보존 (--perms 암시) --xattrs, -X 확장 속성 보존 --owner, -o 소유자 보존 (슈퍼유저 전용) --group, -g 그룹 보존 --devices 장치 파일 보존 (슈퍼유저 전용) --copy-devices 장치 내용을 일반 파일로 복사 --write-devices 장치에 파일처럼 쓰기 (--inplace 암시) --specials 특수 파일 보존 -D --devices --specials와 동일 --times, -t 수정 시간 보존 --atimes, -U 접근 (사용) 시간 보존 --open-noatime 열린 파일의 atime 변경 방지 --crtimes, -N 생성 시간 (새로움) 보존 --omit-dir-times, -O --times에서 디렉토리 제외 --omit-link-times, -J --times에서 심볼릭 링크 제외 --super 수신자가 슈퍼유저 활동 시도 --fake-super 확장 속성을 사용하여 특권 속성 저장/복구 --sparse, -S 널 시퀀스를 스파스 블록으로 전환 --preallocate 쓰기 전에 대상 파일 미리 할당 --dry-run, -n 변경 없이 시범 실행 --whole-file, -W 파일 전체 복사 (델타-전송 알고리즘 없이) --checksum-choice=STR 체크섬 알고리즘 선택 (일명 --cc) --one-file-system, -x 파일 시스템 경계를 넘지 않음 --block-size=SIZE, -B 고정된 체크섬 블록 크기 강제 적용 --rsh=COMMAND, -e 사용할 원격 셸 지정 --rsync-path=PROGRAM 원격 머신에서 실행할 rsync 지정 --existing 수신자 측에 새 파일 생성 건너뛰기 --ignore-existing 수신자 측에 이미 존재하는 파일 업데이트 건너뛰기 --remove-source-files 전송자가 동기화된 파일 제거 (비디렉토리) --del --delete-during의 별칭 --delete 대상 디렉토리에서 불필요한 파일 삭제 --delete-before 수신자가 전송 전에 삭제, 전송 중 아님 --delete-during 수신자가 전송 중에 삭제 --delete-delay 전송 중에 삭제 사항 찾고, 전송 후에 삭제 --delete-after 수신자가 전송 후에 삭제, 전송 중 아님 --delete-excluded 제외된 파일도 대상 디렉토리에서 삭제 --ignore-missing-args 오류 없이 누락된 소스 인자 무시 --delete-missing-args 누락된 소스 인자를 대상에서 삭제 --ignore-errors I/O 오류가 있더라도 삭제 --force 비어 있지 않더라도 디렉토리 강제 삭제 --max-delete=NUM NUM개 이상의 파일 삭제 금지 --max-size=SIZE SIZE보다 큰 파일 전송 금지 --min-size=SIZE SIZE보다 작은 파일 전송 금지 --max-alloc=SIZE 메모리 할당 관련 제한 변경 --partial 부분적으로 전송된 파일 유지 --partial-dir=DIR 부분적으로 전송된 파일을 DIR에 저장 --delay-updates 모든 업데이트된 파일을 마지막에 배치 --prune-empty-dirs, -m 파일 목록에서 빈 디렉토리 체인 정리 --numeric-ids 사용자/그룹 이름으로 uid/gid 값 매핑 안 함 --usermap=STRING 사용자 이름 사용자 정의 매핑 --groupmap=STRING 그룹 이름 사용자 정의 매핑 --chown=USER:GROUP 간단한 사용자 이름/그룹 이름 매핑 --timeout=SECONDS I/O 타임아웃을 초 단위로 설정 --contimeout=SECONDS 데몬 연결 타임아웃을 초 단위로 설정 --ignore-times, -I 크기 및 시간 일치 파일 건너뛰지 않음 --size-only 크기만 일치하는 파일 건너뛰기 --modify-window=NUM, -@ 수정 시간 비교 정확도 설정 --temp-dir=DIR, -T 임시 파일을 DIR 디렉토리에 생성 --fuzzy, -y 대상 파일이 없으면 유사한 파일 찾기 --compare-dest=DIR DIR에 상대적인 대상 파일도 비교 --copy-dest=DIR ... 그리고 변경되지 않은 파일의 복사본 포함 --link-dest=DIR 변경되지 않은 파일은 DIR의 파일에 하드 링크 --compress, -z 전송 중 파일 데이터 압축 --compress-choice=STR 압축 알고리즘 선택 (일명 --zc) --compress-level=NUM 명시적으로 압축 수준 설정 (일명 --zl) --skip-compress=LIST LIST에 접미사가 있는 파일 압축 건너뛰기 --cvs-exclude, -C CVS와 동일한 방식으로 파일 자동 무시 --filter=RULE, -f 파일 필터링 RULE 추가 -F --filter='dir-merge /.rsync-filter'와 동일 반복 사용 시: --filter='- .rsync-filter' --exclude=PATTERN PATTERN과 일치하는 파일 제외 --exclude-from=FILE FILE에서 제외 패턴 읽기 --include=PATTERN PATTERN과 일치하는 파일 제외하지 않음 --include-from=FILE FILE에서 포함 패턴 읽기 --files-from=FILE FILE에서 소스 파일 이름 목록 읽기 --from0, -0 모든 *-from/filter 파일은 0으로 구분됨 --old-args 최신 인자 보호 관용구 비활성화 --secluded-args, -s 프로토콜을 사용하여 인자를 안전하게 전송 --trust-sender 원격 전송자의 파일 목록 신뢰 --copy-as=USER[:GROUP] 복사 작업에 사용자 및 선택적 그룹 지정 --address=ADDRESS 데몬으로 나가는 소켓의 바인딩 주소 --port=PORT 이중 콜론 대체 포트 번호 지정 --sockopts=OPTIONS 사용자 정의 TCP 옵션 지정 --blocking-io 원격 셸에 블로킹 I/O 사용 --outbuf=N|L|B 출력 버퍼링을 None, Line, Block으로 설정 --stats 파일 전송 통계 제공 --8-bit-output, -8 출력에서 높은 비트 문자 이스케이프 안 함 --human-readable, -h 숫자를 사람이 읽기 쉬운 형식으로 출력 --progress 전송 중 진행 상황 표시 -P --partial --progress와 동일 --itemize-changes, -i 모든 업데이트에 대한 변경 요약 출력 --remote-option=OPT, -M OPT를 원격 측으로만 전송 --out-format=FORMAT 지정된 FORMAT을 사용하여 업데이트 출력 --log-file=FILE 지정된 FILE에 작업 내용 기록 --log-file-format=FMT 지정된 FMT을 사용하여 업데이트 기록 --password-file=FILE FILE에서 데몬-액세스 암호 읽기 --early-input=FILE 데몬의 초기 exec 입력에 FILE 사용 --list-only 파일 복사 대신 나열 --bwlimit=RATE 소켓 I/O 대역폭 제한 --stop-after=MINS MINS분 경과 후 rsync 중지 --stop-at=y-m-dTh:m 지정된 시간에 rsync 중지 --fsync 모든 쓰여진 파일 fsync --write-batch=FILE FILE에 일괄 업데이트 기록 --only-write-batch=FILE --write-batch와 유사하지만 대상 업데이트 안 함 --read-batch=FILE FILE에서 일괄 업데이트 읽기 --protocol=NUM 이전 프로토콜 버전 강제 사용 --iconv=CONVERT_SPEC 파일 이름의 문자셋 변환 요청 --checksum-seed=NUM 블록/파일 체크섬 시드 설정 (고급) --ipv4, -4 IPv4 선호 --ipv6, -6 IPv6 선호 --version, -V 버전 및 기타 정보 출력 후 종료 --help, -h (*) 이 도움말 표시 (* -h는 단독 사용 시에만 도움말) Rsync는 데몬으로 실행될 수도 있으며, 이 경우 다음 옵션이 허용됩니다: --daemon rsync 데몬으로 실행 --address=ADDRESS 지정된 주소에 바인딩 --bwlimit=RATE 소켓 I/O 대역폭 제한 --config=FILE 대체 rsyncd.conf 파일 지정 --dparam=OVERRIDE, -M 전역 데몬 구성 매개변수 재정의 --no-detach 부모로부터 분리하지 않음 --port=PORT 대체 포트 번호로 수신 --log-file=FILE \"로그 파일\" 설정 재정의 --log-file-format=FMT \"로그 형식\" 설정 재정의 --sockopts=OPTIONS 사용자 정의 TCP 옵션 지정 --verbose, -v 상세 정보 출력 증가 --ipv4, -4 IPv4 선호 --ipv6, -6 IPv6 선호 --help, -h 이 도움말 표시 (--daemon과 함께 사용 시) OPTIONS Rsync는 긴 옵션(이중 대시 + 단어)과 짧은 옵션(단일 대시 + 문자)을 모두 허용합니다. 사용 가능한 모든 옵션 목록은 아래에 설명되어 있습니다. 옵션을 두 가지 이상의 방법으로 지정할 수 있는 경우, 선택 사항은 쉼표로 구분됩니다. 일부 옵션은 긴 변형만 있고 짧은 변형은 없습니다. 옵션에 매개변수가 필요한 경우, 매개변수는 긴 변형 뒤에만 나열되어 있지만, 짧은 변형에도 지정되어야 합니다. 매개변수를 지정할 때는 --option=param, --option param, -o=param, -o param 또는 -oparam 형식을 사용할 수 있습니다 (후자의 선택은 옵션에 짧은 변형이 있다고 가정합니다). 매개변수는 셸의 명령줄 파싱을 통과하기 위해 어떤 식으로든 인용(quote)되어야 할 수 있습니다. 또한 경로 이름의 선행 틸데(~)는 셸에 의해 치환되므로, 로컬 셸이 이를 확장하도록 하려면 옵션 이름과 경로 이름 사이에 공백을 사용하여 구분해야 합니다. --help 도움말 rsync에서 사용 가능한 옵션에 대한 짧은 도움말 페이지를 출력하고 종료합니다. 다른 옵션 없이 사용될 때 -h를 --help 대신 사용할 수도 있습니다 (일반적으로 --human-readable을 의미하기 때문입니다). --version, -V rsync 버전 및 기타 정보를 출력하고 종료합니다. 반복해서 사용하면 정보가 여전히 읽기 쉬운 JSON 형식으로 출력됩니다 (클라이언트 측만). 출력에는 컴파일된 기능 목록, 최적화 목록, 기본 체크섬 알고리즘 목록, 기본 압축 알고리즘 목록, 기본 데몬 인증 다이제스트 목록, rsync 웹 사이트 링크 및 몇 가지 다른 항목이 포함됩니다. --verbose, -v 이 옵션은 전송 중에 제공되는 정보의 양을 늘립니다. 기본적으로 rsync는 조용히 작동합니다. 단일 -v는 어떤 파일이 전송되는지에 대한 정보와 마지막에 간략한 요약을 제공합니다. 두 개의 -v 옵션은 어떤 파일이 건너뛰어지는지에 대한 정보와 마지막에 약간 더 많은 정보를 제공합니다. 두 개 이상의 -v 옵션은 rsync를 디버깅하는 경우에만 사용해야 합니다. 실행 종료 요약은 원격 rsync로 전송된 바이트 수(로컬 복사 시 수신 측), 원격 호스트로부터 수신된 바이트 수, 그리고 rsync 실행 전체 기간 동안 계산된 전송된 데이터의 초당 평균 바이트 수를 알려줍니다. 두 번째 줄은 rsync가 전송을 고려한 모든 파일 크기의 합계인 총 크기(바이트 단위)를 보여줍니다. 또한 \"속도 향상(speedup)\" 값도 보여주는데, 이는 총 파일 크기를 전송 및 수신된 바이트의 합계로 나눈 비율입니다 (이는 단순히 기분 좋은 \"더 클수록 좋다\"는 숫자입니다). 이러한 바이트 값은 --human-readable (또는 --no-human-readable) 옵션을 사용하여 더 (또는 덜) 사람이 읽기 쉽게 만들 수 있습니다. 최신 rsync에서 -v 옵션은 --info 및 --debug 옵션 그룹 설정과 동일합니다. -v 사용에 추가하거나 대신 이러한 새로운 옵션을 사용할 수 있으며, 모든 세분화된 설정은 -v의 암시된 설정을 재정의합니다. --info와 --debug 모두 상세도 증가에 따라 어떤 플래그가 설정되는지 정확히 알려주는 도움말 요청 방법이 있습니다. 그러나 데몬의 \"최대 상세도(max verbosity)\" 설정은 데몬 측에서 다양한 개별 플래그를 설정할 수 있는 수준을 제한한다는 점을 명심하십시오. 예를 들어, 최대값이 2이면, -vv에 의해 설정되는 값보다 높은 값으로 설정된 모든 정보 및/또는 디버그 플래그는 데몬 로깅에서 -vv 수준으로 다운그레이드됩니다. --info=FLAGS 이 옵션을 사용하면 보고 싶은 정보 출력에 대해 세분화된 제어를 할 수 있습니다. 개별 플래그 이름 뒤에는 레벨 번호가 올 수 있으며, 0은 해당 출력을 침묵시키고, 1은 기본 출력 레벨이며, 더 높은 숫자는 해당 플래그의 출력을 증가시킵니다 (더 높은 레벨을 지원하는 플래그의 경우). 사용 가능한 모든 플래그 이름, 출력 내용 및 상세도 증가에 따라 추가되는 플래그 이름을 보려면 --info=help를 사용하십시오. 몇 가지 예시: rsync -a --info=progress2 src/ dest/ rsync -avv --info=stats2,misc1,flist0 src/ dest/ --info=name의 출력은 --out-format 및 --itemize-changes (-i) 옵션에 영향을 받습니다. 출력 내용 및 시기에 대한 자세한 정보는 해당 옵션을 참조하십시오. 이 옵션은 3.1.0에 추가되었으므로, 서버 측의 오래된 rsync는 세분화된 제어 시도를 거부할 수 있습니다 (하나 이상의 플래그를 서버로 보내야 하고 서버가 이를 이해하기에는 너무 오래된 경우). 데몬을 다룰 때 위에 언급된 \"최대 상세도(max verbosity)\" 주의사항도 참조하십시오. --debug=FLAGS 이 옵션을 사용하면 보고 싶은 디버그 출력에 대해 세분화된 제어를 할 수 있습니다. 개별 플래그 이름 뒤에는 레벨 번호가 올 수 있으며, 0은 해당 출력을 침묵시키고, 1은 기본 출력 레벨이며, 더 높은 숫자는 해당 플래그의 출력을 증가시킵니다 (더 높은 레벨을 지원하는 플래그의 경우). 사용 가능한 모든 플래그 이름, 출력 내용 및 상세도 증가에 따라 추가되는 플래그 이름을 보려면 --debug=help를 사용하십시오. 몇 가지 예시: rsync -avvv --debug=none src/ dest/ rsync -avA --del --debug=del2,acl src/ dest/ 특히 I/O 및 버퍼 디버깅과 관련된 일부 디버그 메시지는 --stderr=all 옵션이 지정된 경우에만 출력됩니다. 3.2.0부터 이 옵션은 더 이상 서버 측으로 자동 전달되지 않습니다. 이는 전송의 각 측에 대해 다른 디버그 값을 지정하고, rsync 버전 중 하나에만 존재하는 새로운 디버그 옵션을 지정할 수 있도록 하기 위함입니다. 양쪽에 동일한 옵션을 복제하려면 괄호 확장을 사용하여 타이핑을 줄일 수 있습니다. 이는 zsh 및 bash에서 작동합니다: rsync -aiv {-M,}--debug=del2 src/ dest/ --stderr=errors|all|client 이 옵션은 어떤 프로세스가 stderr로 출력하고 정보 메시지도 stderr로 변경되는지 제어합니다. 모드 문자열은 약어로 지정할 수 있으므로 단일 문자 값을 사용해도 됩니다. 가능한 3가지 선택은 다음과 같습니다: o errors - (기본값) 모든 rsync 프로세스가 오류를 stderr로 직접 보냅니다. 프로세스가 전송의 원격 측에 있더라도 마찬가지입니다. 정보 메시지는 프로토콜 스트림을 통해 클라이언트 측으로 전송됩니다. stderr를 사용할 수 없는 경우 (즉, 소켓을 통해 데몬에 직접 연결할 때) 오류는 프로토콜 스트림을 통해 전송되는 것으로 대체됩니다. o all - 모든 rsync 메시지(정보 및 오류)가 모든 (가능한) 프로세스에서 stderr로 직접 쓰여지도록 합니다. 이로 인해 stderr가 라인-버퍼링되고 (원시 버퍼링 대신) 정보 및 오류 메시지를 파일 핸들별로 분할하는 기능이 사라집니다. 디버깅을 하거나 여러 수준의 상세도를 사용하는 경우 이 옵션은 전송 스트림이 막히는 것을 방지하여 (교착 상태 버그가 발생하는 것을 방지해야 합니다) 도움이 될 수 있습니다. 또한 --debug가 일부 추가 I/O 관련 메시지를 활성화하도록 허용합니다. o client - 모든 rsync 메시지가 프로토콜 스트림을 통해 클라이언트 측으로 전송되도록 합니다. 하나의 클라이언트 프로세스가 모든 메시지를 출력하며, 오류는 stderr로, 정보 메시지는 stdout으로 출력됩니다. 이는 이전 rsync 버전의 기본값이었지만, 많은 전송 데이터가 메시지보다 앞에 있을 때 오류 지연을 유발할 수 있습니다. 오래된 rsync로 파일을 푸시하는 경우, --stderr=all을 사용하는 것이 좋습니다. 이 구문은 여러 릴리스 동안 사용되었습니다. 이 옵션은 rsync 3.2.3에 추가되었습니다. 이 버전은 또한 비기본 설정이 원격 측으로 전달되기 시작했으며, rsync는 이전 버전과의 호환성을 위해 --msgs2stderr 및 --no-msgs2stderr 옵션을 각각 all 및 client 설정으로 나타냅니다. 새로운 rsync는 호환성을 유지하기 위해 이러한 이전 옵션 이름을 계속 허용합니다. --quiet, -q 이 옵션은 전송 중에 제공되는 정보의 양을 줄이며, 특히 원격 서버의 정보 메시지를 억제합니다. 이 옵션은 cron에서 rsync를 호출할 때 유용합니다. --no-motd 이 옵션은 데몬 전송 시작 시 클라이언트가 출력하는 정보에 영향을 줍니다. 이것은 오늘의 메시지(MOTD) 텍스트를 억제하지만, 데몬이 \"rsync host::\" 요청에 응답하여 보내는 모듈 목록에도 영향을 미칩니다 (rsync 프로토콜의 제한 때문입니다). 따라서 데몬에서 모듈 목록을 요청하려면 이 옵션을 생략하십시오. --ignore-times, -I 일반적으로 rsync는 이미 동일한 크기이며 동일한 수정 타임스탬프를 가진 파일을 건너뜁니다. 이 옵션은 이 \"빠른 검사\" 동작을 끄고, 모든 파일이 업데이트되도록 합니다. 이 옵션은 --ignore-existing 및 --ignore-non-existing과 비교할 때 혼란스러울 수 있습니다. 이들은 rsync가 더 적은 파일을 전송하도록 하는 반면, 이 옵션은 rsync가 더 많은 파일을 전송하도록 합니다. --size-only 이것은 전송해야 할 파일을 찾는 rsync의 \"빠른 검사\" 알고리즘을 수정하여, (기본적으로) 크기 또는 최종 수정 시간이 변경된 파일을 전송하는 대신 크기가 변경된 파일만 찾도록 변경합니다. 이는 타임스탬프를 정확하게 보존하지 못할 수 있는 다른 미러링 시스템을 사용한 후 rsync를 사용하기 시작할 때 유용합니다. --modify-window=NUM, -@ 두 타임스탬프를 비교할 때, rsync는 타임스탬프가 modify-window 값 이하로 차이가 나는 경우 동일한 것으로 간주합니다. 기본값은 0이며, 정수 초 단위로만 일치합니다. 음수 값을 지정하면 (수신자가 3.1.3 버전 이상인 경우) 나노초도 고려됩니다. 1을 지정하는 것은 MS Windows FAT 파일 시스템으로/로부터 복사할 때 유용합니다. FAT는 시간을 2초 해상도로 나타내므로 (원래 시간과 최대 1초 차이 허용) 그렇습니다. 모든 전송의 기본값이 나노초 비교를 사용하도록 하려면, ~/.popt 파일을 생성하고 다음 줄을 추가할 수 있습니다: rsync alias -a -a@-1 rsync alias -t -t@-1 이것이 기본값으로 설정되면, 나노초를 무시하도록 재정의하려면 --modify-window=0 (일명 -@0)을 지정해야 합니다. 예를 들어, ext3와 ext4 사이를 복사하거나 수신 rsync가 3.1.3보다 오래된 경우에 그렇습니다. --checksum, -c 이것은 rsync가 파일이 변경되었고 전송이 필요한지 확인하는 방식을 변경합니다. 이 옵션이 없으면 rsync는 (기본적으로) 전송자와 수신자 간에 각 파일의 크기와 최종 수정 시간이 일치하는지 확인하는 \"빠른 검사\"를 사용합니다. 이 옵션은 일치하는 크기를 가진 각 파일에 대해 128비트 체크섬을 비교하도록 변경합니다. 체크섬을 생성한다는 것은 양쪽 모두 전송 중인 모든 파일의 데이터를 읽는 데 많은 디스크 I/O를 소모한다는 것을 의미하므로, 이로 인해 속도가 상당히 느려질 수 있습니다 (그리고 이는 변경된 파일을 전송하기 위해 수행될 모든 읽기 작업 전에 발생합니다). 보내는 측은 사용 가능한 파일 목록을 생성하는 파일 시스템 스캔을 수행하는 동안 체크섬을 생성합니다. 수신 측은 변경된 파일을 스캔할 때 체크섬을 생성하며, 해당 보내는 측 파일과 동일한 크기를 가진 모든 파일의 체크섬을 확인합니다. 크기가 변경되었거나 체크섬이 변경된 파일은 전송 대상으로 선택됩니다. rsync는 항상 전송된 각 파일이 수신 측에서 올바르게 재구성되었는지 확인하기 위해 파일 전송 중 생성되는 전체 파일 체크섬을 확인하지만, 이 자동 전송 후 확인은 이 옵션의 전송 전 \"이 파일을 업데이트해야 하는가?\" 확인과는 관련이 없습니다. 사용되는 체크섬은 클라이언트와 서버 간에 자동 협상되지만, --checksum-choice (--cc) 옵션 또는 해당 옵션 섹션에서 논의된 환경 변수를 사용하여 재정의할 수 있습니다. --archive, -a 이것은 -rlptgoD와 동일합니다. 재귀와 거의 모든 것을 보존하려는 경우에 빠르게 지정할 수 있는 방법입니다. ACL(-A), 확장 속성(-X), 접근 시간(-U), 생성 시간(-N), 하드 링크 찾기 및 보존(-H)은 포함되지 않는다는 점에 유의하십시오. 위 등가성(equivalence)의 유일한 예외는 --files-from이 지정될 때입니다. 이 경우 -r은 암시되지 않습니다. --no-OPTION 옵션 이름 앞에 \"no-\"를 붙여 하나 이상의 암시된 옵션을 끌 수 있습니다. 모든 긍정 옵션이 부정 옵션을 가지는 것은 아니지만, 많은 옵션이 그러하며, 여기에는 암시된 옵션을 비활성화하는 데 사용될 수 있는 옵션(--no-D, --no-perms 등)이나 다양한 상황에서 다른 기본값을 가지는 옵션(--no-whole-file, --no-blocking-io, --no-dirs 등)이 포함됩니다. 모든 유효한 부정 옵션은 \"no-\" 접두사 뒤에 짧은 옵션 이름과 긴 옵션 이름을 모두 허용합니다(예: --no-R은 --no-relative와 동일합니다). 예를 들어, --archive (-a)를 사용하고 싶지만 --owner (-o)는 사용하고 싶지 않다면, -a를 -rlptgD로 변환하는 대신 -a --no-o (일명 --archive --no-owner)를 지정할 수 있습니다. 옵션의 순서가 중요합니다: --no-r -a를 지정하면 -r 옵션이 결국 켜지게 되며, 이는 -a --no-r과 반대입니다. 또한 --files-from 옵션의 부작용은 위치에 영향을 받지 않습니다. 이 옵션은 여러 옵션의 기본 상태에 영향을 미치고 -a의 의미를 약간 변경합니다 (--files-from 옵션에서 자세한 내용을 참조하십시오). --recursive, -r 이것은 rsync에게 디렉토리를 재귀적으로 복사하도록 지시합니다. 단일 디렉토리 스캔을 허용하는 옵션에 대해서는 --dirs (-d)도 참조하십시오. 전송할 파일 목록을 생성하기 위한 증분 재귀에 대한 설명은 --inc-recursive 옵션을 참조하십시오. --inc-recursive, --i-r 이 옵션은 파일 스캔 시 증분 재귀를 명시적으로 활성화합니다. 이는 --recursive 옵션을 사용하고 전송 양쪽이 rsync 3.0.0 이상을 실행 중일 때 기본적으로 활성화됩니다. 증분 재귀는 비증분 방식보다 훨씬 적은 메모리를 사용하며, 전송을 더 빨리 시작합니다 (전체 전송 계층 구조를 스캔할 필요 없이 파일을 전송하기 시작하므로). 소스 파일에 재귀가 활성화되어 있지 않으면 이 옵션은 효과가 없습니다. 일부 옵션은 rsync가 전체 파일 목록을 알아야 하므로 이러한 옵션은 증분 재귀 모드를 비활성화합니다. 여기에는 다음이 포함됩니다: o --delete-before (--delete의 이전 기본값) o --delete-after o --prune-empty-dirs o --delay-updates --delete를 증분 재귀와 호환시키기 위해 rsync 3.0.0은 --delete-during을 기본 삭제 모드로 만들었습니다 (이는 2.6.4에 처음 추가되었습니다). 증분 재귀의 한 가지 부작용은 재귀적으로 스캔된 디렉토리 내의 누락된 하위 디렉토리가 (기본적으로) 하위 디렉토리로 재귀하기 전에 생성된다는 것입니다. 이러한 조기 생성 지점(비증분 재귀와 비교하여)은 rsync가 완료된 디렉토리의 수정 시간을 즉시 설정할 수 있도록 합니다 (많은 재귀적 복사가 완료될 때까지 기다릴 필요 없이). 그러나 이러한 초기 디렉토리는 아직 완료된 모드, mtime 또는 소유권이 설정되지 않았습니다. 하위 디렉토리 복사가 실제로 시작될 때까지는 더 제한적인 권한을 가집니다. 이러기 조기-생성 관용구는 --omit-dir-times 옵션을 사용하여 피할 수 있습니다. 증분 재귀는 --no-inc-recursive (--no-i-r) 옵션을 사용하여 비활성화할 수 있습니다. --no-inc-recursive, --no-i-r --recursive 옵션의 새로운 증분 재귀 알고리즘을 비활성화합니다. 이렇게 하면 rsync는 파일 전송을 시작하기 전에 전체 파일 목록을 스캔합니다. 자세한 내용은 --inc-recursive를 참조하십시오. --relative, -R 상대 경로를 사용합니다. 즉, 명령줄에 지정된 전체 경로 이름이 파일 이름의 마지막 부분만 전송되는 대신 서버로 전송됩니다. 이는 여러 디렉토리를 동시에 전송하려는 경우에 특히 유용합니다. 예를 들어, 다음 명령을 사용했다면: rsync -av /foo/bar/baz.c remote:/tmp/ 원격 머신의 /tmp/에 baz.c라는 이름의 파일이 생성됩니다. 대신 다음을 사용했다면: rsync -avR /foo/bar/baz.c remote:/tmp/ 그러면 원격 머신에 /tmp/foo/bar/baz.c라는 이름의 파일이 생성되어 전체 경로가 보존됩니다. 이러한 추가 경로 요소는 \"암시된 디렉토리\"(즉, 위 예시에서 \"foo\" 및 \"foo/bar\" 디렉토리)라고 불립니다. rsync 3.0.0부터 rsync는 이러한 암시된 디렉토리를 항상 파일 목록에 실제 디렉토리로 보냅니다. 경로 요소가 보내는 측에서 실제로 심볼릭 링크인 경우에도 마찬가지입니다. 이는 경로에 심볼릭 링크가 포함되어 있다는 것을 알지 못하고 파일의 전체 경로를 복사할 때 발생하는 예상치 못한 동작을 방지합니다. 서버 측 심볼릭 링크를 복제하려면 심볼릭 링크는 경로를 통해 포함하고, 참조 디렉토리는 실제 경로를 통해 포함하십시오. 오래된 rsync를 보내는 측에서 다루는 경우, --no-implied-dirs 옵션을 사용해야 할 수도 있습니다. 또한 각 경로에 대해 암시된 디렉토리로 전송되는 경로 정보의 양을 제한할 수도 있습니다. 보내는 측에 최신 rsync(2.6.7부터)가 있는 경우, 소스 경로에 점과 슬래시를 삽입할 수 있습니다: rsync -avR /foo/./bar/baz.c remote:/tmp/ 그러면 원격 머신에 /tmp/bar/baz.c가 생성됩니다. (점 뒤에는 슬래시가 와야 하므로 \"/foo/.\"는 축약되지 않습니다.) 오래된 rsync 버전의 경우, 소스 경로를 제한하기 위해 chdir을 사용해야 합니다. 예를 들어, 파일을 푸시할 때: (cd /foo; rsync -avR bar/baz.c remote:/tmp/) (괄호는 두 명령을 서브 셸에 넣으므로 \"cd\" 명령이 향후 명령에 영향을 미치지 않습니다.) 오래된 rsync에서 파일을 가져오는 경우, 이 구문을 사용하십시오 (단, 데몬이 아닌 전송에만 해당): rsync -avR --rsync-path=\"cd /foo; rsync\" \\ remote:bar/baz.c /tmp/ --no-implied-dirs 이 옵션은 --relative 옵션의 기본 동작에 영향을 줍니다. 이 옵션이 지정되면, 소스 이름에서 암시된 디렉토리의 속성은 전송에 포함되지 않습니다. 이는 대상 시스템의 해당 경로 요소가 존재하는 경우 변경되지 않고 유지되며, 누락된 암시된 디렉토리는 기본 속성으로 생성됨을 의미합니다. 이는 이러한 암시된 경로 요소가 수신 측에서 디렉토리에 대한 심볼릭 링크와 같이 큰 차이를 가질 수도 있게 합니다. 예를 들어, 명령줄 인자 또는 files-from 항목이 rsync에게 \"path/foo/file\" 파일을 전송하도록 지시했고, --relative를 사용하면 \"path\" 및 \"path/foo\" 디렉토리가 암시됩니다. 만약 \"path/foo\"가 대상 시스템에서 \"bar\"에 대한 심볼릭 링크인 경우, 수신 rsync는 일반적으로 \"path/foo\"를 삭제하고, 디렉토리로 다시 생성하고, 새 디렉토리로 파일을 수신합니다. --no-implied-dirs를 사용하면 수신 rsync는 기존 경로 요소를 사용하여 \"path/foo/file\"을 업데이트합니다. 이는 파일이 결국 \"path/bar\"에 생성됨을 의미합니다. 이러한 링크 보존을 달성하는 또 다른 방법은 --keep-dirlinks 옵션을 사용하는 것입니다 (이는 전송의 나머지 부분에 있는 디렉토리에 대한 심볼릭 링크에도 영향을 미칩니다). rsync 3.0.0보다 오래된 rsync에서 파일을 가져올 때, 보내는 측에서 요청한 경로에 심볼릭 링크가 있고 암시된 디렉토리가 일반 디렉토리로 전송되기를 원한다면 이 옵션을 사용해야 할 수도 있습니다. --backup, -b 이 옵션을 사용하면 각 파일이 전송되거나 삭제될 때 기존 대상 파일의 이름이 바뀝니다. --backup-dir 및 --suffix 옵션을 사용하여 백업 파일이 저장될 위치와 (있다면) 어떤 접미사가 추가될지 제어할 수 있습니다. --backup-dir을 지정하지 않으면: 1. --omit-dir-times 옵션이 강제 적용됩니다. 2. --delete (단, --delete-excluded 없음)를 사용하면 rsync는 백업 접미사에 대한 \"보호\" 필터 규칙을 기존 모든 필터 끝에 추가합니다. 이 규칙은 이전에 백업된 파일이 삭제되는 것을 방지합니다. 규칙은 다음과 같습니다: -f \"P *~\". 자신만의 필터 규칙을 제공하는 경우, 필터 목록의 더 높은 위치에 자신만의 제외/보호 규칙을 수동으로 삽입하여 효과를 발휘할 수 있는 충분히 높은 우선순위를 가지도록 해야 할 수도 있습니다 (예: 규칙에 후행 포함/제외 *가 지정된 경우 자동 추가된 규칙은 도달되지 않습니다). --backup-dir=DIR 이것은 --backup 옵션을 암시하며, rsync에게 모든 백업을 수신 측의 지정된 디렉토리에 저장하도록 지시합니다. 이는 증분 백업에 사용할 수 있습니다. --suffix 옵션을 사용하여 백업 접미사를 추가로 지정할 수 있습니다 (그렇지 않으면 지정된 디렉토리에 백업된 파일은 원래 파일 이름을 유지합니다). 상대 경로를 지정하면 백업 디렉토리가 대상 디렉토리에 상대적이므로, 절대 경로 또는 \"../\"로 시작하는 경로를 지정하는 것이 좋습니다. rsync 데몬이 수신자인 경우, 백업 디렉토리가 모듈의 경로 계층을 벗어날 수 없으므로, 삭제하거나 그 안에 복사하지 않도록 특별히 주의하십시오. --suffix=SUFFIX 이 옵션을 사용하면 --backup (-b) 옵션과 함께 사용되는 기본 백업 접미사를 재정의할 수 있습니다. --backup-dir이 지정되지 않은 경우 기본 접미사는 ~이며, 그렇지 않으면 빈 문자열입니다. --update, -u 이것은 rsync가 대상에 존재하고 소스 파일보다 수정 시간이 더 새로운 파일을 건너뛰도록 강제합니다. (기존 대상 파일의 수정 시간이 소스 파일의 수정 시간과 동일하다면, 크기가 다른 경우 업데이트됩니다.) 이는 디렉토리, 심볼릭 링크 또는 기타 특수 파일의 복사에는 영향을 미치지 않습니다. 또한, 보내는 측과 받는 측 간의 파일 형식 차이는 날짜에 관계없이 항상 업데이트할 만큼 중요하게 간주됩니다. 다시 말해, 소스에 디렉토리가 있고 대상에 파일이 있는 경우, 타임스탬프와 관계없이 전송이 발생합니다. 이 옵션은 TRANSFER RULE이므로, 어떤 제외 부작용도 기대하지 마십시오. --inplace와 --update를 결합하기로 선택한 사람들을 위한 주의사항: 중단된 전송은 매우 최근 수정 시간을 가진 부분 파일을 수신 측에 남겨두므로, 전송을 다시 실행해도 중단된 파일이 계속 진행되지 않을 수 있습니다. 따라서, 중단된 진행 중인 파일을 처리하기 위한 수동 단계를 구현하지 않는 한, 일반적으로 이 옵션을 --inplace와 결합하는 것을 피하는 것이 가장 좋습니다. --inplace 이 옵션은 데이터 업데이트가 필요한 파일을 rsync가 전송하는 방식을 변경합니다. 파일의 새 복사본을 생성하고 완료 시 제자리로 이동하는 기본 방식 대신, rsync는 업데이트된 데이터를 대상 파일에 직접 씁니다. 이것은 여러 가지 효과를 가집니다: o 하드 링크가 끊어지지 않습니다. 이는 새 데이터가 대상 파일에 대한 다른 하드 링크를 통해서도 보이게 된다는 의미입니다. 또한, 다중 링크된 대상 파일에 다른 소스 파일을 복사하려고 하면 대상 데이터가 계속해서 바뀌는 \"줄다리기\"가 발생할 수 있습니다. o 사용 중인 바이너리는 업데이트할 수 없습니다 (OS가 이를 방지하거나, 데이터를 스왑인하려고 시도하는 바이너리가 오작동하거나 충돌합니다). o 파일의 데이터는 전송 중에 일관성 없는 상태로 유지되며, 전송이 중단되거나 업데이트에 실패하면 그 상태로 남게 됩니다. o rsync가 쓸 수 없는 파일은 업데이트할 수 없습니다. 슈퍼유저는 어떤 파일이든 업데이트할 수 있지만, 일반 사용자는 파일 열기 및 쓰기 권한이 부여되어야 성공적으로 쓸 수 있습니다. o 대상 파일의 일부 데이터가 파일의 나중에 위치한 데이터를 복사하기 전에 덮어쓰여지면 rsync의 델타-전송 알고리즘 효율성이 저하될 수 있습니다. --backup을 사용하는 경우에는 해당되지 않습니다. rsync는 백업 파일을 전송의 기본 파일로 사용하는 데 충분히 똑똑하기 때문입니다. 경고: 이 옵션을 사용하여 다른 사용자가 액세스하는 파일을 업데이트해서는 안 되므로, 복사 시 이 옵션을 사용하기로 선택할 때는 주의하십시오. 이 옵션은 블록 기반 변경 또는 추가된 데이터가 있는 대용량 파일을 전송하는 데 유용하며, 네트워크가 아닌 디스크에 바인딩된 시스템에서도 유용합니다. 또한 사소한 변경만 있는 파일의 전체 내용을 복사-온-쓰기 파일 시스템 스냅샷이 분기하는 것을 방지하는 데 도움이 될 수 있습니다. 이 옵션은 --partial을 암시하지만 (중단된 전송은 파일을 삭제하지 않으므로), --partial-dir 및 --delay-updates와 충돌합니다. rsync 2.6.4 이전에는 --inplace가 --compare-dest 및 --link-dest와도 호환되지 않았습니다. --append 이 특별한 복사 모드는 수신 측의 기존 내용이 전송 측의 내용과 동일하다고 알려진, 크기가 계속 증가하는 파일을 효율적으로 업데이트할 때만 작동합니다. 전송되는 모든 파일이 공유되고 증가하는 파일인지 100% 확신하지 못하는 경우 --append를 사용하는 것은 위험할 수 있습니다. 따라서 이 기준에 맞지 않는 파일을 걸러내기 위해 필터 규칙을 사용해야 합니다. Rsync는 파일의 기존 내용을 확인하지 않고 (추가하는 내용만 확인) 이러한 증가하는 파일을 제자리에서 업데이트합니다. rsync는 수신 측에 존재하고 보내는 측의 관련 파일보다 짧지 않은 파일은 건너뜁니다 (이는 새 파일은 전송됨을 의미합니다). 또한 전송 협상 중에 보내는 측의 파일 크기가 줄어드는 파일은 건너뜁니다 (이러한 경우 rsync는 \"줄어든\" 파일에 대해 경고합니다). 파일 전송이 필요 없는 경우 파일의 내용이 아닌 속성(예: 권한, 소유권 등) 업데이트를 방해하지 않으며, 디렉토리나 비정규 파일의 업데이트에도 영향을 미치지 않습니다. --append-verify 이 특별한 복사 모드는 --append와 유사하게 작동하지만, 파일의 모든 데이터가 체크섬 검증에 포함됩니다 (효율성은 떨어지지만 잠재적으로 더 안전합니다). 전송되는 모든 파일이 공유되고 증가하는 파일인지 100% 확신하지 못하는 경우 이 옵션을 사용하는 것은 위험할 수 있습니다. 자세한 내용은 --append 옵션을 참조하십시오. 참고: rsync 3.0.0 이전에는 --append 옵션이 --append-verify처럼 작동했습니다. 따라서 이전 rsync와 상호 작용하는 경우 (또는 전송이 30 이전 프로토콜을 사용하는 경우) 두 append 옵션 중 하나를 지정하면 --append-verify 전송이 시작됩니다. --dirs, -d 보내는 측에게 발견되는 모든 디렉토리를 포함하도록 지시합니다. --recursive와 달리, 디렉토리 이름이 \".\"으로 지정되었거나 후행 슬래시로 끝나는 경우(예: \".\", \"dir/.\", \"dir/\", 등)를 제외하고는 디렉토리의 내용은 복사되지 않습니다. 이 옵션이나 --recursive 옵션이 없으면 rsync는 발견하는 모든 디렉토리를 건너뜁니다 (그리고 각 디렉토리에 대해 해당 메시지를 출력합니다). --dirs와 --recursive를 모두 지정하면 --recursive가 우선합니다. --dirs 옵션은 --files-from 옵션 또는 --list-only 옵션(암시된 --list-only 사용 포함)에서 --recursive가 지정되지 않았을 경우 암시됩니다 (그래서 디렉토리가 목록에 보이게 됩니다). 이를 끄고 싶다면 --no-dirs (또는 --no-d)를 지정하십시오. 오래된 rsync가 재귀 없이 단일 디렉토리를 나열하도록 하기 위해 -r --exclude='/*/*' 해킹을 사용하도록 rsync에 지시하는 역방향 호환성 도우미 옵션인 --old-dirs (--old-d)도 있습니다. --mkpath 대상 경로의 누락된 모든 경로 구성 요소를 생성합니다. 기본적으로 rsync는 대상 경로의 마지막 구성 요소만 존재하지 않을 수 있도록 허용합니다. 이는 대상 경로의 유효성을 검사하는 데 도움이 되기 위한 시도입니다. 이 옵션을 사용하면 rsync는 mkdir -p $DEST_PATH가 수신 측에서 실행된 것처럼 누락된 모든 대상 경로 구성 요소를 생성합니다. 대상 경로를 지정할 때 후행 슬래시를 포함하면 파일 목록에 단일 항목이 있더라도 전체 경로가 생성될 디렉토리 이름으로 처리됩니다. rsync가 최종 대상 경로 구성 요소를 디렉토리로 생성해야 할지 여부를 결정하는 방법에 대한 자세한 내용은 COPYING TO A DIFFERENT NAME 섹션을 참조하십시오. 새로 생성된 대상 디렉토리가 보내는 측의 디렉토리와 일치하도록 하려면 --mkpath 대신 --relative (-R)를 사용해야 합니다. 예를 들어, 다음 두 명령은 동일한 대상 트리를 생성하지만, 두 번째 명령만 \"some/extra/path\" 구성 요소가 보내는 측의 디렉토리와 일치하도록 보장합니다: rsync -ai --mkpath host:some/extra/path/*.c some/extra/path/ rsync -aiR host:some/extra/path/*.c ./ --links, -l 각 심볼릭 링크에 대해 \"비정규 파일\" 경고를 시끄럽게 무시하는 대신, 전송된 파일에 심볼릭 링크를 추가합니다. --info=nonreg0을 지정하여 경고를 억제할 수도 있습니다. 심볼릭 링크의 기본 처리는 수신 측에서 각 심볼릭 링크의 변경되지 않은 값을 다시 생성하는 것입니다. 다중 옵션 정보는 SYMBOLIC LINKS 섹션을 참조하십시오. --copy-links, -L 보내는 측은 전송에서 발견된 각 심볼릭 링크를 참조 항목으로 변환하고, 심볼릭 링크 체인을 따라 참조하는 파일 또는 디렉토리로 이동합니다. 심볼릭 링크 체인이 끊어지면 오류가 출력되고 파일은 전송에서 제외됩니다. 이 옵션은 전송에서 심볼릭 링크가 남지 않으므로, 심볼릭 링크에 영향을 미치는 다른 옵션보다 우선합니다. 이 옵션은 수신 측의 기존 심볼릭 링크 처리를 변경하지 않습니다. 이는 rsync 2.6.3 이전 버전과 달리 수신 측에게도 심볼릭 링크를 따르도록 지시하는 부작용이 있었던 것과 다릅니다. 최신 rsync는 이 옵션을 원격 수신자에게 전달하지 않으므로 (보내는 측만 알면 되므로), 이 주의사항은 rsync 클라이언트가 2.6.7보다 오래된 버전(이때 -L이 수신자에게 전달되지 않게 됨)을 사용하는 경우에만 영향을 미칩니다. 디렉토리에 대한 심볼릭 링크가 수신 측에서 실제 디렉토리로 처리되도록 해야 하는 경우 --keep-dirlinks (-K)를 참조하십시오. 다중 옵션 정보는 SYMBOLIC LINKS 섹션을 참조하십시오. --copy-unsafe-links 이것은 rsync에게 복사된 트리 외부를 가리키는 심볼릭 링크의 참조 대상을 복사하도록 지시합니다. 절대 심볼릭 링크도 일반 파일처럼 처리되며, --relative를 사용할 때는 소스 경로 자체에 있는 심볼릭 링크도 마찬가지입니다. 참고로, 차단 지점은 전송의 최상단입니다. 이는 rsync가 자세한 출력에서 언급하지 않는 경로의 일부입니다. 만약 \"/src/subdir\"를 \"/dest/\"로 복사한다면, \"subdir\" 디렉토리는 전송 트리 내부의 이름이지 전송의 최상단(/src)이 아니므로, 생성된 상대 심볼릭 링크가 /src 및 /dest 디렉토리 내부의 다른 이름을 참조하는 것은 합법적입니다. 대신 \"/src/subdir/\" (후행 슬래시 포함)를 \"/dest/subdir\"로 복사한다면, \"subdir\" 외부의 어떤 파일에도 심볼릭 링크를 허용하지 않습니다. 안전한 심볼릭 링크는 --links가 지정되거나 암시된 경우에만 복사됩니다. --copy-unsafe-links 옵션은 --copy-links와 결합될 때 추가적인 효과가 없습니다. 다중 옵션 정보는 SYMBOLIC LINKS 섹션을 참조하십시오. --safe-links 이것은 수신 rsync에게 복사된 트리 외부를 가리키는 전송의 모든 심볼릭 링크를 무시하도록 지시합니다. 모든 절대 심볼릭 링크도 무시됩니다. 이 무시는 수신 측에서 발생하므로, 보내는 측이 심볼릭 링크를 변형했더라도 (--munge-links를 사용할 때) 여전히 효과적입니다. 심볼릭 링크가 안전하지 않은 것으로 간주되어 건너뛰어질 때, 전송에 파일이 존재하면 수신 측의 일치하는 파일이 삭제되는 것을 방지하므로 삭제에도 영향을 미칩니다. 이 옵션은 --links (또는 --archive)와 함께 사용되어야 전송에서 심볼릭 링크가 조건부로 무시될 수 있습니다. 그 효과는 --copy-unsafe-links에 의해 재정의됩니다. --relative와 함께 이 옵션을 사용하면 예상치 못한 결과가 발생할 수 있습니다. 다중 옵션 정보는 SYMBOLIC LINKS 섹션을 참조하십시오. --munge-links 이 옵션은 전송의 한쪽에만 영향을 미치며, rsync에게 파일을 수신할 때 심볼릭 링크 값을 변형하거나 파일을 보낼 때 심볼릭 링크 값을 복원하도록 지시합니다. 변형된 값은 심볼릭 링크를 디스크에서 사용할 수 없게 만들지만, 심볼릭 링크의 원래 내용을 복구할 수 있도록 합니다. 서버 측 rsync는 클라이언트의 지식 없이 이 옵션을 종종 활성화합니다. 예를 들어 rsync 데몬의 구성 파일에서 또는 rrsync (제한된 rsync) 스크립트에 주어진 옵션에 의해 그렇습니다. 클라이언트 측에서 지정할 경우, 클라이언트 측이 변형된 심볼릭 링크를 가지고 있거나 필요로 한다면 옵션을 정상적으로 지정하거나, -M--munge-links를 사용하여 서버가 변형된 심볼릭 링크를 가지고 있거나 필요로 할 때 서버에 옵션을 제공하십시오. 로컬 전송의 경우, 클라이언트가 보내는 측이므로, 옵션을 직접 지정하면 심볼릭 링크가 복원되고, 원격 옵션으로 지정하면 심볼릭 링크가 변형됩니다. 이 옵션은 --remote-option을 통해 데몬으로 전송될 때 효과가 없습니다. 데몬은 \"munge symlinks\" 매개변수를 통해 변형된 심볼릭 링크를 원하는지 구성하기 때문입니다. 심볼릭 링크 값은 전송 시 변형/복원되므로, 심볼릭 링크를 비심볼릭 링크로 변환하는 모든 옵션은 --safe-links를 제외하고 변형/복원보다 먼저 발생합니다. --safe-links는 수신자가 내리는 결정이므로, 변형/복원된 값을 기반으로 결정을 내립니다. 이는 수신자가 변형을 활성화한 경우, --safe-links를 사용하면 모든 심볼릭 링크가 무시됨을 의미합니다 (모두 절대 경로이므로). rsync가 심볼릭 링크를 변형하는 방법은 각 값 앞에 문자열 \"/rsyncd-munged/\"를 붙이는 것입니다. 이는 디렉토리가 존재하지 않는 한 링크를 사용할 수 없게 합니다. 이 옵션이 활성화되면, rsync는 해당 경로가 디렉토리이거나 디렉토리에 대한 심볼릭 링크이면 실행을 거부합니다 (시작 시에만 확인합니다). 제자리에서 하나 이상의 심볼릭 링크를 변형/복원하는 방법은 소스 코드의 support 디렉토리에 있는 \"munge-symlinks\" python 스크립트도 참조하십시오. --copy-dirlinks, -k 이 옵션은 보내는 측이 디렉토리에 대한 심볼릭 링크를 실제 디렉토리처럼 처리하도록 합니다. 이는 --copy-links를 사용할 때처럼 비디렉토리 심볼릭 링크가 영향을 받지 않도록 하려는 경우에 유용합니다. 이 옵션이 없으면, 보내는 측이 디렉토리를 디렉토리에 대한 심볼릭 링크로 교체한 경우, 받는 측은 새 심볼릭 링크를 방해하는 모든 것을 삭제합니다. 디렉토리 계층 구조도 포함합니다 (--force 또는 --delete가 적용 중인 한). 수신 측에 대한 유사한 옵션은 --keep-dirlinks를 참조하십시오. --copy-dirlinks는 소스에 있는 디렉토리에 대한 모든 심볼릭 링크에 적용됩니다. 지정된 몇몇 심볼릭 링크만 따르려면, 후행 슬래시와 함께 추가 소스 인자로 전달하고, --relative를 사용하여 경로가 올바르게 일치하도록 하는 트릭을 사용할 수 있습니다. 예를 들어: rsync -r --relative src/./ src/./follow-me/ dest/ 이것이 작동하는 이유는 rsync가 주어진 소스 인자에 lstat(2)를 호출하고, 후행 슬래시가 lstat(2)가 심볼릭 링크를 따르도록 하여 파일 목록에 디렉토리가 생성되고, 이는 \"src/./\" 스캔 중에 발견된 심볼릭 링크를 재정의하기 때문입니다. 다중 옵션 정보는 SYMBOLIC LINKS 섹션을 참조하십시오. --keep-dirlinks, -K 이 옵션은 수신 측이 디렉토리에 대한 심볼릭 링크를 실제 디렉토리처럼 처리하도록 하지만, 보내는 측의 실제 디렉토리와 일치하는 경우에만 그렇습니다. 이 옵션이 없으면 수신자의 심볼릭 링크는 삭제되고 실제 디렉토리로 교체됩니다. 예를 들어, \"file\"을 포함하는 디렉토리 \"foo\"를 전송한다고 가정해 봅시다. 그러나 \"foo\"는 수신자 측에서 디렉토리 \"bar\"에 대한 심볼릭 링크입니다. --keep-dirlinks가 없으면 수신자는 심볼릭 링크 \"foo\"를 삭제하고, 디렉토리로 다시 생성하고, 새 디렉토리로 파일을 수신합니다. --keep-dirlinks를 사용하면 수신자는 심볼릭 링크를 유지하고 \"file\"은 \"bar\"에 저장됩니다. 주의할 점: --keep-dirlinks를 사용하는 경우, 복사본의 모든 심볼릭 링크를 신뢰하거나 수신 측에서 --munge-links 옵션을 활성화해야 합니다! 신뢰할 수 없는 사용자가 실제 디렉토리에 대한 자신만의 심볼릭 링크를 생성할 수 있는 경우, 해당 사용자는 (후속 복사 시) 심볼릭 링크를 실제 디렉토리로 교체하고 심볼릭 링크가 참조하는 디렉토리의 내용에 영향을 미칠 수 있습니다. 백업 복사의 경우, 수신 계층 구조를 수정하기 위해 심볼릭 링크 대신 바인드 마운트와 같은 것을 사용하는 것이 좋습니다. 보내는 측에 대한 유사한 옵션은 --copy-dirlinks를 참조하십시오. 다중 옵션 정보는 SYMBOLIC LINKS 섹션을 참조하십시오. --hard-links, -H 이것은 rsync에게 소스에서 하드 링크된 파일을 찾아 대상에서 해당 파일을 함께 링크하도록 지시합니다. 이 옵션이 없으면 소스의 하드 링크된 파일은 별개의 파일처럼 처리됩니다. 이 옵션은 대상의 하드 링크 패턴이 소스의 패턴과 정확히 일치하도록 반드시 보장하지는 않습니다. 대상이 추가 하드 링크로 끝날 수 있는 경우는 다음과 같습니다: o 대상이 불필요한 하드 링크를 포함하는 경우 (소스 파일 목록에 있는 것보다 더 많은 링크), 복사 알고리즘은 명시적으로 링크를 끊지 않습니다. 그러나 하나 이상의 경로에 내용 차이가 있는 경우, 일반 파일 업데이트 프로세스는 해당 추가 링크를 끊습니다 (--inplace 옵션을 사용하지 않는 한). o 하드 링크를 포함하는 --link-dest 디렉토리를 지정하는 경우, 대상 파일을 --link-dest 파일과 링크하는 것은 --link-dest 연결로 인해 대상의 일부 경로가 함께 링크될 수 있습니다. rsync는 전송 집합 내에 있는 파일 간의 하드 링크만 감지할 수 있습니다. rsync가 전송 외부 파일에 추가 하드 링크 연결이 있는 파일을 업데이트하면 해당 링크는 끊어집니다. 이러한 손상을 피하기 위해 --inplace 옵션을 사용하려는 경우, 의도하지 않은 변경이 남은 하드 링크로 인해 발생하지 않도록 파일이 업데이트되는 방식을 신중하게 파악해야 합니다 (--inplace 옵션에 대한 추가 주의사항 참조). 증분 재귀가 활성화된 경우 (--inc-recursive 참조), rsync는 계층 구조의 다른 곳에 해당 내용에 대한 다른 링크가 존재한다는 것을 발견하기 전에 누락된 하드 링크된 파일을 전송할 수 있습니다. 이는 전송의 정확성(즉, 어떤 파일이 하드 링크되어 있는지)에는 영향을 미치지 않지만, 효율성(즉, 나중에 전송에서 하드 링크된 파일 집합의 다른 구성원에서 발견될 수 있었던 하드 링크된 파일의 새, 초기 복사본에 대한 데이터 복사)에는 영향을 미칩니다. 이러한 비효율성을 피하는 한 가지 방법은 --no-inc-recursive 옵션을 사용하여 증분 재귀를 비활성화하는 것입니다. --perms, -p 이 옵션은 수신 rsync가 대상 권한을 소스 권한과 동일하게 설정하도록 합니다. (--chmod 옵션은 rsync가 소스 권한으로 간주하는 것을 수정하는 방법을 참조하십시오.) 이 옵션이 꺼져 있으면 권한은 다음과 같이 설정됩니다: o 기존 파일 (업데이트된 파일 포함)은 기존 권한을 유지하지만, --executability 옵션은 파일의 실행 권한만 변경할 수 있습니다. o 새 파일은 수신 프로세스의 umask 또는 대상 디렉토리의 기본 ACL을 통해 지정된 권한과 함께 소스 파일의 권한이 마스킹되어 \"일반\" 권한 비트가 설정되고, 새 디렉토리가 부모 디렉토리에서 setgid 비트를 상속하는 경우를 제외하고는 특수 권한 비트는 비활성화됩니다. 따라서 --perms와 --executability가 모두 비활성화되면 rsync의 동작은 cp(1) 및 tar(1)와 같은 다른 파일 복사 유틸리티와 동일합니다. 요약하자면: 대상 파일(기존 파일과 새 파일 모두)에 소스 권한을 부여하려면 --perms를 사용하십시오. 새 파일에 대상 기본 권한을 부여하려면(기존 파일은 변경하지 않고), --perms 옵션이 꺼져 있는지 확인하고 --chmod=ugo=rwX를 사용하십시오(모든 비마스킹된 비트가 활성화되도록 보장합니다). 이 후자의 동작을 더 쉽게 타이핑하고 싶다면, 다음과 같이 ~/.popt 파일에 이 줄을 추가하여 popt 별칭을 정의할 수 있습니다(다음은 -Z 옵션을 정의하며, 대상 디렉토리의 기본 그룹을 사용하기 위해 --no-g를 포함합니다): rsync alias -Z --no-p --no-g --chmod=ugo=rwX 그런 다음 이 새로운 옵션을 다음과 같은 명령에 사용할 수 있습니다: rsync -avZ src/ dest/ (주의: -a가 -Z 뒤에 오지 않도록 하십시오. 그렇지 않으면 위에 언급된 두 --no-* 옵션이 다시 활성화됩니다.) --perms가 꺼져 있을 때 새로 생성된 디렉토리의 대상 setgid 비트 보존은 rsync 2.6.7에 추가되었습니다. 오래된 rsync 버전은 --perms가 꺼져 있을 때 새로 생성된 파일에 대해 세 가지 특수 권한 비트를 잘못 보존했으며, 새로 생성된 디렉토리의 대상 setgid 비트 설정을 재정의했습니다. 기본 ACL 준수는 rsync 2.6.7용 ACL 패치에 추가되었으므로, 오래된 (또는 ACL이 활성화되지 않은) rsync는 기본 ACL이 존재하더라도 umask를 사용합니다. (이러한 동작에 영향을 미치는 것은 수신 rsync의 버전임을 명심하십시오.) --executability, -E 이 옵션은 --perms가 활성화되지 않았을 때 rsync가 일반 파일의 실행 가능성(또는 비실행 가능성)을 보존하도록 합니다. 일반 파일은 권한에 'x' 중 하나 이상이 켜져 있으면 실행 가능한 것으로 간주됩니다. 기존 대상 파일의 실행 가능성이 해당 소스 파일과 다를 때, rsync는 대상 파일의 권한을 다음과 같이 수정합니다: o 파일을 비실행 가능하게 만들려면 rsync는 모든 'x' 권한을 끕니다. o 파일을 실행 가능하게 만들려면 rsync는 해당 'r' 권한이 활성화된 모든 'x' 권한을 켭니다. --perms가 활성화되면 이 옵션은 무시됩니다. --acls, -A 이 옵션은 rsync가 대상 ACL을 소스 ACL과 동일하게 업데이트하도록 합니다. 이 옵션은 또한 --perms를 암시합니다. 이 옵션이 제대로 작동하려면 소스 및 대상 시스템이 호환 가능한 ACL 항목을 가지고 있어야 합니다. 호환되지 않는 ACL을 백업하고 복원하는 방법에 대해서는 --fake-super 옵션을 참조하십시오. --xattrs, -X 이 옵션은 rsync가 대상 확장 속성을 소스 확장 속성과 동일하게 업데이트하도록 합니다. 확장-속성 네임스페이스를 지원하는 시스템의 경우, 슈퍼유저가 수행하는 복사는 system.*를 제외한 모든 네임스페이스를 복사합니다. 일반 사용자는 user.* 네임스페이스만 복사합니다. 일반 사용자로 비-user 네임스페이스를 백업하고 복원하려면 --fake-super 옵션을 참조하십시오. 위 이름 필터링은 x 수정자와 함께 하나 이상의 필터 옵션을 사용하여 재정의할 수 있습니다. xattr에 영향을 미치는 필터 규칙을 지정하면 rsync는 자체 시스템/사용자 필터링은 물론, 어떤 xattr 이름을 복사하고 어떤 이름을 삭제할 수 있는지에 대한 추가 필터링을 수행하도록 요구합니다. 예를 들어, system 네임스페이스를 건너뛰려면 다음과 같이 지정할 수 있습니다: --filter='-x system.*' user 네임스페이스를 제외한 모든 네임스페이스를 건너뛰려면 다음과 같이 부정-user 일치를 지정할 수 있습니다: --filter='-x! user.*' 어떤 속성도 삭제되지 않도록 하려면 모든 이름을 제외하는 수신자 전용 규칙을 지정할 수 있습니다: --filter='-xr *' -X 옵션은 --fake-super에 사용되는 것과 같은 rsync의 특수 xattr 값을 복사하지 않습니다 (옵션을 반복하지 않는 한, 예: -XX). 이 \"모든 xattrs 복사\" 모드는 --fake-super와 함께 사용할 수 없습니다. --chmod=CHMOD 이 옵션은 rsync에게 전송 중인 파일의 권한에 하나 이상의 쉼표로 구분된 \"chmod\" 모드를 적용하도록 지시합니다. 결과 값은 보내는 측이 파일에 제공한 권한인 것처럼 처리됩니다. 이는 --perms가 활성화되지 않은 경우 이 옵션이 기존 파일에 영향을 미 미치지 않는 것처럼 보일 수 있다는 의미입니다. chmod(1) 맨페이지에 지정된 일반적인 파싱 규칙 외에도, 디렉토리에만 적용되어야 하는 항목은 'D'를 접두사로 붙여 지정할 수 있고, 파일에만 적용되어야 하는 항목은 'F'를 접두사로 붙여 지정할 수 있습니다. 예를 들어, 다음은 모든 디렉토리가 set-gid로 표시되고, 어떤 파일도 다른 사용자가 쓸 수 없으며, 둘 다 사용자-쓰기 가능하고 그룹-쓰기 가능하며, 모든 비트에 걸쳐 일관된 실행 가능성을 갖도록 보장합니다: --chmod=Dg+s,ug+w,Fo-w,+X 8진수 모드 숫자 사용도 허용됩니다: --chmod=D2775,F664 여러 --chmod 옵션을 지정하는 것도 합법적입니다. 각 추가 옵션은 만들 변경 목록에 단순히 추가됩니다. 결과 권한 값이 전송 중인 파일에 어떻게 적용될 수 있는지에 대한 내용은 --perms 및 --executability 옵션을 참조하십시오. --owner, -o 이 옵션은 rsync가 대상 파일의 소유자를 소스 파일과 동일하게 설정하도록 하지만, 수신 rsync가 슈퍼유저로 실행되는 경우에만 그렇습니다 (--super 및 --fake-super 옵션도 참조). 이 옵션이 없으면 새 파일 및/또는 전송된 파일의 소유자는 수신 측의 호출 사용자로 설정됩니다. 소유권 보존은 기본적으로 일치하는 이름을 연결하지만, 일부 상황에서는 ID 번호를 사용하는 것으로 대체될 수 있습니다 (--numeric-ids 옵션에서 자세한 내용을 참조하십시오). --group, -g 이 옵션은 rsync가 대상 파일의 그룹을 소스 파일과 동일하게 설정하도록 합니다. 수신 프로그램이 슈퍼유저로 실행되지 않거나 (--no-super가 지정된 경우) 수신 측의 호출 사용자가 구성원인 그룹만 보존됩니다. 이 옵션이 없으면 그룹은 수신 측의 호출 사용자의 기본 그룹으로 설정됩니다. 그룹 정보 보존은 기본적으로 일치하는 이름을 연결하지만, 일부 상황에서는 ID 번호를 사용하는 것으로 대체될 수 있습니다 (--numeric-ids 옵션에서 자세한 내용을 참조하십시오). --devices 이 옵션은 rsync가 문자 및 블록 장치 파일을 원격 시스템으로 전송하여 이러한 장치를 다시 생성하도록 합니다. 수신 rsync가 슈퍼유저로 실행되지 않는 경우, rsync는 장치 파일 생성을 조용히 건너뜁니다 (--super 및 --fake-super 옵션도 참조). 기본적으로 rsync는 이 옵션이 설정되지 않았을 때 발견되는 각 장치 파일에 대해 \"비정규 파일 건너뛰기\" 경고를 생성합니다. --info=nonreg0을 지정하여 경고를 억제할 수 있습니다. --specials 이 옵션은 rsync가 명명된 소켓 및 FIFO와 같은 특수 파일을 전송하도록 합니다. 수신 rsync가 슈퍼유저로 실행되지 않는 경우, rsync는 특수 파일 생성을 조용히 건너뜁니다 (--super 및 --fake-super 옵션도 참조). 기본적으로 rsync는 이 옵션이 설정되지 않았을 때 발견되는 각 특수 파일에 대해 \"비정규 파일 건너뛰기\" 경고를 생성합니다. --info=nonreg0을 지정하여 경고를 억제할 수 있습니다. -D -D 옵션은 \"--devices --specials\"와 동일합니다. --copy-devices 이것은 rsync에게 보내는 측의 장치를 일반 파일처럼 처리하도록 지시하여, 일반 대상 파일로 (또는 --write-devices도 지정된 경우 다른 장치로) 복사할 수 있도록 합니다. 이 옵션은 rsync 데몬에 의해 기본적으로 거부됩니다. --write-devices 이것은 rsync에게 받는 측의 장치를 일반 파일처럼 처리하도록 지시하여, 파일 데이터를 장치에 쓸 수 있도록 합니다. 이 옵션은 --inplace 옵션을 암시합니다. 특히 rsync를 root로 실행할 때 전송의 받는 측에 어떤 장치가 있는지 알아야 하므로 이 옵션을 사용할 때 주의해야 합니다. 이 옵션은 rsync 데몬에 의해 기본적으로 거부됩니다. --times, -t 이것은 rsync에게 파일과 함께 수정 시간을 전송하고 원격 시스템에서 업데이트하도록 지시합니다. 이 옵션을 사용하지 않으면 수정되지 않은 파일을 제외하는 최적화가 효과적일 수 없습니다. 다시 말해, -t (또는 -a)가 없으면 다음 전송은 --ignore-times (-I)를 사용한 것처럼 동작하여 모든 파일이 업데이트됩니다 (비록 rsync의 델타-전송 알고리즘은 파일이 실제로 변경되지 않았다면 업데이트를 상당히 효율적으로 만들겠지만, -t를 사용하는 것이 훨씬 좋습니다). 전송 프로토콜 30 또는 31을 사용하는 최신 rsync는 최대 8바이트를 사용하여 수정 시간을 전달합니다. rsync가 이전 프로토콜을 사용하도록 강제되면 (아마도 원격 rsync가 3.0.0보다 오래되었기 때문에) 4바이트를 사용하여 수정 시간을 전달합니다. 3.2.7 이전에는 이러한 짧은 값들이 1901년 12월 13일부터 2038년 1월 19일까지의 날짜 범위를 전달할 수 있었습니다. 3.2.7부터 이러한 4바이트 값은 이제 1970년 1월 1일부터 2106년 2월 7일까지의 날짜 범위를 전달합니다. 1970년 이전 날짜의 파일이 있다면, 날짜의 전체 범위를 전달할 수 있도록 rsync 실행 파일을 업그레이드했는지 확인하십시오. --atimes, -U 이것은 rsync에게 대상 파일의 접근(사용) 시간을 소스 파일과 동일하게 설정하도록 지시합니다. 반복해서 사용하면 --open-noatime 옵션도 설정됩니다. 이는 파일을 전송한 후 rsync를 한 번 더 실행할 필요 없이 보내는 시스템과 받는 시스템이 전송된 파일에 대해 동일한 접근 시간을 가지도록 돕습니다. 일부 오래된 rsync 버전(3.2.0 이전)은 이 옵션을 반복할 때 --open-noatime을 암시하지 않는 사전 릴리스 --atimes 패치로 빌드되었을 수 있다는 점에 유의하십시오. --open-noatime 이것은 rsync에게 O_NOATIME 플래그(이를 지원하는 시스템에서)로 파일을 열어 전송 중인 파일의 접근 시간을 변경하지 않도록 지시합니다. 운영 체제가 O_NOATIME 플래그를 지원하지 않으면 rsync는 이 옵션을 조용히 무시합니다. 또한 일부 파일 시스템은 O_NOATIME 플래그가 설정되지 않아도 읽기 접근 시 atime 업데이트를 피하도록 마운트된다는 점에 유의하십시오. --crtimes, -N 이것은 rsync에게 대상 파일의 생성 시간(새로움)을 소스 파일과 동일하게 설정하도록 지시합니다. --omit-dir-times, -O 이것은 rsync에게 수정, 접근 및 생성 시간을 보존할 때 디렉토리를 생략하도록 지시합니다. NFS가 수신 측에서 디렉토리를 공유하는 경우 -O를 사용하는 것이 좋습니다. 이 옵션은 --backup을 --backup-dir 없이 사용하면 추론됩니다. 이 옵션은 또한 --inc-recursive 섹션에서 논의된 바와 같이 증분 재귀가 활성화되었을 때 누락된 하위 디렉토리의 조기 생성을 피하는 부작용이 있습니다. --omit-link-times, -J 이것은 rsync에게 수정, 접근 및 생성 시간을 보존할 때 심볼릭 링크를 생략하도록 지시합니다. --super 이것은 수신 rsync가 슈퍼유저가 실행한 것이 아니더라도 슈퍼유저 활동을 시도하도록 지시합니다. 이러한 활동에는 다음이 포함됩니다: --owner 옵션을 통한 사용자 보존, --group 옵션을 통한 모든 그룹 보존(현재 사용자의 그룹뿐만 아니라), --devices 옵션을 통한 장치 복사. 이것은 슈퍼유저가 아니어도 그러한 활동을 허용하는 시스템에 유용하며, 수신 측이 슈퍼유저로 실행되지 않는 경우 오류를 얻는 데도 유용합니다. 슈퍼유저 활동을 끄려면 슈퍼유저는 --no-super를 사용할 수 있습니다. --fake-super 이 옵션이 활성화되면 rsync는 특수 확장 속성(필요에 따라 각 파일에 첨부됨)을 통해 특권 속성을 저장/복원하여 슈퍼유저 활동을 시뮬레이션합니다. 여기에는 파일의 소유자 및 그룹(기본값이 아닌 경우), 파일의 장치 정보(장치 및 특수 파일은 빈 텍스트 파일로 생성됨), 실제 파일에 설정되지 않을 권한 비트(예: 실제 파일은 안전을 위해 u-s,g-s,o-t를 가짐) 또는 소유자의 접근을 제한하는 권한 비트(실제 슈퍼유저는 항상 파일에 접근/변경할 수 있으므로, 우리가 생성하는 파일은 항상 생성 사용자가 접근/변경할 수 있음)가 포함됩니다. 이 옵션은 ACL(--acls가 지정된 경우) 및 비-사용자 확장 속성(--xattrs가 지정된 경우)도 처리합니다. 이것은 슈퍼유저를 사용하지 않고 데이터를 백업하고, 호환되지 않는 시스템의 ACL을 저장하는 좋은 방법입니다. --fake-super 옵션은 옵션이 사용된 측에만 영향을 미칩니다. 원격-셸 연결의 원격 측에 영향을 미치려면 --remote-option (-M) 옵션을 사용하십시오: rsync -av -M--fake-super /src/ host:/dest/ 로컬 복사의 경우, 이 옵션은 소스 및 대상 모두에 영향을 미칩니다. 로컬 복사에서 이 옵션을 대상 파일에만 활성화하려면 -M--fake-super를 지정하십시오. 로컬 복사에서 이 옵션을 소스 파일에만 활성화하려면 --fake-super를 -M--super와 결합하십시오. 이 옵션은 --super 및 --no-super 모두에 의해 재정의됩니다. 데몬의 rsyncd.conf 파일에 있는 fake super 설정도 참조하십시오. --sparse, -S 스파스 파일을 효율적으로 처리하여 대상에서 공간을 덜 차지하도록 합니다. --inplace와 결합하면 일부 커널 버전 및/또는 파일 시스템 유형 조합에서는 생성된 파일이 스파스 블록으로 끝나지 않을 수 있습니다. --whole-file이 적용되는 경우(예: 로컬 복사의 경우) rsync가 업데이트된 버전을 쓰기 전에 파일을 잘라내므로 항상 작동합니다. rsync 3.1.3 이전 버전은 --sparse와 --inplace의 조합을 거부한다는 점에 유의하십시오. --preallocate 이것은 수신자에게 데이터를 파일에 쓰기 전에 각 대상 파일을 최종 크기로 할당하도록 지시합니다. Rsync는 Linux의 fallocate(2) 시스템 호출 또는 Cygwin의 posix_fallocate(3)가 제공하는 실제 파일 시스템 수준의 사전 할당 지원만 사용하며, 각 블록에 널 바이트를 쓰는 느린 glibc 구현은 사용하지 않습니다. 이 옵션이 없으면 큰 파일은 파일 시스템에서 완전히 연속적이지 않을 수 있지만, 이 옵션을 사용하면 rsync가 더 느리게 복사될 수 있습니다. 대상이 익스텐트(extent)를 지원하는 파일 시스템(예: ext4, xfs, NTFS 등)이 아니면 이 옵션은 전혀 긍정적인 효과를 내지 못할 수 있습니다. --sparse와 결합하면, 할당된 데이터에 구멍을 생성하는 커널 버전 및 파일 시스템 유형이 지원하는 경우에만 파일에 스파스 블록(할당된 널 바이트 시퀀스 대신)이 생성됩니다. --dry-run, -n 이것은 rsync가 변경 사항을 만들지 않는 시범 실행을 수행하도록 합니다 (실제 실행과 대부분 동일한 출력을 생성합니다). 가장 일반적으로 --verbose (-v) 및/또는 --itemize-changes (-i) 옵션과 함께 사용하여 rsync 명령이 실제로 실행되기 전에 무엇을 할지 확인하는 데 사용됩니다. --itemize-changes의 출력은 드라이 런과 후속 실제 실행에서 정확히 동일해야 합니다 (의도적인 속임수 및 시스템 호출 실패 제외). 만약 동일하지 않다면 버그입니다. 다른 출력은 대부분 변경되지 않아야 하지만, 일부 영역에서 다를 수 있습니다. 특히, 드라이 런은 파일 전송을 위한 실제 데이터를 보내지 않으므로, --progress는 효과가 없고, \"전송된 바이트\", \"수신된 바이트\", \"리터럴 데이터\", \"일치하는 데이터\" 통계가 너무 작으며, \"속도 향상\" 값은 파일 전송이 필요 없는 실행과 동일합니다. --whole-file, -W 이 옵션은 rsync의 델타-전송 알고리즘을 비활성화하여, 전송되는 모든 파일이 통째로 보내지도록 합니다. 소스와 대상 머신 간의 대역폭이 디스크 대역폭보다 높은 경우 (특히 \"디스크\"가 실제로 네트워크 파일 시스템인 경우) 이 옵션을 사용하면 전송이 더 빨라질 수 있습니다. 이는 소스와 대상이 모두 로컬 경로로 지정될 때의 기본값이지만, 일괄 쓰기(batch-writing) 옵션이 적용되지 않는 경우에만 해당합니다. --no-whole-file, --no-W 로컬 전송에 대해 기본적으로 전체 파일 업데이트가 활성화된 경우 이를 비활성화합니다. 이는 일반적으로 rsync 속도를 늦추지만, 대상 파일에 대한 쓰기를 최소화하려는 경우(--inplace와 결합 시) 또는 체크섬 기반 업데이트 알고리즘을 테스트하는 데 유용할 수 있습니다. --whole-file 옵션도 참조하십시오. --checksum-choice=STR, --cc=STR 이 옵션은 체크섬 알고리즘을 재정의합니다. 하나의 알고리즘 이름이 지정되면, 전송 체크섬과 ( --checksum이 지정되었다고 가정하면) 전송 전 체크섬 모두에 사용됩니다. 두 개의 쉼표로 구분된 이름이 제공되면, 첫 번째 이름은 전송 체크섬에 영향을 미치고, 두 번째 이름은 전송 전 체크섬(-c)에 영향을 미칩니다. 사용할 수 있는 체크섬 옵션은 다음과 같습니다: o auto (기본 자동 선택) o xxh128 o xxh3 o xxh64 (일명 xxhash) o md5 o md4 o sha1 o none rsync --version을 실행하여 자신의 버전에 컴파일된 기본 체크섬 목록을 확인하십시오 (위 목록과 다를 수 있습니다). 첫 번째 (또는 유일한) 이름에 \"none\"이 지정되면, --whole-file 옵션이 강제 적용되고 전송된 데이터에 대한 체크섬 검증이 수행되지 않습니다. 두 번째 (또는 유일한) 이름에 \"none\"이 지정되면, --checksum 옵션을 사용할 수 없습니다. \"auto\" 옵션은 기본값이며, rsync는 클라이언트와 서버 간의 협상에 따라 알고리즘을 선택합니다: 전송의 양쪽이 최소 3.2.0 버전인 경우, rsync는 클라이언트의 선택 목록에 있는 첫 번째 알고리즘 중에서 서버의 선택 목록에도 있는 것을 선택합니다. 공통 체크섬 선택이 발견되지 않으면 rsync는 오류로 종료됩니다. 원격 rsync가 너무 오래되어 체크섬 협상을 지원하지 않으면, 프로토콜 버전에 따라 값이 선택됩니다 (프로토콜 기간에 따라 MD5와 다양한 MD4 버전 중 하나를 선택합니다). 기본 순서는 환경 변수 RSYNC_CHECKSUM_LIST를 허용되는 체크섬 이름의 공백으로 구분된 목록으로 설정하여 사용자 정의할 수 있습니다. 문자열에 \"&\" 문자가 포함된 경우 \"클라이언트 문자열 & 서버 문자열\"로 분리되고, 그렇지 않으면 동일한 문자열이 둘 다에 적용됩니다. 문자열(또는 문자열 부분)에 공백이 아닌 문자가 없으면 기본 체크섬 목록이 사용됩니다. 유효하지 않은 이름만 있는 목록은 협상 실패로 이어집니다. --checksum-choice 옵션을 사용하면 이 환경 목록이 재정의됩니다. --one-file-system, -x 이것은 rsync에게 재귀할 때 파일 시스템 경계를 넘지 않도록 지시합니다. 이는 사용자가 여러 파일 시스템에서 복사할 항목을 지정하는 능력을 제한하지 않고, 사용자가 지정한 각 디렉토리 계층 구조 내에서 rsync의 재귀만 제한하며, 삭제 시 수신 측에서도 유사한 재귀를 제한합니다. 또한 rsync는 동일한 장치에 대한 \"바인드\" 마운트를 동일한 파일 시스템에 있는 것으로 처리한다는 점을 명심하십시오. 이 옵션을 반복하면 rsync는 모든 마운트 지점 디렉토리를 복사에서 제외합니다. 그렇지 않으면, 발견되는 각 마운트 지점에 빈 디렉토리를 포함합니다 (마운트된 디렉토리의 속성을 사용하며, 기본 마운트 지점 디렉토리의 속성은 접근할 수 없습니다). rsync가 심볼릭 링크를 축소하도록 지시받은 경우 (--copy-links 또는 --copy-unsafe-links를 통해), 다른 장치에 대한 디렉토리 심볼릭 링크는 마운트 지점처럼 처리됩니다. 비디렉토리 심볼릭 링크는 이 옵션의 영향을 받지 않습니다. --ignore-non-existing, --existing 이것은 rsync에게 대상에 아직 존재하지 않는 파일(디렉토리 포함) 생성을 건너뛰도록 지시합니다. 이 옵션을 --ignore-existing 옵션과 결합하면 어떤 파일도 업데이트되지 않습니다 (이는 불필요한 파일만 삭제하려는 경우 유용할 수 있습니다). 이 옵션은 TRANSFER RULE이므로, 어떤 제외 부작용도 기대하지 마십시오. --ignore-existing 이것은 rsync에게 대상에 이미 존재하는 파일 업데이트를 건너뛰도록 지시합니다 (기존 디렉토리는 무시하지 않으므로, 아무것도 수행되지 않을 것입니다). --ignore-non-existing도 참조하십시오. 이 옵션은 TRANSFER RULE이므로, 어떤 제외 부작용도 기대하지 마십시오. 이 옵션은 중단된 백업 실행을 계속해야 할 때 --link-dest 옵션을 사용하여 백업을 수행하는 사람들에게 유용할 수 있습니다. --link-dest 실행이 새 디렉토리 계층으로 복사되므로 (제대로 사용될 때), --ignore-existing을 사용하면 이미 처리된 파일이 수정되지 않도록 보장합니다 (하드 링크된 파일의 권한 변경을 방지합니다). 이는 이 옵션이 대상 계층 자체의 기존 파일만 본다는 것을 의미합니다. --info=skip2가 사용될 때 rsync는 \"FILENAME exists (INFO)\" 메시지를 출력하며, 여기서 INFO는 \"type change\", \"sum change\" (-c 필요), \"file change\" (빠른 검사 기반), \"attr change\", 또는 \"uptodate\" 중 하나를 나타냅니다. --info=skip1 (2개의 -v 옵션으로도 암시됨)을 사용하면 INFO 접미사 없이 exists 메시지를 출력합니다. --remove-source-files 이것은 rsync에게 전송의 일부이며 수신 측에서 성공적으로 복제된 파일 (비디렉토리 의미)을 보내는 측에서 제거하도록 지시합니다. 이 옵션은 정지(quiescent)된 소스 파일에만 사용해야 합니다. 특정 디렉토리에 나타나는 파일을 다른 호스트로 이동하는 데 이 옵션을 사용하는 경우, 완성된 파일이 소스 디렉토리에 직접 쓰이는 것이 아니라 소스 디렉토리로 이름이 변경되도록 하십시오. 이렇게 하면 rsync가 아직 완전히 쓰여지지 않은 파일을 전송할 가능성을 막을 수 있습니다. 파일을 먼저 다른 디렉토리에 쓸 수 없다면, rsync가 아직 완성되지 않은 파일을 전송하는 것을 피할 수 있는 명명 관용구(예: 파일이 쓰여질 때 \"foo.new\"로 이름 지정, 완료되면 \"foo\"로 이름 변경, 그리고 rsync 전송에 --exclude='*.new' 옵션 사용)를 사용해야 합니다. 3.1.0부터 rsync는 파일의 크기 또는 수정 시간이 변경되지 않은 경우 보내는 측에서의 제거를 건너뛰고 (오류를 출력합니다). 3.2.6부터 로컬 rsync 복사는 사용자가 실수로 소스 및 대상 디렉토리를 동일한 경로로 지정한 경우와 같이, 수신자가 방금 확인한 파일을 보내는 측이 제거하지 않도록 합니다. --delete 이것은 rsync에게 수신 측에서 불필요한 파일(보내는 측에 없는 파일)을 삭제하도록 지시하지만, 동기화되는 디렉토리에 대해서만 그렇습니다. 와일드카드를 사용하여 디렉토리의 내용(예: \"dir/*\")을 지정하지 않고 rsync에게 전체 디렉토리(예: \"dir\" 또는 \"dir/\")를 보내도록 요청해야 합니다. 와일드카드는 셸에 의해 확장되고 rsync는 개별 파일을 전송하도록 요청받기 때문입니다. 전송에서 제외된 파일도 --delete-excluded 옵션을 사용하거나 규칙을 보내는 측에서만 일치하도록 표시하지 않으면 삭제에서 제외됩니다 (FILTER RULES 섹션의 포함/제외 수정자 참조). rsync 2.6.7 이전에는 이 옵션이 --recursive가 활성화되지 않으면 효과가 없었습니다. 2.6.7부터는 --dirs (-d)가 활성화될 때도 삭제가 발생하지만, 내용이 복사되는 디렉토리에 대해서만 그렇습니다. 이 옵션은 잘못 사용하면 위험할 수 있습니다! --dry-run (-n) 옵션을 사용하여 어떤 파일이 삭제될 것인지 먼저 확인하는 것이 매우 좋습니다. 보내는 측에서 I/O 오류가 감지되면 대상의 파일 삭제가 자동으로 비활성화됩니다. 이는 보내는 측의 일시적인 파일 시스템 오류(예: NFS 오류)가 대상의 대량 파일 삭제를 유발하는 것을 방지하기 위함입니다. --ignore-errors 옵션으로 이를 재정의할 수 있습니다. --delete 옵션은 --delete-WHEN 옵션 중 하나와 충돌 없이 결합될 수 있으며, --delete-excluded와도 결합될 수 있습니다. 그러나 --delete-WHEN 옵션 중 어느 것도 지정되지 않은 경우, rsync 3.0.0 이상과 통신할 때는 --delete-during 알고리즘을, 오래된 rsync와 통신할 때는 --delete-before 알고리즘을 선택합니다. --delete-delay 및 --delete-after도 참조하십시오. --delete-before 전송 시작 전에 수신 측에서 파일 삭제를 수행하도록 요청합니다. 파일 삭제에 대한 자세한 내용은 --delete (암시됨)를 참조하십시오. 전송 전에 삭제하는 것은 파일 시스템 공간이 부족하여 불필요한 파일을 제거하는 것이 전송을 가능하게 하는 데 도움이 될 때 유용합니다. 그러나 전송 시작 전에 지연이 발생하며, 이 지연으로 인해 전송이 타임아웃될 수 있습니다 (--timeout이 지정된 경우). 또한 rsync가 전송의 모든 파일을 한 번에 메모리로 스캔해야 하는 오래된 비증분 재귀 알고리즘을 사용하도록 강제합니다 (--recursive 참조). --delete-during, --del 전송 중에 수신 측에서 파일 삭제를 점진적으로 수행하도록 요청합니다. 디렉토리별 삭제 스캔은 각 디렉토리가 업데이트를 위해 검사되기 직전에 수행되므로, 더 효율적인 --delete-before처럼 작동하며, 디렉토리별 필터 파일이 업데이트되기 전에 삭제를 수행합니다. 이 옵션은 rsync 버전 2.6.4에 처음 추가되었습니다. 파일 삭제에 대한 자세한 내용은 --delete (암시됨)를 참조하십시오. --delete-delay 수신 측에서 파일 삭제가 전송 중에 계산된 후 (--delete-during처럼) 전송 완료 후에 제거되도록 요청합니다. 이는 --delay-updates 및/또는 --fuzzy와 결합할 때 유용하며, --delete-after를 사용하는 것보다 효율적입니다 (그러나 --delete-after는 모든 업데이트가 완료된 후 별도의 패스에서 삭제를 계산하므로 다르게 작동할 수 있습니다). 제거된 파일 수가 내부 버퍼를 초과하면 수신 측에 이름을 저장할 임시 파일이 생성됩니다 (열려 있는 동안 제거되므로 전송 중에는 보이지 않습니다). 임시 파일 생성이 실패하면 rsync는 --delete-after를 사용하는 것으로 대체하려고 시도합니다 (그러나 --recursive가 증분 스캔을 수행하는 경우에는 불가능합니다). 파일 삭제에 대한 자세한 내용은 --delete (암시됨)를 참조하십시오. --delete-after 전송이 완료된 후에 수신 측에서 파일 삭제를 수행하도록 요청합니다. 이는 전송의 일부로 새로운 디렉토리별 병합 파일을 보내고 해당 제외가 현재 전송의 삭제 단계에 영향을 미치도록 하려는 경우에 유용합니다. 또한 rsync가 전송의 모든 파일을 한 번에 메모리로 스캔해야 하는 오래된 비증분 재귀 알고리즘을 사용하도록 강제합니다 (--recursive 참조). 파일 삭제에 대한 자세한 내용은 --delete (암시됨)를 참조하십시오. 전송이 끝날 때 삭제가 발생하기를 원하는 사람들에게 더 빠른 선택일 수 있는 --delete-delay 옵션도 참조하십시오. --delete-excluded 이 옵션은 불확정적인 제외/포함 규칙을 수신자의 삭제에 영향을 미치지 않는 서버 측 규칙으로 전환합니다. 기본적으로 제외 또는 포함은 서버 측 효과(서버의 파일 목록을 생성할 때 파일을 \"숨기거나\" \"표시함\")와 수신자 측 효과(삭제가 발생할 때 파일을 \"보호하거나\" \"위험에 노출시킴\")를 모두 가집니다. 어떤 측에서 실행될지 지정하는 수정자가 없는 모든 규칙은 대신 서버 측 규칙으로만 처리되어 규칙의 \"보호\" 효과를 피합니다. 이 옵션이 지정되더라도 규칙에 전송자와 수신자 수정자 문자(예: -f'-sr foo')가 모두 주어지면 규칙은 여전히 양쪽에 적용될 수 있습니다. 수신자 측 보호/위험 규칙은 삭제를 제한하기 위해 명시적으로 지정될 수도 있습니다. 이것은 많은 -f'- foo' 규칙을 -f'-s foo' (일명 -f'H foo') 규칙으로 편집하는 수고를 덜어줍니다 (해당 포함 규칙은 말할 것도 없습니다). 자세한 내용은 FILTER RULES 섹션을 참조하십시오. 삭제에 대한 자세한 내용은 --delete (암시됨)를 참조하십시오. --ignore-missing-args rsync가 명시적으로 요청된 소스 파일(예: 명령줄 인자 또는 --files-from 항목)을 처음 처리할 때, 파일을 찾을 수 없으면 일반적으로 오류가 발생합니다. 이 옵션은 해당 오류를 억제하고 파일을 전송하려고 시도하지 않습니다. 이는 파일이 처음에 존재한다고 발견되었지만 나중에 더 이상 존재하지 않는 경우의 후속 파일 사라짐 오류에는 영향을 미치지 않습니다. --delete-missing-args 이 옵션은 (암시된) --ignore-missing-args 옵션의 동작을 한 단계 더 나아갑니다: 각 누락된 인자는 수신 측의 해당 대상 파일에 대한 삭제 요청이 됩니다 (존재하는 경우). 대상 파일이 비어 있지 않은 디렉토리인 경우, --force 또는 --delete가 적용되어야만 성공적으로 삭제됩니다. 그 외에는 이 옵션은 다른 유형의 삭제 처리와는 무관합니다. 누락된 소스 파일은 --list-only 출력에서 \"*missing\" 항목으로 표시되는 특수 파일-목록 항목으로 표현됩니다. --ignore-errors I/O 오류가 있을 때도 파일을 삭제하도록 --delete에게 지시합니다. --force 이 옵션은 rsync에게 비어 있지 않은 디렉토리를 비디렉토리로 교체할 때 삭제하도록 지시합니다. 이는 삭제가 활성화되어 있지 않을 때만 관련됩니다 (--delete에서 자세한 내용을 참조하십시오). 오래된 rsync 버전 참고: --force는 --delete-after를 사용할 때도 필요했으며, --recursive 옵션도 활성화되지 않으면 작동하지 않았습니다. --max-delete=NUM 이것은 rsync에게 NUM개 이상의 파일이나 디렉토리를 삭제하지 않도록 지시합니다. 이 제한을 초과하면 나머지 모든 삭제는 전송이 끝날 때까지 건너뛰어집니다. 마지막에 rsync는 경고(건너뛴 삭제 수 포함)를 출력하고 오류 코드 25로 종료합니다 (더 중요한 다른 오류 조건이 발생하지 않은 경우). 버전 3.0.0부터 --max-delete=0을 지정하여 대상에 있는 모든 불필요한 파일에 대한 경고를 받을 수 있지만 아무것도 제거하지는 않습니다. 오래된 클라이언트는 이것을 \"무제한\"으로 해석했으므로, 클라이언트 버전을 모르는 경우, 어떤 삭제도 허용되지 않도록 지정하는 역방향 호환성 방법으로 덜 명확한 --max-delete=-1을 사용할 수 있습니다 (그러나 아주 오래된 버전은 제한을 초과했을 때 경고하지 않았습니다). --max-size=SIZE 이것은 rsync에게 지정된 SIZE보다 큰 파일은 전송하지 않도록 지시합니다. 숫자 값 뒤에 단위 문자열을 붙이거나, 접미사 없이 바이트를 지정할 수 있습니다. --max-size=1.5m와 같이 분수 값을 단위와 함께 자유롭게 사용하십시오. 이 옵션은 TRANSFER RULE이므로, 어떤 제외 부작용도 기대하지 마십시오. 단위 문자열의 첫 글자는 B (바이트), K (킬로), M (메가), G (기가), T (테라), P (페타)일 수 있습니다. 문자열이 단일 문자이거나 \"ib\"가 추가된 경우 (예: \"G\" 또는 \"GiB\") 단위는 1024의 배수입니다. \"B\"로 끝나는 두 글자 접미사 (예: \"kb\")를 사용하면 1000의 배수 단위가 됩니다. 문자열의 문자는 원하는 대로 대소문자를 혼합할 수 있습니다. 마지막으로, 문자열이 \"+1\" 또는 \"-1\"로 끝나면 지정된 방향으로 1바이트만큼 오프셋됩니다. 가능한 가장 큰 값은 일반적으로 8192P-1입니다. 예: --max-size=1.5mb-1은 1499999 바이트이고, --max-size=2g+1은 2147483649 바이트입니다. rsync 3.1.0 이전 버전은 --max-size=0을 허용하지 않았다는 점에 유의하십시오. --min-size=SIZE 이것은 rsync에게 지정된 SIZE보다 작은 파일은 전송하지 않도록 지시합니다. 이는 작고 쓸모없는 파일을 전송하지 않는 데 도움이 될 수 있습니다. SIZE 및 기타 정보에 대한 설명은 --max-size 옵션을 참조하십시오. rsync 3.1.0 이전 버전은 --min-size=0을 허용하지 않았다는 점에 유의하십시오. --max-alloc=SIZE 기본적으로 rsync는 개별 malloc/realloc을 약 1GB로 제한합니다. 대부분의 사람들에게 이 제한은 잘 작동하며 프로토콜 오류로 인해 rsync가 엄청난 양의 메모리를 요청하는 것을 방지합니다. 그러나 전송할 파일이 수백만 개, 서버 메모리가 매우 많고, 전송을 여러 부분으로 나누고 싶지 않다면, 할당당 제한을 더 크게 늘릴 수 있으며 rsync는 더 많은 메모리를 소비할 것입니다. 이것은 할당된 총 메모리 크기에 대한 제한이 아니라는 점을 명심하십시오. 각 개별 할당에 대한 건전성 검사 값입니다. SIZE를 지정하는 방법에 대한 설명은 --max-size 옵션을 참조하십시오. 접미사가 제공되지 않으면 기본값은 바이트입니다. 3.2.3부터 0 값은 제한 없음(no limit)을 지정합니다. 환경 변수 RSYNC_MAX_ALLOC을 이 옵션이 지원하는 SIZE 값과 동일하게 사용하여 기본값을 설정할 수 있습니다. 원격 rsync가 --max-alloc 옵션을 이해하지 못하는 경우, --max-alloc=1g를 지정하여 환경 값을 재정의할 수 있습니다. 이렇게 하면 rsync는 원격 측으로 옵션을 보내지 않습니다 ( \"1G\"가 기본값이기 때문입니다). --block-size=SIZE, -B 이것은 rsync의 델타-전송 알고리즘에 사용되는 블록 크기를 고정된 값으로 강제합니다. 일반적으로 업데이트되는 각 파일의 크기에 따라 선택됩니다. 자세한 내용은 기술 보고서를 참조하십시오. 3.2.3부터 SIZE는 --max-size 옵션에 자세히 설명된 접미사와 함께 지정될 수 있습니다. 이전 버전은 바이트 수만 허용했습니다. --rsh=COMMAND, -e 이 옵션을 사용하면 로컬 rsync 복사본과 원격 rsync 복사본 간의 통신에 사용할 대체 원격 셸 프로그램을 선택할 수 있습니다. 일반적으로 rsync는 기본적으로 ssh를 사용하도록 구성되지만, 로컬 네트워크에서는 rsh를 사용하는 것을 선호할 수 있습니다. 이 옵션이 [user@]host::module/path와 함께 사용되면, 원격 셸 COMMAND는 원격 호스트에서 rsync 데몬을 실행하는 데 사용되며, 모든 데이터는 실행 중인 원격 rsync 데몬에 대한 직접 소켓 연결을 통하는 대신 해당 원격 셸 연결을 통해 전송됩니다. 위의 USING RSYNC-DAEMON FEATURES VIA A REMOTE-SHELL CONNECTION 섹션을 참조하십시오. rsync 3.2.0부터, 데몬 연결이 원격-셸 연결을 통해 이루어질 때 RSYNC_PORT 환경 변수가 설정됩니다. 기본 데몬 포트가 가정되면 0으로 설정되거나, --port 옵션 또는 rsync:// URL의 비어 있지 않은 포트 값을 통해 지정된 rsync 포트 값으로 설정됩니다. 이를 통해 스크립트는 비기본 포트가 요청되는지 여부를 식별하여, SSL 또는 stunnel 도우미 스크립트가 기본 또는 대체 포트에 연결하는 것과 같은 작업을 수행할 수 있도록 합니다. COMMAND가 단일 인자로 rsync에 제시되는 한, COMMAND에서 명령줄 인자가 허용됩니다. 명령과 인자를 서로 분리하려면 공백(탭 또는 다른 공백 문자 아님)을 사용해야 하며, 인자 내 공백을 보존하기 위해 단일 또는 이중 따옴표를 사용할 수 있습니다 (단, 역슬래시는 안 됨). 단일 따옴표로 묶인 문자열 내에서 단일 따옴표를 두 번 사용하면 단일 따옴표가 됩니다. 이중 따옴표도 마찬가지입니다 (어떤 따옴표를 셸이 파싱하고 어떤 따옴표를 rsync가 파싱하는지 주의해야 합니다). 몇 가지 예시: -e 'ssh -p 2234' -e 'ssh -o \"ProxyCommand nohup ssh firewall nc -w1 %h %p\"' (ssh 사용자는 .ssh/config 파일에서 사이트별 연결 옵션을 사용자 정의할 수도 있습니다.) RSYNC_RSH 환경 변수를 사용하여 원격 셸 프로그램을 선택할 수도 있으며, 이 변수는 -e와 동일한 범위의 값을 허용합니다. 이 옵션에 영향을 받는 --blocking-io 옵션도 참조하십시오. --rsync-path=PROGRAM 원격 머신에서 rsync를 시작하기 위해 실행할 프로그램을 지정하는 데 사용합니다. rsync가 기본 원격 셸의 경로에 없는 경우(예: --rsync-path=/usr/local/bin/rsync) 종종 사용됩니다. PROGRAM은 셸의 도움을 받아 실행되므로, rsync가 통신에 사용하는 표준 입력 및 표준 출력을 손상시키지 않는 한 모든 프로그램, 스크립트 또는 명령 시퀀스가 될 수 있습니다. 한 가지 까다로운 예는 --relative 옵션과 함께 사용하기 위해 원격 머신에서 다른 기본 디렉토리를 설정하는 것입니다. 예를 들어: rsync -avR --rsync-path=\"cd /a/b && rsync\" host:c/d /e/ --remote-option=OPTION, -M 이 옵션은 전송의 한쪽에만 특정 효과를 제한하려는 고급 상황에 사용됩니다. 예를 들어, --log-file=FILE 및 --fake-super를 원격 시스템에 전달하려면 다음과 같이 지정하십시오: rsync -av -M --log-file=foo -M--fake-super src/ dest/ 옵션이 일반적으로 양쪽에 영향을 미치지만, 로컬 측에만 영향을 미치도록 하려면 원격 측에 해당 옵션의 부정을 보내십시오. 예: rsync -av -x -M--no-x src/ dest/ 이 옵션을 사용할 때는 주의하십시오. rsync가 소켓을 통해 다음에 어떤 데이터를 예상해야 하는지에 대해 다른 개념을 갖게 하는 옵션을 토글할 수 있으며, 이는 알 수 없는 방식으로 실패하게 만들 것입니다. 전달하려는 원격 옵션마다 별도의 -M 옵션을 사용해야 합니다. 오래된 rsync 버전에서는 remote-option 인자에 공백이 있으면 별도의 원격 인자로 분할될 수 있었지만, 최신 rsync에서는 --old-args를 사용해야 합니다. 로컬 전송을 수행할 때 \"로컬\" 측은 전송자이고 \"원격\" 측은 수신자입니다. popt 옵션 파싱 라이브러리의 일부 버전에는 짧은 옵션 문자 옆에 등호가 있는 인자(--log-file=/tmp/foo와 같은)를 사용할 수 없게 하는 버그가 있다는 점에 유의하십시오. 이 버그가 popt 버전에 영향을 미치는 경우, rsync에 포함된 popt 버전을 사용할 수 있습니다. --cvs-exclude, -C 이것은 시스템 간에 전송하고 싶지 않은 광범위한 파일을 제외하는 데 유용한 약식입니다. CVS와 유사한 알고리즘을 사용하여 파일을 무시해야 하는지 여부를 결정합니다. 제외 목록은 다음 항목을 제외하도록 초기화됩니다 (이 초기 항목은 소멸성으로 표시됩니다 -- FILTER RULES 섹션 참조): RCS SCCS CVS CVS.adm RCSLOG cvslog.* tags TAGS .make.state .nse_depinfo *~ #* .#* ,* _$* *$ *.old *.bak *.BAK *.orig *.rej .del-* *.a *.olb *.o *.obj *.so *.exe *.Z *.elc *.ln core .svn/ .git/ .hg/ .bzr/ 그런 다음, $HOME/.cvsignore에 나열된 파일이 목록에 추가되고, CVSIGNORE 환경 변수에 나열된 모든 파일이 추가됩니다 (모든 cvsignore 이름은 공백으로 구분됩니다). 마지막으로, .cvsignore 파일과 동일한 디렉토리에 있고 그 안에 나열된 패턴 중 하나와 일치하는 모든 파일은 무시됩니다. rsync의 필터/제외 파일과 달리, 이러한 패턴은 공백으로 분할됩니다. 자세한 내용은 cvs(1) 매뉴얼을 참조하십시오. -C를 자신만의 --filter 규칙과 결합하는 경우, 이러한 CVS 제외는 명령줄에서 -C가 어디에 배치되었는지와 관계없이 자신만의 규칙 끝에 추가된다는 점을 유의해야 합니다. 이것은 명시적으로 지정한 규칙보다 우선순위가 낮다는 의미입니다. 이러한 CVS 제외가 필터 규칙에 삽입되는 위치를 제어하려면, 명령줄 옵션으로 -C를 생략하고 --filter=:C 및 --filter=-C 조합을 사용해야 합니다 (명령줄에서 또는 다른 규칙과 함께 필터 파일에 \":C\" 및 \"-C\" 규칙을 넣어서). 첫 번째 옵션은 .cvsignore 파일에 대한 디렉토리별 스캔을 켭니다. 두 번째 옵션은 위에 언급된 CVS 제외를 한 번 가져옵니다. --filter=RULE, -f 이 옵션을 사용하면 전송할 파일 목록에서 특정 파일을 선택적으로 제외하는 규칙을 추가할 수 있습니다. 이것은 재귀적 전송과 함께 사용할 때 가장 유용합니다. 명령줄에서 원하는 만큼 많은 --filter 옵션을 사용하여 제외할 파일 목록을 만들 수 있습니다. 필터에 공백이 포함되어 있으면 셸이 규칙을 rsync에 단일 인자로 전달하도록 반드시 인용(quote)하십시오. 아래 텍스트에는 규칙과 그 인자를 구분하는 공백 대신 밑줄을 사용할 수 있다는 내용도 언급되어 있습니다. 이 옵션에 대한 자세한 정보는 FILTER RULES 섹션을 참조하십시오. -F -F 옵션은 명령에 두 가지 --filter 규칙을 추가하는 약식입니다. 처음 사용될 때는 다음 규칙의 약식입니다: --filter='dir-merge /.rsync-filter' 이것은 rsync에게 계층 구조에 흩어져 있는 디렉토리별 .rsync-filter 파일을 찾아 전송 중인 파일을 필터링하는 데 그 규칙을 사용하도록 지시합니다. -F를 반복하면 다음 규칙의 약식입니다: --filter='exclude .rsync-filter' 이것은 .rsync-filter 파일 자체를 전송에서 필터링합니다. 이 옵션들이 어떻게 작동하는지에 대한 자세한 정보는 FILTER RULES 섹션을 참조하십시오. --exclude=PATTERN 이 옵션은 --filter 옵션의 단순화된 형태로, 제외 규칙을 지정하며 일반 필터 규칙의 전체 규칙 파싱 구문을 허용하지 않습니다. 이는 -f'- PATTERN'을 지정하는 것과 동일합니다. 이 옵션에 대한 자세한 정보는 FILTER RULES 섹션을 참조하십시오. --exclude-from=FILE 이 옵션은 --exclude 옵션과 관련이 있지만, 제외 패턴(줄당 하나)이 포함된 FILE을 지정합니다. 파일의 빈 줄은 무시되며, ';' 또는 '#'으로 시작하는 전체 줄 주석도 무시됩니다 (이러한 문자를 포함하는 파일 이름 규칙은 영향을 받지 않습니다). 줄이 \"- \" (대시, 공백) 또는 \"+ \" (플러스, 공백)으로 시작하면, 규칙 유형이 명시적으로 제외 또는 포함으로 지정된 것입니다. 이러한 접두사가 없는 모든 규칙은 제외로 간주됩니다. 줄이 단지 \"!\"로 구성되면, 추가 규칙을 추가하기 전에 현재 필터 규칙이 지워집니다. FILE이 '-'이면 목록은 표준 입력에서 읽힙니다. --include=PATTERN 이 옵션은 --filter 옵션의 단순화된 형태로, 포함 규칙을 지정하며 일반 필터 규칙의 전체 규칙 파싱 구문을 허용하지 않습니다. 이는 -f'+ PATTERN'을 지정하는 것과 동일합니다. 이 옵션에 대한 자세한 정보는 FILTER RULES 섹션을 참조하십시오. --include-from=FILE 이 옵션은 --include 옵션과 관련이 있지만, 포함 패턴(줄당 하나)이 포함된 FILE을 지정합니다. 파일의 빈 줄은 무시되며, ';' 또는 '#'으로 시작하는 전체 줄 주석도 무시됩니다 (이러한 문자를 포함하는 파일 이름 규칙은 영향을 받지 않습니다). 줄이 \"- \" (대시, 공백) 또는 \"+ \" (플러스, 공백)으로 시작하면, 규칙 유형이 명시적으로 제외 또는 포함으로 지정된 것입니다. 이러한 접두사가 없는 모든 규칙은 포함으로 간주됩니다. 줄이 단지 \"!\"로 구성되면, 추가 규칙을 추가하기 전에 현재 필터 규칙이 지워집니다. FILE이 '-'이면 목록은 표준 입력에서 읽힙니다. --files-from=FILE 이 옵션을 사용하면 전송할 파일의 정확한 목록을 지정할 수 있습니다 (지정된 FILE 또는 표준 입력에서 '-'로 읽음). 또한 rsync의 기본 동작을 조정하여 지정된 파일과 디렉토리만 더 쉽게 전송할 수 있도록 합니다: o --relative (-R) 옵션이 암시됩니다. 이는 파일의 각 항목에 대해 지정된 경로 정보를 보존합니다 (--no-relative 또는 --no-R을 사용하여 끄고 싶다면). o --dirs (-d) 옵션이 암시됩니다. 이는 목록에 지정된 디렉토리를 시끄럽게 건너뛰는 대신 대상에 생성합니다 (--no-dirs 또는 --no-d를 사용하여 끄고 싶다면). o --archive (-a) 옵션의 동작은 --recursive (-r)를 암시하지 않으므로, 원한다면 명시적으로 지정하십시오. o 이러한 부작용은 rsync의 기본 상태를 변경하므로, 명령줄에서 --files-from 옵션의 위치는 다른 옵션이 파싱되는 방식에 영향을 미치지 않습니다 (예: -a는 --files-from 앞이나 뒤에서 동일하게 작동하며, --no-R 및 다른 모든 옵션도 마찬가지입니다). FILE에서 읽은 파일 이름은 모두 소스 디렉토리에 상대적입니다. 선행 슬래시는 제거되며 \"..\" 참조는 소스 디렉토리보다 상위로 이동할 수 없습니다. 예를 들어, 이 명령을 살펴보십시오: rsync -a --files-from=/tmp/foo /usr remote:/backup 만약 /tmp/foo에 \"bin\" (또는 \"/bin\")이라는 문자열이 포함되어 있다면, /usr/bin 디렉토리가 원격 호스트에 /backup/bin으로 생성됩니다. 만약 \"bin/\" (후행 슬래시 주의)이 포함되어 있다면, 디렉토리의 즉각적인 내용도 전송됩니다 (파일에 명시적으로 언급할 필요 없이 -- 이것은 버전 2.6.4부터 시작되었습니다). 두 경우 모두, -r 옵션이 활성화된 경우, 해당 디렉토리의 전체 계층 구조도 전송됩니다 ( -r은 --files-from과 함께 명시적으로 지정되어야 하며, -a에 의해 암시되지 않는다는 점을 명심하십시오. 또한 (기본적으로 활성화된) -r 옵션의 효과는 파일에서 읽은 경로 정보만 복제하는 것이며, 소스-지정 경로(이 경우 /usr)의 복제를 강제하지는 않습니다). 또한, --files-from 파일은 파일 앞에 \"host:\"를 지정하면 로컬 호스트 대신 원격 호스트에서 읽을 수 있습니다 (호스트는 전송의 한쪽 끝과 일치해야 합니다). 간단히, \":\" 접두사만 지정하여 \"전송의 원격 끝을 사용하라\"는 의미로 사용할 수 있습니다. 예를 들어: rsync -a --files-from=:/path/file-list src:/ /tmp/copy 이것은 원격 \"src\" 호스트에 있는 /path/file-list 파일에 지정된 모든 파일을 복사합니다. --iconv 및 --secluded-args 옵션이 지정되고 --files-from 파일 이름이 한 호스트에서 다른 호스트로 전송되는 경우, 파일 이름은 보내는 호스트의 문자셋에서 받는 호스트의 문자셋으로 번역됩니다. 참고: --files-from 입력에서 파일 목록을 정렬하면 rsync의 효율성이 향상됩니다. 인접 항목 간에 공유되는 경로 요소를 다시 방문하는 것을 피할 수 있기 때문입니다. 입력이 정렬되지 않으면 일부 경로 요소(암시된 디렉토리)가 여러 번 스캔될 수 있으며, rsync는 파일 목록 요소로 변환된 후 결국 중복을 제거합니다. --from0, -0 이것은 rsync에게 파일에서 읽는 규칙/파일 이름이 NL, CR 또는 CR+LF가 아닌 널('\\0') 문자로 끝난다고 알려줍니다. 이는 --exclude-from, --include-from, --files-from 및 --filter 규칙에 지정된 모든 병합 파일에 영향을 미칩니다. --cvs-exclude에는 영향을 미치지 않습니다 ( .cvsignore 파일에서 읽은 모든 이름은 공백으로 분할되기 때문입니다). --old-args 이 옵션은 rsync에게 원격 측의 인자 값이 의도치 않은 단어 분할이나 기타 오해로부터 보호하려는 시도를 중단하도록 지시합니다. 또한 클라이언트가 빈 인자를 오류를 생성하는 대신 \".\"으로 처리하도록 허용합니다. 최신 rsync의 기본값은 \"셸-활성\" 문자(공백 포함)가 원격 셸로 전송되는 인자에서 역슬래시로 이스케이프되는 것입니다. 와일드카드 문자 *, ?, [, & ]는 파일 이름 인자에서는 이스케이프되지 않지만 (여러 파일 이름으로 확장될 수 있도록 허용), --usermap과 같은 옵션 인자에서는 보호됩니다. 파일 이름에서 이전 스타일 인자 분할을 사용하려는 스크립트가 있다면 이 옵션을 한 번 지정하십시오. 원격 셸에 역슬래시 이스케이프에 문제가 있다면 이 옵션을 두 번 지정하십시오. RSYNC_OLD_ARGS 환경 변수를 통해서도 이 설정을 제어할 수 있습니다. 이 변수가 \"1\" 값이라면 rsync는 단일 옵션 설정으로 기본값이 지정됩니다. \"2\" (또는 그 이상) 값이라면 rsync는 반복 옵션 설정으로 기본값이 지정됩니다. \"0\"이라면 기본 이스케이프 동작을 얻게 됩니다. 환경 변수는 항상 수동으로 지정된 긍정 또는 부정 옵션에 의해 재정의됩니다 (부정은 --no-old-args입니다). 이 옵션은 원격 전송자가 요청하지 않은 추가 최상위 항목을 파일 목록에 포함하지 않도록 보장하는 3.2.5에 추가된 추가 안전 점검도 비활성화합니다. 원격 셸이 인자를 해석할 때 어떤 이름을 예상해야 할지 확실히 알 수 없으므로 이 부작용은 필요합니다. 이 옵션은 --secluded-args 옵션과 충돌합니다. --secluded-args, -s 이 옵션은 모든 파일 이름과 대부분의 옵션을 프로토콜을 통해 원격 rsync로 보냅니다 (원격 셸 명령줄이 아님). 이는 원격 셸이 이들을 수정하는 것을 방지합니다. 와일드카드는 셸 대신 rsync에 의해 원격 호스트에서 확장됩니다. 이는 3.2.4에 추가된 인자의 기본 역슬래시 이스케이핑과 유사합니다 (--old-args 참조). 즉, 공백 분할 및 원치 않는 특수 문자 부작용과 같은 것을 방지합니다. 그러나 오래된 rsync 버전(3.0.0 이전)과 호환되지 않고, 안전을 위해 모든 옵션 값을 검사하려는 제한된 셸에 의해 거부될 수 있다는 단점이 있습니다. 이 옵션은 인자의 문자셋이 원격 호스트에 맞게 변환되어야 하는 경우, 원격 셸이 기본 역슬래시 이스케이프 방식과 호환되지 않는 경우, 또는 대부분의 옵션과 인자가 원격 셸의 명령줄을 우회하도록 하려는 다른 이유가 있는 경우에 유용합니다. 이 옵션을 --iconv와 결합하면 원격 측과 관련된 인자는 로컬 문자셋에서 원격 문자셋으로 번역됩니다. 번역은 와일드카드가 확장되기 전에 발생합니다. --files-from 옵션도 참조하십시오. RSYNC_PROTECT_ARGS 환경 변수를 통해서도 이 설정을 제어할 수 있습니다. 이 변수가 0이 아닌 값을 가지면 이 설정은 기본적으로 활성화되고, 그렇지 않으면 기본적으로 비활성화됩니다. 어떤 상태든 이 옵션의 수동으로 지정된 긍정 또는 부정 버전으로 재정의됩니다 (--no-s 및 --no-secluded-args는 부정 버전입니다). 이 환경 변수는 0이 아닌 RSYNC_OLD_ARGS 내보내기로도 재정의됩니다. 이 옵션은 --old-args 옵션과 충돌합니다. 이 옵션은 이전에 --protect-args (3.2.6 이전)라고 불렸으며, 그 이전 이름도 여전히 사용할 수 있습니다 (그러나 -s로 지정하는 것이 항상 가장 쉽고 호환성이 높습니다). --trust-sender 이 옵션은 로컬 클라이언트가 원격 전송자가 생성한 파일 목록에 대해 수행하는 두 가지 추가 유효성 검사(--multi-host-security 섹션에 자세히 설명됨)를 비활성화합니다. 이 옵션은 전송자가 파일 목록에 악의적인 것을 넣지 않을 것이라고 신뢰하는 경우에만 사용해야 합니다 (수정된 rsync, 수정된 셸 또는 유사한 조작을 통해 발생할 수 있습니다). 일반적으로 rsync 클라이언트(버전 3.2.5 기준)는 원격 rsync에서 파일을 가져올 때 두 가지 추가 유효성 검사를 실행합니다: o 전송 상단에 추가 인자 항목이 추가되지 않았는지 확인합니다. o 파일 목록의 항목 중 제외되었어야 할 이름이 없는지 확인합니다 (필터 규칙이 지정된 경우). 다양한 옵션은 유효성 검사와 충돌하는 경우 이러한 확인 중 하나 또는 둘 다를 비활성화할 수 있습니다. 예를 들어: o 디렉토리별 필터 파일을 사용하면 서버만 아는 필터 규칙을 읽으므로 필터 검사가 비활성화됩니다. o --old-args 옵션을 사용하면 전송자가 요청된 인자를 조작할 수 있으므로 인자 검사가 비활성화됩니다. o 서버 측에서 files-from 목록을 읽으면 클라이언트가 인자 목록을 알 수 없으므로 인자 검사가 비활성화됩니다. o --read-batch를 사용하면 배치 파일의 내용이 생성될 때 이미 확인되었으므로 두 검사 모두 비활성화됩니다. 이 옵션은 추가 패턴 일치가 대규모 전송에서 속도를 늦출 때 성능이 낮은 클라이언트 서버에 도움이 될 수 있습니다. 또한 신뢰할 수 있는 전송자로부터의 전송에 대한 검증 로직의 현재 알려지지 않은 버그를 해결하는 데 사용될 수도 있습니다. 이 옵션을 사용할 때는 MULTI-HOST SECURITY 섹션에서 설명된 대로 전용 대상 디렉토리를 지정하는 것이 좋습니다. --copy-as=USER[:GROUP] 이 옵션은 rsync에게 복사 작업에 USER와 (콜론 뒤에 지정된 경우) GROUP을 사용하도록 지시합니다. 이는 rsync를 실행하는 사용자가 사용자 변경 권한을 가지고 있는 경우에만 작동합니다. 그룹이 지정되지 않으면 사용자의 기본 그룹이 사용됩니다. 이 옵션은 루트 권한으로 실행되는 rsync가 실시간 변경이 발생할 수 있는 디렉토리로/로부터 실행될 때 시스템 파일에 대한 루트 수준 읽기 또는 쓰기 작업이 불가능하도록 보장하여 위험을 줄이는 데 도움이 될 수 있습니다. 대신 rsync 전체를 지정된 사용자로 실행할 수도 있지만, 때로는 루트 수준 호스트 액세스 자격 증명을 사용해야 할 때가 있으므로, 이 옵션을 사용하면 원격-셸 또는 데몬 연결이 설정된 후 rsync가 작업의 복사 부분에 대해 루트 권한을 포기할 수 있습니다. 이 옵션은 전송이 로컬인 경우를 제외하고는 전송의 한쪽에만 영향을 미칩니다. 로컬인 경우에는 양쪽에 영향을 미칩니다. 원격 측에 영향을 미치려면 -M--copy-as=joe와 같이 --remote-option을 사용하십시오. 로컬 전송의 경우, lsh (또는 lsh.sh) 지원 파일은 \"localhost:\" 또는 \"lh:\" 호스트-지정을 설정할 필요 없이 원격 셸을 사용하지 않고도 지정할 수 있는 로컬-셸 도우미 스크립트를 제공합니다. 이를 통해 호스트-지정을 사용하는 전송 측에 영향을 미치는 원격 옵션을 지정할 수 있습니다 (호스트 이름 \"lh\"를 사용하면 원격 디렉토리가 사용자의 홈 디렉토리로 재정의되는 것을 피할 수 있습니다). 예를 들어, 다음 rsync는 로컬 파일을 사용자 \"joe\"로 씁니다: sudo rsync -aiv --copy-as=joe host1:backups/joe/ /home/joe/ 이렇게 하면 모든 파일이 사용자 \"joe\"의 소유가 되고, 그룹은 해당 사용자가 사용할 수 있는 그룹으로 제한되며, joe 사용자가 경로를 시기적절하게 악용하여 joe 사용자가 변경 권한이 없는 파일을 변경하는 것이 불가능해집니다. 다음 명령은 \"joe\" 사용자로 \"dest/\" 디렉토리로 로컬 복사를 수행합니다 (PATH에 support/lsh가 설치되어 있다고 가정): sudo rsync -aive lsh -M--copy-as=joe src/ lh:dest/ --temp-dir=DIR, -T 이 옵션은 rsync에게 수신 측에서 전송된 파일의 임시 복사본을 만들 때 DIR을 스크래치 디렉토리로 사용하도록 지시합니다. 기본 동작은 각 임시 파일을 해당 대상 파일과 동일한 디렉토리에 만드는 것입니다. rsync 3.1.1부터 지정된 DIR 내의 임시 파일 이름은 추가 점(.)으로 접두사가 붙지 않습니다 (하지만 여전히 임의의 접미사가 추가됩니다). 이 옵션은 대부분 수신 디스크 파티션에 전송에서 가장 큰 파일의 복사본을 보관할 충분한 여유 공간이 없는 경우에 사용됩니다. 이 경우 (즉, 스크래치 디렉토리가 다른 디스크 파티션에 있는 경우) rsync는 수신된 각 임시 파일의 이름을 해당 대상 파일 위에 덮어쓸 수 없으며, 대신 제자리로 복사해야 합니다. rsync는 파일을 대상 파일 위에 복사하여 이를 수행합니다. 이는 이 복사 중에 대상 파일에 잘린 데이터가 포함됨을 의미합니다. 이러한 방식으로 수행되지 않으면 (대상 파일이 먼저 제거되고, 데이터가 로컬로 대상 디렉토리의 임시 파일로 복사된 다음, 제자리로 이름이 변경되더라도) 이전 파일이 디스크 공간을 계속 차지할 수 있고 (누군가 파일을 열어둔 경우), 따라서 디스크에 새 버전을 동시에 저장할 충분한 공간이 없을 수 있습니다. 디스크 공간 부족 이외의 이유로 이 옵션을 사용하는 경우, --delay-updates 옵션과 결합하는 것을 고려할 수 있습니다. 이 옵션은 복사된 모든 파일이 대상 계층 구조의 하위 디렉토리에 배치되어 전송이 끝날 때까지 기다리도록 보장합니다. 대상 파티션에 도착하는 모든 파일을 복제할 충분한 공간이 없는 경우, rsync에게 디스크 공간에 대해 지나치게 걱정하지 않아도 된다고 알리는 또 다른 방법은 상대 경로와 함께 --partial-dir 옵션을 사용하는 것입니다. 이는 rsync에게 대상 계층의 하위 디렉토리에 단일 파일의 복사본을 저장해도 괜찮다고 알립니다. rsync는 partial-dir을 스테이징 영역으로 사용하여 복사된 파일을 가져온 다음, 거기서부터 제자리로 이름을 변경합니다. (절대 경로와 함께 --partial-dir을 지정하는 것은 이러한 부작용이 없습니다.) --fuzzy, -y 이 옵션은 rsync에게 누락된 대상 파일에 대한 기준 파일을 찾도록 지시합니다. 현재 알고리즘은 대상 파일과 동일한 디렉토리에서 동일한 크기와 수정 시간을 가진 파일 또는 유사한 이름의 파일을 찾습니다. 찾으면 rsync는 퍼지 기준 파일을 사용하여 전송 속도를 높이려고 합니다. 옵션을 반복하면 --compare-dest, --copy-dest 또는 --link-dest를 통해 지정된 일치하는 대체 대상 디렉토리에서도 퍼지 스캔이 수행됩니다. --delete 옵션을 사용하면 잠재적인 퍼지 일치 파일이 제거될 수 있으므로, 이를 방지해야 하는 경우 --delete-after를 사용하거나 일부 파일 이름 제외를 지정하십시오. --compare-dest=DIR 이 옵션은 rsync에게 전송을 수행할 때 (파일이 대상 디렉토리에 없는 경우) 대상 머신의 DIR을 대상 파일을 비교할 추가 계층 구조로 사용하도록 지시합니다. DIR에서 보내는 측의 파일과 동일한 파일이 발견되면, 해당 파일은 대상 디렉토리로 전송되지 않습니다. 이는 이전 백업에서 변경된 파일만으로 구성된 스파스 백업을 생성하는 데 유용합니다. 이 옵션은 일반적으로 비어 있는 (또는 새로 생성된) 디렉토리로 복사할 때 사용됩니다. 버전 2.6.4부터 여러 --compare-dest 디렉토리를 제공할 수 있으며, 이 경우 rsync는 정확한 일치를 찾기 위해 지정된 순서대로 목록을 검색합니다. 속성만 다른 일치 항목이 발견되면 로컬 복사본이 생성되고 속성이 업데이트됩니다. 일치 항목이 발견되지 않으면 전송 속도를 높이기 위해 DIR 중 하나에서 기준 파일이 선택됩니다. DIR이 상대 경로인 경우, 대상 디렉토리에 상대적입니다. --copy-dest 및 --link-dest도 참조하십시오. 참고: 버전 3.1.0부터 rsync는 대상 계층 구조가 비어 있지 않은 경우, 비교-대상 계층 구조 중 하나에서 정확한 일치 항목이 발견되면 파일을 제거합니다 (최종 결과가 새 복사본과 더 가깝게 일치하도록 함). --copy-dest=DIR 이 옵션은 --compare-dest처럼 작동하지만, rsync는 DIR에서 발견된 변경되지 않은 파일을 로컬 복사를 사용하여 대상 디렉토리로 복사합니다. 이는 기존 파일을 손상시키지 않고 새로운 대상으로 전송을 수행한 다음, 모든 파일이 성공적으로 전송되면 즉시 전환(flash-cutover)을 수행하는 데 유용합니다. 여러 --copy-dest 디렉토리를 제공할 수 있으며, rsync는 변경되지 않은 파일을 찾기 위해 지정된 순서대로 목록을 검색합니다. 일치 항목이 발견되지 않으면 전송 속도를 높이기 위해 DIR 중 하나에서 기준 파일이 선택됩니다. DIR이 상대 경로인 경우, 대상 디렉토리에 상대적입니다. --compare-dest 및 --link-dest도 참조하십시오. --link-dest=DIR 이 옵션은 --copy-dest처럼 작동하지만, 변경되지 않은 파일은 DIR에서 대상 디렉토리로 하드 링크됩니다. 파일이 링크되려면 보존된 모든 속성(예: 권한, 경우에 따라 소유권)이 동일해야 합니다. 예: rsync -av --link-dest=$PWD/prior_dir host:src_dir/ new_dir/ 파일이 링크되지 않는다면 속성을 다시 확인하십시오. 또한 rsync의 제어 밖에서 강제되는 속성이 없는지 확인하십시오. 예를 들어, root를 단일 사용자로 압축하는 마운트 옵션이나 일반적인 소유권으로 이동식 드라이브를 마운트하는 경우(OS X의 \"이 볼륨의 소유권 무시\" 옵션과 같은). 버전 2.6.4부터 여러 --link-dest 디렉토리를 제공할 수 있으며, 이 경우 rsync는 정확한 일치를 찾기 위해 지정된 순서대로 목록을 검색합니다 (이러한 디렉토리는 20개로 제한됩니다). 속성만 다른 일치 항목이 발견되면 로컬 복사본이 생성되고 속성이 업데이트됩니다. 일치 항목이 발견되지 않으면 전송 속도를 높이기 위해 DIR 중 하나에서 기준 파일이 선택됩니다. 이 옵션은 비어 있는 대상 계층 구조로 복사할 때 가장 잘 작동합니다. 기존 파일의 속성이 수정될 수 있으며, 이는 하드 링크를 통해 대체 대상 파일에 영향을 미칠 수 있기 때문입니다. 또한, 변경 사항을 항목화하는 것이 다소 복잡해질 수 있습니다. 버전 3.1.0 이전에는 대상 파일이 이미 존재할 때 대체 디렉토리에서 정확한 일치 항목이 발견되지 않았습니다(대상으로 링크되지도 않았습니다). 이 옵션을 --ignore-times와 결합하면 rsync는 어떤 파일도 링크하지 않습니다. 파일을 전송하는 대신 동일한 파일을 함께 링크하는 것을 대체 수단으로만 사용하고, 파일 업데이트 후 추가 검사로는 사용하지 않기 때문입니다. DIR이 상대 경로인 경우, 대상 디렉토리에 상대적입니다. --compare-dest 및 --copy-dest도 참조하십시오. rsync 2.6.1 이전 버전에는 --owner (-o)가 지정되었을 때 (또는 암시되었을 때) 비-슈퍼유저에 대해 --link-dest가 제대로 작동하지 않을 수 있는 버그가 있었습니다. 이 버그를 해결하려면 오래된 rsync로 보낼 때 -o 옵션을 피하거나 (--no-o를 사용하십시오). --compress, -z 이 옵션을 사용하면 rsync는 파일 데이터를 대상 머신으로 보낼 때 압축하여 전송되는 데이터 양을 줄입니다. 이는 느린 연결에서 유용합니다. Rsync는 여러 압축 방법을 지원하며, --compress-choice (--zc) 옵션을 사용하여 선택을 강제하지 않는 한 자동으로 하나를 선택합니다. rsync --version을 실행하여 자신의 버전에 컴파일된 기본 압축 목록을 확인하십시오. 전송의 양쪽이 최소 3.2.0 버전인 경우, rsync는 클라이언트의 선택 목록에 있는 첫 번째 알고리즘 중에서 서버의 선택 목록에도 있는 것을 선택합니다. 공통 압축 선택이 발견되지 않으면 rsync는 오류로 종료됩니다. 원격 rsync가 너무 오래되어 체크섬 협상을 지원하지 않으면, 해당 목록은 \"zlib\"라고 가정됩니다. 기본 순서는 환경 변수 RSYNC_COMPRESS_LIST를 허용되는 압축 이름의 공백으로 구분된 목록으로 설정하여 사용자 정의할 수 있습니다. 문자열에 \"&\" 문자가 포함된 경우 \"클라이언트 문자열 & 서버 문자열\"로 분리되고, 그렇지 않으면 동일한 문자열이 둘 다에 적용됩니다. 문자열(또는 문자열 부분)에 공백이 아닌 문자가 없으면 기본 압축 목록이 사용됩니다. 알려지지 않은 압축 이름은 목록에서 제거되지만, 유효하지 않은 이름만 있는 목록은 협상 실패로 이어집니다. 일부 오래된 rsync 버전은 -z 옵션을 거부하고 -zz 사용을 요구하도록 구성되었습니다. 이는 압축 라이브러리가 기본 zlib 압축 방식과 호환되지 않았기 때문입니다. rsync 서버가 불평하며 -zz를 지정하도록 지시하지 않는 한, 이 이상한 점은 일반적으로 무시해도 됩니다. --compress-choice=STR, --zc=STR 이 옵션은 --compress가 사용될 때 발생하는 압축 알고리즘의 자동 협상을 재정의하는 데 사용할 수 있습니다. \"none\"이 지정되지 않은 한 --compress를 암시하며, \"none\"이 지정되면 --no-compress를 암시합니다. 사용할 수 있는 압축 옵션은 다음과 같습니다: o zstd o lz4 o zlibx o zlib o none rsync --version을 실행하여 자신의 버전에 컴파일된 기본 압축 목록을 확인하십시오 (위 목록과 다를 수 있습니다). --old-compress 또는 --new-compress라는 옵션에 대한 오류가 표시되는 경우, 이는 rsync가 --compress-choice=zlib 또는 --compress-choice=zlibx 옵션을 더 많은 rsync 버전이 이해하는 역방향 호환 방식으로 보내려고 시도하는 것입니다. 이 오류는 서버의 오래된 rsync 버전이 압축 유형을 강제하는 것을 허용하지 않음을 나타냅니다. \"zlibx\" 압축 알고리즘은 압축 스트림에서 일치하는 데이터를 제외한 \"zlib\" 알고리즘입니다 (외부 zlib 구현과 더 호환되도록 시도하기 위함입니다). --compress-level=NUM, --zl=NUM 기본값으로 두는 대신 사용할 압축 수준을 명시적으로 설정합니다 (--compress, -z 참조). 선택된 수준이 적용 중인 압축 알고리즘에 대해 \"압축 안 함\" 수준이 아닌 한 --compress 옵션이 암시됩니다 (예: zlib 압축은 수준 0을 \"해제\"로 처리합니다). 수준 값은 적용되는 체크섬에 따라 다릅니다. rsync는 기본적으로 체크섬 선택을 협상하므로 (원격 rsync가 충분히 최신인 경우), 이 옵션을 --compress-choice (--zc) 옵션과 결합하는 것이 좋습니다. (어떤 선택이 적용되는지 확실하지 않은 경우). 예를 들어: rsync -aiv --zc=zstd --zl=22 host:src/ dest/ zlib 및 zlibx 압축의 경우 유효한 값은 1에서 9까지이며 기본값은 6입니다. --zl=0을 지정하면 압축이 꺼지고, --zl=-1을 지정하면 기본 수준인 6이 선택됩니다. zstd 압축의 경우 유효한 값은 -131072에서 22까지이며 기본값은 3입니다. 0을 지정하면 기본값인 3이 선택됩니다. lz4 압축의 경우 수준이 없으므로 값은 항상 0입니다. 너무 크거나 너무 작은 값을 지정하면 해당 숫자는 유효한 값으로 조용히 제한됩니다. 이를 통해 --zl=999999999와 같이 지정하여 어떤 알고리즘이 선택되든 최대 압축 수준을 얻을 수 있습니다. 적용 중인 압축 수준을 알고 싶다면 --debug=nstr을 지정하여 \"negotiated string\" 결과를 확인하십시오. 이는 \"Client compress: zstd (level 3)\"와 같이 보고됩니다 (적용 중인 체크섬 선택과 함께). --skip-compress=LIST 참고: 현재 어떤 압축 방법도 파일별 압축 변경을 지원하지 않으므로 이 옵션은 효과가 없습니다. 가능한 한 적게 압축될 파일 접미사 목록을 재정의합니다. Rsync는 파일의 접미사에 따라 파일별로 압축 수준을 설정합니다. 압축 알고리즘에 \"해제(off)\" 수준이 있는 경우 해당 파일에 대해서는 압축이 수행되지 않습니다. 스트리밍 수준을 즉시 변경하는 것을 지원하는 다른 알고리즘은 일치하는 파일에 대해 CPU 사용량을 최대한 줄이기 위해 수준이 최소화됩니다. LIST는 슬래시(/)로 구분된 하나 이상의 파일 접미사(점 없이)여야 합니다. 어떤 파일도 건너뛰지 않도록 빈 문자열을 지정할 수 있습니다. 간단한 문자 클래스 일치가 지원됩니다: 각 클래스는 대괄호 안에 문자 목록으로 구성되어야 합니다 (예: \"[:alpha:]\"와 같은 특수 클래스는 지원되지 않으며, '-'는 특수 의미가 없습니다). 별표(*)와 물음표(?) 문자는 특수 의미가 없습니다. 다음은 건너뛸 6개의 접미사를 지정하는 예입니다 (5개 규칙 중 1개가 2개의 접미사와 일치하기 때문입니다): --skip-compress=gz/jpg/mp[34]/7z/bz2 이 rsync 버전의 skip-compress 목록에 있는 기본 파일 접미사는 다음과 같습니다: 3g2 3gp 7z aac ace apk avi bz2 deb dmg ear f4v flac flv gpg gz iso jar jpeg jpg lrz lz lz4 lzma lzo m1a m1v m2a m2ts m2v m4a m4b m4p m4r m4v mka mkv mov mp1 mp2 mp3 mp4 mpa mpeg mpg mpv mts odb odf odg odi odm odp ods odt oga ogg ogm ogv ogx opus otg oth otp ots ott oxt png qt rar rpm rz rzip spx squashfs sxc sxd sxg sxm sxw sz tbz tbz2 tgz tlz ts txz tzo vob war webm webp xz z zip zst 이 목록은 한 가지 상황을 제외하고는 사용자 정의한 --skip-compress 목록으로 대체됩니다. 데몬 rsync로부터의 복사는 건너뛴 접미사를 압축하지 않는 파일 목록에 추가합니다 (그리고 해당 목록은 다른 기본값으로 구성될 수 있습니다). --numeric-ids 이 옵션을 사용하면 rsync는 사용자 및 그룹 이름을 사용하는 대신 숫자 그룹 및 사용자 ID를 전송하고 양쪽에서 매핑합니다. 기본적으로 rsync는 파일에 어떤 소유권을 부여할지 결정하기 위해 사용자 이름과 그룹 이름을 사용합니다. 특수 uid 0 및 특수 그룹 0은 --numeric-ids 옵션이 지정되지 않았더라도 사용자/그룹 이름으로 매핑되지 않습니다. 사용자 또는 그룹이 소스 시스템에 이름이 없거나 대상 시스템에 일치하는 이름이 없으면, 소스 시스템의 숫자 ID가 대신 사용됩니다. chroot 설정이 rsync가 사용자 및 그룹의 이름을 조회하는 능력에 어떻게 영향을 미치는지, 그리고 이에 대해 무엇을 할 수 있는지에 대한 몇 가지 의견은 rsyncd.conf 맨페이지의 use chroot 설정을 참조하십시오. --usermap=STRING, --groupmap=STRING 이 옵션은 수신 측에서 다른 값으로 매핑되어야 하는 사용자 및 그룹을 지정할 수 있도록 합니다. STRING은 쉼표로 구분된 하나 이상의 FROM:TO 값 쌍입니다. 전송자로부터의 일치하는 FROM 값은 수신자로부터의 TO 값으로 대체됩니다. FROM 및 TO 값에 사용자 이름 또는 사용자 ID를 지정할 수 있으며, FROM 값은 와일드카드 문자열일 수도 있습니다. 이것은 전송자의 이름과 일치합니다 (와일드카드는 ID 번호와 일치하지 않지만, 아래에서 '*'가 모든 것을 일치시키는 이유를 참조하십시오). 포함 범위: LOW-HIGH를 통해 ID 번호 범위를 지정할 수도 있습니다. 예를 들어: --usermap=0-99:nobody,wayne:admin,*:normal --groupmap=usr:1,1:usr 목록의 첫 번째 일치 항목이 사용됩니다. 단일 --usermap 옵션을 사용하여 모든 사용자 매핑을 지정하고, 단일 --groupmap 옵션을 사용하여 모든 그룹 매핑을 지정해야 합니다. 사용자 0 및 그룹 0에 대한 보내는 측의 이름은 받는 측으로 전송되지 않으므로, 이 값은 0을 사용하여 일치시키거나, 받는 측에서 사용 중인 이름(일반적으로 \"root\")을 사용해야 합니다. 다른 모든 FROM 이름은 보내는 측에서 사용 중인 이름과 일치합니다. 모든 TO 이름은 받는 측에서 사용 중인 이름과 일치합니다. 보내는 측에 이름이 없는 모든 ID는 일치 목적을 위해 빈 이름을 가진 것으로 처리됩니다. 이를 통해 \"*\" 또는 빈 이름을 사용하여 일치시킬 수 있습니다. 예를 들어: --usermap=:nobody --groupmap=*:nobody --numeric-ids 옵션이 사용될 때, 보내는 측은 어떤 이름도 보내지 않으므로 모든 ID는 빈 이름을 가진 것으로 처리됩니다. 이는 이름 없는 ID를 다른 값으로 매핑하려면 숫자 FROM 값을 지정해야 함을 의미합니다. --usermap 옵션이 작동하려면 수신자는 슈퍼유저로 실행되어야 합니다 (--super 및 --fake-super 옵션도 참조). --groupmap 옵션이 작동하려면 수신자는 해당 그룹을 설정할 권한이 있어야 합니다. rsync 3.2.4부터 --usermap 옵션은 --owner (-o) 옵션을 암시하며, --groupmap 옵션은 --group (-g) 옵션을 암시합니다 (매핑 옵션이 작동하려면 rsync가 해당 옵션을 활성화해야 하므로). 오래된 rsync 클라이언트는 와일드카드 문자에 대한 불만을 피하기 위해 -s를 사용해야 할 수도 있지만, 최신 rsync는 이를 자동으로 처리합니다. --chown=USER:GROUP 이 옵션은 모든 파일이 USER가 소유하고 GROUP에 속하도록 강제합니다. 이는 --usermap 및 --groupmap을 직접 사용하는 것보다 간단한 인터페이스이지만, 내부적으로 해당 옵션을 사용하여 구현되므로 혼합하여 사용할 수 없습니다. USER 또는 GROUP이 비어 있으면 생략된 사용자/그룹에 대한 매핑은 발생하지 않습니다. GROUP이 비어 있으면 후행 콜론을 생략할 수 있지만, USER가 비어 있으면 선행 콜론을 제공해야 합니다. \"--chown=foo:bar\"를 지정하는 것은 \"--usermap=*:foo --groupmap=*:bar\"를 지정하는 것과 정확히 동일하며, 더 쉽습니다 (그리고 동일한 암시된 --owner 및/또는 --group 옵션을 가집니다). 오래된 rsync 클라이언트는 와일드카드 문자에 대한 불만을 피하기 위해 -s를 사용해야 할 수도 있지만, 최신 rsync는 이를 자동으로 처리합니다. --timeout=SECONDS 이 옵션을 사용하면 최대 I/O 타임아웃을 초 단위로 설정할 수 있습니다. 지정된 시간 동안 데이터가 전송되지 않으면 rsync가 종료됩니다. 기본값은 0이며, 이는 타임아웃이 없음을 의미합니다. --contimeout=SECONDS 이 옵션은 rsync가 rsync 데몬에 연결하는 데 성공할 때까지 기다릴 시간을 설정할 수 있도록 합니다. 타임아웃에 도달하면 rsync는 오류로 종료됩니다. --address=ADDRESS 기본적으로 rsync는 rsync 데몬에 연결할 때 와일드카드 주소에 바인딩합니다. --address 옵션을 사용하면 특정 IP 주소(또는 호스트 이름)에 바인딩하도록 지정할 수 있습니다. --address 옵션의 데몬 버전도 참조하십시오. --port=PORT 이것은 기본값 873 대신 사용할 대체 TCP 포트 번호를 지정합니다. 이는 이중 콜론(::) 구문을 사용하여 rsync 데몬에 연결할 때만 필요합니다 (URL 구문에는 URL의 일부로 포트를 지정하는 방법이 있기 때문입니다). --port 옵션의 데몬 버전도 참조하십시오. --sockopts=OPTIONS 이 옵션은 시스템을 최대한 튜닝하는 것을 좋아하는 사람들에게 끝없는 재미를 제공할 수 있습니다. 전송 속도를 높이거나(또는 늦출 수 있는) 모든 종류의 소켓 옵션을 설정할 수 있습니다. 설정할 수 있는 일부 옵션에 대한 자세한 내용은 setsockopt() 시스템 호출 맨페이지를 참조하십시오. 기본적으로 특수 소켓 옵션은 설정되지 않습니다. 이것은 원격 rsync 데몬에 대한 직접 소켓 연결에만 영향을 미칩니다. --sockopts 옵션의 데몬 버전도 참조하십시오. --blocking-io 이것은 rsync에게 원격 셸 전송을 시작할 때 블로킹 I/O를 사용하도록 지시합니다. 원격 셸이 rsh 또는 remsh인 경우, rsync는 기본적으로 블로킹 I/O를 사용하며, 그렇지 않으면 논블로킹 I/O를 사용합니다. (ssh는 논블로킹 I/O를 선호합니다.) --outbuf=MODE 이것은 출력 버퍼링 모드를 설정합니다. 모드는 None (일명 Unbuffered), Line, 또는 Block (일명 Full)이 될 수 있습니다. 모드의 경우 한 글자만 지정할 수 있으며, 대소문자를 구분하지 않습니다. 이 옵션의 주요 용도는 rsync의 출력이 파일이나 파이프로 전달될 때 전체 버퍼링을 라인 버퍼링으로 변경하는 것입니다. --itemize-changes, -i 각 파일에 적용되는 변경 사항, 속성 변경 사항을 포함한 간단한 항목별 목록을 요청합니다. 이것은 --out-format='%i %n%L'을 지정하는 것과 정확히 동일합니다. 옵션을 반복하면 변경되지 않은 파일도 출력되지만, 수신 rsync가 2.6.7 버전 이상인 경우에만 그렇습니다 (이전 rsync 버전에서는 -vv를 사용할 수 있지만, 이는 다른 상세 메시지 출력도 켭니다). \"%i\" 이스케이프는 11글자의 암호 같은 출력을 가집니다. 일반적인 형식은 YXcstpoguax와 같으며, Y는 수행되는 업데이트 유형으로 대체되고, X는 파일 유형으로 대체되며, 다른 글자는 수정될 수 있는 속성을 나타냅니다. Y를 대체하는 업데이트 유형은 다음과 같습니다: o A <는 파일이 원격 호스트로 전송되고 있음을 의미합니다 (보냄). o A >는 파일이 로컬 호스트로 전송되고 있음을 의미합니다 (받음). o A c는 항목에 대한 로컬 변경/생성이 발생하고 있음을 의미합니다 (예: 디렉토리 생성 또는 심볼릭 링크 변경 등). o A h는 항목이 다른 항목에 대한 하드 링크임을 의미합니다 (--hard-links 필요). o A .는 항목이 업데이트되지 않고 있음을 의미합니다 (하지만 속성이 수정될 수 있음). o A *는 항목별 출력 영역의 나머지 부분에 메시지가 포함되어 있음을 의미합니다 (예: \"deleting\"). X를 대체하는 파일 유형은 다음과 같습니다: 파일의 경우 f, 디렉토리의 경우 d, 심볼릭 링크의 경우 L, 장치의 경우 D, 특수 파일(예: 명명된 소켓 및 FIFO)의 경우 S. 문자열의 다른 글자는 파일의 일부 속성이 변경되었는지 여부를 나타내며, 다음과 같습니다: o \".\" - 속성이 변경되지 않았습니다. o \"+\" - 파일이 새로 생성되었습니다. o \" \" - 모든 속성이 변경되지 않았습니다 (모든 점이 공백으로 바뀝니다). o \"?\" - 변경 사항을 알 수 없습니다 (원격 rsync가 오래된 경우). o 문자는 속성이 업데이트되고 있음을 나타냅니다. 각 문자와 관련된 속성은 다음과 같습니다: o c는 일반 파일의 체크섬이 다르거나 (--checksum 필요) 심볼릭 링크, 장치 또는 특수 파일의 값이 변경되었음을 의미합니다. rsync 3.0.1 이전 버전으로 파일을 보내는 경우, 이 변경 플래그는 체크섬이 다른 일반 파일에만 나타납니다. o s는 일반 파일의 크기가 다르며 파일 전송에 의해 업데이트될 것임을 의미합니다. o t는 수정 시간이 다르며 전송자의 값으로 업데이트되고 있음을 의미합니다 (--times 필요). 대체 값 T는 수정 시간이 전송 시간으로 설정됨을 의미합니다. 이는 파일/심볼릭 링크/장치가 --times 없이 업데이트될 때, 그리고 심볼릭 링크가 변경되고 수신자가 시간을 설정할 수 없을 때 발생합니다. (참고: rsync 3.0.0 클라이언트를 사용할 때, 이 시간 설정 실패에 대해 적절한 T 플래그 대신 s 플래그가 t와 결합된 것을 볼 수 있습니다.) o p는 권한이 다르며 전송자의 값으로 업데이트되고 있음을 의미합니다 (--perms 필요). o o는 소유자가 다르며 전송자의 값으로 업데이트되고 있음을 의미합니다 (--owner 및 슈퍼유저 권한 필요). o g는 그룹이 다르며 전송자의 값으로 업데이트되고 있음을 의미합니다 (--group 및 그룹 설정 권한 필요). o o u|n|b는 다음 정보를 나타냅니다: u 접근(사용) 시간이 다르며 전송자의 값으로 업데이트되고 있음을 의미합니다 (--atimes 필요) o n 생성 시간(새로움)이 다르며 전송자의 값으로 업데이트되고 있음을 의미합니다 (--crtimes 필요) o b 접근 시간과 생성 시간 모두 업데이트되고 있음을 의미합니다 o a는 ACL 정보가 변경되고 있음을 의미합니다. o x는 확장 속성 정보가 변경되고 있음을 의미합니다. 또 다른 출력이 가능합니다: 파일을 삭제할 때, \"%i\"는 제거되는 각 항목에 대해 \"*deleting\" 문자열을 출력합니다 (충분히 최신 rsync와 통신하여 삭제를 상세 메시지로 출력하는 대신 로그에 기록하는 경우). --out-format=FORMAT 이것은 rsync 클라이언트가 사용자에게 업데이트별로 정확히 무엇을 출력할지 지정할 수 있도록 합니다. 형식은 퍼센트(%) 문자로 시작하는 내장된 단일 문자 이스케이프 시퀀스를 포함하는 텍스트 문자열입니다. --info=name 또는 -v가 지정되면 \"%n%L\"의 기본 형식이 가정됩니다 (이는 파일 이름과, 항목이 링크인 경우 가리키는 위치만 알려줍니다). 가능한 이스케이프 문자의 전체 목록은 rsyncd.conf 맨페이지의 log format 설정을 참조하십시오. --out-format 옵션을 지정하면 --info=name 옵션이 암시됩니다. 이 옵션은 상당한 방식으로 업데이트되는 각 파일, 디렉토리 등을 언급합니다 (전송된 파일, 재구성된 심볼릭 링크/장치, 또는 터치된 디렉토리). 또한, 항목별 변경 이스케이프(%i)가 문자열에 포함된 경우 (예: --itemize-changes 옵션이 사용된 경우), 이름 로깅은 어떤 식으로든 변경된 모든 항목을 언급하도록 증가합니다 (수신 측이 최소 2.6.4 버전인 한). \"%i\" 출력에 대한 설명은 --itemize-changes 옵션을 참조하십시오. rsync는 파일 전송 전에 out-format 문자열을 출력합니다. 단, 전송 통계 이스케이프 중 하나가 요청된 경우에는 파일 전송이 끝날 때 로깅이 수행됩니다. 이 후반 로깅이 적용되고 --progress도 지정되면, rsync는 진행 정보 앞에 전송 중인 파일의 이름도 출력합니다 (물론 out-format 출력 뒤에). --log-file=FILE 이 옵션은 rsync가 수행하는 작업을 파일에 기록하도록 합니다. 이는 데몬이 수행하는 로깅과 유사하지만, 클라이언트 측 및/또는 비데몬 전송의 서버 측에 대해 요청할 수 있습니다. 클라이언트 옵션으로 지정하면, 기본 형식인 \"%i %n%L\"로 전송 로깅이 활성화됩니다. 이를 재정의하려면 --log-file-format 옵션을 참조하십시오. 다음은 원격 측에서 발생하는 일을 기록하도록 요청하는 명령의 예시입니다: rsync -av --remote-option=--log-file=/tmp/rlog src/ dest/ 연결이 예기치 않게 종료되는 이유를 디버깅해야 할 때 매우 유용합니다. --log-file 옵션의 데몬 버전도 참조하십시오. --log-file-format=FORMAT 이것은 --log-file 옵션으로 지정된 파일에 어떤 업데이트별 로깅이 기록될지 정확히 지정할 수 있도록 합니다 (--log-file 옵션도 이 옵션이 효과를 가지려면 지정되어야 합니다). 빈 문자열을 지정하면 업데이트된 파일은 로그 파일에 언급되지 않습니다. 가능한 이스케이프 문자의 목록은 rsyncd.conf 맨페이지의 log format 설정을 참조하십시오. --log-file이 지정되고 이 옵션이 지정되지 않은 경우 사용되는 기본 FORMAT은 '%i %n%L'입니다. --log-file-format 옵션의 데몬 버전도 참조하십시오. --stats 이것은 rsync에게 파일 전송에 대한 자세한 통계 집합을 출력하도록 지시하여, 데이터에 대한 rsync의 델타-전송 알고리즘이 얼마나 효과적인지 알 수 있도록 합니다. 이 옵션은 0 또는 1개의 -v 옵션과 결합하면 --info=stats2와 동일하며, 2개 이상의 -v 옵션과 결합하면 --info=stats3와 동일합니다. 현재 통계는 다음과 같습니다: o Number of files (파일 수)는 디렉토리, 심볼릭 링크 등을 포함한 모든 \"파일\"(일반적인 의미에서)의 개수입니다. 총 개수 뒤에는 파일 유형별 개수 목록이 표시됩니다 (총 개수가 0이 아닌 경우). 예: \"(reg: 5, dir: 3, link: 2, dev: 1, special: 1)\"는 일반 파일, 디렉토리, 심볼릭 링크, 장치 및 특수 파일의 총 개수를 나열합니다. 값이 0인 경우 목록에서 완전히 생략됩니다. o Number of created files (생성된 파일 수)는 생성된 (업데이트된 것과 반대되는) \"파일\"(일반적인 의미에서)의 개수입니다. 총 개수 뒤에는 파일 유형별 개수 목록이 표시됩니다 (총 개수가 0이 아닌 경우). o Number of deleted files (삭제된 파일 수)는 삭제된 \"파일\"(일반적인 의미에서)의 개수입니다. 총 개수 뒤에는 파일 유형별 개수 목록이 표시됩니다 (총 개수가 0이 아닌 경우). 이 줄은 삭제가 적용 중일 때만 출력되며, 프로토콜 31이 사용 중일 때만 출력됩니다 (rsync 3.1.x의 기본값). o Number of regular files transferred (전송된 일반 파일 수)는 rsync의 델타-전송 알고리즘을 통해 업데이트된 일반 파일의 개수입니다. 이는 디렉토리, 심볼릭 링크 등을 포함하지 않습니다. rsync 3.1.0부터 이 제목에 \"regular\"라는 단어가 추가되었습니다. o Total file size (총 파일 크기)는 전송에서 모든 파일 크기의 총합입니다. 이는 디렉토리나 특수 파일의 크기는 계산하지 않지만, 심볼릭 링크의 크기는 포함합니다. o Total transferred file size (총 전송 파일 크기)는 전송된 파일만의 모든 파일 크기의 총합입니다. o Literal data (리터럴 데이터)는 업데이트된 파일을 재구성하기 위해 수신자에게 보내야 했던 일치하지 않는 파일 업데이트 데이터의 양입니다. o Matched data (일치하는 데이터)는 업데이트된 파일을 재구성할 때 수신자가 로컬에서 얻은 데이터의 양입니다. o File list size (파일 목록 크기)는 전송자가 수신자에게 보낼 때 파일 목록 데이터의 크기였습니다. 이는 rsync가 목록을 보낼 때 중복 데이터를 압축하기 때문에 파일 목록의 메모리 내 크기보다 작습니다. o File list generation time (파일 목록 생성 시간)은 전송자가 파일 목록을 생성하는 데 소요된 시간(초)입니다. 이것이 존재하려면 보내는 측에 최신 rsync가 필요합니다. o File list transfer time (파일 목록 전송 시간)은 전송자가 수신자에게 파일 목록을 보내는 데 소요된 시간(초)입니다. o Total bytes sent (총 보낸 바이트)는 rsync가 클라이언트 측에서 서버 측으로 보낸 모든 바이트의 개수입니다. o Total bytes received (총 받은 바이트)는 rsync가 클라이언트 측에서 서버 측으로부터 수신한 비메시지 바이트의 총 개수입니다. \"비메시지\" 바이트는 서버가 우리에게 보낸 상세 메시지의 바이트를 계산하지 않는다는 의미이며, 이는 통계를 더 일관성 있게 만듭니다. --8-bit-output, -8 이것은 rsync에게 모든 높은 비트 문자를 현재 로케일에서 유효한지 테스트하고 유효하지 않은 문자를 이스케이프하려고 시도하는 대신, 출력에서 이스케이프되지 않은 채로 두도록 지시합니다. 이 옵션 설정과 관계없이 모든 제어 문자(탭은 제외)는 항상 이스케이프됩니다. 2.6.7부터 시작된 이스케이프 관용구는 리터럴 역슬래시(\\)와 해시(#)를 출력한 다음 정확히 3개의 8진수 숫자를 출력하는 것입니다. 예를 들어, 새 줄은 \"\\#012\"로 출력됩니다. 파일 이름에 있는 리터럴 역슬래시는 해시와 3개의 숫자(0-9)가 뒤따르지 않는 한 이스케이프되지 않습니다. --human-readable, -h 숫자를 사람이 읽기 쉬운 형식으로 출력합니다. 가능한 수준은 3가지입니다: 1. 세 자리마다 구분 기호(소수점이 마침표 또는 쉼표로 표시되는지에 따라 쉼표 또는 마침표)를 사용하여 숫자를 출력합니다. 2. 1000 단위로 숫자를 출력합니다 (더 큰 단위에는 문자 접미사 사용 -- 아래 참조). 3. 1024 단위로 숫자를 출력합니다. 기본값은 사람이 읽기 쉬운 수준 1입니다. 각 -h 옵션은 수준을 1씩 증가시킵니다. --no-human-readable (--no-h) 옵션을 지정하여 수준을 0으로 낮출 수 있습니다 (순수 숫자로 출력). 수준 2와 3에서 추가되는 단위 문자는 다음과 같습니다: K (킬로), M (메가), G (기가), T (테라), P (페타). 예를 들어, 1234567바이트 파일은 수준 2에서 1.23M으로 출력됩니다 (마침표가 로컬 소수점이라고 가정). 하위 호환성 참고: rsync 3.1.0 이전 버전은 사람이 읽기 쉬운 수준 1을 지원하지 않으며, 기본적으로 수준 0을 사용합니다. 따라서 하나 또는 두 개의 -h 옵션을 지정하는 것은 하나 이상의 -h 옵션 이전에 --no-h 옵션을 지정하지 않는 한 이전 및 새 버전에서 유사한 방식으로 작동합니다. 한 가지 차이점에 대해서는 --list-only 옵션을 참조하십시오. --partial 기본적으로 rsync는 전송이 중단되면 부분적으로 전송된 파일을 삭제합니다. 어떤 경우에는 부분적으로 전송된 파일을 유지하는 것이 더 바람직합니다. --partial 옵션을 사용하면 rsync는 부분 파일을 유지하도록 지시하여 후속 파일 전송을 훨씬 빠르게 할 수 있습니다. --partial-dir=DIR 이 옵션은 --partial 옵션의 동작을 수정하는 동시에 이 옵션이 활성화되도록 암시합니다. 이 향상된 부분 파일 방식은 부분적으로 전송된 모든 파일을 대상 파일로 직접 쓰는 대신 지정된 DIR에 저장합니다. 다음 전송 시 rsync는 이 디렉토리에서 발견된 파일을 데이터로 사용하여 전송 재개 속도를 높이고, 역할을 다하면 삭제합니다. --whole-file이 지정되었거나 (또는 암시되었을 때), 업데이트되는 파일에 대해 발견된 partial-dir 파일은 단순히 제거됩니다 (rsync는 rsync의 델타-전송 알고리즘을 사용하지 않고 파일을 보내기 때문입니다). Rsync는 DIR이 누락된 경우 생성하지만, 마지막 디렉토리만 생성하며 전체 경로는 생성하지 않습니다. 이는 상대 경로(예: \"--partial-dir=.rsync-partial\")를 사용하여 rsync가 필요할 때 대상 파일의 디렉토리에 부분 디렉토리를 생성한 다음, 부분 파일이 삭제될 때 다시 제거하도록 하는 것을 쉽게 만듭니다. 이 디렉토리 제거는 상대 경로에 대해서만 수행됩니다. 절대 경로는 partial-dir 작업 전용 디렉토리로 예상되기 때문입니다. partial-dir 값이 절대 경로가 아닌 경우, rsync는 기존 모든 제외 규칙 끝에 제외 규칙을 추가합니다. 이것은 보내는 측에 존재할 수 있는 partial-dir 파일 전송을 방지하고, 수신 측에서 partial-dir 항목이 시기적절하게 삭제되는 것을 방지합니다. 예: 위 --partial-dir 옵션은 다른 필터 규칙 끝에 이와 동등한 \"소멸성\" 제외 규칙을 추가합니다: -f '-p .rsync-partial/' 자신만의 제외 규칙을 제공하는 경우, 다음 이유로 인해 partial-dir에 대한 자신만의 제외/숨김/보호 규칙을 추가해야 할 수도 있습니다: 4. 자동 추가된 규칙이 다른 규칙의 끝에서는 효과적이지 않을 수 있거나, 5. rsync의 제외 선택을 재정의하고 싶을 수 있기 때문입니다. 예를 들어, rsync가 남아 있는 partial-dir을 정리하도록 하려면 --delete-after를 지정하고 \"위험\" 필터 규칙을 추가해야 합니다. 예: -f 'R .rsync-partial/'. --delete-before 또는 --delete-during 사용은 현재 실행 중에 rsync가 남아 있는 partial-dir 데이터를 사용할 필요가 없는 경우가 아니면 피하십시오. 중요: --partial-dir은 다른 사용자가 쓸 수 없어야 합니다. 그렇지 않으면 보안 위험이 있습니다! 예: \"/tmp\"는 피하십시오! RSYNC_PARTIAL_DIR 환경 변수에 partial-dir 값을 설정할 수도 있습니다. 환경에 이것을 설정하는 것은 부분 전송이 활성화되도록 강제하지 않지만, --partial이 지정되었을 때 부분 파일이 어디로 가는지에 영향을 미칩니다. 예를 들어, --progress와 함께 --partial-dir=.rsync-tmp를 사용하는 대신, 환경에 RSYNC_PARTIAL_DIR=.rsync-tmp를 설정하고 -P 옵션을 사용하여 부분 전송에 .rsync-tmp 디렉토리 사용을 켤 수 있습니다. --partial 옵션이 이 환경 값을 찾지 않는 유일한 경우는 다음과 같습니다: 6. --inplace가 지정되었을 때 (--inplace는 --partial-dir과 충돌하므로), 그리고 7. --delay-updates가 지정되었을 때 (아래 참조). 최신 rsync가 partial-dir의 파일 전송을 재개할 때, 해당 부분 파일은 이제 또 다른 임시 파일 복사본을 만드는 대신 제자리에서 업데이트됩니다 (따라서 dest + partial + tmp 대신 dest + tmp에서 최대화됩니다). 이는 전송의 양쪽 끝이 최소 버전 3.2.0이어야 합니다. 데몬-구성의 \"refuse options\" 설정 목적상, --partial-dir은 --partial을 암시하지 않습니다. 이는 --partial 옵션의 거부가 부분 전송으로 대상 파일을 덮어쓰는 것을 허용하지 않으면서도 --partial-dir이 제공하는 더 안전한 관용구를 허용할 수 있도록 하기 위함입니다. --delay-updates 이 옵션은 각 업데이트된 파일의 임시 파일을 전송이 끝날 때까지 보류 디렉토리에 넣은 다음, 모든 파일의 이름이 순식간에 제자리로 변경되도록 합니다. 이는 파일 업데이트를 좀 더 원자적으로 만들기 위한 시도입니다. 기본적으로 파일은 각 파일의 대상 디렉토리 내에 .~tmp~라는 디렉토리에 배치되지만, --partial-dir 옵션을 지정한 경우 해당 디렉토리가 대신 사용됩니다. .~tmp~ 디렉토리가 전송에서 제외되는 방법과 rsync가 남아 있을 수 있는 오래된 .~tmp~ 디렉토리를 정리하도록 하려면 --partial-dir 섹션의 설명을 참조하십시오. --inplace 및 --append와 충돌합니다. 이 옵션은 끝에서 반복할 수 있도록 전체 파일 목록이 메모리에 있어야 하므로 --no-inc-recursive를 암시합니다. 이 옵션은 수신 측에서 더 많은 메모리(전송된 파일당 1비트)를 사용하며, 업데이트된 모든 파일의 추가 복사본을 저장할 수 있는 충분한 여유 디스크 공간이 수신 측에 필요합니다. 또한 --partial-dir에 절대 경로를 사용해서는 안 됩니다. 다음 경우를 제외하고는: 8. 전송되는 파일 중 이름이 같은 파일이 있을 가능성이 전혀 없을 때 (경로가 절대 경로인 경우 모든 업데이트된 파일이 단일 디렉토리에 배치되므로), 그리고 9. 계층 구조에 마운트 지점이 없을 때 (지연된 업데이트가 제자리로 이름이 변경되지 않으면 실패하므로). \"atomic-rsync\" python 스크립트도 \"support\" 하위 디렉토리에 있습니다. 이 스크립트는 훨씬 더 원자적인 업데이트 알고리즘을 사용합니다 (--link-dest 및 파일의 병렬 계층 구조 사용). --prune-empty-dirs, -m 이 옵션은 수신 rsync에게 파일 목록에서 빈 디렉토리, 비디렉토리 자식이 없는 중첩된 디렉토리를 포함하여 제거하도록 지시합니다. 이는 보내는 rsync가 포함/제외/필터 규칙을 사용하여 파일 계층 구조를 재귀적으로 스캔할 때 쓸모없는 디렉토리가 많이 생성되는 것을 방지하는 데 유용합니다. 이 옵션은 TRANSFER_RULES를 사용하는 경우 수신 측에 빈 디렉토리를 남길 수도 있습니다. 파일 목록이 실제로 정리되고 있기 때문에 이 옵션은 삭제가 활성화되었을 때 어떤 디렉토리가 삭제될지에도 영향을 미칩니다. 그러나 제외된 파일 및 디렉토리가 소스 파일을 숨기고 대상 파일을 보호하는 두 가지 이유로 기존 항목이 삭제되는 것을 방지할 수 있다는 점을 명심하십시오. 이를 피하는 방법에 대해서는 소멸성 필터 규칙 옵션을 참조하십시오. 글로벌 \"보호\" 필터를 사용하여 파일 목록에서 특정 빈 디렉토리가 정리되는 것을 방지할 수 있습니다. 예를 들어, 이 옵션은 \"emptydir\" 디렉토리가 파일 목록에 유지되도록 보장합니다: --filter 'protect emptydir/' 다음은 계층 구조에 있는 모든 .pdf 파일을 복사하고, .pdf 파일을 보관하는 데 필요한 대상 디렉토리만 생성하며, 대상의 불필요한 파일 및 디렉토리가 제거되도록 하는 예시입니다 (비디렉토리를 숨기는 필터가 제외 대신 사용됨을 주의하십시오): rsync -avm --del --include='*.pdf' -f 'hide,! */' src/ dest 불필요한 대상 파일을 제거하고 싶지 않다면, --include='*/' --exclude='*'라는 더 오래된 옵션이 hide-filter 대신 잘 작동할 것입니다 (더 자연스럽게 느껴진다면). --progress 이 옵션은 rsync에게 전송 진행 상황을 보여주는 정보를 출력하도록 지시합니다. 이는 지루한 사용자에게 볼거리를 제공합니다. 최신 rsync에서는 --info=flist2,name,progress를 지정하는 것과 동일하지만, 해당 정보 플래그에 대한 사용자 제공 설정이 우선합니다 (예: --info=flist0 --progress). rsync가 일반 파일을 전송하는 동안, 다음과 같은 진행 상황 줄을 업데이트합니다: 782448 63% 110.64kB/s 0:00:04 이 예시에서 수신자는 보내는 측 파일의 782448바이트 또는 63%를 재구성했으며, 초당 110.64킬로바이트의 속도로 재구성되고 있으며, 현재 속도가 유지되면 4초 후에 전송이 완료될 것입니다. rsync의 델타-전송 알고리즘이 사용 중인 경우 이러한 통계는 오해의 소지가 있을 수 있습니다. 예를 들어, 보내는 측 파일이 기준 파일과 추가 데이터로 구성된 경우, 수신자가 리터럴 데이터에 도달하면 보고된 속도가 극적으로 떨어질 수 있으며, 파일의 일치하는 부분을 완료할 때 수신자가 예상한 것보다 전송 완료 시간이 훨씬 오래 걸릴 수 있습니다. 파일 전송이 완료되면 rsync는 진행 상황 줄을 다음과 같은 요약 줄로 바꿉니다: 1,238,099 100% 146.38kB/s 0:00:08 (xfr#5, to-chk=169/396) 이 예시에서 파일의 총 길이는 1,238,099바이트였고, 전체 파일의 평균 전송 속도는 완료하는 데 걸린 8초 동안 초당 146.38킬로바이트였습니다. 이것은 현재 rsync 세션 동안 일반 파일의 5번째 전송이었고, 파일 목록의 총 396개 파일 중 수신자가 확인해야 할 파일은 169개 남아 있습니다 (최신 상태인지 여부를 확인하기 위해). 증분 재귀 스캔에서 rsync는 스캔이 끝날 때까지 파일 목록의 총 파일 수를 알지 못하지만, 스캔 중에 파일을 전송하기 시작하므로 목록의 전체 크기를 알 때까지 \"to-chk\" 대신 \"ir-chk\"(증분 재귀 확인) 텍스트가 포함된 줄을 표시합니다. 따라서 \"ir-chk\"를 보면 파일 목록의 총 파일 수가 계속 증가할 것임을 알 수 있습니다 (그리고 그럴 때마다 확인해야 할 파일 수는 목록에 추가된 파일 수만큼 증가합니다). -P -P 옵션은 \"--partial --progress\"와 동일합니다. 그 목적은 중단될 수 있는 긴 전송에 대해 이 두 옵션을 지정하는 것을 훨씬 쉽게 만드는 것입니다. 전체 전송을 기반으로 통계를 출력하는 --info=progress2 옵션도 있습니다. 파일 이름 없이 이 플래그를 사용하십시오 (예: -v를 피하거나 --info=name0을 지정하십시오). 화면을 많은 이름으로 스크롤하지 않고 전송이 어떻게 진행되고 있는지 보고 싶다면. (--info=progress2를 사용하기 위해 --progress 옵션을 지정할 필요는 없습니다.) 마지막으로, SIGINFO 또는 SIGVTALRM 신호를 rsync에 보내면 즉시 진행 보고서를 받을 수 있습니다. BSD 시스템에서는 Ctrl+T를 입력하여 SIGINFO가 생성됩니다 (Linux는 현재 SIGINFO 신호를 지원하지 않습니다). 클라이언트 측 프로세스가 이러한 신호 중 하나를 받으면 단일 진행 보고서를 출력하기 위한 플래그를 설정하고, 현재 파일 전송이 완료될 때 출력됩니다 (따라서 신호가 도착할 때 큰 파일이 처리 중이면 약간의 시간이 걸릴 수 있습니다). 파일 이름이 출력된 후 (필요한 경우) --info=progress2 형식의 진행 정보가 출력됩니다. 3개의 rsync 프로세스 중 어느 것이 클라이언트 프로세스인지 모르는 경우, 모든 프로세스에 신호를 보내도 괜찮습니다 (비클라이언트 프로세스는 신호를 무시합니다). 주의: 오래된 rsync (3.2.0 이전)에 SIGVTALRM을 보내면 종료됩니다. --password-file=FILE 이 옵션을 사용하면 파일 또는 표준 입력(-)을 통해 rsync 데몬에 액세스할 때 암호를 제공할 수 있습니다. 파일은 첫 줄에 암호만 포함해야 합니다 (다른 모든 줄은 무시됩니다). FILE이 전 세계적으로 읽을 수 있거나 root로 실행되는 rsync 명령이 root 소유가 아닌 파일을 발견하면 rsync는 오류와 함께 종료됩니다. 이 옵션은 ssh와 같은 원격 셸 전송에 암호를 제공하지 않습니다. 이를 수행하는 방법은 원격 셸의 설명서를 참조하십시오. 원격 셸을 전송으로 사용하여 rsync 데몬에 액세스할 때, 이 옵션은 원격 셸이 인증을 마친 후에만 적용됩니다 (즉, 데몬의 구성 파일에도 암호를 지정한 경우). --early-input=FILE 이 옵션을 사용하면 rsync가 \"early exec\" 스크립트의 표준 입력으로 최대 5K의 데이터를 보낼 수 있습니다. 이 데이터의 한 가지 가능한 용도는 스크립트에 암호화된 파일 시스템을 마운트하는 데 사용할 수 있는 비밀을 제공하는 것입니다 (\"post-xfer exec\" 스크립트에서 마운트 해제해야 합니다). 데몬은 최소 3.2.1 버전이어야 합니다. --list-only 이 옵션은 소스 파일이 전송되는 대신 나열되도록 합니다. 이 옵션은 단일 소스 인자가 있고 대상이 지정되지 않은 경우 추론되므로, 주요 용도는 다음과 같습니다: 10. 대상 인자를 포함하는 복사 명령을 파일 목록 명령으로 전환하거나, 11. 두 개 이상의 소스 인자를 지정할 수 있도록 합니다. 참고: 대상을 반드시 포함하십시오. 주의: 와일드카드가 있는 소스 인자는 셸에 의해 여러 인자로 확장되므로, 이 옵션을 추론하기 위해 단일 와일드카드 인자를 지정하는 것은 안전하지 않습니다. 안전한 예시는 다음과 같습니다: rsync -av --list-only foo* dest/ 이 옵션은 항상 다음과 유사한 출력 형식을 사용합니다: drwxrwxr-x 4,096 2022/09/30 12:53:11 support -rw-rw-r-- 80 2005/01/11 10:37:37 support/Makefile 이 출력 스타일에 영향을 미치는 유일한 옵션은 (3.1.0 기준) --human-readable (-h) 옵션입니다. 기본값은 숫자 구분 기호가 있는 바이트 수(14자 너비 열)로 크기를 출력하는 것입니다. 하나 이상의 -h 옵션을 지정하면 크기가 단위 접미사와 함께 출력됩니다. 숫자 구분 기호가 없는 이전 스타일 바이트 수 크기(11자 너비 열)를 원하면 --no-h를 사용하십시오. 호환성 참고: rsync 2.6.3 이하 버전에서 원격 파일 목록을 요청할 때 비재귀적 목록을 요청하면 오류가 발생할 수 있습니다. 이는 파일 목록이 --recursive 없이 --dirs 옵션을 암시하고, 오래된 rsync에는 해당 옵션이 없기 때문입니다. 이 문제를 피하려면 --no-dirs 옵션을 지정하거나 (디렉토리 내용을 확장할 필요가 없는 경우), 재귀를 켜고 하위 디렉토리 내용을 제외하십시오: -r --exclude='/*/*'. --bwlimit=RATE 이 옵션은 소켓을 통해 전송되는 데이터의 최대 전송 속도를 초당 단위로 지정할 수 있도록 합니다. RATE 값 뒤에는 크기 승수를 나타내는 문자열을 붙일 수 있으며, 분수 값일 수도 있습니다 (예: --bwlimit=1.5m). 접미사가 지정되지 않으면 값은 1024바이트 단위로 가정됩니다 (마치 \"K\" 또는 \"KiB\"가 추가된 것처럼). 사용 가능한 모든 접미사에 대한 설명은 --max-size 옵션을 참조하십시오. 0 값은 제한 없음을 지정합니다. 하위 호환성을 위해 속도 제한은 가장 가까운 KiB 단위로 반올림되므로, 초당 1024바이트보다 작은 속도는 불가능합니다. Rsync는 소켓을 통해 데이터를 블록 단위로 기록하며, 이 옵션은 rsync가 쓰는 블록의 크기를 제한하고 요청된 제한에서 평균 전송 속도를 유지하려고 합니다. rsync가 데이터 블록을 쓰고 평균 속도를 준수하기 위해 잠시 대기하는 경우 일부 버스티니스(burstiness)가 나타날 수 있습니다. 내부적인 데이터 버퍼링으로 인해 --progress 옵션은 데이터 전송 속도를 정확하게 반영하지 못할 수 있습니다. 일부 파일은 데이터가 빠르게 버퍼링될 때 빠르게 전송되는 것처럼 보일 수 있고, 다른 파일은 출력 버퍼 플러싱이 발생할 때 매우 느리게 전송되는 것처럼 보일 수 있기 때문입니다. 이것은 향후 버전에서 수정될 수 있습니다. --bwlimit 옵션의 데몬 버전도 참조하십시오. --stop-after=MINS, (--time-limit=MINS) 이 옵션은 rsync에게 지정된 시간이 경과하면 복사를 중지하도록 지시합니다. 최대한의 유연성을 위해 rsync는 이 옵션을 원격 rsync에 통신하지 않습니다. 연결의 한쪽이 지정된 대로 종료하는 것으로 충분하기 때문입니다. 이를 통해 연결의 한쪽만 이 옵션을 지원하는 경우에도 사용할 수 있습니다. 필요하다면 --remote-option (-M)을 사용하여 원격 측에 시간 제한을 알릴 수 있습니다. --time-limit 버전의 이 옵션은 더 이상 사용되지 않습니다. --stop-at=y-m-dTh:m 이 옵션은 rsync에게 지정된 시간에 복사를 중지하도록 지시합니다. 날짜 및 시간은 로컬 시간대에서 연-월-일Th시:분 (예: 2000-12-31T23:59)의 숫자 형식으로 완전히 지정할 수 있습니다. 날짜 숫자를 대시 대신 슬래시로 구분해도 됩니다. 값은 또한 2자리 연도를 지정하거나 다양한 값을 생략하는 등 여러 가지 방식으로 축약될 수 있습니다. 모든 경우에 이 값은 제공된 정보와 일치하는 다음 가능한 시간 지점으로 간주됩니다. 값이 현재 시간 또는 과거 시간을 지정하면 rsync는 오류와 함께 종료됩니다. 예를 들어, \"1-30\"은 다음 1월 30일 (로컬 자정)을 지정하고, \"14:00\"은 다음 오후 2시를 지정하며, \"1\"은 다음 달 1일 자정을 지정하고, \"31\"은 다음 달 중 31일에 멈출 수 있는 다음 달을 지정하며, \":59\"는 시간당 다음 59분을 지정합니다. 최대한의 유연성을 위해 rsync는 이 옵션을 원격 rsync에 통신하지 않습니다. 연결의 한쪽이 지정된 대로 종료하는 것으로 충분하기 때문입니다. 이를 통해 연결의 한쪽만 이 옵션을 지원하는 경우에도 사용할 수 있습니다. 필요하다면 --remote-option (-M)을 사용하여 원격 측에 시간 제한을 알릴 수 있습니다. 원격 호스트가 로컬 호스트와 다른 기본 시간대를 가질 수 있다는 점을 명심하십시오. --fsync 수신 측에서 각 완료된 파일을 fsync하도록 합니다. 이는 전송 속도를 늦출 수 있지만, 중요한 파일을 업데이트할 때 안심할 수 있도록 도움이 됩니다. --write-batch=FILE 나중에 --read-batch로 다른 동일한 대상에 적용할 수 있는 파일을 기록합니다. 자세한 내용은 \"BATCH MODE\" 섹션을 참조하고, --only-write-batch 옵션도 참조하십시오. 이 옵션은 협상된 체크섬 및 압축 목록을 재정의하며 항상 구식 md5/md4/zlib 선택을 기반으로 선택을 협상합니다. 더 현대적인 선택을 원한다면 --checksum-choice (--cc) 및/또는 --compress-choice (--zc) 옵션을 사용하십시오. --only-write-batch=FILE --write-batch처럼 작동하지만, 배치를 생성할 때 대상 시스템에 업데이트를 수행하지 않습니다. 이를 통해 다른 수단을 통해 변경 사항을 대상 시스템으로 전송한 다음, --read-batch를 통해 변경 사항을 적용할 수 있습니다. 배치 파일을 휴대용 미디어에 직접 쓰는 것이 자유롭다는 점에 유의하십시오. 만약 이 미디어가 전송이 끝나기 전에 용량을 채우면, 해당 부분 전송을 대상에 적용하고 전체 프로세스를 반복하여 나머지 변경 사항을 얻을 수 있습니다 (다중 업데이트 주기가 진행되는 동안 부분적으로 업데이트된 대상 시스템이 있어도 괜찮다면). 또한 원격 시스템에 변경 사항을 푸시할 때만 대역폭을 절약할 수 있습니다. 이는 배치된 데이터를 보내는 측에서 배치 파일로 전환하여 수신자에게 유선을 통해 흐르도록 할 필요 없이 만들 수 있기 때문입니다 (가져올 때, 보내는 측이 원격에 있으므로 배치를 쓸 수 없습니다). --read-batch=FILE FILE에 저장된 모든 변경 사항을 적용합니다. FILE은 이전에 --write-batch에 의해 생성된 파일입니다. FILE이 '-'이면 배치 데이터는 표준 입력에서 읽힙니다. 자세한 내용은 \"BATCH MODE\" 섹션을 참조하십시오. --protocol=NUM 이전 프로토콜 버전을 강제로 사용합니다. 이는 이전 버전의 rsync와 호환되는 배치 파일을 생성하는 데 유용합니다. 예를 들어, rsync 2.6.4가 --write-batch 옵션과 함께 사용되지만, rsync 2.6.3이 --read-batch 옵션을 실행하는 데 사용될 예정이라면, 배치 파일 생성 시 \"--protocol=28\"을 사용하여 배치 파일에서 이전 프로토콜 버전이 사용되도록 강제해야 합니다 (읽기 시스템의 rsync를 업그레이드할 수 없다고 가정할 때). --iconv=CONVERT_SPEC Rsync는 이 옵션을 사용하여 문자셋 간에 파일 이름을 변환할 수 있습니다. CONVERT_SPEC으로 \".\"을 사용하면 rsync에게 로케일 설정을 통해 기본 문자셋을 찾도록 지시합니다. 또는 --iconv=LOCAL,REMOTE와 같이 쉼표로 구분된 로컬 및 원격 문자셋을 순서대로 지정하여 변환할 내용을 완전히 지정할 수 있습니다. 예를 들어 --iconv=utf8,iso88591. 이 순서는 파일을 푸시하든 풀링하든 옵션이 동일하게 유지되도록 보장합니다. 마지막으로, --no-iconv 또는 CONVERT_SPEC으로 \"-\"를 지정하여 모든 변환을 끌 수 있습니다. 이 옵션의 기본 설정은 사이트별로 다르며, RSYNC_ICONV 환경 변수를 통해서도 영향을 받을 수 있습니다. 로컬 iconv 라이브러리가 지원하는 문자셋 이름 목록을 보려면 \"iconv --list\"를 실행하십시오. --secluded-args (-s) 옵션을 지정하면 rsync는 명령줄에 지정된 파일 이름 중 원격 호스트로 전송되는 파일 이름을 번역합니다. --files-from 옵션도 참조하십시오. rsync는 필터 파일(포함/제외 파일 포함)의 이름을 변환하지 않는다는 점에 유의하십시오. 전송의 양쪽에서 일치할 수 있는 규칙을 지정하는 것은 사용자에게 달려 있습니다. 예를 들어, 양쪽에 파일 이름 차이가 있어 처리해야 하는 경우 추가 포함/제외 규칙을 지정할 수 있습니다. --iconv 옵션을 허용하는 rsync 데몬에 전달할 때, 데몬은 실제로 전달하는 원격 문자셋과 관계없이 \"charset\" 구성 매개변수에 지정된 문자셋을 사용합니다. 따라서 데몬 전송의 경우 로컬 문자셋만 지정해도 됩니다 (예: --iconv=utf8). --ipv4, -4 또는 --ipv6, -6 rsync에게 소켓을 생성하거나 ssh를 실행할 때 IPv4/IPv6를 선호하도록 지시합니다. 이는 rsync가 직접 제어하는 소켓, 예를 들어 rsync 데몬에 직접 연결할 때 나가는 소켓, 그리고 ssh가 원격 셸로 사용된다는 것을 rsync가 추론할 수 있을 때 ssh에 -4 또는 -6 옵션을 전달하는 것에 영향을 미칩니다. 다른 원격 셸의 경우 \"--rsh SHELL -4\" 옵션 (또는 해당 셸이 사용하는 IPv4/IPv6 힌트 옵션)을 직접 지정해야 합니다. 이 옵션의 데몬 버전도 참조하십시오. rsync가 IPv6 지원 없이 컴파일된 경우, --ipv6 옵션은 효과가 없습니다. rsync --version 출력에 \"no IPv6\"가 포함되어 있으면 이러한 경우입니다. --checksum-seed=NUM 체크섬 시드를 정수 NUM으로 설정합니다. 이 4바이트 체크섬 시드는 각 블록 및 MD4 파일 체크섬 계산에 포함됩니다 (더 현대적인 MD5 파일 체크섬은 시드를 사용하지 않습니다). 기본적으로 체크섬 시드는 서버에 의해 생성되며 현재 시간()으로 기본값이 지정됩니다. 이 옵션은 특정 체크섬 시드를 설정하는 데 사용되며, 반복 가능한 블록 체크섬을 원하는 애플리케이션이나 사용자가 더 무작위적인 체크섬 시드를 원하는 경우에 유용합니다. NUM을 0으로 설정하면 rsync는 체크섬 시드에 대해 기본값인 time()을 사용합니다. DAEMON OPTIONS rsync 데몬을 시작할 때 허용되는 옵션은 다음과 같습니다: --daemon 이것은 rsync에게 데몬으로 실행하도록 지시합니다. 실행 중인 데몬은 host::module 또는 rsync://host/module/ 구문을 사용하여 rsync 클라이언트로 접근할 수 있습니다. 표준 입력이 소켓인 경우 rsync는 inetd를 통해 실행 중이라고 가정하고, 그렇지 않으면 현재 터미널에서 분리되어 백그라운드 데몬이 됩니다. 데몬은 클라이언트가 연결할 때마다 구성 파일(rsyncd.conf)을 읽고 요청에 따라 응답합니다. 자세한 내용은 rsyncd.conf(5) 맨페이지를 참조하십시오. --address=ADDRESS 기본적으로 rsync는 --daemon 옵션으로 데몬으로 실행될 때 와일드카드 주소에 바인딩합니다. --address 옵션을 사용하면 특정 IP 주소(또는 호스트 이름)에 바인딩하도록 지정할 수 있습니다. 이는 --config 옵션과 함께 가상 호스팅을 가능하게 합니다. rsyncd.conf 맨페이지의 address 전역 옵션과 --address 옵션의 클라이언트 버전도 참조하십시오. --bwlimit=RATE 이 옵션은 데몬이 소켓을 통해 보내는 데이터의 최대 전송 속도를 지정할 수 있도록 합니다. 클라이언트는 여전히 더 작은 --bwlimit 값을 지정할 수 있지만, 더 큰 값은 허용되지 않습니다. 몇 가지 추가 세부 사항은 --bwlimit 옵션의 클라이언트 버전을 참조하십시오. --config=FILE 이것은 기본값 대신 대체 구성 파일을 지정합니다. 이는 --daemon이 지정된 경우에만 관련됩니다. 데몬이 원격 셸 프로그램을 통해 실행되고 원격 사용자가 슈퍼유저가 아닌 경우를 제외하고 기본값은 /etc/rsyncd.conf입니다. 이 경우 기본값은 현재 디렉토리(일반적으로 $HOME)의 rsyncd.conf입니다. --dparam=OVERRIDE, -M 이 옵션은 데몬 모드에서 rsync를 시작할 때 데몬-구성 매개변수를 설정하는 데 사용될 수 있습니다. 이는 첫 번째 모듈 정의 이전에 전역 설정 끝에 매개변수를 추가하는 것과 동일합니다. 매개변수 이름은 원하는 경우 공백 없이 지정할 수 있습니다. 예를 들어: rsync --daemon -M pidfile=/path/rsync.pid --no-detach 데몬으로 실행할 때, 이 옵션은 rsync에게 자신을 분리하여 백그라운드 프로세스가 되지 않도록 지시합니다. 이 옵션은 Cygwin에서 서비스로 실행할 때 필요하며, daemontools 또는 AIX의 System Resource Controller와 같은 프로그램에 의해 rsync가 감독될 때도 유용할 수 있습니다. --no-detach는 rsync가 디버거에서 실행될 때도 권장됩니다. 이 옵션은 rsync가 inetd 또는 sshd에서 실행될 때 효과가 없습니다. --port=PORT 이것은 데몬이 기본값 873 대신 수신할 대체 TCP 포트 번호를 지정합니다. --port 옵션의 클라이언트 버전과 rsyncd.conf 맨페이지의 port 전역 설정도 참조하십시오. --log-file=FILE 이 옵션은 rsync 데몬에게 구성 파일의 \"log file\" 설정 대신 주어진 로그-파일 이름을 사용하도록 지시합니다. --log-file 옵션의 클라이언트 버전도 참조하십시오. --log-file-format=FORMAT 이 옵션은 rsync 데몬에게 구성 파일의 \"log format\" 설정 대신 주어진 FORMAT 문자열을 사용하도록 지시합니다. 또한 문자열이 비어 있지 않으면 \"전송 로깅\"을 활성화하고, 문자열이 비어 있으면 전송 로깅을 끕니다. --log-file-format 옵션의 클라이언트 버전도 참조하십시오. --sockopts 이것은 rsyncd.conf 파일의 소켓 옵션 설정을 재정의하며, 구문은 동일합니다. --sockopts 옵션의 클라이언트 버전도 참조하십시오. --verbose, -v 이 옵션은 데몬이 시작 단계에서 기록하는 정보의 양을 늘립니다. 클라이언트가 연결되면 데몬의 상세도 수준은 클라이언트가 사용한 옵션과 모듈 구성 섹션의 \"max verbosity\" 설정에 의해 제어됩니다. --verbose 옵션의 클라이언트 버전도 참조하십시오. --ipv4, -4 또는 --ipv6, -6 rsync 데몬이 연결을 수신하기 위해 사용하는 수신 소켓을 생성할 때 IPv4/IPv6를 선호하도록 지시합니다. 이 옵션 중 하나는 Linux의 오래된 버전에서 커널의 IPv6 버그를 해결하기 위해 필요할 수 있습니다 (다른 아무것도 포트를 사용하고 있지 않은데 \"address already in use\" 오류가 발생하면 데몬을 시작할 때 --ipv6 또는 --ipv4를 지정해 보십시오). 이 옵션의 클라이언트 버전도 참조하십시오. rsync가 IPv6 지원 없이 컴파일된 경우, --ipv6 옵션은 효과가 없습니다. rsync --version 출력에 \"no IPv6\"가 포함되어 있으면 이러한 경우입니다. --help, -h --daemon 다음에 지정되면 rsync 데몬을 시작하는 데 사용할 수 있는 옵션을 설명하는 짧은 도움말 페이지를 출력합니다. FILTER RULES 필터 규칙은 파일 처리 방식의 여러 측면을 사용자 정의로 제어할 수 있도록 합니다: o 보내는 측이 전송 계층 구조를 설명하는 파일 목록에 어떤 파일을 넣을지 제어합니다. o 삭제 시, 파일이 전송자의 파일 목록에 없을 때 받는 측이 어떤 파일을 삭제로부터 보호할지 제어합니다. o xattrs를 복사할 때 어떤 확장 속성 이름을 건너뛸지 제어합니다. 규칙은 옵션 인자를 통해 직접 지정되거나, 하나 이상의 파일에서 읽어올 수 있습니다. 필터-규칙 파일은 복사되는 파일 계층 구조의 일부일 수 있으며, 트리의 다른 부분에 다른 방식으로 영향을 미칠 수 있습니다. SIMPLE INCLUDE/EXCLUDE RULES 먼저 포함 및 제외 규칙이 전송되는 파일에 어떤 영향을 미치는지 기본적인 사항을 다루고, 삭제 부작용은 무시하겠습니다. 필터 규칙은 주로 rsync가 \"재귀적으로\" 들어가는 디렉토리의 내용에 영향을 미치지만, 인자로 지정된 전송의 최상위 항목에도 영향을 미칠 수 있습니다. 일치하지 않는 모든 파일/디렉토리의 기본값은 전송에 포함되는 것이며, 이는 파일/디렉토리를 보내는 측의 파일 목록에 넣습니다. 제외 규칙을 사용하면 일치하는 하나 이상의 파일/디렉토리가 보내는 측의 파일 목록에서 제외됩니다. 포함 규칙은 너무 많은 파일과 일치하는 제외 규칙의 영향을 제한하는 데 사용될 수 있습니다. 규칙의 순서가 중요합니다. 첫 번째로 일치하는 규칙이 적용되기 때문입니다. 따라서, 초기 규칙이 파일을 제외하면 그 뒤에 오는 포함 규칙은 아무런 효과도 가질 수 없습니다. 이는 포함 재정의를 의도한 제외 규칙보다 앞서 어딘가에 배치해야 한다는 것을 의미합니다. 디렉토리가 제외되면 해당 내용과 하위 내용도 모두 제외됩니다. 보내는 측은 어떤 내용도 전혀 스캔하지 않으므로, 불필요한 큰 하위 트리를 건너뛸 때 많은 시간을 절약할 수 있습니다. 또한 포함/제외 규칙은 보내는 측이 재귀하는 모든 파일 및 디렉토리에 적용된다는 것을 이해하는 것이 중요합니다. 따라서 특정 깊은 파일을 포함하려면, 해당 파일로 가는 경로에서 반드시 거쳐야 할 디렉토리 중 어느 것도 제외되지 않았는지 확인해야 합니다. 그렇지 않으면 파일이 포함될 것으로 발견되지 않을 것입니다. 예를 들어, \"a/path\" 디렉토리가 전송 인자로 주어졌고 \"a/path/down/deep/wanted.txt\" 파일이 전송의 일부가 되도록 하려면, 보내는 측은 파일 트리를 스캔하면서 \"a/path\", \"a/path/down\", \"a/path/down/deep\" 디렉토리를 제외해서는 안 됩니다. 규칙 작업을 할 때, rsync에게 무엇이 제외/포함되고 왜 그런지 알려달라고 요청하는 것이 도움이 될 수 있습니다. --debug=FILTER 또는 (파일을 가져올 때) -M--debug=FILTER를 지정하면 FILTER 디버그 정보의 수준 1이 켜지고, 파일이나 디렉토리가 포함되거나 제외될 때마다 어떤 규칙과 일치했는지 메시지가 출력됩니다. 3.2.4부터는 필터 규칙에 후행 공백이 있는 경우에도 경고합니다. \"foo \" (후행 공백 포함) 제외는 \"foo\"라는 파일을 제외하지 않기 때문입니다. 제외 및 포함 규칙은 파일 접미사 또는 파일 이름의 일부와 같은 것을 일치시킬 수 있는 와일드카드 PATTERN MATCHING RULES (셸 와일드카드와 유사)를 지정할 수 있습니다. 규칙은 파일 이름 뒤에 후행 슬래시를 붙여 디렉토리에만 영향을 미치도록 제한할 수 있습니다. SIMPLE INCLUDE/EXCLUDE EXAMPLE 보내는 측에 다음 파일 트리가 생성되었다고 가정합니다: mkdir x/ touch x/file.txt mkdir x/y/ touch x/y/file.txt touch x/y/zzz.txt mkdir x/z/ touch x/z/file.txt 그러면 다음 rsync 명령은 \"x/y/file.txt\" 파일과 해당 파일을 보관하는 데 필요한 디렉토리를 전송하여 원격 호스트에 \"/tmp/x/y/file.txt\" 경로가 존재하도록 합니다: rsync -ai -f'+ x/' -f'+ x/y/' -f'+ x/y/file.txt' -f'- *' x host:/tmp/ 참고: 이 복사는 -R 옵션을 사용하여도 달성할 수 있었습니다 (단, 삭제가 활성화된 경우 두 명령은 다르게 작동합니다): rsync -aiR x/y/file.txt host:/tmp/ 다음 명령은 \"x\" 디렉토리가 전송의 일부가 아니므로 (후행 슬래시 주의) 포함할 필요가 없습니다. 이 명령을 실행하면 \"y\" 및 \"z\" 디렉토리가 제외되므로 \"/tmp/x/file.txt\"만 복사됩니다: rsync -ai -f'+ file.txt' -f'- *' x/ host:/tmp/x/ 이 명령은 \"zzz.txt\" 파일을 제외하고 \"x\"와 그 안에 포함된 모든 것을 복사합니다: rsync -ai -f'- zzz.txt' x host:/tmp/ FILTER RULES WHEN DELETING 삭제 옵션이 없으면 디렉토리별 규칙은 보내는 측에만 관련되므로, 병합 파일 자체를 제외해도 전송에 영향을 주지 않습니다. 이를 쉽게 하기 위해 'e' 수정자가 이 제외를 자동으로 추가합니다. 다음 두 가지 동등한 명령에서 볼 수 있습니다: rsync -av --filter=': .excl' --exclude=.excl host:src/dir /dest rsync -av --filter=':e .excl' host:src/dir /dest 그러나 수신 측에서 삭제를 수행하고 일부 파일을 삭제에서 제외하고 싶다면, 수신 측이 어떤 파일을 제외해야 하는지 알아야 합니다. 가장 쉬운 방법은 디렉토리별 병합 파일을 전송에 포함하고 --delete-after를 사용하는 것입니다. 이는 삭제를 시도하기 전에 수신 측이 보내는 측과 동일한 모든 제외 규칙을 받도록 보장하기 때문입니다: rsync -avF --delete-after host:src/dir /dest 그러나 병합 파일이 전송의 일부가 아닌 경우, 전역 제외 규칙을 지정하거나 (즉, 명령줄에 지정), 수신 측에서 자체 디렉토리별 병합 파일을 유지해야 합니다. 첫 번째 예시는 다음과 같습니다 (원격 .rules 파일이 자신을 제외한다고 가정): rsync -av --filter=': .rules' --filter='. /my/extra.rules' --delete host:src/dir /dest 위 예시에서 extra.rules 파일은 전송의 양쪽에 영향을 미칠 수 있지만, (보내는 측에서는) 디렉토리별 병합 규칙 이후에 지정되었으므로 .rules 파일에서 병합된 규칙보다 하위입니다. 마지막 예시로, 원격 측은 전송에서 .rsync-filter 파일을 제외하고 있지만, 수신 측에서 무엇이 삭제될지 제어하기 위해 자체 .rsync-filter 파일을 사용하고 싶습니다. 이를 위해 디렉토리별 병합 파일을 명시적으로 제외하고 (삭제되지 않도록), 로컬 파일에 다른 무엇이 삭제되지 않아야 하는지 제어하는 규칙을 추가해야 합니다. 다음 명령 중 하나와 같이: rsync -av --filter=':e /.rsync-filter' --delete \\ host:src/dir /dest rsync -avFF --delete host:src/dir /dest FILTER RULES IN DEPTH Rsync는 이전 스타일 포함/제외 규칙과 새로운 스타일 필터 규칙을 지원합니다. 이전 규칙은 --include 및 --exclude뿐만 아니라 --include-from 및 --exclude-from을 사용하여 지정됩니다. 이들은 동작이 제한적이지만 \"-\" 또는 \"+\" 접두사가 필요하지 않습니다. 이전 스타일 제외 규칙은 \"- name\" 필터 규칙(수정자 없음)으로 변환되고, 이전 스타일 포함 규칙은 \"+ name\" 필터 규칙(수정자 없음)으로 변환됩니다. Rsync는 명령줄에 지정되었거나 파일에서 읽어온 필터 규칙의 순서 있는 목록을 빌드합니다. 새로운 스타일 필터 규칙은 다음 구문을 가집니다: RULE [PATTERN_OR_FILENAME] RULE,MODIFIERS [PATTERN_OR_FILENAME] 아래 설명된 짧은 또는 긴 RULE 이름을 선택하여 사용할 수 있습니다. 짧은 이름의 규칙을 사용하는 경우 RULE과 MODIFIERS를 구분하는 ','는 선택 사항입니다. 그 뒤에 오는 PATTERN 또는 FILENAME (존재하는 경우)은 단일 공백 또는 밑줄(_) 뒤에 와야 합니다. 추가 공백 및/또는 밑줄은 패턴 이름의 일부로 간주됩니다. 사용 가능한 규칙 접두사는 다음과 같습니다: exclude, '-' 숨김(hide)과 보호(protect)를 모두 수행하는 제외 패턴을 지정합니다 (기본값). include, '+' 표시(show)와 위험(risk)을 모두 수행하는 포함 패턴을 지정합니다 (기본값). merge, '.' 더 많은 규칙을 읽을 클라이언트 측의 병합 파일(merge-file)을 지정합니다. dir-merge, ':' 디렉토리별 병합 파일(per-directory merge-file)을 지정합니다. 이러한 종류의 필터 규칙을 사용하려면 보내는 측의 필터 확인을 신뢰해야 하므로, --trust-sender 옵션에 언급된 부작용이 있습니다. hide, 'H' 전송에서 파일을 숨기기 위한 패턴을 지정합니다. 보내는 측 전용 제외와 동일하므로 -f'H foo'는 -f'-s foo'로도 지정할 수 있습니다. show, 'S' 패턴과 일치하는 파일은 숨겨지지 않습니다. 보내는 측 전용 포함과 동일하므로 -f'S foo'는 -f'+s foo'로도 지정할 수 있습니다. protect, 'P' 삭제로부터 파일을 보호하기 위한 패턴을 지정합니다. 받는 측 전용 제외와 동일하므로 -f'P foo'는 -f'-r foo'로도 지정할 수 있습니다. risk, 'R' 패턴과 일치하는 파일은 보호되지 않습니다. 받는 측 전용 포함과 동일하므로 -f'R foo'는 -f'+r foo'로도 지정할 수 있습니다. clear, '!' 현재 포함/제외 목록을 지웁니다 (인자를 받지 않습니다). 규칙이 파일에서 읽어올 때 (merge 또는 dir-merge 사용), 빈 줄은 무시되며, '#'으로 시작하는 전체 줄 주석도 무시됩니다 (해시 문자를 포함하는 파일 이름 규칙은 영향을 받지 않습니다). 또한 --filter, --include 및 --exclude 옵션은 각각 하나의 규칙/패턴만 받습니다. 여러 개를 추가하려면 명령줄에서 옵션을 반복하거나, --filter 옵션의 병합 파일 구문을 사용하거나, --include-from / --exclude-from 옵션을 사용하십시오. PATTERN MATCHING RULES 위에 언급된 대부분의 규칙은 규칙이 무엇과 일치해야 하는지 지정하는 인자를 받습니다. rsync가 디렉토리 계층을 재귀적으로 탐색하는 경우, 각 패턴은 rsync가 보낼 파일 이름을 찾으면서 하위 경로의 모든 디렉토리 이름과 일치하는지 확인된다는 점을 명심하십시오. 패턴 인자에 대한 일치 규칙은 여러 형태를 가집니다: o 패턴에 / (후행 슬래시 제외) 또는 \"**\" (슬래시와 일치할 수 있음)가 포함된 경우, 패턴은 전송 내의 모든 선행 디렉토리를 포함한 전체 경로 이름과 일치합니다. 패턴에 (후행이 아닌) / 또는 \"**\"가 포함되어 있지 않으면, 파일 이름 또는 경로 이름의 마지막 구성 요소와만 일치합니다. 예를 들어, foo는 최종 경로 구성 요소가 \"foo\"여야 함을 의미하며, foo/bar는 경로의 마지막 두 요소와 일치합니다 (두 요소 모두 전송 내에 있는 한). o /로 끝나는 패턴은 디렉토리만 일치시키고, 일반 파일, 심볼릭 링크 또는 장치는 일치시키지 않습니다. o /로 시작하는 패턴은 전송 경로의 끝이 아닌 시작에 고정됩니다. 예를 들어, /foo/** 또는 /foo/bar/**는 경로의 선행 요소만 일치시킵니다. 규칙이 디렉토리별 필터 파일에서 읽힌 경우, 일치하는 전송 경로는 전송의 최상단이 아닌 필터 파일 수준에서 시작됩니다. 전송의 루트에서 일치하는 패턴을 지정하는 방법에 대한 전체 논의는 ANCHORING INCLUDE/EXCLUDE PATTERNS 섹션을 참조하십시오. Rsync는 패턴에 다음 세 가지 와일드카드 문자 중 하나가 포함되어 있는지 확인하여 간단한 문자열 일치와 와일드카드 일치 중 하나를 선택합니다: '*', '?', '[' : o '?'는 슬래시(/)를 제외한 모든 단일 문자와 일치합니다. o '*'는 슬래시가 아닌 0개 이상의 문자와 일치합니다. o '**'는 슬래시를 포함하여 0개 이상의 문자와 일치합니다. o '['는 문자 클래스(예: [a-z] 또는 [[:alpha:]])를 도입하며, 한 문자와 일치해야 합니다. o 패턴의 후행 ***는 디렉토리와 그 모든 내용을 단일 규칙으로 일치시킬 수 있는 약식입니다. 예를 들어, \"dir_name/***\"를 지정하면 \"dir_name\" 디렉토리(마치 \"dir_name/\"이 지정된 것처럼)와 디렉토리 내의 모든 내용(마치 \"dir_name/**\"이 지정된 것처럼)을 모두 일치시킵니다. o 역슬래시는 와일드카드 문자를 이스케이프하는 데 사용할 수 있지만, 일치 패턴에 와일드카드 문자가 하나 이상 있는 경우에만 이스케이프 문자로 해석됩니다. 예를 들어, 패턴 \"foo\\bar\"는 단일 역슬래시를 리터럴로 일치시키지만, 패턴 \"foo\\bar*\"는 \"\\b\"가 단순히 \"b\"가 되는 것을 피하기 위해 \"foo\\\\bar*\"로 변경되어야 합니다. 다음은 제외/포함 일치 예시입니다: o 옵션 -f'- *.o'는 .o로 끝나는 모든 파일 이름을 제외합니다. o 옵션 -f'- /foo'는 전송-루트 디렉토리의 foo라는 파일(또는 디렉토리)을 제외합니다. o 옵션 -f'- foo/'는 foo라는 모든 디렉토리를 제외합니다. o 옵션 -f'- foo/*/bar'는 foo라는 디렉토리 아래 두 단계에 있는 bar라는 파일/디렉토리를 제외합니다 (foo가 전송에 포함된 경우). o 옵션 -f'- /foo/**/bar'는 최상위 디렉토리 foo 아래 두 단계 이상에 있는 bar라는 파일/디렉토리를 제외합니다 (단, /foo/bar는 이것에 의해 제외되지 않습니다). o 옵션 -f'+ */' -f'+ *.c' -f'- *'는 모든 디렉토리와 .c 소스 파일만 포함하고 다른 것은 포함하지 않습니다. o 옵션 -f'+ foo/' -f'+ foo/bar.c' -f'- *'는 foo 디렉토리와 foo/bar.c만 포함합니다 (foo 디렉토리는 명시적으로 포함되어야 합니다. 그렇지 않으면 \"- *\"에 의해 제외됩니다). FILTER RULE MODIFIERS 다음 수정자는 포함(+) 또는 제외(-) 규칙 뒤에 허용됩니다: o /는 포함/제외 규칙이 현재 항목의 절대 경로 이름과 일치해야 함을 지정합니다. 예를 들어, -f'-/ /etc/passwd'는 전송이 \"/etc\" 디렉토리에서 파일을 보낼 때마다 passwd 파일을 제외하며, \"-/ subdir/foo\"는 \"foo\"가 \"subdir\"라는 디렉토리에 있을 때 항상 \"foo\"를 제외합니다. \"foo\"가 현재 전송의 루트에 있더라도 마찬가지입니다. o !는 패턴이 일치하지 않을 때 포함/제외가 적용되어야 함을 지정합니다. 예를 들어, -f'-! */'는 모든 비디렉토리를 제외합니다. o C는 모든 전역 CVS-제외 규칙이 \"-C\" 대신 제외로 삽입되어야 함을 나타내는 데 사용됩니다. 인자가 뒤따르지 않습니다. o s는 규칙이 보내는 측에 적용됨을 나타내는 데 사용됩니다. 규칙이 보내는 측에 영향을 미칠 때, 이는 보내는 측의 파일 목록에 어떤 파일이 포함될지에 영향을 미칩니다. 기본값은 규칙이 양쪽에 영향을 미치는 것이지만, --delete-excluded가 지정된 경우 기본 규칙은 보내는 측에만 적용됩니다. 보내는 측의 포함/제외를 지정하는 다른 방법인 hide (H) 및 show (S) 규칙도 참조하십시오. o r은 규칙이 수신 측에 적용됨을 나타내는 데 사용됩니다. 규칙이 수신 측에 영향을 미칠 때, 이는 파일이 삭제되는 것을 방지합니다. 자세한 내용은 s 수정자를 참조하십시오. 수신 측의 포함/제외를 지정하는 다른 방법인 protect (P) 및 risk (R) 규칙도 참조하십시오. o p는 규칙이 소멸성(perishable)임을 나타냅니다. 즉, 삭제되는 디렉토리에서는 무시됩니다. 예를 들어, --cvs-exclude (-C) 옵션의 \"CVS\" 및 \"*.o\"와 같은 것을 제외하는 기본 규칙은 소멸성으로 표시되며, 소스에서 제거된 디렉토리가 대상에서 삭제되는 것을 방지하지 않습니다. o x는 규칙이 xattr 복사/삭제 작업에서 xattr 이름에 영향을 미치고 (따라서 파일/디렉토리 이름과 일치할 때는 무시됩니다) 있음을 나타냅니다. xattr-일치 규칙이 지정되지 않으면 기본 xattr 필터링 규칙이 사용됩니다 (--xattrs 옵션 참조). MERGE-FILE FILTER RULES 병합(.) 또는 디렉토리 병합(:) 필터 규칙을 지정하여 전체 파일을 필터 규칙에 병합할 수 있습니다 (위의 FILTER RULES 섹션에서 소개됨). 병합 파일에는 단일-인스턴스('.')와 디렉토리별(':') 두 가지 종류가 있습니다. 단일-인스턴스 병합 파일은 한 번 읽히며, 그 규칙은 \".\" 규칙 위치의 필터 목록에 통합됩니다. 디렉토리별 병합 파일의 경우, rsync는 탐색하는 모든 디렉토리에서 명명된 파일을 스캔하고, 파일이 존재할 때 그 내용을 현재 상속된 규칙 목록에 병합합니다. 이러한 디렉토리별 규칙 파일은 보내는 측에서 생성되어야 합니다. 보내는 측이 전송할 수 있는 파일을 스캔하기 때문입니다. 또한, 삭제되지 않아야 할 파일에 영향을 미치려면 이러한 규칙 파일을 수신 측으로 전송해야 할 수도 있습니다 (아래 PER-DIRECTORY RULES AND DELETE 참조). 몇 가지 예시: merge /etc/rsync/default.rules . /etc/rsync/default.rules dir-merge .per-dir-filter dir-merge,n- .non-inherited-per-dir-excludes :n- .non-inherited-per-dir-excludes 다음 수정자는 병합 또는 디렉토리 병합 규칙 뒤에 허용됩니다: o -는 파일이 오직 제외 패턴으로만 구성되어야 하며, 인-파일 주석을 제외하고는 다른 규칙 파싱은 없어야 함을 지정합니다. o +는 파일이 오직 포함 패턴으로만 구성되어야 하며, 인-파일 주석을 제외하고는 다른 규칙 파싱은 없어야 함을 지정합니다. o C는 파일이 CVS 호환 방식으로 읽혀야 함을 지정하는 방법입니다. 이것은 'n', 'w', '-'를 켜지만, 목록 지우기 토큰(!)도 지정할 수 있도록 합니다. 파일 이름이 제공되지 않으면 \".cvsignore\"가 가정됩니다. o e는 병합 파일 이름을 전송에서 제외합니다. 예: \"dir-merge,e .rules\"는 \"dir-merge .rules\"와 \"- .rules\"와 같습니다. o n은 규칙이 하위 디렉토리에 상속되지 않음을 지정합니다. o w는 규칙이 일반적인 줄 분할 대신 공백으로 단어 분할됨을 지정합니다. 이것은 또한 주석을 끕니다. 참고: 접두사와 규칙을 분리하는 공백은 특별하게 처리되므로, \"- foo + bar\"는 두 개의 규칙으로 파싱됩니다 (접두사 파싱도 비활성화되지 않았다고 가정할 때). o \"+\" 또는 \"-\" 규칙 (위 참조)에 대한 수정자 중 하나를 지정할 수도 있습니다. 이는 파일에서 읽어온 규칙이 해당 수정자가 설정된 것으로 기본값을 갖도록 하기 위함입니다 (유용하지 않을 ! 수정자는 제외). 예를 들어, \"merge,-/ .excl\"은 .excl의 내용을 절대 경로 제외로 처리하고, \"dir-merge,s .filt\" 및 \":sC\"는 각각 디렉토리별 규칙을 보내는 측에만 적용되도록 합니다. 병합 규칙이 영향을 미칠 측면을 지정하는 경우 (s 또는 r 수정자 또는 둘 다를 통해), 파일의 규칙은 측면을 지정해서는 안 됩니다 (수정자 또는 hide와 같은 규칙 접두사를 통해). 디렉토리별 규칙은 'n' 수정자가 사용되지 않는 한 병합 파일이 발견된 디렉토리의 모든 하위 디렉토리에서 상속됩니다. 각 하위 디렉토리의 규칙은 부모로부터 상속된 디렉토리별 규칙 앞에 추가되어, 최신 규칙이 상속된 규칙보다 높은 우선순위를 가집니다. 전체 dir-merge 규칙 집합은 병합 파일이 지정된 위치에 함께 그룹화되므로, 전역 규칙 목록에서 이전에 지정된 규칙을 통해 dir-merge 규칙을 재정의할 수 있습니다. 디렉토리별 파일에서 목록 지우기 규칙(\"!\")이 읽히면 현재 병합 파일에 대한 상속된 규칙만 지워집니다. dir-merge 파일의 단일 규칙이 상속되는 것을 막는 또 다른 방법은 선행 슬래시로 고정하는 것입니다. 디렉토리별 병합 파일의 고정된 규칙은 병합 파일의 디렉토리에 상대적이므로, 패턴 \"/foo\"는 dir-merge 필터 파일이 발견된 디렉토리의 \"foo\" 파일만 일치시킬 것입니다. 다음은 --filter=\". file\"을 통해 지정할 수 있는 예시 필터 파일입니다: merge /home/user/.global-filter - *.gz dir-merge .rules + *.[ch] - *.o - foo* 이것은 목록 시작 부분에 /home/user/.global-filter 파일의 내용을 병합하고, \".rules\" 파일 이름을 디렉토리별 필터 파일로 전환합니다. 디렉토리 스캔 시작 이전에 읽어온 모든 규칙은 전역 고정 규칙을 따릅니다 (즉, 선행 슬래시는 전송의 루트와 일치합니다). 디렉토리별 병합 파일이 첫 번째 전송 디렉토리의 상위 디렉토리 경로로 지정된 경우, rsync는 해당 시작 지점에서 전송 디렉토리까지의 모든 상위 디렉토리에서 지정된 디렉토리별 파일을 스캔합니다. 예를 들어, 다음은 일반적인 필터입니다 (-F 참조): --filter=': /.rsync-filter' 그 규칙은 rsync에게 정상적인 디렉토리 스캔이 시작되기 전에 루트부터 전송의 상위 디렉토리까지의 모든 디렉토리에서 .rsync-filter 파일을 스캔하도록 지시합니다. (참고: rsync 데몬의 경우, 루트는 항상 모듈의 \"path\"와 동일합니다.) 디렉토리별 파일에 대한 사전 스캔의 몇 가지 예시: rsync -avF /src/path/ /dest/dir rsync -av --filter=': ../../.rsync-filter' /src/path/ /dest/dir rsync -av --filter=': .rsync-filter' /src/path/ /dest/dir 위의 처음 두 명령은 정상 스캔이 \"/src/path\" 및 그 하위 디렉토리에서 파일을 찾기 시작하기 전에 \"/\" 및 \"/src\"에서 \".rsync-filter\"를 찾을 것입니다. 마지막 명령은 상위 디렉토리 스캔을 피하고 전송의 일부인 각 디렉토리에서만 \".rsync-filter\" 파일을 찾습니다. 패턴에 \".cvsignore\"의 내용을 포함하려면, \":C\" 규칙을 사용해야 합니다. 이것은 .cvsignore 파일의 dir-merge를 생성하지만, CVS 호환 방식으로 파싱됩니다. 이것을 사용하여 --cvs-exclude (-C) 옵션의 디렉토리별 .cvsignore 파일 포함 위치를 필터 규칙 내에서 원하는 위치에 배치하여 제어할 수 있습니다. 그렇지 않으면 rsync는 .cvsignore 파일에 대한 dir-merge 규칙을 다른 모든 규칙의 끝에 추가합니다 (명령줄 규칙보다 낮은 우선순위를 부여합니다). 예를 들어: cat <<EOT | rsync -avC --filter='. -' a/ b + foo.o :C - *.old EOT rsync -avC --include=foo.o -f :C --exclude='*.old' a/ b 위의 두 rsync 명령은 동일합니다. 각 명령은 모든 디렉토리별 .cvsignore 규칙을 목록의 끝이 아닌 중간에 병합합니다. 이를 통해 해당 디렉토리별 규칙이 :C 이후의 규칙을 재정의할 수 있게 되며, 모든 규칙에 종속되지 않습니다. 다른 CVS 제외 규칙(즉, 기본 제외 목록, $HOME/.cvsignore의 내용, $CVSIGNORE 값)에 영향을 미치려면 -C 명령줄 옵션을 생략하고 대신 필터 규칙에 \"-C\" 규칙을 삽입해야 합니다. 예를 들어 \"--filter=-C\"와 같이요. LIST-CLEARING FILTER RULE \"!\" 필터 규칙을 사용하여 현재 포함/제외 목록을 지울 수 있습니다 (위 FILTER RULES 섹션에서 소개됨). \"현재\" 목록은 전역 규칙 목록(필터 옵션을 파싱하는 동안 규칙이 발견된 경우) 또는 디렉토리별 규칙 집합(자체 하위 목록으로 상속되므로 하위 디렉토리에서 이를 사용하여 부모의 규칙을 지울 수 있음)입니다. ANCHORING INCLUDE/EXCLUDE PATTERNS 앞서 언급했듯이, 전역 포함/제외 패턴은 \"전송의 루트\"에 고정됩니다 (디렉토리별 패턴은 병합 파일의 디렉토리에 고정됩니다). 전송을 보내는 측에서 받는 측으로 전송되는 이름의 서브트리라고 생각한다면, 전송 루트는 트리가 대상 디렉토리에서 복제되기 시작하는 곳입니다. 이 루트는 슬래시로 시작하는 패턴이 어디에서 일치하는지 지배합니다. 일치가 전송 루트에 상대적이기 때문에, 소스 경로의 후행 슬래시를 변경하거나 --relative 옵션 사용을 변경하면 일치에 사용해야 하는 경로에 영향을 미칩니다 (파일 트리의 얼마나 많은 부분이 대상 호스트에 복제되는지 변경하는 것 외에도). 다음 예시는 이를 보여줍니다. 두 개의 소스 파일, 하나는 \"/home/me/foo/bar\"의 절대 경로를 가지고 있고 다른 하나는 \"/home/you/bar/baz\"의 경로를 가지고 있다고 가정해 봅시다. 2-소스 전송에 대한 다양한 명령 선택 사항은 다음과 같이 다릅니다: 예시 명령: rsync -a /home/me /home/you /dest +/- 패턴: /me/foo/bar +/- 패턴: /you/bar/baz 대상 파일: /dest/me/foo/bar 대상 파일: /dest/you/bar/baz 예시 명령: rsync -a /home/me/ /home/you/ /dest +/- 패턴: /foo/bar (\"me\"가 누락됨에 주의) +/- 패턴: /bar/baz (\"you\"가 누락됨에 주의) 대상 파일: /dest/foo/bar 대상 파일: /dest/bar/baz 예시 명령: rsync -a --relative /home/me/ /home/you /dest +/- 패턴: /home/me/foo/bar (전체 경로 주의) +/- 패턴: /home/you/bar/baz (동일) 대상 파일: /dest/home/me/foo/bar 대상 파일: /dest/home/you/bar/baz 예시 명령: cd /home; rsync -a --relative me/foo you/ /dest +/- 패턴: /me/foo/bar (지정된 경로에서 시작) +/- 패턴: /you/bar/baz (동일) 대상 파일: /dest/me/foo/bar 대상 파일: /dest/you/bar/baz 필터링해야 할 이름을 확인하는 가장 쉬운 방법은 --verbose를 사용하여 출력되는 이름을 보고 이름 앞에 /를 붙이는 것입니다 (--dry-run 옵션을 사용하면 아직 파일을 복사할 준비가 되지 않은 경우에 유용합니다). PER-DIRECTORY RULES AND DELETE 삭제 옵션이 없으면 디렉토리별 규칙은 보내는 측에만 관련되므로, 병합 파일 자체를 제외해도 전송에 영향을 주지 않습니다. 이를 쉽게 하기 위해 'e' 수정자가 이 제외를 자동으로 추가합니다. 다음 두 가지 동등한 명령에서 볼 수 있습니다: rsync -av --filter=': .excl' --exclude=.excl host:src/dir /dest rsync -av --filter=':e .excl' host:src/dir /dest 그러나 수신 측에서 삭제를 수행하고 일부 파일을 삭제에서 제외하고 싶다면, 수신 측이 어떤 파일을 제외해야 하는지 알아야 합니다. 가장 쉬운 방법은 디렉토리별 병합 파일을 전송에 포함하고 --delete-after를 사용하는 것입니다. 이는 삭제를 시도하기 전에 수신 측이 보내는 측과 동일한 모든 제외 규칙을 받도록 보장하기 때문입니다: rsync -avF --delete-after host:src/dir /dest 그러나 병합 파일이 전송의 일부가 아닌 경우, 전역 제외 규칙을 지정하거나 (즉, 명령줄에 지정), 수신 측에서 자체 디렉토리별 병합 파일을 유지해야 합니다. 첫 번째 예시는 다음과 같습니다 (원격 .rules 파일이 자신을 제외한다고 가정): rsync -av --filter=': .rules' --filter='. /my/extra.rules' --delete host:src/dir /dest 위 예시에서 extra.rules 파일은 전송의 양쪽에 영향을 미칠 수 있지만, (보내는 측에서는) 디렉토리별 병합 규칙 이후에 지정되었으므로 .rules 파일에서 병합된 규칙보다 하위입니다. 마지막 예시로, 원격 측은 전송에서 .rsync-filter 파일을 제외하고 있지만, 수신 측에서 무엇이 삭제될지 제어하기 위해 자체 .rsync-filter 파일을 사용하고 싶습니다. 이를 위해 디렉토리별 병합 파일을 명시적으로 제외하고 (삭제되지 않도록), 로컬 파일에 다른 무엇이 삭제되지 않아야 하는지 제어하는 규칙을 추가해야 합니다. 다음 명령 중 하나와 같이: rsync -av --filter=':e /.rsync-filter' --delete \\ host:src/dir /dest rsync -avFF --delete host:src/dir /dest TRANSFER RULES 보내는 측과 (삭제 시) 받는 측에서 파일 목록을 생성하는 재귀적 파일 스캔에 영향을 미치는 FILTER RULES 외에도 전송 규칙이 있습니다. 이러한 규칙은 제외 필터 규칙의 부작용 없이 생성기가 전송해야 한다고 결정하는 파일에 영향을 미칩니다. 전송 규칙은 파일에만 영향을 미치고 디렉토리에는 영향을 미치지 않습니다. 전송 규칙은 보내는 측 (및 받는 측)의 파일 목록에 포함되는 항목에 영향을 미치지 않으므로, 받는 측에서 삭제되는 파일에 어떤 영향도 미칠 수 없습니다. 예를 들어, \"foo\" 파일이 보내는 측 목록에 있지만 크기 때문에 전송 규칙에 의해 생략되는 경우, 받는 측은 파일을 요청하지 않습니다. 그러나 파일 목록에 파일이 존재한다는 것은 삭제 패스가 받는 측에서 \"foo\"라는 이름의 일치하는 파일을 제거하지 않을 것임을 의미합니다. 반면에, 서버 측에서 \"foo\" 파일을 제외(숨김)하면 파일이 서버의 파일 목록에서 제외되고, 받는 측 제외(보호)가 없으면 삭제가 요청될 경우 받는 측은 \"foo\"라는 이름의 일치하는 파일을 제거할 것입니다. 파일이 여전히 보내는 측의 파일 목록에 있다는 점을 감안할 때, --prune-empty-dirs 옵션은 전송 규칙이 생략한 파일만 포함하더라도 디렉토리를 비어 있다고 판단하지 않을 것입니다. 마찬가지로, 전송 규칙은 수신 측에서 삭제되는 파일에 추가적인 영향을 미치지 않으므로, 전송에 대한 최대 파일 크기를 설정한다고 해서 큰 파일이 삭제되는 것을 막지는 못합니다. 전송 규칙의 예시로는 기본 \"빠른 검사\" 알고리즘(크기 및 수정 시간 비교), --update 옵션, --max-size 옵션, --ignore-non-existing 옵션 및 기타 몇 가지가 있습니다. BATCH MODE 배치 모드는 동일한 업데이트 집합을 여러 동일한 시스템에 적용하는 데 사용할 수 있습니다. 소스 트리가 여러 호스트에 복제되어 있다고 가정해 봅시다. 이제 이 소스 트리에 변경 사항이 생겼고, 이 변경 사항을 다른 호스트에 전파해야 한다고 가정해 봅시다. 배치 모드를 사용하여 이를 수행하려면, write-batch 옵션으로 rsync를 실행하여 소스 트리에 대한 변경 사항을 대상 트리 중 하나에 적용합니다. write-batch 옵션은 rsync 클라이언트가 이 작업을 다른 동일한 대상 트리에 대해 반복하는 데 필요한 모든 정보를 \"배치 파일\"에 저장하도록 합니다. 배치 파일을 한 번 생성하면 여러 대상 트리를 업데이트할 때 파일 상태, 체크섬 및 데이터 블록 생성을 한 번 이상 수행할 필요가 없습니다. 멀티캐스트 전송 프로토콜을 사용하여 동일한 데이터를 각 호스트에 개별적으로 보내는 대신 배치 업데이트 파일을 여러 호스트에 동시에 병렬로 전송할 수 있습니다. 기록된 변경 사항을 다른 대상 트리에 적용하려면, read-batch 옵션으로 rsync를 실행하고, 동일한 배치 파일 이름과 대상 트리를 지정하십시오. Rsync는 배치 파일에 저장된 정보를 사용하여 대상 트리를 업데이트합니다. 편의를 위해, write-batch 옵션이 사용될 때 스크립트 파일도 생성됩니다. 이 스크립트 파일은 배치 파일과 동일한 이름에 \".sh\"가 추가됩니다. 이 스크립트 파일에는 연관된 배치 파일을 사용하여 대상 트리를 업데이트하는 데 적합한 명령줄이 포함되어 있습니다. Bourne (또는 Bourne과 유사한) 셸을 사용하여 실행할 수 있으며, 선택적으로 대체 대상 트리 경로 이름을 전달할 수 있습니다. 이 경우 원래 대상 경로 대신 해당 경로가 사용됩니다. 이는 현재 호스트의 대상 트리 경로가 배치 파일을 생성하는 데 사용된 경로와 다른 경우에 유용합니다. 예: $ rsync --write-batch=foo -a host:/source/dir/ /adest/dir/ $ scp foo* remote: $ ssh remote ./foo.sh /bdest/dir/ $ rsync --write-batch=foo -a /source/dir/ /adest/dir/ $ ssh remote rsync --read-batch=- -a /bdest/dir/ <foo 이 예시에서 rsync는 /source/dir/에서 /adest/dir/를 업데이트하는 데 사용되며, 이 작업을 반복하는 정보는 \"foo\"와 \"foo.sh\"에 저장됩니다. 호스트 \"remote\"는 배치된 데이터가 디렉토리 /bdest/dir로 이동하면서 업데이트됩니다. 두 예시의 차이점은 배치를 처리하는 방식에 있어 사용 가능한 유연성을 보여줍니다: o 첫 번째 예시는 초기 복사가 로컬일 필요가 없음을 보여줍니다. 원하는 대로 원격 셸 구문 또는 rsync 데몬 구문을 사용하여 원격 호스트로/로부터 데이터를 푸시하거나 가져올 수 있습니다. o 첫 번째 예시는 생성된 \"foo.sh\" 파일을 사용하여 원격 호스트에서 read-batch 명령을 실행할 때 올바른 rsync 옵션을 얻습니다. o 두 번째 예시는 표준 입력을 통해 배치 데이터를 읽으므로 배치 파일을 먼저 원격 머신으로 복사할 필요가 없습니다. 이 예시는 수정된 --read-batch 옵션을 사용해야 했기 때문에 foo.sh 스크립트를 피하지만, 원한다면 스크립트 파일을 편집할 수 있습니다 (--exclude-from=- 옵션과 같이 다른 옵션이 표준 입력을 사용하려고 하지 않는지 확인하십시오). 주의사항: read-batch 옵션은 업데이트하는 대상 트리가 배치 업데이트 파일셋을 생성하는 데 사용된 대상 트리와 동일하다고 예상합니다. 대상 트리 간의 차이가 발견되면 업데이트는 경고와 함께 폐기될 수 있거나 (파일이 이미 최신 상태인 것처럼 보이면), 파일-업데이트가 시도된 후 파일이 확인에 실패하면 오류와 함께 업데이트가 폐기될 수 있습니다. 이는 명령이 중단된 경우 read-batch 작업을 다시 실행해도 안전해야 함을 의미합니다. 파일의 크기 및 날짜와 관계없이 배치 업데이트를 항상 시도하도록 강제하려면 -I 옵션(배치를 읽을 때)을 사용하십시오. 오류가 발생하면 대상 트리는 부분적으로 업데이트된 상태가 될 것입니다. 이 경우 rsync는 일반(비배치) 작동 모드에서 대상 트리를 수정하는 데 사용할 수 있습니다. 모든 대상에서 사용되는 rsync 버전은 배치 파일을 생성하는 데 사용된 버전보다 최신이어야 합니다. 배치 파일의 프로토콜 버전이 배치-읽기 rsync가 처리하기에는 너무 새로운 경우 rsync는 오류와 함께 종료됩니다. 이전 rsync가 이해할 수 있는 배치 파일을 생성하는 방법은 --protocol 옵션도 참조하십시오. (배치 파일은 버전 2.6.3에서 형식이 변경되었으므로, 그보다 오래된 버전과 새로운 버전을 혼합하면 작동하지 않습니다.) 배치 파일을 읽을 때 rsync는 배치 파일의 데이터와 일치하도록 특정 옵션의 값을 강제합니다. 배치-쓰기 명령과 동일하게 설정하지 않은 경우에 그렇습니다. 다른 옵션은 변경할 수 있습니다 (그리고 변경해야 합니다). 예를 들어, --write-batch는 --read-batch로 변경되고, --files-from은 삭제되며, --filter / --include / --exclude 옵션은 --delete 옵션 중 하나가 지정되지 않는 한 필요하지 않습니다. BATCH.sh 파일을 생성하는 코드는 모든 필터/포함/제외 옵션을 셸 스크립트 파일에 \"여기 문서\"로 추가되는 단일 목록으로 변환합니다. 고급 사용자는 이를 사용하여 --delete에 의해 삭제되는 내용을 변경하고 싶을 때 제외 목록을 수정할 수 있습니다. 일반 사용자는 이 세부 사항을 무시하고 셸 스크립트를 배치된 데이터에 대한 적절한 --read-batch 명령을 실행하는 쉬운 방법으로 사용할 수 있습니다. SYMBOLIC LINKS rsync가 소스 디렉토리에서 심볼릭 링크를 발견할 때 세 가지 기본적인 동작이 가능합니다. 기본적으로 심볼릭 링크는 전혀 전송되지 않습니다. 존재하는 모든 심볼릭 링크에 대해 \"비정규 파일 건너뛰기\" 메시지가 발생합니다. --links가 지정되면 심볼릭 링크가 전송에 추가되고 (시끄럽게 무시하는 대신), 기본 처리는 대상에서 동일한 대상을 가리키도록 다시 생성하는 것입니다. --archive는 --links를 암시합니다. --copy-links가 지정되면 심볼릭 링크는 심볼릭 링크 자체가 아닌 참조 대상을 복사하여 \"축소\"됩니다. Rsync는 또한 \"안전한\" 심볼릭 링크와 \"안전하지 않은\" 심볼릭 링크를 구별할 수 있습니다. 이것이 사용될 수 있는 예시는 복사되는 rsync 모듈이 사이트의 공개 섹션에 /etc/passwd에 대한 심볼릭 링크를 포함하지 않도록 하려는 웹 사이트 미러입니다. --copy-unsafe-links를 사용하면 모든 링크가 대상에서 가리키는 파일로 복사됩니다. --safe-links를 사용하면 안전하지 않은 링크가 수신자에 의해 생략됩니다. ( --safe-links가 효과를 가지려면 --links를 지정하거나 암시해야 한다는 점에 유의하십시오.) 심볼릭 링크는 절대 심볼릭 링크( '/'로 시작), 비어 있거나, 전송의 최상단에서 벗어날 만큼 충분한 \"..\" 구성 요소를 포함하는 경우 안전하지 않은 것으로 간주됩니다. 다음은 심볼릭 링크 옵션이 어떻게 해석되는지에 대한 요약입니다. 목록은 우선순위 순서이므로, 옵션 조합이 언급되지 않았다면, 옵션의 완전한 하위 집합인 첫 번째 줄을 사용하십시오: --copy-links 모든 심볼릭 링크를 일반 파일 및 디렉토리로 전환합니다 (다른 옵션이 영향을 미칠 심볼릭 링크를 전송에 남기지 않습니다). --copy-dirlinks 디렉토리에 대한 심볼릭 링크만 실제 디렉토리로 전환하고, 다른 모든 심볼릭 링크는 아래 설명된 대로 처리되도록 둡니다. --links --copy-unsafe-links 모든 안전하지 않은 심볼릭 링크를 파일로 전환하고, 모든 안전한 심볼릭 링크를 생성합니다. --copy-unsafe-links 모든 안전하지 않은 심볼릭 링크를 파일로 전환하고, 모든 안전한 심볼릭 링크는 시끄럽게 건너뜁니다. --links --safe-links 수신자는 전송에서 발견된 안전하지 않은 심볼릭 링크 생성을 건너뛰고 안전한 심볼릭 링크를 생성합니다. --links 모든 심볼릭 링크를 생성합니다. --munge-links의 효과에 대해서는 해당 옵션 섹션의 설명을 참조하십시오. --keep-dirlinks 옵션은 전송의 심볼릭 링크에는 영향을 미치지 않고, 대신 rsync가 수신 측에 이미 존재하는 디렉토리에 대한 심볼릭 링크를 처리하는 방식에 영향을 미친다는 점에 유의하십시오. 해당 옵션 섹션의 경고를 참조하십시오. DIAGNOSTICS Rsync는 때때로 다소 이해하기 어려운 오류 메시지를 생성합니다. 가장 혼란을 야기하는 메시지는 \"protocol version mismatch -- is your shell clean?\"입니다. 이 메시지는 일반적으로 시작 스크립트 또는 원격 셸 기능이 rsync가 전송에 사용하는 스트림에 원치 않는 쓰레기 데이터를 생성할 때 발생합니다. 이 문제를 진단하는 방법은 다음과 같이 원격 셸을 실행하는 것입니다: ssh remotehost /bin/true > out.dat 그런 다음 out.dat 파일을 확인하십시오. 모든 것이 올바르게 작동하면 out.dat는 0 길이 파일이어야 합니다. rsync에서 위 오류가 발생하면 out.dat에 일부 텍스트 또는 데이터가 포함되어 있을 것입니다. 내용을 살펴보고 무엇이 이를 생성하는지 파악해 보십시오. 가장 흔한 원인은 비대화형 로그인에 대한 출력 문을 포함하는 잘못 구성된 셸 시작 스크립트(.cshrc 또는 .profile과 같은)입니다. 필터 패턴을 디버깅하는 데 문제가 있다면 -vv 옵션을 지정해 보십시오. 이 상세도 수준에서는 rsync가 각 개별 파일이 포함되거나 제외되는 이유를 보여줍니다. EXIT VALUES o 0 - 성공 o 1 - 구문 또는 사용 오류 o 2 - 프로토콜 비호환성 o 3 - 입력/출력 파일, 디렉토리 선택 오류 o o 4 - 요청된 작업이 지원되지 않습니다. 다음 중 하나일 수 있습니다: 64비트 파일을 지원할 수 없는 플랫폼에서 64비트 파일을 조작하려는 시도 o 클라이언트에서 지원하지만 서버에서는 지원하지 않는 옵션이 지정됨 o 5 - 클라이언트-서버 프로토콜 시작 오류 o 6 - 데몬이 로그 파일에 추가할 수 없음 o 10 - 소켓 I/O 오류 o 11 - 파일 I/O 오류 o 12 - rsync 프로토콜 데이터 스트림 오류 o 13 - 프로그램 진단 오류 o 14 - IPC 코드 오류 o 20 - SIGUSR1 또는 SIGINT 수신 o 21 - waitpid()에서 반환된 일부 오류 o 22 - 코어 메모리 버퍼 할당 오류 o 23 - 오류로 인한 부분 전송 o 24 - 사라진 소스 파일로 인한 부분 전송 o 25 - --max-delete 제한으로 삭제 중지 o 30 - 데이터 송수신 타임아웃 o 35 - 데몬 연결 대기 타임아웃 ENVIRONMENT VARIABLES CVSIGNORE CVSIGNORE 환경 변수는 .cvsignore 파일의 무시 패턴을 보완합니다. 자세한 내용은 --cvs-exclude 옵션을 참조하십시오. RSYNC_ICONV 이 환경 변수를 사용하여 기본 --iconv 설정을 지정합니다. 3.0.0부터 처음 지원됩니다. RSYNC_OLD_ARGS --old-args 옵션을 기본적으로 활성화하려면 \"1\"을 지정하고, 반복 옵션 상태로 활성화하려면 \"2\" (또는 그 이상)를 지정하거나, 기본적으로 비활성화되도록 하려면 \"0\"을 지정하십시오. 이 환경 변수가 0이 아닌 값으로 설정되면 RSYNC_PROTECT_ARGS 변수를 재정의합니다. 이 변수는 --old-args, --no-old-args 또는 --secluded-args가 명령줄에 지정된 경우 무시됩니다. 3.2.4부터 처음 지원됩니다. RSYNC_PROTECT_ARGS --secluded-args 옵션을 기본적으로 활성화하려면 0이 아닌 숫자 값을 지정하고, 기본적으로 비활성화되도록 하려면 0 값을 지정하십시오. 이 변수는 --secluded-args, --no-secluded-args 또는 --old-args가 명령줄에 지정된 경우 무시됩니다. 3.1.0부터 처음 지원됩니다. 3.2.4부터 이 변수는 RSYNC_OLD_ARGS가 0이 아닌 값으로 설정되면 무시됩니다. RSYNC_RSH 이 환경 변수를 사용하면 rsync의 전송으로 사용되는 기본 셸을 재정의할 수 있습니다. --rsh (-e) 옵션과 마찬가지로 명령 이름 뒤에 명령줄 옵션이 허용됩니다. RSYNC_PROXY 이 환경 변수를 사용하면 rsync 데몬에 연결할 때 rsync 클라이언트가 웹 프록시를 사용하도록 리디렉션할 수 있습니다. RSYNC_PROXY를 hostname:port 쌍으로 설정해야 합니다. RSYNC_PASSWORD 이 환경 변수를 사용하면 rsync 데몬 연결에 대한 암호를 설정하여 암호 프롬프트를 피할 수 있습니다. 이는 ssh와 같은 원격 셸 전송에 암호를 제공하지 않는다는 점에 유의하십시오 (이를 수행하는 방법은 해당 설명서를 참조하십시오). USER or LOGNAME USER 또는 LOGNAME 환경 변수는 rsync 데몬으로 전송되는 기본 사용자 이름을 결정하는 데 사용됩니다. 둘 다 설정되지 않으면 사용자 이름은 \"nobody\"로 기본값이 지정됩니다. 둘 다 설정된 경우 USER가 우선합니다. RSYNC_PARTIAL_DIR 이 환경 변수는 부분 전송이 활성화되도록 암시하지 않고 --partial 전송에 사용할 디렉토리를 지정합니다. 자세한 내용은 --partial-dir 옵션을 참조하십시오. RSYNC_COMPRESS_LIST 이 환경 변수를 사용하면 대체 순서 또는 축소된 이름 목록을 지정하여 압축 알고리즘 협상을 사용자 정의할 수 있습니다. 사용 가능한 압축 이름을 확인하려면 rsync --version 명령을 사용하십시오. 자세한 내용은 --compress 옵션을 참조하십시오. RSYNC_CHECKSUM_LIST 이 환경 변수를 사용하면 대체 순서 또는 축소된 이름 목록을 지정하여 체크섬 알고리즘 협상을 사용자 정의할 수 있습니다. 사용 가능한 체크섬 이름을 확인하려면 rsync --version 명령을 사용하십시오. 자세한 내용은 --checksum-choice 옵션을 참조하십시오. RSYNC_MAX_ALLOC 이 환경 변수는 --max-alloc 옵션을 사용한 것처럼 할당 최대값을 설정합니다. RSYNC_PORT 이 환경 변수는 rsync에 의해 읽히지 않지만, rsync가 원격 셸을 데몬 연결과 함께 실행할 때 하위 환경에 설정됩니다. 이를 통해 rsync-ssl과 같은 스크립트가 명령줄에 사용자가 지정한 포트 번호를 알 수 있습니다. HOME 이 환경 변수는 사용자의 기본 .cvsignore 파일을 찾는 데 사용됩니다. RSYNC_CONNECT_PROG 이 환경 변수는 주로 디버그 설정에서 데몬 연결을 할 때 사용할 프로그램을 설정하는 데 사용됩니다. 자세한 내용은 CONNECTING TO AN RSYNC DAEMON을 참조하십시오. RSYNC_SHELL 이 환경 변수는 주로 디버그 설정에서 RSYNC_CONNECT_PROG에 의해 지정된 프로그램을 실행하는 데 사용할 프로그램을 설정하는 데 사용됩니다. 자세한 내용은 CONNECTING TO AN RSYNC DAEMON을 참조하십시오. FILES /etc/rsyncd.conf 또는 rsyncd.conf SEE ALSO rsync-ssl(1), rsyncd.conf(5), rrsync(1) BUGS o 시간은 *nix time_t 값으로 전송됩니다. o FAT 파일 시스템으로 전송할 때 rsync가 수정되지 않은 파일을 다시 동기화할 수 있습니다. --modify-window 옵션에 대한 설명을 참조하십시오. o 파일 권한, 장치 등은 원시 숫자 값으로 전송됩니다. o --delete 옵션에 대한 설명도 참조하십시오. 버그를 보고해 주십시오! 웹 사이트 https://rsync.samba.org/를 참조하십시오. VERSION 이 맨페이지는 rsync 버전 3.2.7에 대한 것입니다. INTERNAL OPTIONS --server 및 --sender 옵션은 rsync 내부적으로 사용되며, 정상적인 상황에서는 사용자가 직접 입력해서는 안 됩니다. 이러한 옵션에 대한 인식이 특정 시나리오, 예를 들어 rsync 명령만 실행할 수 있는 로그인을 설정할 때 필요할 수 있습니다. 예를 들어, rsync 배포판의 support 디렉토리에는 rrsync (제한된 rsync)라는 예시 스크립트가 있으며, 제한된 ssh 로그인과 함께 사용할 수 있습니다. CREDITS Rsync는 GNU General Public License에 따라 배포됩니다. 자세한 내용은 COPYING 파일을 참조하십시오. rsync 웹 사이트는 https://rsync.samba.org/에서 사용할 수 있습니다. 이 사이트에는 이 설명서 페이지에서 다루지 않은 질문에 답할 수 있는 FAQ-O-Matic이 포함되어 있습니다. rsync github 프로젝트는 https://github.com/WayneD/rsync입니다. 이 프로그램을 좋아하신다면 기꺼이 여러분의 의견을 듣겠습니다. rsync@lists.samba.org 메일링 리스트로 연락주십시오. 이 프로그램은 Jean-loup Gailly와 Mark Adler가 작성한 훌륭한 zlib 압축 라이브러리를 사용합니다. THANKS 특별히 John Van Essen, Matt McCutchen, Wesley W. Terpstra, David Dykstra, Jos Backus, Sebastian Krahmer, Martin Pool, 그리고 우리를 떠났지만 잊혀지지 않을 동지 J.W. Schultz에게 감사드립니다. Richard Brent, Brendan Mackay, Bill Waite, Stephen Rothwell, David Bell에게도 감사드립니다. 몇몇 분들을 놓쳤을 수도 있습니다. 죄송합니다. AUTHOR Rsync는 원래 Andrew Tridgell과 Paul Mackerras가 작성했습니다. 많은 사람들이 나중에 기여했습니다. 현재 Wayne Davison이 유지 관리하고 있습니다. 지원 및 개발을 위한 메일링 리스트는 https://lists.samba.org/에서 사용할 수 있습니다. rsync 3.2.7 2022년 10월 20일 rsync(1)",
      "frontmatter": {
        "date": "2025-09-03T15:49:56+09:00",
        "lastmod": "2025-10-16T10:03:19+09:00"
      }
    },
    "무제 8": {
      "path": "/무제-8/",
      "filename": "무제 8",
      "content": "",
      "frontmatter": {
        "date": "2025-10-06T23:28:32+09:00",
        "lastmod": "2025-10-06T23:28:40+09:00"
      }
    },
    "무제 9": {
      "path": "/무제-9/",
      "filename": "무제 9",
      "content": "generatefilestructure.py 로 파일 구조를 만들었어 클라이언트 사이드 렌더링 방식으로 github page 에 정적파일을 호스팅 하고 싶어 md 들이 잘 보이도록 index.html style.css script.js generatefilestructure.py file_structure.json (파이썬으로 만들어짐) 이렇게 4개의 파일을 만들어줘 디자인 : tailwind 를 사용해 디자인은 너가 알아서 정해 멋진데 가독성이 좋도록 단 좌측은 마크다운 렌더링 : 라이브러리는 자율롭게 사용하되 현재 모든 마크다운은 상단에 frontmatter 가 위치하고 있어 기능 : 검색기능 sprin",
      "frontmatter": {
        "tags": [
          "auto"
        ],
        "date": "2025-09-04T14:42:47+09:00",
        "lastmod": "2025-10-16T10:03:19+09:00"
      }
    },
    "무제": {
      "path": "/06.university/network/무제/",
      "filename": "무제",
      "content": "알겠습니다. 컴퓨터공학 전공자로서 기술적 내용에 중점을 둔 5G 및 6G 네트워크 통신 기술 보고서를 작성해 드리겠습니다. 먼저 요청하신 대로 목차를 다시 제시하고, 이어서 각 항목에 대한 기술 중심의 설명을 제공하겠습니다. 목차 서론 차세대 이동통신 기술의 등장 배경 및 기술적 요구사항 연구의 목적 및 기술적 범위 정의 5G 네트워크 통신 기술 분석 5G의 핵심 성능 지표(KPI) 및 표준화 동향 (3GPP 릴리즈 기반) 5G 핵심 기술 요소 상세 분석 새로운 무선 접속 기술 (NR: New Radio) 주파수 대역 확장 (Sub-6GHz, mmWave) 및 특성 Massive MIMO (Multiple-Input Multiple-Output) 및 빔포밍 유연한 프레임 구조 및 OFDM 변형 기술 (예: CP-OFDM, DFT-s-OFDM) 고급 채널 코딩 (LDPC, Polar Codes) 5G 코어 네트워크 (5GC) 아키텍처 서비스 기반 아키텍처 (SBA: Service-Based Architecture) 네트워크 슬라이싱 (Network Slicing) 모바일 엣지 컴퓨팅 (MEC: Multi-access Edge Computing) NFV (Network Functions Virtualization) 및 SDN (Software-Defined Networking) 적용 5G 주요 서비스 시나리오별 기술적 요구사항 eMBB (Enhanced Mobile Broadband): 고대역폭, 고속 데이터 전송 URLLC (Ultra-Reliable Low Latency Communications): 고신뢰성, 저지연 통신 mMTC (Massive Machine Type Communications): 대규모 디바이스 연결 5G 상용화 현황 및 기술적 과제 (예: 커버리지, 장비 호환성, 간섭 문제) 6G 네트워크 통신 기술 전망 6G 비전 및 주요 성능 목표 (5G 대비 향상점 중심) 6G 예상 핵심 기술 후보군 및 연구 동향 테라헤르츠(THz) 및 서브-THz 대역 통신 기술 AI/ML 기반 네트워크 지능화 및 자동화 (예: AI 기반 RAN, 코어망 운영) 공간 통신 기술 (예: 위성 통합, 무인항공기(UAV) 활용, Reconfigurable Intelligent Surfaces - RIS) 네트워크-컴퓨팅-센싱 융합 (예: ISAC - Integrated Sensing and Communication) 차세대 코어 네트워크 및 아키텍처 (예: 분산형 AI, 양자 통신 연동 가능성) 고정밀 측위 및 동기화 기술 6G 기술의 표준화 로드맵 및 기술적 난제 5G 및 6G 기술의 기술적 파급 효과 및 영향 분석 긍정적 영향 (기술적 구현 가능성 및 새로운 서비스 창출 중심) 실생활 변화: 기술적 진보가 가져올 구체적 서비스 변화 완전 몰입형 XR (확장현실) 서비스: 초고대역폭, 초저지연 통신 기반 홀로그래픽 통신: 대용량 데이터 실시간 전송 및 처리 디지털 트윈 및 사이버 물리 시스템(CPS) 고도화: 정밀 센싱, 실시간 제어 브레인-컴퓨터 인터페이스(BCI) 연동 가능성 (장기적 관점) 사회 인프라 변화: 지능형 인프라 구축 및 운영 효율화 자율주행 네트워크 및 지능형 교통 시스템 (ITS)의 완전 자율화 원격 로봇 수술 및 정밀 의료 서비스 확대 스마트 시티 운영 시스템의 실시간 최적화 경제/산업 구조 변화: 기술 기반 신산업 및 생산 방식 혁신 산업용 IoT(IIoT) 및 스마트 팩토리의 초지능화 UAM(도심항공교통) 등 신규 모빌리티 산업 지원 인프라 데이터 중심 경제 가속화 및 AI 기반 서비스 확산 부정적 영향 및 기술적 해결 과제 (보안, 프라이버시, 에너지 효율 등) 기술적 문제점 및 우려 보안 취약성 증대: 연결성 확장 및 복잡도 증가로 인한 공격 표면 확대 프라이버시 침해 가능성: 방대한 데이터 수집 및 AI 분석에 따른 위험 네트워크 및 단말기의 에너지 소비 증가 문제: 고성능화에 따른 전력 효율성 확보 과제 전파 인체 유해성 논란 (특히 고주파 대역) 및 기술적 검증 필요성 기술 구현의 복잡성 및 상호 운용성 확보 문제 사회·경제적 문제점 (기술적 관점에서 파생되는 문제) 기술 격차 (Digital Divide) 심화: 고도화된 기술 접근성 불평등 고용 구조 변화 가속화: 자동화 및 AI 대체에 따른 일자리 문제 인프라 투자 비용 및 유지보수 부담 증가 결론: 기술적 과제와 미래 전망 (본인 의견 포함) 5G 및 6G 기술의 핵심적 의의 및 기술 발전 방향 요약 기술적 난제 극복 및 지속 가능한 발전을 위한 컴퓨터공학도의 역할과 제언 보안 및 프라이버시 강화 기술 개발의 중요성 (예: 차세대 암호 기술, 연합 학습 등) 에너지 효율적인 네트워크 및 단말 기술 연구 개방형 표준 및 오픈소스 생태계 활성화를 통한 기술 혁신 가속화 AI 기술의 책임감 있는 활용 및 통제 방안 연구 (네트워크 오작동 방지 등) 참고문헌 이제 각 목차 항목에 대한 내용을 기술 중심으로 작성해 드리겠습니다. 서론 차세대 이동통신 기술의 등장 배경 및 기술적 요구사항 이동통신 기술은 지난 수십 년간 비약적인 발전을 거듭하며 우리 생활과 산업 전반에 혁명적인 변화를 가져왔습니다. 1세대(1G) 아날로그 음성 통화에서 시작하여, 2세대(2G) 디지털 음성 및 문자 메시지, 3세대(3G) 모바일 인터넷 접속, 4세대(4G) LTE를 통한 모바일 광대역 서비스로 진화해왔습니다. 특히 4G LTE는 스마트폰의 대중화를 이끌며 다양한 모바일 애플리케이션과 서비스의 기반이 되었습니다. 그러나 모바일 데이터 트래픽의 폭발적인 증가, 사물인터넷(IoT) 기기의 확산, 증강현실(AR)·가상현실(VR)과 같은 실감형 미디어 서비스의 등장, 자율주행차, 스마트 팩토리 등 새로운 산업 분야의 출현은 기존 4G 기술의 한계를 드러내기 시작했습니다. 이러한 배경 속에서 더 높은 전송 속도(throughput), 더 짧은 지연 시간(latency), 그리고 더 많은 기기를 동시에 수용할 수 있는 연결성(connectivity) 이라는 명확한 기술적 요구사항과 함께 5G 기술이 등장하게 되었습니다. 5G는 단순히 4G보다 빠른 기술을 넘어, 다양한 산업 분야와의 융합을 통해 새로운 가치를 창출하고 디지털 전환(Digital Transformation)을 가속화하는 핵심 인프라로 주목받고 있습니다. 나아가 현재 활발히 연구가 진행 중인 6G는 5G의 성능을 극한으로 끌어올리는 동시에, 인공지능(AI)과의 완전한 통합, 통신과 센싱의 융합, 공간 제약 없는 통신 등을 목표로 하여 더욱 혁신적인 미래 사회의 청사진을 제시하고 있습니다. 연구의 목적 및 기술적 범위 정의 본 보고서의 목적은 현재 상용화되어 우리 생활 깊숙이 들어오고 있는 5G 네트워크 통신 기술의 핵심 요소와 아키텍처를 기술적으로 심층 분석하고, 차세대 기술로 연구개발이 진행 중인 6G의 비전과 예상 핵심 기술들을 조망하는 데 있습니다. 이를 위해 5G의 경우, 3GPP 표준화 문서를 기반으로 한 핵심 성능 지표, 새로운 무선 접속 기술(NR), 5G 코어 네트워크(5GC) 아키텍처 및 주요 서비스 시나리오별 기술적 요구사항을 상세히 다룰 것입니다. 특히 mmWave 주파수 활용, Massive MIMO, 네트워크 슬라이싱, 모바일 엣지 컴퓨팅 등 5G를 특징짓는 핵심 기술들의 원리와 구현 방식을 중심으로 설명합니다. 6G에 대해서는 현재 논의되고 있는 비전과 성능 목표를 제시하고, 테라헤르츠 대역 통신, AI/ML 기반 네트워크 지능화, 공간 통신 기술, 네트워크-컴퓨팅-센싱 융합 등 유력한 후보 기술들의 개념과 연구 동향, 그리고 예상되는 기술적 난제들을 살펴볼 것입니다. 궁극적으로 이러한 기술 분석을 바탕으로 5G 및 6G 기술이 가져올 실생활, 사회, 경제 전반의 긍정적·부정적 영향을 기술적 관점에서 구체적인 예를 들어 예측하고, 컴퓨터공학도로서 기술적 과제를 해결하고 바람직한 미래를 만들어가기 위한 방안을 제시하고자 합니다. 본 보고서는 인문학적 접근보다는 기술의 원리, 구현 방식, 성능 지표, 그리고 기술적 파급 효과에 초점을 맞추어 전개될 것입니다. 5G 네트워크 통신 기술 분석 5G의 핵심 성능 지표(KPI) 및 표준화 동향 (3GPP 릴리즈 기반) 5G 이동통신 기술은 국제전기통신연합(ITU)에서 정의한 IMT-2020 표준을 기반으로 하며, 주요 기술 규격은 이동통신 표준화 기술협력기구인 3GPP(3rd Generation Partnership Project)를 통해 개발되고 있습니다. 5G의 주요 핵심 성능 지표(KPI)는 다음과 같습니다. 최대 전송 속도 (Peak Data Rate): 다운링크 20 Gbps, 업링크 10 Gbps 사용자 체감 전송 속도 (User Experienced Data Rate): 다운링크 100 Mbps, 업링크 50 Mbps (도심 밀집 환경 기준) 주파수 효율 (Spectrum Efficiency): LTE 대비 3배 향상 (eMBB 기준) 이동성 (Mobility): 최대 500 km/h 속도에서도 통신 지원 지연 시간 (Latency): 무선 구간 1ms (URLLC 기준), 종단간(E2E) 1ms 목표 연결 밀도 (Connection Density): 1 km² 당 100만 개 기기 연결 지원 (mMTC 기준) 에너지 효율 (Network Energy Efficiency): LTE 대비 100배 향상 면적 당 트래픽 용량 (Area Traffic Capacity): 1 m² 당 10 Mbps (eMBB 실내 핫스팟 기준) 3GPP는 5G 표준을 여러 릴리즈(Release)를 통해 순차적으로 개발해왔습니다. 릴리즈 15 (Rel-15): 2018년 완료. 5G NR의 초기 표준(Phase 1)으로, Non-Standalone (NSA) 운영 (LTE 코어망 활용) 및 Standalone (SA) 운영 (5G 코어망 활용)을 모두 지원합니다. eMBB 서비스에 중점을 두었습니다. 릴리즈 16 (Rel-16): 2020년 완료. 5G NR Phase 2 표준으로, URLLC 기능 강화, 산업용 IoT(IIoT) 지원, V2X(Vehicle-to-Everything) 통신, 비면허 대역 기반 5G NR(NR-U) 등을 포함합니다. 릴리즈 17 (Rel-17): 2022년 완료. 위성 통신 연동(NTN: Non-Terrestrial Networks), NR-Light(RedCap: Reduced Capability)을 통한 중급 IoT 지원, 사이드링크 기능 강화, XR(확장현실) 지원 향상 등을 포함합니다. 릴리즈 18 (Rel-18): 현재 진행 중. \"5G-Advanced\"의 시작으로, AI/ML 기술의 네트워크 적용, 센싱 기능 통합, XR 기능 심화 등을 목표로 합니다. 5G 핵심 기술 요소 상세 분석 새로운 무선 접속 기술 (NR: New Radio) 5G NR은 다양한 서비스 요구사항을 만족시키기 위해 유연하고 확장 가능한 무선 인터페이스를 제공합니다. 주파수 대역 확장 (Sub-6GHz, mmWave) 및 특성: Sub-6GHz (FR1: Frequency Range 1): 410MHz ~ 7.125GHz 대역. 기존 LTE 대역을 포함하며, 넓은 커버리지 확보에 유리하지만, 가용 대역폭이 제한적입니다. 주로 전국망 커버리지 및 기본적인 5G 서비스 제공에 활용됩니다. 밀리미터파 (mmWave, FR2: Frequency Range 2): 24.25GHz ~ 52.6GHz 대역. 매우 넓은 대역폭(수백 MHz ~ 수 GHz)을 활용하여 초고속 데이터 전송이 가능하지만, 직진성이 강하고 장애물 투과율이 낮아 커버리지가 좁고 구축 비용이 높습니다. 핫스팟, 인빌딩, 특정 산업 현장 등에서 주로 활용됩니다. mmWave는 대기 중 산소나 수증기에 의한 흡수 손실, 경로 손실이 크다는 기술적 과제가 있습니다. Massive MIMO (Multiple-Input Multiple-Output) 및 빔포밍 (Beamforming): Massive MIMO: 기지국에 수십에서 수백 개의 안테나 소자를 집적하여 공간 다중화(Spatial Multiplexing) 이득을 극대화하고, 여러 사용자에게 동시에 데이터를 전송하거나 특정 사용자에게 집중적으로 전송하는 기술입니다. 이를 통해 주파수 효율과 셀 용량을 크게 향상시킬 수 있습니다. 안테나 수가 증가함에 따라 채널 추정의 복잡성 증가, RF 체인 비용 증가 등의 기술적 고려사항이 있습니다. 빔포밍: 다수의 안테나에서 방사되는 신호의 위상과 진폭을 조절하여 특정 방향으로 전파 에너지를 집중시키는 기술입니다. 이를 통해 특정 사용자에게 신호 강도를 높이고, 다른 사용자에 대한 간섭을 줄일 수 있습니다. mmWave 대역의 높은 경로 손실을 극복하고 커버리지를 확보하는 데 필수적인 기술입니다. 아날로그 빔포밍, 디지털 빔포밍, 하이브리드 빔포밍 방식이 있으며, 복잡도와 성능 간의 트레이드오프가 존재합니다. 유연한 프레임 구조 및 OFDM 변형 기술 (예: CP-OFDM, DFT-s-OFDM): 5G NR은 다양한 서비스 시나리오(eMBB, URLLC, mMTC)와 주파수 대역(Sub-6GHz, mmWave)에 유연하게 대응하기 위해 가변적인 부반송파 간격(SCS: Subcarrier Spacing), 슬롯 길이, 미니-슬롯(mini-slot) 등을 지원하는 유연한 프레임 구조를 채택했습니다. SCS는 15kHz, 30kHz, 60kHz, 120kHz, 240kHz 등 다양하게 설정 가능하며, 이는 지연 시간에 민감한 서비스(URLLC)나 고주파 대역(mmWave) 운영에 적합합니다. CP-OFDM (Cyclic Prefix Orthogonal Frequency Division Multiplexing): 대부분의 5G 다운링크 및 업링크 전송에 사용되는 기본 파형입니다. 다중 경로 페이딩에 강하고 주파수 효율이 높습니다. DFT-s-OFDM (Discrete Fourier Transform spread OFDM): 업링크 전송에서 단말기의 PAPR(Peak-to-Average Power Ratio)을 낮추기 위해 선택적으로 사용됩니다. 이는 단말기의 전력 증폭기 효율을 높이고 배터리 소모를 줄이는 데 기여합니다. 고급 채널 코딩 (LDPC, Polar Codes): 데이터 전송 시 발생하는 오류를 정정하기 위해 강력한 채널 코딩 기술을 사용합니다. LDPC (Low-Density Parity-Check) 부호: 데이터 채널(eMBB)에 주로 사용되며, 높은 코딩 이득과 병렬 처리 용이성으로 고속 데이터 전송에 적합합니다. Polar 부호: 제어 채널(URLLC의 일부 제어 정보)에 주로 사용되며, 상대적으로 짧은 블록 길이에서도 우수한 성능을 보입니다. 5G 코어 네트워크 (5GC) 아키텍처 5GC는 다양한 서비스 요구사항을 효율적으로 지원하기 위해 클라우드 네이티브 기술을 기반으로 설계되었습니다. 서비스 기반 아키텍처 (SBA: Service-Based Architecture): 기존 4G EPC(Evolved Packet Core)의 점대점(Point-to-Point) 인터페이스 방식 대신, 각 네트워크 기능(NF: Network Function)들이 잘 정의된 API를 통해 서로 서비스를 제공하고 소비하는 방식으로 구성됩니다. 이는 마이크로서비스 아키텍처(MSA)와 유사하며, 네트워크 기능의 모듈화, 유연성, 확장성을 높여줍니다. 주요 NF로는 AMF(Access and Mobility Management Function), SMF(Session Management Function), UPF(User Plane Function), AUSF(Authentication Server Function1), NRF(NF Repository Function) 등이 있습니다. 네트워크 슬라이싱 (Network Slicing): 하나의 물리적인 네트워크 인프라를 다수의 독립적인 가상 네트워크(네트워크 슬라이스)로 분할하여 각 슬라이스마다 특정 서비스(eMBB, URLLC, mMTC 등)에 최적화된 특성을 제공하는 기술입니다. 각 슬라이스는 독립적인 네트워크 자원(RAN, 코어, 전송망) 할당, 토폴로지, 기능 등을 가질 수 있으며, 서비스 맞춤형 네트워크를 온디맨드(on-demand) 방식으로 제공할 수 있게 합니다. 이를 통해 통신 사업자는 다양한 버티컬 산업의 요구사항을 효과적으로 만족시킬 수 있습니다. 슬라이스 간 격리(isolation) 보장 및 효율적인 자원 관리가 기술적 핵심입니다. 모바일 엣지 컴퓨팅 (MEC: Multi-access Edge Computing): 데이터 처리 및 서비스 기능을 사용자 단말과 가까운 네트워크 엣지(edge)에 분산 배치하여, 데이터 전송 지연을 최소화하고 네트워크 부하를 줄이는 기술입니다. URLLC 서비스(예: 자율주행, 스마트 팩토리)나 대용량 콘텐츠 전송(예: AR/VR)에 필수적입니다. MEC 플랫폼은 애플리케이션 개발자에게 개방되어 새로운 엣지 기반 서비스를 창출할 수 있는 기반을 제공합니다. NFV (Network Functions Virtualization) 및 SDN (Software-Defined Networking) 적용: NFV: 라우터, 방화벽, 로드밸런서 등 기존 하드웨어 기반의 네트워크 장비 기능을 소프트웨어 형태로 가상화하여 범용 하드웨어(서버, 스토리지, 스위치)에서 실행하는 기술입니다. 이를 통해 장비 도입 비용 절감, 서비스 배포 시간 단축, 운영 유연성 향상이 가능합니다. 5GC의 NF들은 NFV 기반으로 구현됩니다. SDN: 네트워크 제어부(Control Plane)와 데이터 전달부(Data Plane 또는 User Plane)를 분리하고, 제어부를 중앙 집중화하여 네트워크 전체를 소프트웨어로 프로그래밍하고 관리하는 기술입니다. 네트워크 자원의 효율적인 할당, 트래픽 최적화, 신규 서비스 도입 용이성 등을 제공합니다. UPF와 SMF 간의 제어 인터페이스 등에서 SDN 원리가 활용됩니다. 5G 주요 서비스 시나리오별 기술적 요구사항 eMBB (Enhanced Mobile Broadband): 고대역폭, 고속 데이터 전송 기술적 요구사항: 매우 높은 데이터 전송률, 넓은 대역폭, 높은 스펙트럼 효율. 주요 기술: mmWave, Massive MIMO, 캐리어 어그리게이션(CA), LDPC 코딩. 활용 예: 초고화질 동영상 스트리밍, AR/VR, 고속 모바일 인터넷. URLLC (Ultra-Reliable Low Latency Communications): 고신뢰성, 저지연 통신 기술적 요구사항: 극도로 낮은 지연 시간 (1ms 이내), 매우 높은 신뢰도 (99.999% 이상). 주요 기술: 미니-슬롯, Grant-free 전송, 중복 전송(Duplication), MEC, 네트워크 슬라이싱, Polar 코딩. 활용 예: 원격 수술, 자율주행차 제어, 스마트 팩토리 로봇 제어, 공공 안전 통신. mMTC (Massive Machine Type Communications): 대규모 디바이스 연결 기술적 요구사항: 매우 높은 연결 밀도, 저전력 소모, 저비용 단말기. 주요 기술: NB-IoT(Narrowband-IoT), LTE-M(LTE for Machine-Type Communication)의 진화, Power Saving Mode (PSM), Extended Discontinuous Reception (eDRX), 단순화된 프로토콜 스택. 활용 예: 스마트 미터링, 스마트 시티 센서, 환경 모니터링, 스마트 농업. 5G 상용화 현황 및 기술적 과제 5G는 전 세계적으로 빠르게 상용화되고 있으며, NSA 방식에서 SA 방식으로 전환이 이루어지고 있습니다. 주요 도시를 중심으로 mmWave 망 구축도 진행 중이지만, 다음과 같은 기술적 과제들이 존재합니다. 커버리지 확보: 특히 mmWave 대역은 회절성이 낮고 장애물 투과 손실이 커서 넓은 지역에 균일한 커버리지를 제공하기 어렵습니다. 이를 해결하기 위해 더 많은 기지국(스몰셀) 설치, 중계기 활용, RIS(Reconfigurable Intelligent Surfaces)와 같은 신기술 연구가 필요합니다. 장비 호환성 및 비용: 다양한 벤더의 장비 간 상호 운용성 확보는 여전히 중요한 문제이며, mmWave 장비 및 단말기의 가격이 높아 보급 확대에 걸림돌이 될 수 있습니다. 간섭 문제: 다양한 주파수 대역 사용, 특히 비면허 대역 활용(NR-U) 시 기존 시스템과의 간섭 또는 5G 시스템 내 간섭 관리가 중요합니다. 에너지 효율성: 기지국 및 단말기의 에너지 소모를 줄이는 것은 네트워크 운영 비용 절감 및 환경 보호 측면에서 중요합니다. Massive MIMO, mmWave 등은 에너지 소모를 증가시키는 요인이 될 수 있어 효율적인 전력 관리 기술이 요구됩니다. 보안: 네트워크 슬라이싱, 가상화된 코어 네트워크 등 새로운 아키텍처 도입에 따른 보안 위협에 대한 대비가 필요합니다. 6G 네트워크 통신 기술 전망 5G가 본격적으로 확산되는 가운데, 학계와 산업계에서는 이미 다음 세대 이동통신 기술인 6G에 대한 연구 개발을 시작했습니다. 6G는 2030년경 상용화를 목표로 하며, 5G의 성능을 비약적으로 향상시키는 동시에 새로운 차원의 서비스를 제공할 것으로 기대됩니다. 6G 비전 및 주요 성능 목표 (5G 대비 향상점 중심) 6G의 비전은 \"모든 것이 연결되고 지능적으로 융합되는 세상(Connecting Everything with Ambient Intelligence)\" 등으로 요약될 수 있습니다. 주요 성능 목표는 5G 대비 더욱 극한의 성능을 추구합니다. 최대 전송 속도: 1 Tbps (5G 대비 50배) 사용자 체감 전송 속도: 1 Gbps (5G 대비 10배) 지연 시간: 무선 구간 0.1ms (5G 대비 1/10), 종단간 1ms 이하의 초저지연 지속 신뢰도: 99.99999% (Six-nines) 이상의 초고신뢰도 (URLLC 보다 한 단계 높은 수준) 연결 밀도: 1 km² 당 1,000만 개 기기 (5G 대비 10배) 이동성: 1,000 km/h 이상 (고속철도, 항공기 등 지원) 주파수 효율: 5G 대비 2~3배 향상 에너지 효율: 5G 대비 10~100배 향상 (목표치가 다양함) 정밀 측위: cm 수준의 실내외 정밀 측위 (현재 GPS는 수 m 수준) 센싱 기능: 통신 네트워크 자체를 센서로 활용하여 주변 환경 감지 6G 예상 핵심 기술 후보군 및 연구 동향 6G는 5G의 기술을 기반으로 더욱 혁신적인 기술들을 통합할 것으로 예상됩니다. 테라헤르츠(THz) 및 서브-THz 대역 통신 기술: 100GHz ~ 10THz 사이의 주파수 대역을 활용하여 Tbps급의 초고속 데이터 전송을 목표로 합니다. THz파는 매우 넓은 가용 대역폭을 제공하지만, mmWave보다 더 심각한 전파 손실, 대기 흡수, 부품 기술의 미성숙 등의 기술적 난제가 존재합니다. 초소형 안테나 어레이, 고효율 RF 부품 개발, 새로운 파형 및 신호 처리 기술 연구가 활발히 진행 중입니다. AI/ML 기반 네트워크 지능화 및 자동화 (예: AI 기반 RAN, 코어망 운영): 네트워크 설계, 구축, 운영, 최적화 전반에 AI/ML 기술을 깊숙이 통합하여 네트워크의 자율성과 효율성을 극대화합니다. 예를 들어, AI 기반 빔포밍 최적화, 동적 네트워크 슬라이스 관리, 예측 기반 자원 할당, 지능형 간섭 제거, 이상 트래픽 탐지 및 자동 복구 등이 가능해집니다. 이를 위해 경량화된 AI 모델, 실시간 학습 및 추론 기술, 데이터 수집 및 관리 플랫폼 구축이 중요합니다. 공간 통신 기술 (예: 위성 통합, 무인항공기(UAV) 활용, Reconfigurable Intelligent Surfaces - RIS): 위성 통합 (NTN: Non-Terrestrial Networks): 저궤도(LEO), 중궤도(MEO), 정지궤도(GEO) 위성 통신망을 지상망과 통합하여 음영 지역 없는 글로벌 커버리지를 제공합니다. 5G Rel-17에서 시작된 NTN 연구가 6G에서는 더욱 심화될 것입니다. UAV 활용: 드론이나 HAPS(High Altitude Platform Station)를 이동형 기지국 또는 중계기로 활용하여 재난 지역 통신 지원, 임시 커버리지 확장 등에 사용됩니다. RIS (Reconfigurable Intelligent Surfaces): 전파 환경을 동적으로 제어할 수 있는 메타표면(metasurface)을 건물 벽면이나 유리창 등에 설치하여, 전파를 원하는 방향으로 반사하거나 투과시켜 통신 품질을 개선하고 음영 지역을 해소하는 기술입니다. 수동형 RIS와 능동형 RIS 연구가 진행 중입니다. 네트워크-컴퓨팅-센싱 융합 (예: ISAC - Integrated Sensing and Communication): 통신 신호를 이용하여 주변 환경에 대한 정보를 감지(sensing)하고, 이 정보를 다시 통신 성능 향상이나 새로운 서비스 제공에 활용하는 기술입니다. 예를 들어, 기지국이 전파를 이용해 주변의 물체, 사람, 환경 변화 등을 감지하고, 이를 자율주행, 스마트 시티, 재난 감지 등에 활용할 수 있습니다. 통신과 센싱 기능을 동일한 하드웨어 및 주파수 자원에서 효율적으로 통합하는 것이 핵심입니다. 차세대 코어 네트워크 및 아키텍처 (예: 분산형 AI, 양자 통신 연동 가능성): 6G 코어 네트워크는 AI 기능을 내재화하고, 더욱 분산된 컴퓨팅 환경을 지원하며, 극한의 서비스 요구사항을 만족시키기 위한 새로운 아키텍처가 필요합니다. 데이터 중심 설계, 지능형 서비스 오케스트레이션, 종단간 보안 강화 등이 중요하며, 장기적으로는 양자 컴퓨팅 및 양자 통신 기술과의 연동 가능성도 탐색되고 있습니다. 고정밀 측위 및 동기화 기술: 센티미터(cm) 수준의 초정밀 실내외 위치 정보를 제공하여, 자율주행, 로보틱스, XR 등 다양한 위치 기반 서비스의 정확도를 향상시킵니다. 이를 위해 넓은 대역폭 신호, Massive MIMO, AI 기반 측위 알고리즘 등이 활용될 수 있습니다. 또한, 네트워크 전체의 정밀한 시간 동기화는 URLLC 및 분산 시스템 운영에 필수적입니다. 6G 기술의 표준화 로드맵 및 기술적 난제 6G 기술의 표준화는 ITU-R의 IMT-2030 프레임워크 논의를 시작으로, 3GPP에서는 릴리즈 20 이후부터 본격적으로 다뤄질 것으로 예상됩니다. 상용화는 2028년~2030년경으로 전망되고 있습니다. 기술적 난제는 다음과 같습니다. THz 대역 기술 성숙도: THz RF 부품의 성능 및 집적도 향상, 채널 모델링, 효율적인 신호 처리 기술 확보가 시급합니다. AI 기술의 신뢰성 및 설명 가능성: 네트워크 운영에 AI를 적용할 때, AI 결정의 신뢰성, 안정성, 그리고 설명 가능성(explainability) 확보가 중요합니다. 네트워크 복잡도 증가: 다양한 기술의 융합으로 인해 네트워크 설계 및 운영의 복잡도가 크게 증가할 수 있습니다. 에너지 효율성 확보: Tbps급 속도, 초연결 환경에서도 지속 가능한 에너지 효율을 달성해야 합니다. 글로벌 표준화 및 주파수 확보 경쟁: 국가별, 지역별 이해관계에 따른 표준화 및 주파수 확보 경쟁이 예상됩니다. 사회적 수용성: 새로운 기술에 대한 사회적, 윤리적 논의와 합의 과정이 필요합니다. 5G 및 6G 기술의 기술적 파급 효과 및 영향 분석 5G 및 6G 기술은 단순히 통신 속도를 높이는 것을 넘어, 다양한 기술과 융합하여 실생활, 사회 인프라, 경제/산업 구조 전반에 걸쳐 광범위하고 심층적인 변화를 가져올 것입니다. 긍정적 영향 (기술적 구현 가능성 및 새로운 서비스 창출 중심) 실생활 변화: 기술적 진보가 가져올 구체적 서비스 변화 완전 몰입형 XR (확장현실) 서비스: 5G의 eMBB(초고속)와 URLLC(초저지연)는 AR/VR/MR 서비스의 품질을 크게 향상시킵니다. 6G에서는 Tbps급 전송 속도와 0.1ms 수준의 초저지연을 통해 시각, 청각뿐만 아니라 촉각까지 포함하는 완전 몰입형 XR 경험(예: 고해상도 햅틱 피드백 연동)이 가능해질 것입니다. 이는 게임, 엔터테인먼트는 물론 원격 교육, 가상 쇼핑, 협업 등 다양한 분야에 활용될 수 있습니다. MEC 기술은 XR 콘텐츠 렌더링 및 데이터 처리를 엣지에서 수행하여 단말기의 부담을 줄이고 반응 속도를 높이는 데 기여합니다. 홀로그래픽 통신: 6G의 초고대역폭과 초저지연은 3차원 공간 정보를 실시간으로 전송하고 재현하는 홀로그래픽 통신을 현실화할 수 있습니다. 이는 원격 회의, 원격 진료, 실감형 교육 등에 활용되어 물리적 거리의 제약을 극복하는 새로운 소통 방식을 제공할 것입니다. Massive MIMO와 빔포밍 기술은 홀로그램 데이터와 같은 대용량 정보를 안정적으로 전송하는 데 필요합니다. 디지털 트윈 및 사이버 물리 시스템(CPS) 고도화: 현실 세계의 사물이나 시스템을 가상 공간에 동일하게 복제하는 디지털 트윈은 5G의 mMTC(초연결) 및 URLLC를 통해 실현 가능성이 높아졌습니다. 6G에서는 ISAC(통합 센싱 및 통신) 기술과 초정밀 측위 기술을 통해 더욱 정교하고 실시간성이 높은 디지털 트윈 구축이 가능해집니다. 이를 통해 도시, 공장, 인체 등의 복잡한 시스템을 시뮬레이션하고, 예측, 최적화, 원격 제어하는 CPS의 고도화가 이루어질 것입니다. 브레인-컴퓨터 인터페이스(BCI) 연동 가능성 (장기적 관점): 6G의 초저지연, 초고신뢰 통신은 뇌파와 같은 생체 신호를 실시간으로 분석하고, 이를 통해 기기를 제어하거나 의사소통하는 BCI 기술의 발전에 기여할 수 있습니다. 이는 의료, 재활 분야뿐만 아니라 새로운 사용자 인터페이스로서의 가능성을 열어줄 수 있으나, 기술적, 윤리적 과제가 많이 남아있는 장기적인 전망입니다. 사회 인프라 변화: 지능형 인프라 구축 및 운영 효율화 자율주행 네트워크 및 지능형 교통 시스템 (ITS)의 완전 자율화: 5G의 V2X 통신은 차량 간(V2V), 차량-인프라 간(V2I) 통신을 지원하여 자율주행의 안전성을 높입니다. 6G에서는 더욱 향상된 URLLC, eMBB, 그리고 ISAC 기술을 통해 차량이 주변 환경을 보다 정밀하게 인지하고, 실시간으로 복잡한 교통 상황에 대응하는 완전 자율주행(레벨 5)의 실현을 앞당길 것입니다. 네트워크 슬라이싱은 자율주행차량의 안전 관련 통신에 높은 우선순위와 품질을 보장하는 데 활용됩니다. 원격 로봇 수술 및 정밀 의료 서비스 확대: 5G URLLC는 원격 로봇 수술 시 촉각 정보 전달과 정밀 제어를 가능하게 합니다. 6G에서는 지연 시간이 더욱 단축되고 신뢰도가 극대화되어, 수술의 정밀도와 안정성이 향상될 것입니다. 또한, AI 기반 의료 영상 분석, 웨어러블 기기를 통한 실시간 건강 모니터링 및 맞춤형 원격 진료 서비스가 고도화될 것입니다. 스마트 시티 운영 시스템의 실시간 최적화: 5G mMTC는 도시 전역에 설치된 수많은 센서와 기기를 연결하여 데이터를 수집하고, 6G에서는 AI 기반 분석 및 ISAC 기술을 통해 도시 운영의 효율성을 극대화합니다. 교통 흐름 최적화, 에너지 사용량 관리, 환경 오염 모니터링, 재난 예측 및 대응 시스템 등이 실시간으로 지능화될 것입니다. 경제/산업 구조 변화: 기술 기반 신산업 및 생산 방식 혁신 산업용 IoT(IIoT) 및 스마트 팩토리의 초지능화: 5G URLLC와 네트워크 슬라이싱은 스마트 팩토리 내 로봇 제어, 공정 자동화, 실시간 품질 관리 등을 지원합니다. 6G에서는 AI와 디지털 트윈 기술이 결합되어, 생산 라인의 완전 자율 운영, 예지 보전, 맞춤형 대량 생산 등이 가능해질 것입니다. THz 대역을 활용한 고정밀 센싱은 제조 공정의 미세한 오류까지 감지하는 데 기여할 수 있습니다. UAM(도심항공교통) 등 신규 모빌리티 산업 지원 인프라: UAM과 같은 차세대 모빌리티는 안전한 운항을 위해 매우 높은 신뢰성과 저지연성을 갖춘 통신 인프라가 필수적입니다. 5G-Advanced 및 6G의 NTN(위성 통합), 고정밀 측위, URLLC 기술은 UAM의 관제, 자율 비행, 충돌 방지 시스템 등을 지원하는 핵심 인프라가 될 것입니다. 데이터 중심 경제 가속화 및 AI 기반 서비스 확산: 5G 및 6G 환경에서는 엄청난 양의 데이터가 생성, 수집, 분석될 것입니다. 이는 AI 모델 학습 및 서비스 개발을 위한 핵심 자원이 되며, 데이터 기반의 새로운 비즈니스 모델과 서비스가 다양한 산업 분야에서 확산될 것입니다. 엣지 컴퓨팅은 데이터 처리의 분산화와 실시간성을 높여 이러한 변화를 가속화합니다. 부정적 영향 및 기술적 해결 과제 (보안, 프라이버시, 에너지 효율 등) 기술적 문제점 및 우려 보안 취약성 증대: mMTC로 인해 연결되는 디바이스 수가 기하급수적으로 증가하고, 네트워크 슬라이싱, NFV/SDN, MEC 등 아키텍처가 복잡해지면서 잠재적인 공격 표면(attack surface)이 크게 확대됩니다. IoT 디바이스의 보안 취약점, 가상화 환경의 보안 문제, 슬라이스 간 격리 실패 등이 새로운 보안 위협으로 대두될 수 있습니다. 엔드-투-엔드 암호화 강화, AI 기반 이상행위 탐지, 제로 트러스트(Zero Trust) 보안 모델 도입 등 다층적인 보안 기술 개발이 시급합니다. 프라이버시 침해 가능성: 6G의 ISAC, 고정밀 측위, AI 기반 데이터 분석 기술은 개인의 위치, 행동, 생체 정보 등 민감한 데이터를 이전보다 훨씬 정밀하게 수집하고 분석할 수 있게 합니다. 이는 프라이버시 침해의 위험을 크게 높일 수 있습니다. 데이터 익명화/가명화 기술, 차등 프라이버시(Differential Privacy), 연합 학습(Federated Learning)과 같은 프라이버시 강화 기술(PET: Privacy Enhancing Technologies)의 연구 및 적용이 중요합니다. 네트워크 및 단말기의 에너지 소비 증가 문제: Tbps급 전송 속도, THz 대역 활용, Massive MIMO 안테나 수 증가, AI 연산량 증가는 네트워크 장비와 단말기의 전력 소비를 크게 증가시킬 수 있습니다. 이는 통신 사업자의 운영 비용 증가뿐만 아니라 탄소 배출량 증가 등 환경 문제로 이어질 수 있습니다. 저전력 RF 부품 개발, 에너지 효율적인 파형 및 프로토콜 설계, AI 기반 지능형 전력 관리 기술 등 에너지 효율성 확보를 위한 다각적인 노력이 필요합니다. 전파 인체 유해성 논란 (특히 고주파 대역) 및 기술적 검증 필요성: mmWave 및 THz와 같은 고주파 대역의 전자파가 인체에 미치는 영향에 대한 사회적 우려가 존재합니다. 이에 대한 과학적이고 객관적인 연구를 통해 안전 기준을 마련하고, 대중과의 투명한 소통을 통해 기술적 신뢰를 확보해야 합니다. 또한, 전파 노출을 최소화할 수 있는 빔포밍 기술, 저출력 통신 기술 등의 연구도 병행되어야 합니다. 기술 구현의 복잡성 및 상호 운용성 확보 문제: 6G는 다양한 첨단 기술들이 복합적으로 융합되기 때문에 시스템 설계 및 구현의 복잡성이 매우 높습니다. 이종 기술 간, 서로 다른 벤더 장비 간의 원활한 상호 운용성을 확보하기 위한 표준화 노력과 철저한 검증 과정이 필수적입니다. 사회·경제적 문제점 (기술적 관점에서 파생되는 문제) 기술 격차 (Digital Divide) 심화: 고도화된 5G/6G 인프라 구축에는 막대한 비용이 소요되므로, 경제적 여건에 따라 국가별, 지역별, 계층별 기술 접근성과 활용 수준에 격차가 발생할 수 있습니다. 이는 정보 불평등을 심화시키고 사회적 양극화를 초래할 수 있습니다. 포용적인 인프라 구축 전략과 디지털 리터러시 교육 강화가 필요합니다. 고용 구조 변화 가속화: AI 기반 자동화, 스마트 팩토리, 자율주행 등 5G/6G 기술이 가져올 산업 혁신은 기존 일자리를 대체하고 고용 구조에 큰 변화를 야기할 수 있습니다. 새로운 기술에 적응하기 위한 재교육 프로그램 확대 및 사회 안전망 강화가 중요합니다. 인프라 투자 비용 및 유지보수 부담 증가: THz 대역 활용을 위한 초고밀도 셀 구축, AI 연산을 위한 고성능 컴퓨팅 인프라 확보 등은 막대한 초기 투자 비용을 요구합니다. 또한, 고도로 복잡해진 네트워크의 유지보수 및 운영 비용도 증가할 수 있어, 효율적인 투자 및 운영 모델 개발이 필요합니다. 결론: 기술적 과제와 미래 전망 (본인 의견 포함) 5G 및 6G 기술의 핵심적 의의 및 기술 발전 방향 요약 5G 기술은 초고속, 초저지연, 초연결이라는 3대 핵심 성능을 바탕으로 모바일 브로드밴드의 고도화를 넘어 다양한 산업 분야와의 융합을 가능하게 하는 디지털 전환의 핵심 인프라로서의 의의를 지닙니다. mmWave, Massive MIMO, 네트워크 슬라이싱, MEC 등의 핵심 기술들은 이러한 성능 목표를 달성하고, 스마트 팩토리, 자율주행, 실감형 미디어 등 새로운 서비스를 현실화하는 기반을 제공했습니다. 6G 기술은 5G의 성능을 극한으로 끌어올려 Tbps급 전송 속도, 0.1ms 수준의 지연 시간, cm급 정밀 측위 등을 목표로 하며, 여기에 더해 AI/ML의 완전한 네트워크 내재화, 통신-센싱-컴퓨팅의 융합, 위성을 포함한 공간 통신이라는 새로운 차원의 기술적 진보를 추구합니다. THz 통신, RIS, ISAC 등의 기술은 6G가 단순히 통신망을 넘어 지능형 인프라 플랫폼으로 진화하는 데 핵심적인 역할을 할 것으로 기대됩니다. 이는 현실 세계와 가상 세계를 완벽하게 연결하고, 모든 사물과 공간에 지능을 부여하여 인간의 삶과 산업의 방식을 근본적으로 변화시킬 잠재력을 가지고 있습니다. 기술 발전 방향은 단순히 성능 지표를 높이는 것을 넘어, 네트워크의 유연성, 개방성, 지능성, 그리고 지속 가능성을 확보하는 방향으로 나아가고 있습니다. 클라우드 네이티브 아키텍처, 오픈랜(Open RAN)과 같은 개방형 인터페이스, AI 기반의 자율 운영, 에너지 효율성 극대화 등이 주요 기술 트렌드가 될 것입니다. 기술적 난제 극복 및 지속 가능한 발전을 위한 컴퓨터공학도의 역할과 제언 5G 및 6G 기술이 가져올 혁신적인 미래는 매우 기대되지만, 동시에 해결해야 할 기술적 난제와 사회적 고려 사항도 산적해 있습니다. 이러한 과제를 극복하고 지속 가능한 기술 발전을 이루기 위해 컴퓨터공학도로서 다음과 같은 역할과 노력이 필요하다고 생각합니다. 보안 및 프라이버시 강화 기술 개발의 중요성: 의견: 초연결, 초지능화된 네트워크 환경에서는 보안과 프라이버시가 기술의 성패를 좌우하는 가장 중요한 요소 중 하나가 될 것입니다. 단순히 기능 구현에 그치지 않고, 설계 단계부터 보안(Security by Design)과 프라이버시(Privacy by Design)를 핵심 원칙으로 고려해야 합니다. 기술적 제언: 차세대 암호 기술 연구: 양자컴퓨팅 시대에도 안전한 양자내성암호(PQC), 동형암호, 영지식증명 등 첨단 암호 기술을 네트워크 프로토콜 및 서비스에 적극적으로 적용해야 합니다. AI 기반 보안 위협 탐지 및 대응 시스템 고도화: 네트워크 트래픽, 시스템 로그 등을 AI로 분석하여 알려지지 않은 공격(Zero-day attack)이나 지능형 지속 위협(APT)을 실시간으로 탐지하고 자동 대응하는 기술 개발이 중요합니다. 프라이버시 강화 기술(PET) 적용 확대: 데이터 수집 및 활용 과정에서 개인정보를 보호하기 위해 연합 학습, 차등 프라이버시, 안전한 다자간 계산(Secure Multi-Party Computation) 등의 기술을 실제 서비스에 효과적으로 통합하는 연구가 필요합니다. 에너지 효율적인 네트워크 및 단말 기술 연구: 의견: 6G의 성능 목표를 달성하기 위한 기술들은 자칫 엄청난 에너지 소비를 초래할 수 있습니다. 이는 환경 문제뿐만 아니라 네트워크 운영 비용 증가로 이어져 기술 보급의 장벽이 될 수 있습니다. 따라서 성능 향상과 동시에 에너지 효율을 극대화하는 것은 기술적 성숙도를 판단하는 중요한 기준이 될 것입니다. 기술적 제언: 저전력 반도체 및 RF 부품 개발: THz 대역 통신, Massive MIMO 등을 지원하면서도 에너지 효율이 높은 반도체 및 RF 부품 기술 개발에 집중해야 합니다. AI 기반 지능형 전력 관리: 트래픽 상황, 사용자 요구 등을 AI로 예측하여 기지국, 네트워크 장비, 단말기의 전력 소모를 동적으로 최적화하는 기술을 개발해야 합니다. (예: 특정 시간대나 지역의 트래픽이 적을 경우 일부 셀을 슬립 모드로 전환) 에너지 효율적인 네트워크 아키텍처 및 프로토콜 설계: 데이터 전송 경로 최적화, 불필요한 신호 송수신 최소화 등 에너지 효율을 고려한 네트워크 아키텍처 및 통신 프로토콜 연구가 필요합니다. 개방형 표준 및 오픈소스 생태계 활성화를 통한 기술 혁신 가속화: 의견: 과거 이동통신 기술은 특정 대기업 중심의 폐쇄적인 기술 개발 경향이 있었습니다. 그러나 5G부터 오픈랜(O-RAN) 등의 논의가 활발해졌으며, 6G에서는 이러한 개방성이 더욱 중요해질 것입니다. 개방형 표준과 오픈소스는 다양한 아이디어의 접목을 촉진하고, 중소기업 및 스타트업의 참여를 확대하여 기술 혁신을 가속화하며, 특정 벤더에 대한 종속성을 줄일 수 있습니다. 기술적 제언: 오픈랜 및 개방형 인터페이스 표준화 적극 참여 및 기여: 국제 표준화 단체 활동에 적극적으로 참여하고, 국내 기술이 표준에 반영될 수 있도록 노력해야 합니다. 오픈소스 소프트웨어 개발 및 커뮤니티 활성화: 6G 네트워크 기능, AI 모델, 관리 도구 등을 오픈소스로 공개하고, 국내외 개발자 커뮤니티와의 협력을 통해 생태계를 확장해야 합니다. 모듈화된 소프트웨어 아키텍처 설계: 네트워크 기능을 모듈화하고 표준 API를 통해 연동함으로써 다양한 솔루션의 조합과 빠른 서비스 개발을 지원해야 합니다. AI 기술의 책임감 있는 활용 및 통제 방안 연구 (네트워크 오작동 방지 등): 의견: 6G는 AI가 네트워크 전반에 깊숙이 통합되는 \"AI-Native Network\"를 지향합니다. AI는 네트워크 효율성과 자율성을 크게 향상시키지만, 동시에 AI 모델의 편향성, 예측 불가능성, 오작동 가능성 등 새로운 위험 요소도 내포하고 있습니다. 따라서 AI 기술의 책임감 있는 활용과 통제 방안 마련이 필수적입니다. 기술적 제언: 설명 가능한 AI(XAI) 기술 적용: 네트워크 운영에 사용되는 AI 모델의 의사결정 과정을 인간이 이해할 수 있도록 설명 가능성을 확보하여, 문제 발생 시 원인 분석 및 해결을 용이하게 해야 합니다. AI 모델의 견고성(Robustness) 및 안전성 검증 기술 개발: AI 모델이 예기치 않은 입력이나 악의적인 공격에도 안정적으로 동작하고, 심각한 네트워크 장애를 유발하지 않도록 검증하는 기술이 필요합니다. 인간 통제(Human-in-the-loop) 시스템 설계: 완전 자율 운영을 지향하더라도, 중요한 결정이나 비상 상황에서는 인간 운영자가 개입하여 통제할 수 있는 안전장치를 마련해야 합니다. 컴퓨터공학도로서 우리는 이러한 기술적 과제 해결에 적극적으로 참여하고, 끊임없는 학습과 연구를 통해 5G와 6G 기술이 인류 사회에 긍정적으로 기여할 수 있도록 노력해야 할 것입니다. 기술의 발전은 결국 인간을 위한 것이어야 하며, 그 과정에서 발생할 수 있는 문제점들을 예측하고 선제적으로 대응하는 자세가 중요하다고 생각합니다. 참고문헌 본 보고서는 일반적인 5G 및 6G 기술 동향과 전망에 대한 공개된 자료(학술 논문, 기술 백서, 전문 기사, 국제 표준화 기구 문서 등)를 종합하여 작성되었으며, 특정 문서를 직접 인용하지는 않았습니다. 실제 보고서 작성 시에는 구체적인 출처를 명시해야 합니다. 제공해주신 \"James F. Kurose Keith W. Ross - Computer Networking A Top-down Approach-Pearson (2020).pdf\" 파일은 네트워크의 계층적 구조, 링크 계층, LAN 기술 등에 대한 포괄적인 내용을 담고 있어, 5G/6G의 하부 네트워크 인프라 및 프로토콜을 이해하는 데 배경 지식으로 참고할 수 있습니다. 특히, 데이터센터 네트워킹, 이더넷 기술, MAC 주소 및 ARP, 스위치 및 VLAN 등의 내용은 5G/6G 네트워크가 구축되고 운영되는 물리적, 논리적 환경을 이해하는 데 도움이 됩니다. 다만, 해당 PDF는 5G/6G 기술 자체를 직접적으로 다루고 있지는 않아 본 보고서의 핵심 내용에 대한 직접적인 인용은 제한적입니다. 알겠습니다. 제공해주신 \"Chapter5v8.0-NetworkLayer_ControlPlane수정.pptx\" 자료를 기반으로 5G 및 6G 네트워크 통신 기술, 특히 네트워크 제어 평면(Control Plane) 에 초점을 맞춰 범위를 좁히고 각 개별 기술에 대해 더 깊이 있는 분석을 제공하는 보고서를 작성하겠습니다. 먼저, PPT 내용을 반영하여 새롭게 구성한 목차를 제안 드립니다. 이 목차는 전통적인 제어 평면 기술부터 SDN, 네트워크 관리 자동화 기술(NETCONF/YANG)이 5G/6G 환경에서 어떻게 적용되고 발전하는지에 대한 심층적인 내용을 다룰 것입니다. 목차 (PPT 내용 기반 심층 분석안) 서론 5G/6G 시대, 네트워크 제어 평면의 혁신과 그 중요성 다양한 서비스 요구사항(eMBB, URLLC, mMTC)과 제어 평면의 역할 네트워크 유연성, 자동화, 지능화를 위한 제어 평면 기술의 필요성 연구 목적 및 범위 PPT \"Network Layer: Control Plane\" 핵심 기술 심층 분석 분석된 기술의 5G/6G 적용 방안 및 심화된 기술적 고찰 네트워크 제어 평면의 구성 원리 및 핵심 프로토콜 (PPT 내용 심층 탐구) 제어 평면의 두 가지 접근 방식: 전통적 방식 vs. SDN 라우터별 제어 (Per-router control) 방식 분석 (PPT p-6) 논리적 중앙 집중형 제어 (Logically centralized control - SDN) 방식 분석 (PPT p-6) 라우팅 알고리즘 심층 분석 (PPT p-40) 라우팅 알고리즘의 목표 및 분류 (PPT p-9) 링크 상태 (Link State, LS) 라우팅 알고리즘 (PPT p-20) 다익스트라 (Dijkstra) 알고리즘 상세 분석 및 예제 LS 알고리즘의 메시지 복잡도 및 특성 거리 벡터 (Distance Vector, DV) 라우팅 알고리즘 (PPT p-31) 벨만-포드 (Bellman-Ford) 방정식 및 알고리즘 상세 분석 DV 알고리즘의 문제점 (라우팅 루프, Count-to-infinity) 및 해결책 (포이즌 리버스) LS vs. DV 비교 분석 (PPT p) 계층적 라우팅 (Hierarchical Routing)의 필요성 및 개념 (PPT p-35) 인터넷에서의 라우팅 프로토콜: 실제 구현 사례 (PPT p-71) 자율 시스템 (Autonomous Systems, AS) 내부 라우팅: OSPF (PPT p-52) OSPF의 특징 (개방형, LS 기반, 계층 구조 지원 등) OSPF 동작 방식 및 보안 (인증) 자율 시스템 간 라우팅: BGP (PPT p-70) BGP의 역할 및 기본 동작 (eBGP, iBGP) 경로 속성(Path attributes) 및 경로 선택 정책 BGP 라우팅 정책의 중요성 및 예시 왜 내부/외부 라우팅 프로토콜을 분리하는가? (PPT p) 소프트웨어 정의 네트워킹(SDN)과 5G/6G 제어 평면 (PPT p-92) SDN의 핵심 개념 및 아키텍처 (PPT p-78) 제어 평면과 데이터 평면의 분리 데이터 평면 스위치 (플로우 테이블 기반 포워딩) SDN 컨트롤러 (네트워크 OS) 의 역할 및 기능 제어 평면 API (Northbound) 및 데이터 평면 API (Southbound - 예: OpenFlow) OpenFlow 프로토콜 심층 분석 (PPT p-83 내외 OpenFlow 관련 내용) 플로우 테이블 항목 (매치 필드, 액션, 카운터 등) OpenFlow 메시지 유형 (컨트롤러-스위치 간) SDN 컨트롤러 실제 사례 (PPT p ODL, ONOS 등 언급) 5G/6G 네트워크에서의 SDN 적용 심화 5G SBA(Service Based Architecture)와 SDN의 연관성 분석 네트워크 슬라이싱(Network Slicing) 구현을 위한 SDN의 역할 상세 분석 MEC(Multi-access Edge Computing) 트래픽 관리 및 자원 할당에서의 SDN 활용 네트워크 관리 및 자동화와 5G/6G 제어 평면 (PPT p-98) ICMP (Internet Control Message Protocol) (PPT p-94) ICMP 메시지 유형 (오류 보고, 질의) 및 기능 Traceroute 프로그램에서의 ICMP 활용 예시 5G/6G 네트워크 진단 및 모니터링에서의 ICMP 활용 방안 네트워크 관리 프레임워크 개요 (PPT p) 차세대 네트워크 설정 및 관리: NETCONF와 YANG (PPT p-98) NETCONF 프로토콜 심층 분석 (PPT p) 목표 및 동작 원리 (RPC 기반) 주요 오퍼레이션 (&lt;get-config>, &lt;edit-config> 등) 및 XML 기반 메시지 구조 YANG 데이터 모델링 언어 심층 분석 (PPT p) YANG의 역할 및 특징 (데이터 구조, 구문, 의미론 정의) YANG 모듈 구조 및 주요 키워드 (container, list, leaf 등) 데이터 제약 조건 표현 및 유효성 검증 5G/6G 네트워크 자동화 및 오케스트레이션에서의 NETCONF/YANG 활용 가상화된 네트워크 기능(VNF) 및 네트워크 슬라이스 설정 자동화 제로 터치 프로비저닝(Zero Touch Provisioning) 구현 5G/6G 제어 평면 기술의 영향, 과제 및 미래 전망 제어 평면 기술 혁신이 가져올 기술적 파급 효과 네트워크 프로그래머빌리티 향상 및 서비스 민첩성 증대 자원 효율성 극대화 및 운영 비용 절감 지능형 네트워크 서비스 및 맞춤형 네트워크 제공 능력 강화 기술적 과제 및 심층적 해결 방안 중앙 집중형 제어의 확장성, 성능, 안정성 확보 문제 (SDN 컨트롤러) 제어 평면 보안 위협 (DDoS 공격, 설정 오류를 통한 네트워크 마비 등) 및 능동적 방어 전략 다양한 벤더 장비 및 기술 간 상호 운용성 확보의 어려움 6G를 향한 제어 평면의 지능화: AI/ML 통합 시 고려사항 (데이터, 알고리즘, 신뢰성) 결론: 미래 제어 평면 기술 발전을 위한 컴퓨터공학도의 심층적 역할 (본인 의견 포함) 개방형 인터페이스 및 표준화 기여를 통한 생태계 확장 고신뢰/고성능 제어 알고리즘 및 AI 모델 개발 End-to-End 네트워크 슬라이싱 및 자동화 기술 심화 연구 참고자료 제공된 PPT: Chapter5v8.0-NetworkLayer_ControlPlane수정.pptx (필요시 추가 문헌) 위 목차를 바탕으로 각 항목에 대한 심층적인 기술 분석 보고서를 작성해 드리겠습니다. PPT의 슬라이드 번호를 일부 명시하여 내용의 근거를 찾기 쉽도록 하였습니다. 다음 단계로 첫 번째 항목인 \"1. 서론\"부터 내용을 전개해 나가겠습니다. 원하시면 특정 부분부터 시작할 수도 있습니다. 알겠습니다. 사용자의 요청에 맞춰 기술적인 깊이를 약간 조절하고, 실생활, 사회, 경제적 영향에 대한 내용을 더 보강하여 목차를 수정했습니다. PPT 슬라이드 번호는 본문 내용 작성 시에는 생략하도록 하겠습니다. 수정된 목차를 바탕으로 보고서 작성을 시작하겠습니다. 5G와 6G 네트워크 통신 기술: 현재와 미래, 그리고 우리 삶에 미칠 영향 목차 (수정안) 서론 5G를 넘어 6G로: 차세대 네트워크의 비전과 사회 변화의 기대감 연구의 목적: 5G/6G 핵심 기술 이해와 실생활, 사회, 경제적 영향 다각적 분석 보고서의 구성 및 범위 (제어 평면 기술을 포함한 핵심 기술과 그 파급 효과 중심) 5G 및 6G 네트워크의 핵심 기술과 제어 평면의 진화 5G 네트워크의 주요 특징과 서비스 시나리오 초고속(eMBB), 초저지연(URLLC), 초연결(mMTC)의 기술적 기반 개요 서비스 기반 아키텍처(SBA) 및 네트워크 슬라이싱의 개념과 중요성 6G 네트워크의 지향점과 예상 핵심 기술 5G 성능의 확장과 새로운 차원 (AI 통합, 센싱 융합, THz 통신 등) 지능적이고 유연한 네트워크를 위한 제어 평면의 역할 (네트워크 제어 평면 PPT 핵심 내용 연계) 전통적 라우팅에서 SDN(소프트웨어 정의 네트워킹)으로의 전환의 의미 SDN의 기본 원리: 제어부-데이터부 분리와 중앙 집중형 제어를 통한 네트워크 유연성 확보 5G/6G에서 SDN을 활용한 네트워크 프로그래밍, 동적 자원 할당 및 서비스 맞춤화 네트워크 관리 자동화: NETCONF/YANG 프로토콜의 역할과 중요성 복잡다단한 5G/6G 네트워크의 효율적 설정, 모니터링 및 관리 자동화의 필요성 NETCONF/YANG을 통한 자동화된 오케스트레이션, 신속한 서비스 배포 및 제로터치 운영 가능성 5G 및 6G 기술이 실생활, 사회, 경제에 미치는 영향 (긍정적 및 부정적 측면) 우리들의 실생활 변화 긍정적 영향: 현실과 가상을 넘나드는 몰입형 엔터테인먼트 및 초실감형 소통 (XR, 홀로그램, 메타버스 등) 생활 편의를 극대화하는 개인 맞춤형 스마트 환경 (지능형 스마트 홈, 실시간 원격 헬스케어, AI 기반 맞춤형 교육) 시공간의 제약을 허무는 이동성의 혁신 (완전 자율주행차 상용화, 도심항공교통(UAM)의 대중화 기반 마련) 부정적 영향: 디지털 정보 격차 심화 및 새로운 형태의 정보 소외 계층 발생 가능성 개인 식별 정보 및 생체 정보 등 민감 데이터의 프라이버시 침해 및 오용 우려 증가 과도한 기술 의존으로 인한 사회성 약화 및 사이버 중독, 디지털 피로감 확산 사회 구조 및 기능 변화 긍정적 영향: 도시 전체가 유기적으로 연결되는 지능형 도시(스마트 시티) 구현 가속화 및 도시 문제 해결 기여 원격 근무 및 분산형 교육 시스템 보편화, 일과 삶의 균형 및 교육 기회 확대 AI와 빅데이터 기반의 재난안전 시스템 고도화 및 국민 생활 안전망 강화 부정적 영향: AI 및 로봇 기술 발전으로 인한 특정 직군의 자동화 가속화, 일자리 구조 변화 및 고용 불안정 심화 사이버 공격, 가짜 정보(딥페이크 등) 유포 등 기술 악용 범죄의 지능화·고도화로 인한 사회 혼란 야기 개인의 모든 활동이 데이터화되면서 발생하는 감시 사회에 대한 우려 증폭 및 통제 강화 가능성 경제 구조 및 산업 발전 변화 긍정적 영향: 메타버스, 인공지능 서비스, 실감형 콘텐츠 등 새로운 디지털 산업 창출 및 국가 성장 동력 확보 전통 산업(제조, 의료, 농업, 물류 등)의 디지털 전환 가속화 및 생산성 혁신 (스마트 팩토리, 원격 정밀 의료, 스마트팜, 지능형 물류 시스템) 데이터 기반 경제 활성화 및 전 산업의 서비스화 촉진, 글로벌 경쟁력 강화 부정적 영향: 전국적인 5G/6G 인프라 구축 및 유지보수에 요구되는 막대한 초기 투자 비용과 재원 확보 문제 핵심 기술 및 장비의 해외 의존도 심화 시 국가 산업 경쟁력 약화 및 기술 종속 우려 소수의 거대 플랫폼 기업에 의한 시장 독과점 심화 및 불공정 경쟁 문제 발생 가능성 5G/6G 시대의 바람직한 미래를 위한 제언 (본인 의견 중심) 기술 발전과 사회적 가치의 조화로운 추구 포용적 기술 발전 전략: 디지털 접근성 강화 및 정보 격차 해소를 위한 정책적 노력 신뢰할 수 있는 디지털 환경 구축: 강력한 개인정보보호 및 사이버 보안 시스템 마련, 기술 윤리 확립 지속 가능한 발전 지향: 차세대 네트워크의 에너지 효율성 극대화 및 환경 영향 최소화 노력 미래 사회 변화에 대한 능동적이고 선제적인 대응 창의적 융합 인재 양성을 위한 교육 시스템 혁신 및 평생 학습 체계 구축 기술 발전에 따른 법제도 정비 및 새로운 사회적 규범과 윤리적 가이드라인 확립 기술의 혜택이 사회 전체에 공정하게 공유될 수 있도록 사회적 합의 도출 및 공동체적 노력 강화 결론 5G/6G 기술의 핵심적 가치와 무한한 잠재력 요약 기술이 주도하는 미래 사회의 희망과 우려, 그리고 우리의 능동적 준비 자세 강조 서론 5G를 넘어 6G로: 차세대 네트워크의 비전과 사회 변화의 기대감 우리는 불과 몇 년 전 4세대 이동통신(4G LTE)이 제공하는 모바일 광대역 서비스를 통해 스마트폰으로 언제 어디서나 인터넷에 접속하고 다양한 정보를 소비하는 시대를 맞이했습니다. 이제 우리는 5세대 이동통신(5G)의 상용화를 넘어, 그 다음 세대인 6세대 이동통신(6G) 기술을 향한 기대감 속에 살아가고 있습니다. 5G는 단순히 더 빠른 속도의 통신을 의미하는 것을 넘어, 초고속(eMBB), 초저지연(URLLC), 초연결(mMTC)이라는 세 가지 핵심 특징을 바탕으로 가상현실(VR), 증강현실(AR), 스마트 팩토리, 자율주행 등 과거에는 상상하기 어려웠던 혁신적인 서비스들을 우리 생활과 산업 현장 곳곳에 스며들게 하고 있습니다. 이러한 5G의 성과를 발판 삼아, 6G는 한층 더 진화된 기술을 통해 인간과 사물, 공간 모두가 유기적으로 연결되고 지능화되는 새로운 차원의 시대를 열 것으로 전망됩니다. 테라헤르츠(THz)와 같은 초고주파 대역 활용을 통한 초고속·대용량 데이터 전송, 인공지능(AI)의 네트워크 전반 적용을 통한 완전 자동화 및 지능형 서비스 제공, 통신과 센싱 기능의 융합을 통한 현실 세계의 디지털 복제(디지털 트윈) 및 상호작용 강화 등은 6G가 가져올 미래 사회의 모습을 어렴풋이 보여주고 있습니다. 이러한 차세대 네트워크 기술의 발전은 우리 삶의 방식, 사회 시스템, 그리고 경제 구조 전반에 걸쳐 이전과는 비교할 수 없는 거대한 변화를 예고하며 큰 기대감을 불러일으키고 있습니다. 연구의 목적: 5G/6G 핵심 기술 이해와 실생활, 사회, 경제적 영향 다각적 분석 본 보고서의 목적은 현재 우리 사회의 디지털 전환을 이끌고 있는 5G 네트워크 통신 기술과 다가올 미래의 핵심 인프라가 될 6G 네트워크 통신 기술의 주요 특징과 핵심 기술들을 이해하는 데 있습니다. 더 나아가, 이러한 첨단 통신 기술의 발전이 단순한 기술적 진보를 넘어 우리들의 구체적인 실생활, 사회 구조 및 기능, 그리고 경제 산업 전반에 걸쳐 어떠한 긍정적 변화와 기회를 가져다주는 동시에, 어떠한 부정적 영향이나 해결해야 할 과제들을 안겨줄 수 있는지 다각적인 관점에서 심층적으로 분석하고자 합니다. 특히 컴퓨터공학 전공자로서 기술의 원리를 이해하는 것을 바탕으로, 이러한 기술들이 실제로 어떻게 구현되고 활용될 수 있는지, 그리고 그 과정에서 발생할 수 있는 긍정적·부정적 파급 효과들을 구체적인 예를 통해 제시하고, 이에 대한 본인의 비판적이고 건설적인 의견을 제시하는 것을 목표로 합니다. 보고서의 구성 및 범위 (제어 평면 기술을 포함한 핵심 기술과 그 파급 효과 중심) 본 보고서는 먼저 5G 및 6G 네트워크의 핵심 기술 요소들을 살펴보고, 특히 네트워크의 유연성, 지능성, 자동화를 가능하게 하는 핵심 요소인 제어 평면(Control Plane) 기술의 진화(예: 소프트웨어 정의 네트워킹 - SDN, 네트워크 기능 자동화 - NETCONF/YANG 등)에 주목하여 기술적 배경을 설명할 것입니다. 제공된 \"Network Layer: Control Plane\" PPT 자료의 핵심 개념들을 바탕으로, 이러한 제어 평면 기술이 어떻게 5G/6G의 혁신적인 서비스들을 가능하게 하는지 연관 지어 이해를 돕고자 합니다. 이러한 기술적 이해를 바탕으로, 5G 및 6G 기술이 우리 실생활에 가져올 편리함과 새로운 경험, 사회 시스템의 효율화와 공공 서비스 개선, 그리고 신산업 창출과 경제 성장과 같은 긍정적 측면을 구체적인 예시와 함께 제시할 것입니다. 동시에, 디지털 격차 심화, 프라이버시 침해, 일자리 변화, 사이버 보안 위협 증가, 그리고 기술의 오용 가능성과 같은 잠재적인 부정적 영향과 우려 사항들에 대해서도 균형 있는 시각으로 조명할 것입니다. 마지막으로, 이러한 분석을 종합하여 5G와 6G 기술이 만들어갈 미래 사회가 더욱 바람직한 방향으로 나아가기 위해 필요한 노력과 컴퓨터공학도로서 기여할 수 있는 부분에 대한 본인의 의견을 제시하며 보고서를 마무리하고자 합니다. 본 보고서는 기술적 설명의 깊이를 일정 수준으로 유지하면서도, 기술이 사회 전반에 미치는 영향에 대한 폭넓은 이해를 돕는 데 중점을 둘 것입니다.",
      "frontmatter": {
        "date": "2025-05-26T16:21:21+09:00",
        "lastmod": "2025-06-03T05:27:10+09:00"
      }
    },
    "바이브 코딩": {
      "path": "/바이브-코딩/",
      "filename": "바이브 코딩",
      "content": "본 글은 바이브 코딩뿐만 아니라 모든 ai가 코드에 개입하는 상황과 여러 프로젝트를 경험해보고 느낀 글이다 소프트웨어 물리적 개입 분류 실시간 ai 자동완성 ex) cursor ex) gemini cli, claude code 개입 정도 따른 분류 초창기 ai 는 autocomplete 형태로 개입되었다(초창기 copliot, TabNine) 2023년 정도 시점 하지만 시간이 지나면서 cursor 가 나오게 되고 claude code 과 같은 터미널 베이스 로 개입이 되고 있다 각각 autocomplete, 소프트 웨어 공학 용어들 SSoT (Single Source of Truth, 단일 진실 공급원) 정의: 시스템 내 특정 데이터는 오직 하나의 권위 있는 출처에서만 관리되고 갱신된다. 목적: 데이터 중복, 불일치, 오류를 방지하고 일관성과 신뢰성을 확보한다. 응집도 (Cohesion) 정의: 모듈 내부 요소들이 하나의 목적 또는 책임을 위해 얼마나 긴밀하게 연결되어 있는가. 목적: 유지보수성, 재사용성, 테스트 용이성을 높이기 위해 응집도를 최대화한다. 결합도 (Coupling) 정의: 두 모듈 간의 의존 정도, 즉 한 모듈이 다른 모듈의 변경에 얼마나 영향을 받는가. 목적: 결합도를 낮춰(느슨한 결합) 시스템의 유연성과 유지보수성을 향상시킨다. 관심사의 분리 (Separation of Concerns, SoC) 정의: 시스템을 서로 다른 책임 또는 관심사 단위로 분리하여 설계하는 원칙. 목적: 각 구성 요소의 역할을 명확히 해 가독성, 확장성, 재사용성을 높인다. 단일 책임 원칙 (Single Responsibility Principle, SRP) 정의: 클래스나 함수는 오직 하나의 변경 이유만 가져야 한다. 목적: 변경의 영향 범위를 좁혀 코드의 안정성과 유지보수성을 높인다. DRY 원칙 (Don’t Repeat Yourself) 정의: 동일한 지식, 로직, 정보를 시스템 내에서 중복하지 말라. 목적: 중복으로 인한 오류와 유지보수 비용을 줄이고 일관성을 유지한다. KISS 원칙 (Keep It Simple, Stupid) 정의: 시스템은 가능한 한 단순하게 설계되어야 한다. 목적: 불필요한 복잡도를 제거해 이해와 유지보수를 쉽게 한다. YAGNI 원칙 (You Aren’t Gonna Need It) 정의: 현재 필요하지 않은 기능은 구현하지 말라. 목적: 과도한 설계(Over-engineering)를 방지하고 개발 속도와 유연성을 높인다. 의존성 역전 원칙 (Dependency Inversion Principle, DIP) 정의: 상위 모듈은 하위 모듈에 의존하지 않고, 양쪽 모두 추상화(인터페이스)에 의존해야 한다. 목적: 결합도를 낮추고, 테스트 용이성 및 확장성을 높인다. 횡단 관심사 (Cross-cutting Concerns) 정의: 로깅, 보안, 트랜잭션 등 여러 모듈에 걸쳐 반복되는 공통 기능. 목적: SoC를 유지하기 위해 AOP, 데코레이터, 미들웨어 등을 활용해 분리한다. 모듈화 (Modularity) 정의: 시스템을 독립적이고 교체 가능한 단위(모듈)로 분할하는 설계 방식. 목적: 재사용성, 병렬 개발, 테스트 용이성 및 유지보수성을 향상시킨다. 명령-쿼리 분리 원칙 (Command-Query Separation, CQS) 정의: 메서드는 명령(상태를 변경) 또는 쿼리(값을 반환) 중 하나만 수행해야 하며, 둘 다 하면 안 된다. 목적: 부작용을 명확히 분리해 코드의 예측 가능성과 테스트 용이성을 높인다. 의존성 주입 (Dependency Injection, DI) 정의: 객체가 스스로 의존성을 생성하지 않고, 외부에서 주입받는 설계 패턴. 목적: 결합도를 낮추고, 유닛 테스트와 모듈 교체를 쉽게 하며 DIP를 실현한다. 계약에 의한 설계 (Design by Contract, DbC) 정의: 함수나 메서드가 사전조건(Precondition), 사후조건(Postcondition), 불변조건(Invariant)을 명시적으로 정의하는 방식. 목적: 인터페이스의 명확한 계약을 통해 오류를 조기에 발견하고 신뢰성을 높인다. 정보 은닉 (Information Hiding) 정의: 모듈 내부 구현 세부사항을 외부에 노출하지 않고, 인터페이스만 공개하는 원칙. 목적: 구현 변경이 외부에 영향을 주지 않도록 하여 유지보수성과 안정성을 확보한다. 리스코프 치환 원칙 (Liskov Substitution Principle, LSP) 정의: 서브타입은 언제나 기반 타입으로 대체 가능해야 하며, 프로그램의 정확성을 해쳐서는 안 된다. 목적: 상속 구조의 안정성과 예측 가능성을 보장하여 다형성을 안전하게 사용할 수 있게 한다. 인터페이스 분리 원칙 (Interface Segregation Principle, ISP) 정의: 클라이언트는 자신이 사용하지 않는 인터페이스 메서드에 의존하지 않아야 한다. 목적: 불필요한 의존을 줄이고, 인터페이스를 작고 목적에 맞게 분리해 유연한 설계를 가능하게 한다. 불변성 (Immutability) 정의: 객체가 생성된 후 상태를 변경할 수 없도록 설계하는 원칙. 목적: 동시성 안정성, 부작용 방지, 디버깅 용이성, 캐시 가능성 향상. 명시적 인터페이스 (Explicit Interface) 정의: 의도를 명확히 드러내는 이름과 시그니처를 가진 인터페이스를 설계하는 방식. 목적: 코드의 자기 문서화(self-documenting)를 통해 가독성과 협업 효율성을 높인다. 컨텍스트 경계 (Bounded Context) — 도메인 주도 설계(DDD)에서 정의: 도메인 모델이 일관된 의미와 규칙을 갖는 하나의 경계된 영역. 목적: 복잡한 시스템에서 모델의 명확성과 일관성을 유지하고, 마이크로서비스 간 경계를 정의한다. 💡 선택적 추가 팁 SOLID 원칙 전체를 강조하려면, 현재 목록에 OCP, LSP, ISP를 추가하는 것이 자연스럽습니다 (SRP, DIP는 이미 있음). 테스트 용이성, 병렬 처리, 마이크로서비스 아키텍처와 관련된 개념이라면 불변성, DI, Bounded Context가 특히 유용합니다. 함수형 프로그래밍과 접목한다면 CQS, 불변성, 명시적 인터페이스가 중요해집니다. autocomplete 기반 개입 : 최소 개입 최소 개입 틀을 사람이 짜고 각 컴포넌트와 같은 것을 llm 으로 웹 프론트의 (router.js, eventBus.js), 웹 백엔드 (application.propertices, interface) 와 같은 어플리케이션에 광범위한 영향을 줄수 있는 것을 사람이 통제한다 틀을 자연어 문서(markdown) 로 만들고 전부 llm 으로 단순히 어떤 앱을 만들어라고 명령 전부 llm ai 가 생성한 코드가 이상하더라도 치명적일 수 없도록 권한을 통제한다 생성시 정확한 가이드라인을 제시 생성시 관련 링크 이해 부채: LLM이 만든 코드가 남기는 시한폭탄 => https://news.hada.io/topic?id=23384 AI 에이전트를 위한 효과적 컨텍스트 엔지니어링 => https://news.hada.io/topic?id=23483",
      "frontmatter": {
        "tags": [
          "thinking"
        ],
        "date": "2025-10-08T04:59:51+09:00",
        "lastmod": "2025-10-13T13:22:04+09:00"
      }
    },
    "수식 테스트": {
      "path": "/수식-테스트/",
      "filename": "수식 테스트",
      "content": "수식 테스트 문서 이 문서는 KaTeX 수식 렌더링을 테스트하기 위한 문서입니다. 인라인 수식 여기서 $x = 2$ 는 간단한 인라인 수식입니다. 피타고라스 정리: $a^2 + b^2 = c^2$ 제곱근: $\\sqrt{x + y}$ 분수: $\\frac{x}{y}$ 블록 수식 가우스 정규분포: $$ f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$ 적분: $$ \\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi} $$ 행렬: $$ \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} $$ 복잡한 수식 오일러의 공식: $$ e^{i\\pi} + 1 = 0 $$ 리만 제타 함수: $$ \\zeta(s) = \\sum_{n=1}^{\\infty} \\frac{1}{n^s} $$ 수식과 텍스트의 혼합 변수 $x$가 주어졌을 때, 함수 $f(x) = x^2 + 2x + 1$의 도함수는 다음과 같습니다: $$ f'(x) = \\frac{d}{dx}(x^2 + 2x + 1) = 2x + 2 $$ 따라서 $x = 1$에서의 기울기는 $f'(1) = 4$입니다. 화학식 (KaTeX 확장 기능) 물의 화학식: $\\ce{H2O}$ 메탄 연소 반응: $\\ce{CH4 + 2O2 -> CO2 + 2H2O}$ 특수 기호 그리스 문자: $\\alpha, \\beta, \\gamma, \\delta, \\epsilon, \\zeta, \\eta, \\theta$ 화살표: $\\rightarrow, \\leftarrow, \\leftrightarrow, \\Rightarrow, \\Leftarrow, \\Leftrightarrow$ 집합 기호: $\\in, \\notin, \\subset, \\supset, \\cup, \\cap, \\emptyset$ 논리 기호: $\\land, \\lor, \\neg, \\forall, \\exists$",
      "frontmatter": {
        "date": "2025-09-10T12:34:37+09:00",
        "lastmod": "2025-09-11T06:53:53+09:00"
      }
    },
    "영어 파트너 챗 질문지 목록": {
      "path": "/영어-파트너-챗-질문지-목록/",
      "filename": "영어 파트너 챗 질문지 목록",
      "content": "What is something you are mad about? I really enjoy getting enough sleep. why if i get enough sleep and wake up I feel great the next day. What kind of person are you? Actually, I’m not sure exactly what kind of person I am yet, but I know that what I want to be, i went to be easygoing! because Being too serious feels exhausting. sometime i asked for me Could I become more relaxed? It might take time, but I think it’s worth it. When was the last time you felt happy or angry? Can you tell me about it? Yesterday! It was the weekend, so I spent the whole day sleeping. I felt happy just being lazy. Thank goodness for rest—I think that’s something we all need. What’s it gonna take for the good life? I hate having to hurry because of unexpected things. i think good life means staying calm and peaceful most of the time, without constant interruptions. I don’t think money is directly but it still seems like a pretty important factor. what do you think about that? money isn’t everything, but having at least the minimum is essential. but It doesn’t create happiness, it helps avoid stress—so it matters, just not as the main thing. Is there someone you know who seems to be living a really good life? If so, tell me about them! Yes! It’s my friend. He’s really good at keeping stress low because he sees himself objectively—never letting emotions take over. Happiness just seems to come easily to him. I don’t want to be exactly, but I’d love to have that mindset—and honestly, it’s something that can be learned. What sports do you like the most? my favorite sport is soccer second is running Why? Because Soccer is a sport that has a larger market than most other sports, it is many exciting things to watch which is leagues like the EPL, Bundesliga and La Liga, and on the international side, the World Cup. world cup is the most-watched event in human history. isn’t it? What skills are important to play that sport well? (1번에서 말한 스포츠에 대해) i think short burst speed is most important to play soccer. of course stamina is also important but when you're under pressure from an opponent, the most important skill is short-speed Which sport do you think is the most difficult or competitive? i think boxing is most difficult sports Why? I’ve seen this information before—boxing has the highest heart rate of any sport. I even visited a boxing gym once, and it was incredibly hard.",
      "frontmatter": {
        "date": "2025-09-29T09:14:09+09:00",
        "lastmod": "2025-10-27T09:29:25+09:00"
      }
    },
    "코어 덤프(Core Dump)": {
      "path": "/코어-덤프core-dump/",
      "filename": "코어 덤프(Core Dump)",
      "content": "코어 덤프(Core Dump)란 무엇인가? - \"프로세스의 마지막 순간을 담은 사진\" 코어 덤프(Core Dump)는 프로그래밍과 시스템 관리에서 매우 중요한 개념입니다. 가장 쉽게 비유하자면, 코어 덤프는 특정 프로그램(프로세스)이 비정상적으로 종료되는 바로 그 순간의 메모리 상태를 그대로 찍어 저장한 \"스냅샷 파일\"입니다. 마치 비행기 사고 시 원인 분석을 위해 블랙박스를 회수하는 것과 같습니다. \"코어(Core)\" 라는 이름의 유래 이름이 조금 생소할 수 있습니다. \"코어\"라는 단어는 현대의 CPU 코어를 의미하는 것이 아닙니다. 이는 아주 오래전, 컴퓨터의 주기억장치로 자기 코어 메모리(Magnetic Core Memory)를 사용하던 시절에서 유래했습니다. 당시 메모리의 내용을 파일로 '쏟아낸다(dump)'는 의미에서 '코어 덤프'라는 용어가 탄생했고, 메모리 기술이 반도체로 바뀐 지금까지도 그 이름이 그대로 사용되고 있습니다. 코어 덤프는 언제 생성되나요? 코어 덤프는 주로 프로그램이 심각한 오류를 만나 더 이상 실행을 계속할 수 없을 때, 운영체제(커널)에 의해 생성됩니다. 앞서 살펴본 신호(Signal) 목록에서 \"Default Action\"이 create core image 또는 Core 로 표시된 신호를 받았을 때가 바로 그 경우입니다. 대표적인 예시는 다음과 같습니다. SIGSEGV (Segmentation Violation): 허용되지 않은 메모리 영역에 접근하려고 할 때 (가장 흔한 원인). SIGILL (Illegal Instruction): CPU가 이해할 수 없는 잘못된 명령어를 실행하려고 할 때. SIGFPE (Floating-Point Exception): 0으로 나누는 등 잘못된 부동소수점 연산을 수행할 때. SIGABRT (Abort): 프로그램이 스스로 abort() 함수를 호출하여 비정상 종료를 선택했을 때. SIGQUIT (Quit): 사용자가 의도적으로 디버깅을 위해 Ctrl+\\ 키를 눌러 코어 덤프를 생성하며 종료시키고자 할 때. 반면, SIGTERM 이나 SIGINT ( Ctrl+C ) 같은 신호는 프로그램에게 \"정리하고 정상적으로 종료하라\"고 요청하는 신호이므로, 기본적으로는 코어 덤프를 생성하지 않습니다. 코어 덤프 파일에는 무엇이 들어있나요? 코어 덤프 파일은 단순한 텍스트 파일이 아니라, 특정 구조를 가진 바이너리 파일입니다. 여기에는 오류 분석에 필요한 거의 모든 정보가 담겨 있습니다. 프로세스 메모리 이미지 (Process Memory Image): 스택(Stack): 함수 호출 기록, 지역 변수 등의 정보가 담겨 있습니다. 충돌이 발생한 지점까지 어떤 함수들이 어떤 순서로 호출되었는지 추적하는 데 결정적입니다. 힙(Heap): 동적으로 할당된 메모리 영역의 데이터가 들어있습니다. 전역 변수(Global Variables) 및 정적 변수(Static Variables): 프로그램 전역에서 사용되는 변수들의 마지막 상태를 알 수 있습니다. CPU 레지스터 상태 (CPU Registers Status): 프로그램 카운터 (Program Counter, PC 또는 IP): 오류가 발생한 정확한 코드 라인(기계어 명령어 주소)을 가리킵니다. 스택 포인터 (Stack Pointer, SP): 현재 스택의 최상단 위치를 가리킵니다. 범용 레지스터 (General-Purpose Registers): 연산에 사용되던 값들이 그대로 저장되어 있어, 오류 직전의 계산 상태를 파악할 수 있습니다. 프로세스 상태 정보: 프로세스 ID(PID), 사용자 ID(UID) 등 프로세스 기본 정보. 코어 덤프를 유발한 신호(Signal)의 번호. 코어 덤프의 주된 용도: 사후 분석 디버깅 (Post-mortem Debugging) 코어 덤프의 존재 이유는 단 하나, \"사후 분석 디버깅\"을 위해서입니다. 프로그램이 고객 환경이나 운영 서버처럼 개발 환경이 아닌 곳에서 예기치 않게 죽었을 때, 개발자는 실시간으로 디버깅을 할 수 없습니다. 이때 서버에 남겨진 코어 덤프 파일을 가져오면, 마치 프로그램이 방금 죽은 그 순간으로 시간을 되돌려 현장을 조사하는 것처럼 디버깅을 할 수 있습니다. 주로 gdb (GNU Debugger, Linux에서 주로 사용)나 lldb (LLVM Debugger, macOS에서 주로 사용)와 같은 디버거를 사용하여 코어 덤프 파일을 분석합니다. 예를 들어, 다음과 같은 명령어로 분석을 시작할 수 있습니다. # gdb <실행 파일> <코어 덤프 파일> gdb ./my_program core 디버거로 코어 덤프를 열면 개발자는 다음과 같은 강력한 분석을 할 수 있습니다. 백트레이스(Backtrace) 확인: bt 명령어를 통해 프로그램이 어떤 함수 호출 순서를 거치다가 죽었는지 한눈에 파악할 수 있습니다. (가장 먼저 하는 일) 변수 값 확인: 특정 시점의 변수들이 어떤 값을 가지고 있었는지 확인할 수 있습니다. 메모리 상태 조사: 특정 메모리 주소에 어떤 데이터가 있었는지 직접 들여다볼 수 있습니다. 레지스터 값 확인: CPU 레지스터 값을 통해 저수준의 연산 상태를 분석할 수 있습니다. 실용적인 정보 및 주의사항 파일 크기: 코어 덤프는 프로세스가 사용하던 메모리 전체를 저장하므로, 파일 크기가 매우 클 수 있습니다 (수백 MB ~ 수십 GB). 이 때문에 기본적으로 시스템에서 코어 덤프 생성이 비활성화되어 있는 경우가 많습니다. 활성화 방법: 쉘에서 ulimit -c unlimited 명령어를 사용하면 현재 세션에서 코어 덤프 파일 크기 제한을 풀어 생성을 활성화할 수 있습니다. ( ulimit -c 0 은 비활성화) 보안: 코어 덤프 파일에는 메모리의 모든 내용, 즉 비밀번호, 개인 키, 고객 정보 등 매우 민감한 데이터가 평문으로 포함될 수 있습니다. 따라서 코어 덤프 파일은 매우 신중하게 관리해야 하며, 외부 유출에 각별히 주의해야 합니다. 파일 이름 및 위치: 생성되는 코어 덤프 파일의 이름은 보통 core 또는 core.[PID] 형식이며, 파일의 이름과 저장 위치는 운영체제 설정을 통해 변경할 수 있습니다 (Linux의 경우 /proc/sys/kernel/core_pattern ). 요약 코어 덤프는 프로그램의 비정상 종료 시점의 메모리 상태, 레지스터 값 등을 담고 있는 파일로, '사후 분석 디버깅'을 위한 핵심적인 단서입니다. 이를 통해 개발자는 재현하기 어려운 버그의 원인을 정확하고 효율적으로 찾아낼 수 있습니다.",
      "frontmatter": {
        "tags": [
          "ai-content",
          "system-programing"
        ],
        "date": "2025-09-09T23:13:33+09:00",
        "lastmod": "2025-10-19T16:12:19+09:00"
      }
    },
    "클라이언트 어플리케이션 구조 변화": {
      "path": "/클라이언트-어플리케이션-구조-변화/",
      "filename": "클라이언트 어플리케이션 구조 변화",
      "content": "클라이언트 아키텍처의 위대한 진화: 콜백 지옥에서 선언형 UI까지 클라이언트 애플리케이션, 즉 사용자와 직접 상호작용하는 모든 소프트웨어(데스크톱 프로그램, 모바일 앱)의 개발 역사는 본질적으로 '상태(State)와의 전쟁'의 역사입니다. 사용자의 예측 불가능한 입력, 네트워크의 비동기적 응답, 시스템의 예기치 않은 이벤트 속에서 화면은 끊임없이 변화해야 합니다. 로딩 중, 에러 발생, 데이터 목록의 유무, 버튼의 활성화/비활성화 등, 복잡하게 얽히고설킨 상태를 어떻게 하면 ▲안정적이고 ▲테스트 가능하며 ▲유지보수하기 쉽게 관리할 수 있을까? 이 거대한 질문에 답하기 위해 아키텍처는 수십 년에 걸쳐 놀랍도록 유사한 궤적을 그리며 진화해왔습니다. 데스크톱 GUI 프로그래밍의 오랜 역사와 모바일 앱 개발의 역사는 사용하는 기술과 용어는 다를지라도, 근본적으로 같은 문제를 해결하기 위한 여정이었으며, 이는 특정 플랫폼을 넘어선 소프트웨어 공학의 보편적 원리가 존재함을 증명합니다. 이 진화의 과정은 크게 네 단계로 나눌 수 있습니다. 제1단계: 원시 시대의 혼돈, 콜백 지옥 (Callback Hell) 제2단계: 최초의 질서와 거대해진 중재자, MVC (Model-View-Controller) 제3단계: 테스트 가능성을 향한 여정, MVP (Model-View-Presenter) 제4단계: 상태 관리의 자동화와 선언형 패러다임의 완성, MVVM (Model-View-ViewModel) 이제 각 단계를 심층적으로 탐험해 보겠습니다. 제1장: 원시 시대의 혼돈, 콜백 지옥 (Callback Hell) GUI 프로그래밍의 가장 초기 형태는 아키텍처라는 개념 없이, 단순히 이벤트와 그 이벤트를 처리할 함수를 직접 연결하는 방식이었습니다. C/C++ 기반의 순수 Win32 API나 초기 GTK, X-Window 프로그래밍이 이에 해당하며, 이는 모바일 시대의 '거대 뷰 컨트롤러'가 겪는 문제의 원형과 정확히 일치합니다. 핵심 구조 이 시대의 구조는 '구조가 없는 것'이 특징입니다. 구성 요소는 단 두 가지뿐입니다. UI 요소 (View): CreateWindow() , gtk_button_new() 같은 API를 통해 프로그래밍 방식으로 생성된 버튼, 텍스트 상자 등의 화면 요소. 콜백 함수 (Callback Function): 특정 UI 요소에서 특정 이벤트(클릭, 키 입력 등)가 발생했을 때 운영체제나 GUI 시스템에 의해 호출되도록 등록된 함수. 사실상 모든 로직이 이 함수 안에 담깁니다. 이 구조에는 역할 분리라는 개념이 존재하지 않습니다. 모든 것은 '이벤트가 발생하면 이 함수를 실행하라'는 단순한 명제로 귀결됩니다. 동작 방식 개발자는 화면에 필요한 UI 요소들을 코드로 생성하고 배치합니다. 각 UI 요소의 특정 이벤트(예: login_button 의 clicked 이벤트)에 처리 함수( on_login_button_clicked )를 직접 연결(등록)합니다. 사용자가 버튼을 클릭하면, GUI 시스템은 등록된 콜백 함수를 호출합니다. (문제의 지점) 호출된 콜백 함수 내부에서 관련된 모든 작업이 순서 없이, 구분 없이 일어납니다. 다른 View에서 데이터 읽기: 아이디 입력창( email_entry )과 비밀번호 입력창( password_entry )에서 텍스트 값을 직접 가져옵니다. 비즈니스 로직 수행: 가져온 값으로 유효성을 검사하거나, 데이터베이스에 접근하거나, 네트워크 요청을 보내는 등의 핵심 로직을 실행합니다. 다른 View의 상태 변경: 로직 처리 결과에 따라, 상태 메시지를 표시하는 레이블( status_label )의 텍스트를 바꾸거나, 다른 버튼을 비활성화시키는 등 화면의 다른 부분들을 직접 제어합니다. 콜백 방식을 구성하는 핵심 기술들 이 원시적인 방식은 프레임워크의 가장 기본적인 기능들에 의존합니다. 이벤트 루프 (Event Loop): 모든 GUI 시스템의 심장입니다. 운영체제로부터 마우스 클릭, 키보드 입력, 타이머 등의 이벤트를 지속적으로 받아와 큐에 저장하고, 이를 순차적으로 처리하여 적절한 콜백 함수를 호출하는 무한 루프입니다. 개발자가 직접 제어하기보다는 시스템이 제공하는 기반 위에서 동작합니다. 함수 포인터 (Function Pointers) / 시그널과 슬롯 (Signals & Slots)의 원형: 이벤트와 처리 함수를 연결하는 '접착제' 역할을 합니다. C언어에서는 함수 포인터를 직접 전달하여 \"이 이벤트가 발생하면, 이 메모리 주소에 있는 함수를 실행해\"라고 알려줍니다. GTK의 g_signal_connect 는 이러한 함수 포인터 기반의 연결을 더 체계적으로 만든 원시적인 시그널-슬롯 시스템으로 볼 수 있습니다. 실제 프레임워크 예시: GTK (GIMP Toolkit) GTK는 C언어 기반의 대표적인 GUI 툴킷으로, 초기 콜백 방식의 문제점을 명확하게 보여줍니다. 아래는 로그인 버튼 클릭 시의 동작을 콜백 함수로 구현한 상세 예시입니다. #include <gtk/gtk.h> // 데이터베이스나 파일에서 사용자 정보를 검증하는 순수한 비즈니스 로직 함수 (가정) gboolean perform_login(const gchar *email, const gchar *password) { // 실제로는 파일 I/O, DB 조회 등의 로직이 들어감 if (g_strcmp0(email, \"test@example.com\") == 0 && g_strcmp0(password, \"password123\") == 0) { return TRUE; } return FALSE; } // 로그인 버튼이 클릭되었을 때 호출될 콜백 함수 // 이 함수 하나에 모든 책임이 집중되어 있습니다. void on_login_button_clicked(GtkButton *button, gpointer user_data) { // user_data를 통해 메인 윈도우의 다른 위젯(UI 요소)들에 접근합니다. // 이는 이 함수가 다른 UI 요소들의 존재와 구조를 모두 알아야 함을 의미합니다. (강한 결합) GtkWidget *email_entry = GTK_WIDGET(g_hash_table_lookup(GTK_HASH_TABLE(user_data), \"email_entry\")); GtkWidget *password_entry = GTK_WIDGET(g_hash_table_lookup(GTK_HASH_TABLE(user_data), \"password_entry\")); GtkWidget *status_label = GTK_WIDGET(g_hash_table_lookup(GTK_HASH_TABLE(user_data), \"status_label\")); // 1. 다른 View(위젯)에서 직접 데이터를 읽어옴 const gchar *email = gtk_entry_get_text(GTK_ENTRY(email_entry)); const gchar *password = gtk_entry_get_text(GTK_ENTRY(password_entry)); // 2. 비즈니스 로직을 직접 호출하여 수행 gboolean is_login_successful = perform_login(email, password); // 3. 비즈니스 로직의 결과에 따라 다른 View(위젯)의 상태를 직접 변경 if (is_login_successful) { gtk_label_set_text(GTK_LABEL(status_label), \"Login Successful! Welcome.\"); gtk_widget_set_sensitive(GTK_WIDGET(button), FALSE); // 로그인 버튼 비활성화 } else { gtk_label_set_text(GTK_LABEL(status_label), \"Error: Invalid email or password.\"); } } int main(int argc, char *argv[]) { // ... GTK 초기화 및 윈도우, 버튼, 입력창 등 위젯 생성 코드 ... // 이벤트와 콜백 함수를 직접 연결하는 부분 // \"login_button\"에서 \"clicked\" 시그널(이벤트)이 발생하면, // G_CALLBACK 매크로를 통해 on_login_button_clicked 함수를 호출하도록 설정합니다. g_signal_connect(login_button, \"clicked\", G_CALLBACK(on_login_button_clicked), app_widgets_hashtable); // ... } 이 방식의 치명적인 문제점은 명확합니다. 스파게티 코드 (Spaghetti Code): on_login_button_clicked 함수는 UI 데이터 접근, 비즈니스 로직, UI 상태 업데이트라는 세 가지 다른 종류의 책임을 모두 떠안고 있습니다. 애플리케이션이 복잡해지면 수십 개의 콜백 함수가 서로의 상태를 읽고 변경하면서 거미줄처럼 얽히게 되어, 코드의 흐름을 추적하는 것이 불가능에 가까워집니다. 재사용 불가능성: '로그인'이라는 핵심 비즈니스 로직은 on_login_button_clicked 라는 특정 UI 이벤트 처리 함수 내부에 갇혀 있습니다. 만약 다른 곳(예: 자동 로그인)에서 이 로직을 재사용하려면 코드를 복사-붙여넣기 하거나, 로직을 분리하기 위해 복잡한 리팩토링을 거쳐야 합니다. 테스트 불가능성: perform_login 함수 자체는 테스트할 수 있을지 몰라도, 이메일/비밀번호 형식에 따라 로그인 버튼이 활성화/비활성화되는 로직을 테스트하려면 어떻게 해야 할까요? 이 로직은 gtk_entry_get_text 나 gtk_label_set_text 같은 GTK UI 함수에 직접적으로 의존하므로, GUI 환경을 실제로 실행하지 않고서는 단위 테스트를 작성하는 것이 거의 불가능합니다. 이러한 문제들은 필연적으로 아키텍처의 필요성을 낳았고, 그 첫 번째 대답이 바로 MVC였습니다. 제2장: 최초의 질서와 거대해진 중재자, MVC (Model-View-Controller) MVC 패턴은 애플리케이션의 구성 요소를 세 가지 역할(Model, View, Controller)로 분리하려는 최초의 체계적인 시도였습니다. Apple과 Google이 초기에 iOS와 안드로이드 개발의 기본 패턴으로 제시하면서 모바일 개발의 시작을 열었지만, 이론의 우아함과 달리 실제 환경에서는 콜백 지옥의 문제가 다른 형태로 나타나는 한계를 보였습니다. 핵심 구조 Model: 애플리케이션의 데이터와 비즈니스 로직을 담당합니다. 데이터가 무엇인지, 어떻게 저장되고 처리되는지에 대한 규칙을 포함합니다. (예: User 객체, AuthService 클래스) View: 사용자에게 보여지는 UI 요소를 담당합니다. (예: iOS의 UIView , 안드로이드의 XML Layout 과 View 객체들) 사용자의 입력을 감지하여 Controller에게 전달하는 역할을 합니다. Controller: Model과 View 사이의 중재자입니다. View로부터 사용자 입력을 받아 Model에 변경을 요청하고, Model의 데이터가 변경되면 그 결과를 가져와 View에 어떻게 표시할지 결정합니다. 동작 방식 사용자가 View(예: 로그인 버튼)를 터치합니다. View는 이벤트를 감지하고 이를 Controller에게 전달합니다. (예: @IBAction 메소드 호출) Controller는 이벤트에 맞는 로직을 수행하기 위해 Model에게 데이터 처리를 요청합니다. (예: authService.login(...) 호출) Model은 비즈니스 로직을 수행하고, 그 결과를 Controller에게 돌려줍니다. (예: 로그인 성공/실패 결과 반환) (문제의 지점) Controller는 Model로부터 받은 데이터를 가공하여 직접 View 객체의 속성을 변경함으로써 화면을 업데이트합니다. (예: nameLabel.text = \"홍길동\" , loginButton.isEnabled = true ) MVC 패턴을 구성하는 핵심 기술들 MVC는 객체 지향 프로그래밍의 개념을 기반으로, 플랫폼별로 다음과 같은 기술을 통해 구현됩니다. 객체 지향 프로그래밍 (OOP): MVC 자체가 역할과 책임을 객체 단위로 나눈 고전적인 디자인 패턴입니다. 각 구성 요소를 클래스로 정의하고 메시지를 통해 상호작용하는 OOP의 기본 원칙을 따릅니다. 타겟-액션 (Target-Action) 매커니즘 (iOS): 콜백 방식보다 조금 더 구조화된 이벤트 처리 방식입니다. 이벤트가 발생한 객체(예: UIButton )가 지정된 '타겟'( Target , 주로 Controller)에게 '액션'( Action , 특정 메소드)을 실행하라는 메시지를 보냅니다. @IBAction 은 이 매커니즘을 시각적으로 연결해주는 기능입니다. XML 레이아웃과 ID 참조 (Android): 안드로이드에서는 XML 로 View의 구조를 정의하고, 각 View 요소에 고유한 ID 를 부여합니다. Controller 역할을 하는 Activity 나 Fragment 는 findViewById 또는 최신의 View Binding 같은 기술을 사용하여 ID 를 통해 View 객체에 대한 직접적인 참조를 얻어와 제어합니다. 실제 프레임워크 예시: iOS의 Massive View Controller iOS의 UIViewController 는 이름 그대로 Controller의 역할을 하지만, 실제로는 View의 생명주기(viewDidLoad, viewWillAppear 등)를 관리하고 View 객체들을 직접 소유( @IBOutlet )하는 등 View의 역할도 겸합니다. 이로 인해 View와 Controller의 분리가 모호해지고 모든 책임이 UIViewController 로 집중되는 '거대 뷰 컨트롤러' 문제가 발생합니다. 아래는 iOS MVC 패턴의 문제점을 보여주는 상세한 코드 예시입니다. // iOS의 Massive View Controller 예시 import UIKit // Model 레이어 (가정) class AuthService { func login(email: String, password: String, completion: (Result<Bool, Error>) -> Void) { // 실제로는 네트워크 통신이 일어남 if email == \"test@example.com\" && password.count >= 8 { completion(.success(true)) } else { completion(.failure(NSError(domain: \"AuthError\", code: 401))) } } } class LoginViewController: UIViewController { // View 요소들을 Controller가 직접 소유하고 있습니다. (View와 Controller의 강한 결합) @IBOutlet weak var emailTextField: UITextField! @IBOutlet weak var passwordTextField: UITextField! @IBOutlet weak var loginButton: UIButton! // Model(Service)을 Controller가 직접 소유 let authService = AuthService() // View의 생명주기 관리 책임 override func viewDidLoad() { super.viewDidLoad() // View의 초기 상태를 Controller가 직접 설정 loginButton.isEnabled = false loginButton.backgroundColor = .gray } // View의 이벤트를 Controller가 직접 처리 @IBAction func emailDidChange(_ sender: UITextField) { validateInput() } @IBAction func passwordDidChange(_ sender: UITextField) { validateInput() } // 이 함수 하나에 프레젠테이션 로직과 View 제어 로직이 뒤섞여 있습니다. private func validateInput() { // 1. 프레젠테이션 로직 (어떻게 보여줄지를 결정하는 로직) let isEmailValid = emailTextField.text?.contains(\"@\") ?? false let isPasswordValid = (passwordTextField.text?.count ?? 0) >= 8 // 2. View 제어 로직 (Controller가 View를 직접 제어) if isEmailValid && isPasswordValid { loginButton.isEnabled = true loginButton.backgroundColor = .blue } else { loginButton.isEnabled = false loginButton.backgroundColor = .gray } } // View의 이벤트를 처리하며 비즈니스 로직과 화면 전환 로직까지 담당합니다. @IBAction func loginButtonTapped(_ sender: UIButton) { // 1. 비즈니스 로직 호출 authService.login(email: emailTextField.text!, password: passwordTextField.text!) { [weak self] result in DispatchQueue.main.async { // UI 업데이트는 메인 스레드에서 // 2. Controller가 직접 View를 제어 (화면 전환, 얼럿 표시 등) switch result { case .success: // 화면 전환 로직... print(\"로그인 성공, 메인 화면으로 이동합니다.\") case .failure: // 얼럿(Alert)을 띄우는 View 로직... let alert = UIAlertController(title: \"로그인 실패\", message: \"아이디나 비밀번호를 확인해주세요.\", preferredStyle: .alert) alert.addAction(UIAlertAction(title: \"확인\", style: .default)) self?.present(alert, animated: true) } } } } } 이 구조는 콜백 지옥보다 역할이 조금 분리되었을 뿐, 본질적인 문제들은 그대로 남아있습니다. 과도한 책임 (Overloaded Responsibility): LoginViewController 는 View의 생명주기 관리, 사용자 입력 처리, 입력값 유효성 검사(프레젠테이션 로직), View의 속성 업데이트(View 제어), 비즈니스 로직 호출, 화면 전환, 에러 처리(얼럿 표시) 등 너무나 많은 책임을 집니다. 클래스가 수백, 수천 줄로 비대해져 유지보수가 재앙에 가까워집니다. 강한 결합 (Tight Coupling): Controller는 @IBOutlet 을 통해 View의 구체적인 클래스( UITextField , UIButton )를 직접 알고 그 속성( text , isEnabled )을 변경합니다. 만약 디자이너가 UIButton 을 커스텀 View로 교체한다면, Controller의 코드까지 수정해야 합니다. View와 Controller가 한 몸처럼 묶여있어 분리가 불가능합니다. 테스트의 어려움: validateInput 함수의 로직, 즉 \"이메일에 @가 포함되고 비밀번호가 8자 이상이면 로그인 버튼이 활성화된다\"는 중요한 정책을 테스트하고 싶다고 가정해 봅시다. 이 로직은 emailTextField.text 와 loginButton.isEnabled 같은 UIKit 프레임워크의 구체적인 컴포넌트에 의존합니다. 따라서 이 로직을 검증하려면 iOS 시뮬레이터를 띄우고 UI 테스트를 실행해야만 합니다. 순수한 로직만 분리하여 빠르게 실행할 수 있는 단위 테스트(Unit Test)가 거의 불가능합니다. 결국 MVC는 '테스트'와 '역할 분리'라는 과제를 남겼고, 이를 해결하기 위해 MVP가 등장하게 됩니다. 제3장: 테스트 가능성을 향한 여정, MVP (Model-View-Presenter) MVP(Model-View-Presenter) 패턴은 MVC의 '거대 컨트롤러'와 '테스트 불가' 문제를 정면으로 해결하기 위해 등장했습니다. 그 핵심 철학은 View와 로직의 완벽한 분리에 있으며, 이를 위해 '인터페이스(프로토콜)'를 통한 의존성 역전 원칙을 사용합니다. 핵심 구조 Model: MVC와 동일합니다. View: 이제 완전히 수동적이고 '멍청한(Dumb)' 존재가 됩니다. 자신의 역할은 오직 화면을 그리고, 사용자 이벤트를 Presenter에게 전달하는 것뿐입니다. View는 로직을 갖지 않습니다. 중요한 점은, View가 지켜야 할 규칙(인터페이스/프로토콜)을 구현한다는 것입니다. (안드로이드의 Activity / Fragment , iOS의 UIViewController 가 이 역할을 수행합니다.) Presenter: View와 Model 사이의 진정한 중재자. 모든 프레젠테이션 로직(데이터를 어떻게 보여줄지 결정하는 로직)을 담당합니다. 가장 큰 특징은 Presenter가 View의 구체적인 클래스를 전혀 알지 못하고, 오직 View가 구현한 인터페이스에만 의존한다는 것입니다. 이 덕분에 Presenter는 UI 프레임워크로부터 완벽하게 독립될 수 있습니다. 동작 방식 사용자가 View(버튼)를 터치합니다. View는 어떤 로직도 수행하지 않고, 자신이 소유한 Presenter의 메소드를 호출하여 이벤트를 그대로 전달합니다. ( presenter.loginButtonTapped() ) Presenter는 Model에게 데이터를 요청하고 결과를 받습니다. Presenter는 받은 데이터를 가공하여 View에 어떻게 표시할지 결정합니다. Presenter는 자신이 알고 있는 View 인터페이스의 메소드를 호출하여 View에게 \"무엇을 그릴지 명령\"합니다. ( view.enableLoginButton() , view.showLoginError(message: \"에러 발생\") ) View는 이 명령에 따라 실제 UI를 업데이트합니다. MVP 패턴을 완성하는 핵심 기술들 MVP의 정수는 특정 프레임워크 기능이 아닌, 소프트웨어 공학 원칙의 적용에 있습니다. 인터페이스 (Interfaces) / 프로토콜 (Protocols): MVP를 가능하게 하는 가장 핵심적인 기술입니다. Presenter가 View의 구체적인 클래스( LoginViewController )에 의존하는 대신, 추상적인 규칙의 집합( LoginView 프로토콜)에 의존하게 만듭니다. 이는 '의존성 역전 원칙(Dependency Inversion Principle)'의 대표적인 예시로, 이로 인해 Presenter와 View의 결합이 끊어지고 Presenter를 UI 프레임워크로부터 완전히 분리할 수 있게 됩니다. 모의 객체 (Mock Objects)와 의존성 주입 (Dependency Injection): 인터페이스 덕분에 단위 테스트가 가능해집니다. 테스트 코드에서는 실제 View( LoginViewController ) 대신, LoginView 프로토콜을 따르는 가짜 객체( MockLoginView )를 만들어 Presenter에 주입( Injection )할 수 있습니다. 이 모의 객체는 Presenter가 자신에게 내린 명령( enableLoginButton 이 호출되었는가?)을 기록하고, 테스트는 이 기록을 검증함으로써 Presenter의 로직이 올바른지 확인할 수 있습니다. 실제 프레임워크 예시: Swift를 이용한 MVP 구현 MVP 패턴의 정수는 인터페이스를 통한 역할 분리에 있습니다. 아래 코드는 MVC 예제를 MVP로 리팩토링한 상세 버전입니다. import UIKit // Model 레이어는 동일 (AuthService) // --- MVP의 핵심: View와 Presenter 사이의 계약(Contract) 정의 --- // 1. View가 따라야 할 규칙 (인터페이스/프로토콜) // Presenter가 View에게 내릴 수 있는 명령들의 목록입니다. protocol LoginView: AnyObject { func enableLoginButton() func disableLoginButton() func showLoginError(message: String) func navigateToMainScreen() func showLoadingIndicator() func hideLoadingIndicator() } // 2. Presenter (UI 프레임워크 코드 없음! -> import UIKit 불필요) // 이 클래스는 순수한 Swift 코드로만 작성되어 단위 테스트가 매우 용이합니다. class LoginPresenter { // Presenter는 구체적인 ViewController가 아닌, LoginView 프로토콜에만 의존합니다. private weak var view: LoginView? private let authService = AuthService() init(view: LoginView) { self.view = view } // 프레젠테이션 로직이 Controller에서 Presenter로 이동했습니다. func validateInput(email: String?, password: String?) { let isEmailValid = email?.contains(\"@\") ?? false let isPasswordValid = (password?.count ?? 0) >= 8 if isEmailValid && isPasswordValid { // View에게 \"버튼을 활성화하라\"고 명령합니다. view?.enableLoginButton() } else { // View에게 \"버튼을 비활성화하라\"고 명령합니다. view?.disableLoginButton() } } func loginButtonTapped(email: String?, password: String?) { guard let email = email, let password = password else { return } view?.showLoadingIndicator() // 로딩 시작 명령 authService.login(email: email, password: password) { [weak self] result in self?.view?.hideLoadingIndicator() // 로딩 종료 명령 switch result { case .success: self?.view?.navigateToMainScreen() // 화면 전환 명령 case .failure: self?.view?.showLoginError(message: \"로그인 실패\") // 에러 표시 명령 } } } } // 3. View (이제 훨씬 단순해지고, Presenter의 명령을 수행하는 역할만 합니다.) class LoginViewController: UIViewController, LoginView { // Presenter를 소유 var presenter: LoginPresenter! @IBOutlet weak var emailTextField: UITextField! @IBOutlet weak var passwordTextField: UITextField! @IBOutlet weak var loginButton: UIButton! // ... 로딩 스피너 등 추가 UI ... override func viewDidLoad() { super.viewDidLoad() // 자기 자신(LoginView를 구현한 ViewController)을 Presenter에 주입하여 생성 presenter = LoginPresenter(view: self) } // View는 이벤트를 받으면 생각하지 않고 Presenter에게 전달합니다. @IBAction func emailDidChange(_ sender: UITextField) { presenter.validateInput(email: sender.text, password: passwordTextField.text) } @IBAction func passwordDidChange(_ sender: UITextField) { presenter.validateInput(email: emailTextField.text, password: sender.text) } @IBAction func loginButtonTapped(_ sender: UIButton) { presenter.loginButtonTapped(email: emailTextField.text, password: passwordTextField.text) } // --- Presenter의 명령을 수행하는 메소드들 (LoginView 프로토콜 구현) --- func enableLoginButton() { loginButton.isEnabled = true loginButton.backgroundColor = .blue } func disableLoginButton() { loginButton.isEnabled = false loginButton.backgroundColor = .gray } func showLoginError(message: String) { let alert = UIAlertController(title: \"에러\", message: message, preferredStyle: .alert) alert.addAction(UIAlertAction(title: \"확인\", style: .default)) present(alert, animated: true) } func navigateToMainScreen() { print(\"메인 화면으로 이동합니다.\") // 화면 전환 코드 구현 } func showLoadingIndicator() { /* 로딩 스피너 보이기 */ } func hideLoadingIndicator() { /* 로딩 스피너 숨기기 */ } } MVP가 이뤄낸 성과와 새로운 문제는 다음과 같습니다. 해결된 점 (장점): 최고의 테스트 용이성: LoginPresenter 는 import UIKit 이 필요 없는 순수한 Swift 클래스입니다. 따라서 가짜 MockLoginView 객체를 만들어 주입하면, \"validateInput을 호출했을 때 view.enableLoginButton 이 정확히 호출되는가?\"와 같은 모든 로직을 시뮬레이터 없이 수 밀리초 만에 검증하는 단위 테스트가 완벽하게 가능해졌습니다. 명확한 역할 분리: Controller의 비대화 문제가 해결되었습니다. View는 UI 코드만, Presenter는 프레젠테이션 로직만 담당하게 되어 코드의 가독성과 유지보수성이 크게 향상되었습니다. 새로운 문제점: View와 Presenter의 1:1 강한 결합: 비록 인터페이스를 통해 분리했지만, 결국 Presenter는 View가 어떤 기능들을 가지고 있는지( enableLoginButton , showLoginError 등) 속속들이 알아야만 합니다. 화면이 복잡해져 View에 기능이 추가될수록, LoginView 프로토콜과 LoginPresenter 는 함께 비대해지며 서로에게 강하게 묶이는 경향이 있습니다. 반복적인 코드 (Boilerplate) 증가: showLoading() , hideLoading() , updateText(text: String) , showImage(image: UIImage) 등, 모든 사소한 UI 변경 작업을 위해 개발자는 ①프로토콜에 메소드 선언, ②Presenter에서 해당 메소드 호출, ③View에서 메소드 구현이라는 3단계의 반복적인 작업을 계속해야 합니다. 이는 개발의 피로도를 높이는 원인이 됩니다. 결국 MVP는 '테스트'라는 큰 산을 넘었지만, '반복적인 명령'이라는 새로운 과제를 남겼습니다. 이 문제를 해결하기 위해, 아키텍처는 '명령'이 아닌 '상태'에 집중하기 시작합니다. 바로 MVVM의 등장입니다. 제4장: 상태 관리의 자동화와 선언형 패러다임의 완성, MVVM (Model-View-ViewModel) MVP의 반복적인 명령과 1:1 결합 문제를 해결하기 위해 등장한 MVVM(Model-View-ViewModel)은 클라이언트 아키텍처의 패러다임을 근본적으로 바꾸었습니다. 핵심은 데이터 바인딩(Data Binding) 메커니즘을 통해, Presenter의 '수동 명령'을 '상태 변화에 대한 자동 반응'으로 전환한 것입니다. 핵심 구조 Model: MVC, MVP와 동일합니다. View: 화면 UI를 담당합니다. 가장 큰 특징은 더 이상 수동적으로 명령을 기다리지 않고, ViewModel의 상태(State)를 구독(Observe)하고 있다가, 상태가 변경되면 스스로 UI를 갱신한다는 점입니다. ViewModel: 'View를 위한 Model'이라는 의미로, View와 Model 사이의 중재자입니다. View에 표시될 모든 상태(예: 사용자 이름 텍스트, 버튼 활성화 여부)와 View가 실행할 로직(커맨드, 예: 로그인 버튼 클릭 시 실행할 로직)을 가집니다. 가장 혁신적인 부분은 ViewModel은 View의 존재 자체를 전혀 알지 못한다는 것입니다. ViewModel은 그저 자신의 상태를 변경할 뿐, 이 변화가 어떻게 UI에 반영될지는 신경 쓰지 않습니다. 동작 방식 View는 시작 시점에 자신의 UI 속성들을 ViewModel의 상태 프로퍼티에 바인딩(Binding)합니다. (예: loginButton.isEnabled 속성을 viewModel.isLoginEnabled 상태에 바인딩) 사용자가 View와 상호작용합니다. (예: 텍스트 필드에 이메일 입력) (양방향 바인딩) View에 입력된 값은 데이터 바인딩을 통해 자동으로 ViewModel의 상태 프로퍼티에 반영됩니다. ( viewModel.email = \"test@test.com\" ) ViewModel은 자신의 프로퍼티가 변경될 때마다, 관련된 다른 상태 프로퍼티를 비즈니스 로직에 따라 업데이트합니다. (예: email 과 password 상태가 변경되면, isLoginEnabled 상태가 true 또는 false 로 자동 계산됨) (단방향 바인딩) View는 ViewModel의 isLoginEnabled 상태를 구독(바인딩)하고 있으므로, 이 값이 true 로 바뀌는 것을 감지하고 스스로 버튼을 활성화시킵니다. ViewModel이 view.enableButton() 과 같은 명령을 전혀 하지 않습니다. MVVM을 완성하는 핵심 기술들 MVVM 패턴은 프레임워크나 라이브러리의 강력한 지원을 통해 진정으로 완성됩니다. 데이터 바인딩 (Data Binding): View와 ViewModel을 연결하는 '마법의 접착제'입니다. ViewModel의 데이터(상태)가 변경되면, 데이터 바인딩 라이브러리가 이를 감지하고 View의 UI 속성을 자동으로 업데이트합니다. 이를 통해 textView.setText(...) 와 같은 모든 UI 조작 코드가 사라지고, View는 \"이 UI 속성은 저 상태와 같다\"고 선언만 하면 됩니다. 반응형 프로그래밍 (Reactive Programming) 라이브러리: 데이터의 '흐름(Stream)'을 다루는 프로그래밍 패러다임입니다. 데이터가 단일 값이 아닌, 시간의 흐름에 따라 계속 변하는 스트림이라고 보고, 이 스트림을 구독(Subscribe)하거나, 여러 스트림을 조합(Combine), 변환(Map)하는 등의 작업을 통해 상태 변화를 효과적으로 전파합니다. AAC (Android Architecture Components): 구글은 MVVM 패턴을 공식적으로 지원하기 위해 AAC를 발표했습니다. ViewModel: 생명주기를 고려하여 UI 관련 데이터를 저장하고 관리합니다. 화면 회전과 같이 Activity 가 재생성되는 상황에서도 데이터를 안전하게 보존하여 상태 유지를 쉽게 만듭니다. LiveData: 관찰 가능한(Observable) 데이터 홀더 클래스입니다. 안드로이드의 생명주기를 인지하여, View가 활성 상태(STARTED, RESUMED)일 때만 데이터를 업데이트하여 메모리 누수나 비정상 종료를 방지하는 안정적인 반응형 데이터 타입입니다. SwiftUI & Combine (iOS): Apple은 SwiftUI와 Combine 프레임워크를 통해 MVVM 패턴을 네이티브 차원에서 강력하게 지원합니다. SwiftUI: UI 자체를 상태의 함수로 정의하는 선언적 UI 프레임워크입니다. 데이터(상태)가 변경되면 UI가 자동으로 다시 그려지는 구조를 가지고 있어 MVVM에 매우 적합합니다. Combine: 데이터의 변경을 처리하고 전파하기 위한 Apple의 공식 반응형 프로그래밍 프레임워크입니다. ViewModel은 @Published 프로퍼티 래퍼를 통해 데이터의 변화를 외부에 알리고(Publish), View는 이를 구독( onReceive )하여 UI를 갱신합니다. 실제 프레임워크 예시: RxSwift/Combine을 활용한 iOS의 MVVM 아래는 반응형 라이브러리인 RxSwift를 사용하여 MVVM 패턴을 구현한 상세 예시입니다. `` import UIKit import RxSwift import RxCocoa // Model 레이어는 동일 (AuthService) // 1. ViewModel (UI 프레임워크 코드 없음! -> 순수한 로직) // ViewModel은 View의 존재를 전혀 모릅니다. 오직 상태만 외부에 노출합니다. class LoginViewModel { // --- 입력 (View로부터 받는 데이터 스트림) --- // BehaviorRelay는 값을 가질 수 있고, 그 값의 변화를 스트림으로 외부에 알립니다. let email = BehaviorRelay<String>(value: \"\") let password = BehaviorRelay<String>(value: \"\") // --- 출력 (View가 구독할 상태 스트림) --- let isLoginEnabled: Observable<Bool> // 버튼 활성화 여부 상태 let loginResult: PublishSubject<Result<Void, Error>> = PublishSubject() // 로그인 결과 상태 private let authService = AuthService() private let disposeBag = DisposeBag() init() { // 'isLoginEnabled' 상태는 'email'과 'password' 상태에 의해 자동으로 결정됩니다. // 이것이 바로 '반응형 프로그래밍'의 핵심입니다. isLoginEnabled = Observable.combineLatest(email, password) { email, password in // 상태를 조합하여 새로운 상태를 파생 return email.contains(\"@\") && password.count >= 8 } } // View로부터 '로그인' 액션이 트리거될 때 호출될 함수 (커맨드) func loginButtonTapped() { let currentEmail = email.value let currentPassword = password.value authService.login(email: currentEmail, password: currentPassword) { [weak self] result in // 비즈니스 로직 수행 후, 결과 '상태'만 업데이트합니다. // View에게 어떻게 하라고 명령하지 않습니다. switch result { case .success: self?.loginResult.onNext(.success(())) case .failure(let error): self?.loginResult.onNext(.failure(error)) } } } } // 2. View (이제 ViewModel의 상태를 '바인딩'하는 역할만 합니다.) class LoginViewController: UIViewController { var viewModel = LoginViewModel() // disposeBag은 구독을 관리하여 메모리 누수를 방지합니다. let disposeBag = DisposeBag() @IBOutlet weak var emailTextField: UITextField! @IBOutlet weak var passwordTextField: UITextField! @IBOutlet weak var loginButton: UIButton! override func viewDidLoad() { super.viewDidLoad() bindViewModel() } // MVVM의 핵심. View와 ViewModel을 연결하는 과정입니다. private func bindViewModel() { // --- 입력 바인딩 (View의 이벤트 -> ViewModel의 상태) --- // emailTextField의 텍스트가 변경될 때마다 그 값을 viewModel.email에 전달(바인딩) emailTextField.rx.text.orEmpty .bind(to: viewModel.email) .disposed(by: disposeBag) // passwordTextField의 텍스트가 변경될 때마다 그 값을 viewModel.password에 전달(바인딩) passwordTextField.rx.text.orEmpty .bind(to: viewModel.password) .disposed(by: disposeBag) // loginButton이 탭(tap)될 때마다 viewModel.loginButtonTapped 함수를 실행 loginButton.rx.tap .subscribe(onNext: { [weak self] in self?.viewModel.loginButtonTapped() }) .disposed(by: disposeBag) // --- 출력 바인딩 (ViewModel의 상태 -> View의 UI 속성) --- // viewModel.isLoginEnabled 상태가 변경될 때마다 그 값을 loginButton.rx.isEnabled 속성에 전달(바인딩) // 이 한 줄이 MVP의 enable/disableLoginButton 함수와 그 호출 코드를 모두 대체합니다. viewModel.isLoginEnabled .bind(to: loginButton.rx.isEnabled) .disposed(by: disposeBag) // isLoginEnabled 상태에 따라 버튼의 배경색도 자동으로 변경 viewModel.isLoginEnabled .map { isEnabled in isEnabled ? UIColor.blue : UIColor.gray } // 상태를 UI 속성 값으로 변환 .bind(to: loginButton.rx.backgroundColor) .disposed(by: disposeBag) // viewModel.loginResult 상태를 구독하여 결과에 따라 UI 처리 viewModel.loginResult .subscribe(onNext: { [weak self] result in switch result { case .success: print(\"로그인 성공! 메인 화면으로 이동합니다.\") case .failure: // 얼럿 표시 로직 let alert = UIAlertController(title: \"로그인 실패\", message: \"에러 발생\", preferredStyle: .alert) self?.present(alert, animated: true) } }) .disposed(by: disposeBag) } ` MVVM이 이뤄낸 궁극의 성과는 다음과 같습니다. import UIKit view.enableButton() 같은 명령형 코드가 사라졌습니다. 대신 **\"로그인 버튼의 활성화 상태( isEnabled )는 ViewModel의 isLoginEnabled email , password , isLoginEnabled , loginResult` 등)가 ViewModel에 명시적으로 선언되어 있습니다. 복잡한 화면이라도 ViewModel의 상태 프로퍼티만 보면 어떤 상태들이 존재하는지 한눈에 파악할 수 있어, 상태를 추적하고 관리하기 매우 용이합니다. 최종 결론: 상태를 지배하는 아키텍처를 향한 위대한 여정 클라이언트 아키텍처의 발전사는 '어떻게(How)' UI를 조작할 것인가에 대한 고민에서 '무엇(What)' 을 보여줄 것인가에 대한 고민으로의 전환 과정이었습니다. 구분 1세대: 콜백 지옥/MVC 2세대: MVP 3세대: MVVM 핵심 철학 모든 것을 하나의 함수/컨트롤러에서 처리 Presenter가 View에게 명령 ViewModel의 상태를 View가 구독/바인딩 패러다임 명령형 (UI를 직접 조작) 명령형 (UI 조작을 지시) 선언형 (상태를 선언하면 UI가 반응) View-중재자 관계 강한 결합 1:1 강한 결합 (인터페이스 경유) 느슨한 결합 (결합 없음) 테스트 용이성 어려움 높음 (Presenter 테스트 가능) 매우 높음 (ViewModel 테스트 매우 용이) 주요 동기 기본 구조 제공 테스트와 역할 분리 상태 관리 자동화와 반복 코드 제거 백엔드 아키텍처가 의존성 주입(DI)을 통해 각 컴포넌트의 결합을 끊어내고 유연성을 확보했다면, 클라이언트 아키텍처는 데이터 바인딩과 반응형 프로그래밍을 통해 View와 로직의 결합을 끊어내고 복잡한 상태를 효과적으로 관리하는 방향으로 진화했습니다. 오래 지속되고 복잡하게 얽히는 '상태'를 안정적으로 관리하고, 변화에 유연하며, 테스트 가능한 코드를 작성하는 능력. 이것이 바로 콜백 지옥에서 시작하여 MVVM에 이르는 기나긴 여정이 우리에게 가르쳐주는 핵심 교훈이자, 현대 클라이언트 애플리케이션 개발자가 갖춰야 할 가장 중요한 역량입니다.",
      "frontmatter": {
        "date": "2025-10-16T16:48:03+09:00",
        "lastmod": "2025-10-22T13:24:14+09:00"
      }
    },
    "프로젝트 주제": {
      "path": "/프로젝트-주제/",
      "filename": "프로젝트 주제",
      "content": "세종대 allcll 프로젝트 여러명이 있을 때 블루투스 스피커가 없음 근데 크게 듣고 싶음 => 각 기기 동기화 시켜서 동일한 음원 정확하게 같은 시간에 트는거 유튜브 재생목록 받아서 구현( 대형을 보고 스피커 형태 구현하면 더 좋다 => 좌우 전후 등등) universal clipboard 클립보드 완전 동기화 (android, window, mac, ios, linux ... )",
      "frontmatter": {
        "date": "2025-07-05T10:09:57+09:00",
        "lastmod": "2025-10-19T23:19:07+09:00"
      }
    },
    "Drawing 2025-06-02 09.33.51.excalidraw": {
      "path": "/04.excalidraw/drawing-2025-06-02-09.33.51.excalidraw/",
      "filename": "Drawing 2025-06-02 09.33.51.excalidraw",
      "content": "==⚠ Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠== You can decompress Drawing data with the command palette: 'Decompress current Excalidraw file'. For more info check in plugin settings under 'Saving' Drawing N4IgLgngDgpiBcIYA8DGBDANgSwCYCd0B3EAGhADcZ8BnbAewDsEAmcm+gV31TkQAswYKDXgB6MQHNsYfpwBGAOlT0AtmIBeNCtlQbs6RmPry6uA4wC0KDDgLFLUTJ2lH8MTDHQ0YNMWHRJMRZFAEYQljIkT1UYRjAaBABtAF1ydCgoAGUAsD5QSXw8LOwNPkZOTExyHRgiACF0VABrQq5GXABhekx6fAQQAGIAM1GxkABfCaA== %%",
      "frontmatter": {
        "excalidraw-plugin": "parsed",
        "tags": "[excalidraw]"
      }
    },
    "Drawing 2025-06-02 09.38.04.excalidraw": {
      "path": "/04.excalidraw/drawing-2025-06-02-09.38.04.excalidraw/",
      "filename": "Drawing 2025-06-02 09.38.04.excalidraw",
      "content": "==⚠ Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠== You can decompress Drawing data with the command palette: 'Decompress current Excalidraw file'. For more info check in plugin settings under 'Saving' Drawing N4IgLgngDgpiBcIYA8DGBDANgSwCYCd0B3EAGhADcZ8BnbAewDsEAmcm+gV31TkQAswYKDXgB6MQHNsYfpwBGAOlT0AtmIBeNCtlQbs6RmPry6uA4wC0KDDgLFLUTJ2lH8MTDHQ0YNMWHRJMRZFAEYQljIkT1UYRjAaBABtAF1ydCgoAGUAsD5QSXw8LOwNPkZOTExyHRgiACF0VABrQq5GXABhekx6fAQQAGIAM1GxkABfCaA== %%",
      "frontmatter": {
        "excalidraw-plugin": "parsed",
        "tags": "[excalidraw]"
      }
    },
    "Drawing 2025-06-02 09.38.12.excalidraw": {
      "path": "/04.excalidraw/drawing-2025-06-02-09.38.12.excalidraw/",
      "filename": "Drawing 2025-06-02 09.38.12.excalidraw",
      "content": "==⚠ Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠== You can decompress Drawing data with the command palette: 'Decompress current Excalidraw file'. For more info check in plugin settings under 'Saving' Excalidraw Data Text Elements %% Drawing N4KAkARALgngDgUwgLgAQQQDwMYEMA2AlgCYBOuA7hADTgQBuCpAzoQPYB2KqATLZMzYBXUtiRoIACyhQ4zZAHoFAc0JRJQgEYA6bGwC2CgF7N6hbEcK4OCtptbErHALRY8RMpWdx8Q1TdIEfARcZgRmBShcZQUebQBGAFYEmjoghH0EDihmbgBtAF1+CFw4OABlKKhxVFAwSHUMmogiZWlU+oZCBAoAIVxsAGtlUmEOYgBhNnw2Um4IAGIAMxXV jshsEUDsgElq/UrRwYQpmbmJBfiEK6v1iE3Sbag9jP6hkbHJ6dn5qHIOZhwXBPO4PJ4vfQAMUI+HwlRgwXmgg8oK2WWe+0ObGOAHUSOpuHxwBs0btMX9sQh4YiJMiSKjHuiIQAlYRtDjhXJoeL8EmMskZADyQOwahg3HiAAZJbz7qSMRlIZwoJDcPoYeK0IlZWCmfsldlyoQjDUeDLiXL+Qr9AAVLBQACCrS4EmCSygDPB5OBjsebAokhCxG4HCE sJ18ohAFFxg6/QGg/NgaMqBGrRC4ymbfBmqMhONPXrFeRMqyaahQ+GLcxsKNYQANbiJABsAA5tABmZvxACcfcS8XiAHZ4q3W7Ka3X8ABNbgAFg7yUSPHic54HY3PdbC6H2otRjYBm4dU69AIQhq8WJAF8016Mqz88QOcwueg8wXZSMSEaTYTzZ037EJUCBwE2X6kCQACybDEAgMa4JowTBmgSwEGEEEkKcPxoCekC9NMKHvsomi4AAFCuQ7ULww7 UZR1GStoiQAJTrJAzIIMoYbAvMpAkeR64yrwHZCYJDFMaxN53uiWLHMKUDsACx7EpAkiEG0rTSEw3BofgGEqfcnDMFE2QAAqBK+Ig1H8F68qpoSCkIUBEByOnoQgdkQMIzmEByeLEASaAAZ0bBwApRnKfUnTqL5HC+co3CMc2dkCPoh4xRwCVBQkKUQMZgRqi5NSMTyKmQCErCZfMFUII5UBGq5uXGT6kVRRspTcDZHllQ0pTaUFKXlaE8XzEVPF lbeZUYAWuE9fcHWoe5g3QH15yoMFUUYMNVWur5IRzBNKnXlJFpLCWCAceMTAjbNm1ZIhyGdaQtkGdgRBgWgXWeRwarWc93Wbd5RVPS9m36MCxykAAcr9IMA504OkJDCFIQgRFdSdIWaAAVgg2A5OUP1wDBcEo49i16fDJT44QjA2oe+CtQ0OZIukNOcGxcrGQY2aICGYb4J5MxDGjbmU55Z0GOUbPhS6qC6fpm34KEjo03TDP87CN7gJNkBLDC4T Hsd15AA= %%",
      "frontmatter": {
        "excalidraw-plugin": "parsed",
        "tags": "[excalidraw]"
      }
    },
    "Drawing 2025-10-11 13.12.38.excalidraw": {
      "path": "/04.excalidraw/drawing-2025-10-11-13.12.38.excalidraw/",
      "filename": "Drawing 2025-10-11 13.12.38.excalidraw",
      "content": "==⚠ Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠== You can decompress Drawing data with the command palette: 'Decompress current Excalidraw file'. For more info check in plugin settings under 'Saving' Excalidraw Data Text Elements Hello ^c34eIMiq %% Drawing N4KAkARALgngDgUwgLgAQQQDwMYEMA2AlgCYBOuA7hADTgQBuCpAzoQPYB2KqATLZMzYBXUtiRoIACyhQ4zZAHoFAc0JRJQgEYA6bGwC2CgF7N6hbEcK4OCtptbErHALRY8RMpWdx8Q1TdIEfARcZgRmBShcZQUebQBGAFYEmjoghH0EDihmbgBtcDBQMBKIEm4IbABmABYEAEkAWUIAR1SSyFhECqgsKHbSzG5nePia7QAORIA2RIAGadqAdlnp nkT+Uphh+IBOacnpmp4Jiaqqubmlifi+QsgKEnVuGqqE3Y+b6YndmsSq24be5SBCEZTSbjrTaQazKYLcObQiDMKCkNgAawQAGE2Pg2KQKgBieIIEkkgaQTS4bDo5RooQcYg4vEEiSo6zMOC4QLZCkQABmhHw+AAyrB4RJBB4+Si0ZiAOpPSSQpGyjEIMUwCXoKXlJH08EccK5NDxJFsLnYNTbU2XJF04RwerEE2oPIAXSR/PImWd3A4QmFSMIjKw FVwcz59MZRuYroDQeBYQQxG4YyqExqSyW50RwMYLHYXDQEyRBdYnAAcpwxC9pksFvFc8HmAARdK9VNofkEMJIzTCRkAUWCmWyroKHSK91K5QkbCWxAA8gAhAD6UHqAEUeABxCaYJYAJQAWhwsfp4gAJCmlLridDctFUGcAX3unuBQjgxFwnbTKxzEkJy7JmSRIniNIptwPb4H2wK9Jg/QSFeQR4lGlAACp9BUqHCmwfL8pwUAioQRgPjweYdAKxE AGK4PoQo2qgQLUYhUAAIJEMoxboME/L9GWTBQOYBBcWCvHQBafJ6NkuAhkwfpoAm+DmqQYIhgQ2FIbhaEEUiuBCFAbBHuEZEPqiQgIBBCmoWCEKmtoULAkQHDov6gb4IUr6bMUs5dugJgANItOi6JXpgQ7KGQQ4TJhQVzCeHGtvKfL3hUgTYFEHBwkgSJDGg0zxJMpxLDUXw/H8AJIsxzg8A22iJD8iQfNMcy7FUSysaUjzEM8aANYkezrLMuzxE s+ynEikiguCyGoPE0wHBM9VjJmnXdTCOXalRpRqpizL4kSZKknlwJUjSDoMkyuJHWy5AcJy3JZIJwKCsKmrasiuJ6kmqLqoqfXKmgdzUftGrig+33SvqwiGsaabmpa1ppnawJXU6Lr5J+1HegxDQBSpwahgVj7xFGg7ELGrp+Xe8APlU9w+UmCDQaaPDxBMS1VLsiQ1EJhacGmfwCxWHDVhwtZoB8SRzE1DYtu2wT/t2vbWedlMjhkL0TjOtP+RU C7Luum47vuh6nuel43tCdPdBIT5sC+U7vh0OOlN+v4qwtgHAbscwXEVNluR5ibUZBmIBbBYTeb5wJzugRurhu257gex5nhe15pfT4akM+fKk84RzaACMxNTMHVVC1/PArVnMTAkCzTGNuwja1SK9f1C1JE5NQfHMmZZjwqyg6UM32fNxU87UlxNeNEzZjmVQGdtD67QI/0HbdrLoMSp3kv21K0tGN0sj0D1PTyr240KoqQxUuqpqqW8IID3dj5vc oQ1qUNPxTfhJDU0RsCC01IUa2g3hADGzoJzu0gHjX0hNPLE2IGGB2PAAExgRmgfW0Bc4gyZqqVmAVFgrBGmMUWRZuC7CWFQqsNYKJXB4G3UYcxP5lDbB2NmqBo7q2ogOa6Wsxw5FDqpL8P4/w8PGm1P2AcFgcNcu5ZSKCXJsCglHNWSI4BsBDKItAk4OiGI6BvEocwZzwLAMYkooxkg8H7v7IeSwR4zE/jY0uHUahzxuNcJe5wLEfggqEKAOJ9CM RkCmAACro3kKiw57SiKQKAK4QyOBytwPB6QRFKXQNUOoTRWi3hotgIQrpnBzFLpRZx6w+Y/EzCcTakBlC4DgNwJYpcvFczaqcLmSxOZdSIcCLIxAUmMhDMoDJM4MCjhejkiAwVQrhUitFUgsV4qJWSqlW2xTSnDAqUBDmEwgK/AzJ1LqmZtnNNababQvMaj9xWmNICrjEiDLBokzi+cnYzVwMg+JkBhkcS+RQH5AUICOyoEiIIA4KA8L4bHQotNo A4QkAACgAJR5G0NoBQgAWbsADgTC13SEWIqRcikIoFEWyPRRi+BmKbWRUhcSPEKj8RvqUAsIl3DMsksZVpwYwWaCvEOfQbB4AABkc723QOxQuwwObtO+Fmb49Z6yJGuDVeVbVtATSOFUexvNRjOM7kqNMowdUfDbmq64gJpqzQcrwBlsIdov2/odXeEB94nT5BdE+lN3UXw5Fya+hE76fShpIakGhAgylfu/YGC1XXqnDY/H6z9gQGiATgxNoDka wFRlA0+wC4niLBiQ7gGZ/jTHobxRaNaJZSwWgCK4S0eAjyRJ7KRAUZHPIuMce5tdqKEC4crHhll+GlBgVjAx8CICCOHDM8c2Ng7KNQETNRGiYJaLej6AmYikSySiApUgOT13UXxBpDgWkUXoAxVinFBKiUYQoNpeaEA73YrxYS+IxLg4IDsnNNMTknVGRMmZclaBx2oPQY+KoEAEUlCRQnCAMBB4UF3OKqo7LOgEJlThfKOwgIJEzCsXp1qbiarQ HVJaTl9ivANUNcaHCu4JtLMCCegGQZOrXgiJN29z4SC9adH1x8rqMgDfdINz1eRejDQ/SUabY3f3jSqP638U0KZhhmuGWa4wgPPXm5i8Q0bUSnXAr0u7T2qKHSTcMNQsFU2zWeva5bTRdT6Vmf4g6OVMDFhW+tjDuBFRqItHmK1FbcIClBjWQjF36LXdZj2kjvY9qSBcC4A8V37o3ZHLdcEJ2dBvRAZ9r6KgkuyGSphXo6IMSYtwBl7EeWsoQAJP knLRL4Ca2yaSB7iLySNCev5pbSgXv8NenSEg+SGWMqZVgEHUDRfDrZe1U9gMIenEOsFmAtyEC3OK9E+guBInSmyfDwIi7GbeJzZVZH3MUbrlquxdH9X90Y8a4ELHuBseohxh1zlqLOvXnx7EO9joHzOgI0Tp8JMysvsGl6oaPryZ1Ip4HKmQbA40yjrT1FM3FpzQZ8B+bIH2npJjczO78ZWf+WUWzDtEgOfx85gQrmFo3FuL8dt+ZfPULQCvbngt xaBdNEkG4+x2ERdHVF0gVl+yazi/GRLkBO0pd9v8AO5w25ZZLRBdRuXVb5eO0VkrxvqsVfMhSs3UAaV1bQA1voXW+ItewwwYSHXHdSX5cCQ9/XFJDbUpe8bb6pugdmxbyDMuCsQFcv+lbQH1jeXADjcFcA4BiikZMu8M1MissngMBghAEAUBXFD/1oPBP8kr1X/PJT86zN6PoMUbry97xOofQolQRDX3qA3kvl1oct+gHD6T7LO91+yD3jItE5O/ 1TTjyAtfu8N6bwDU1GOO+L/rxkFfmIsfQ1+qUTfE+G9Hh0/js0G+u9b/0EuQzBbNhj6X1PmrtL6UP6P5uBvtFSXh94LtR/1+r6HubKNeV+x+2+HyQKz4oK2Wh+YBn+GQQ4jIUB3yIQYKEKoB4+CB+gKBL6uGEAp8mBT++gtEu6p+2ozmyI2AaIwoAAGhWtmNoK8F1MBPLD8AsA/swNQbiPgAAJovDsIJD/aQBGBsAGCZ4coEBWSQjrYAHgH6Cn7X T44EGUz550gkCVaW4d7qHEBigIDXIsQP46GNBsBoJIG4CaDBCaKG7aHqRnx3SoBIori4hgqkDKBUiooKrUC8DjTeFeGoAVKJDop8imTKCBjcgZTuG4CeEXB+GxG8DxGBHBGyEf474IC34iScCK7xIChkHDJMDjISEApXqWFjqR4HpEAGGLalBXo54R6y6gJGQx7cDVGQD6DciYikCVj4wtHlHAjtGkCdHmGlHS5WSyF2AABWCAWUzAIoV6cAJhZh JRVheW8EpQ1IIkjAmEYh+ARR+C0qyI6QWU1CB6pSxk+gmEuGzO0eeucK26uMaIjeRxmRvEfCQSKIHExxWxOxYiieYAzMCCQo4QGS74r4QAA= %%",
      "frontmatter": {
        "excalidraw-plugin": "parsed",
        "tags": "[excalidraw]"
      }
    },
    "101 문제": {
      "path": "/temp/101-문제/",
      "filename": "101 문제",
      "content": "나의 경우 문장을 단어로 분할을 위해 인덱스를 잡고 vector 에 추가하여 임시로 저장 이후에 vector 를 나중에 Given an input string s, reverse the order of the words. A word is defined as a sequence of non-space characters. The words in s will be separated by at least one space. Return a string of the words in reverse order concatenated by a single space. Note that s may contain leading or trailing spaces or multiple spaces between two words. The returned string should only have a single space separating the words. Do not include any extra spaces. Example 1: Input: s = \"the sky is blue\" Output: \"blue is sky the\" Example 2: Input: s = \" hello world \" Output: \"world hello\" Explanation: Your reversed string should not contain leading or trailing spaces. Example 3: Input: s = \"a good example\" Output: \"example good a\" Explanation: You need to reduce multiple spaces between two words to a single space in the reversed string. Constraints: 1 <= s.length <= 104 s contains English letters (upper-case and lower-case), digits, and spaces ' '. There is at least one word in s. // 처음에 문자가 나오는 인덱스 저장 , 문자가 끝나는 다음 인덱스 저장 배열 // 단어를 지우고 다시 쓴다 class Solution { public: string reverseWords(string s) { vector<pair<int,int>> vector_index; for(int i = 0; i < s.size(); i++) { if (s[i] != ' ') { int start_index = i; while (i < s.size() && s[i] != ' ') { i++; } vector_index.push_back(make_pair(start_index, i)); // i는 다음 공백의 인덱스 } } string result; for(int i = vector_index.size()-1 ; i>= 0 ; i--){ int first_index = vector_index[i].first; int second_index = vector_index[i].second; result += s.substr(first_index, second_index - first_index); result += ' '; } result.pop_back(); return result; } }; vector_index 를 만드는 반복분을 int start_index = -1; for (int i = 0; i < s.size(); i++) { // 단어의 시작을 찾음 if (start_index == -1 && isalpha(s[i])) { start_index = i; } // 단어의 끝을 찾음 else if (start_index != -1 && (s[i] == ' ' || i == s.size() - 1)) { // 문자열의 끝에 도달한 경우, 마지막 단어 처리 if (s[i] != ' ') { i++; // 마지막 단어의 끝을 포함 } word_index.push_back(make_pair(start_index, i)); start_index = -1; // 다음 단어를 위해 초기화 } } 이렇게 할 수 도 있다 아니면 cpp 문자열 split 을 사용할 수도 있다",
      "frontmatter": {
        "date": "2025-01-18T13:38:16+09:00",
        "lastmod": "2025-11-02T02:43:29+09:00"
      }
    },
    "2024-10-25": {
      "path": "/temp/2024-10-25/",
      "filename": "2024-10-25",
      "content": "2024/10/25자 생성 영어 pdf 문서 통번역(정확도 순위별 정렬)(하이퍼 링크) DeepL => 5,000자까지, 로그인(회원가입) 필요 papago => 10MB 이하 1만자 이하, 로그인(회원가입) 필요 google translate => 클릭 이후 상단 문서로 이동 LLM 사용 (AI) 뤼튼 => 무제한, 로그인 필요Pasted image 20241025230788 chatGPT => 거이 무제한, 로그인 필요 Claude => 가장 똑똑하지만 제한량이 있음, 로그인 필요 Microsoft AI Copliot => microsoft AI Google AI gemini => 구글이 만ems AI",
      "frontmatter": {
        "date": "2025-06-03T06:05:16+09:00",
        "lastmod": "2025-09-05T22:04:13+09:00"
      }
    },
    "ANSI escape code": {
      "path": "/temp/ansi-escape-code/",
      "filename": "ANSI escape code",
      "content": "ANSI 이스케이프 시퀀스는 비디오 텍스트 터미널 과 터미널 에뮬레이터 에서 커서 위치, 색상, 글꼴 스타일 및 기타 옵션을 제어하기 위한 인 밴드 신호 표준 source wikipedia 일반적인 상황에서는 터미널 환경에서 사용한다 GUI 에서 사용하더도 대부분 텍스트를 제어하는 데에 사용된다",
      "frontmatter": {
        "date": "2024-07-31T00:56:00+09:00",
        "lastmod": "2025-08-11T01:10:26+09:00"
      }
    },
    "ELF 구조": {
      "path": "/temp/elf-구조/",
      "filename": "ELF 구조",
      "content": "%20image%2020250123105644.png)",
      "frontmatter": {
        "aliases": [
          "object file"
        ],
        "date": "2025-01-23T10:56:00+09:00",
        "lastmod": "2025-01-23T10:56:00+09:00"
      }
    },
    "Makefile": {
      "path": "/temp/makefile/",
      "filename": "Makefile",
      "content": "target : dependency <탭> command tarrget 은 일반적인 command 로 생성된 파일을 의미할 수도 있고 목표하는 의미가 될 수도 있다 예를 들어 target 에 clean 이 들어가면 빌드 생성파일을 삭제한다 변수 매크로의 사용에서 ${..}, $(..), $..를 모두 사용할 수 있습니다. 그러나 대부분의 책에서는 $(..) 을 사용하라고 권하는군요 OBJS = main.o read.o write.o test : $(OBJS) gcc -o test $(OBJS) 미리 정해저 있는 predefind 변수 make -p 자동변수 $@ : 현재 타겟 이름 $^ : 현재 타겟이 의존하는 대상들의 전체 목록 $^ : 의존 대상의 처음 대상",
      "frontmatter": {
        "tags": [
          "c"
        ],
        "date": "2024-01-22T03:36:00+09:00",
        "lastmod": "2024-01-22T03:36:00+09:00"
      }
    },
    "Postgresql": {
      "path": "/temp/postgresql/",
      "filename": "Postgresql",
      "content": "Postgresql binary clusterdb: 데이터베이스 클러스터의 모든 데이터베이스를 클러스터화합니다. createdb: 새로운 데이터베이스를 생성합니다. createuser: 새로운 사용자를 생성합니다. dropdb: 데이터베이스를 삭제합니다. dropuser: 사용자를 삭제합니다. initdb: 새로운 데이터베이스 클러스터를 초기화합니다. oid2name: OID(객체 식별자)와 이름을 매핑합니다. pg_amcheck: 인덱스 접근 방법을 검사합니다. pg_archivecleanup: 아카이브 로그 파일을 정리합니다. pg_basebackup: 데이터베이스 클러스터의 전체 백업을 생성합니다. pgbench: 벤치마킹 도구로, 성능 테스트를 수행합니다. pg_checksums: 데이터베이스 블록의 체크섬을 확인합니다. pg_config: PostgreSQL 설치 정보 및 구성을 출력합니다. pg_controldata: 데이터베이스 클러스터의 제어 정보를 출력합니다. pg_ctl: 데이터베이스 서버를 시작, 중지, 재시작합니다. pg_dump: 데이터베이스의 백업을 생성합니다. pg_dumpall: 모든 데이터베이스의 백업을 생성합니다. pg_isready: 데이터베이스 서버의 상태를 체크합니다. pg_receivewal: WAL(Write Ahead Log)을 수신합니다. pg_recvlogical: 논리적 복제를 위한 WAL을 수신합니다. pg_resetwal: WAL 파일의 상태를 재설정합니다. pgrestore**: pgdump로 생성된 백업을 복원합니다. pg_rewind: 마스터와 슬레이브 간의 데이터 동기화를 수행합니다. pgtestfsync: fsync 성능을 테스트합니다. pgtesttiming: 시간 측정 테스트를 수행합니다. pg_upgrade: 데이터베이스를 새로운 버전으로 업그레이드합니다. pg_verifybackup: 백업의 유효성을 검사합니다. pg_waldump: WAL 파일의 내용을 출력합니다. postgres: PostgreSQL 데이터베이스 서버의 주요 실행 파일입니다. postmaster: PostgreSQL 서버를 시작하는 데 사용되는 심볼릭 링크입니다. psql: PostgreSQL의 명령줄 인터페이스입니다. reindexdb: 데이터베이스의 인덱스를 재생성합니다. vacuumdb: 데이터베이스의 공간을 회수하고, 통계를 업데이트합니다. PostgreSQL에서 Predefined Roles(미리 정의된 역할)은 특정 권한과 기능에 대한 접근을 제공하는 역할로, 데이터베이스 관리자가 사용자나 다른 역할에 부여할 수 있습니다. 이러한 역할은 일반적으로 자주 필요한 권한을 집합적으로 관리할 수 있도록 설계되었습니다. 아래에 각 미리 정의된 역할에 대한 설명을 제공합니다. 미리 정의된 역할 목록과 설명 pgreadall_data 권한: 모든 데이터(테이블, 뷰, 시퀀스)를 읽을 수 있는 권한을 가집니다. SELECT 권한이 없는 경우에도 USAGE 권한이 자동으로 부여됩니다. 주의: RLS(행 수준 보안)가 활성화된 경우, BYPASSRLS 속성이 설정되지 않으면 제한이 있을 수 있습니다. pgwriteall_data 권한: 모든 데이터에 대해 INSERT, UPDATE 및 DELETE 권한을 가집니다. USAGE 권한도 자동으로 부여됩니다. 주의: RLS가 활성화된 경우, BYPASSRLS 속성이 설정되지 않으면 제한이 있을 수 있습니다. pgreadall_settings 권한: 모든 설정 변수(일반 사용자에게는 보이지 않는 것까지 포함)를 읽을 수 있습니다. pgreadall_stats 권한: pgstat* 뷰와 다양한 통계 관련 확장을 읽을 수 있습니다. 일반 사용자에게는 보이지 않는 정보에 접근할 수 있습니다. pgstatscan_tables 권한: 테이블에 ACCESS SHARE 잠금을 걸 수 있는 모니터링 기능을 실행할 수 있습니다. pg_monitor 권한: 다양한 모니터링 뷰와 함수를 읽고 실행할 수 있습니다. 이 역할은 pgreadallsettings, pgreadallstats, pgstatscan_tables의 멤버입니다. pgdatabaseowner 권한: 현재 데이터베이스의 소유자에게만 자동으로 부여되는 역할입니다. 이 역할은 다른 역할의 구성원이 될 수 없습니다. pgsignalbackend 권한: 다른 백엔드에 신호를 보내 쿼리를 취소하거나 세션을 종료할 수 있는 권한을 부여합니다. pgreadserver_files 권한: 데이터베이스가 접근할 수 있는 파일을 읽을 수 있는 권한입니다. pgwriteserver_files 권한: 데이터베이스가 접근할 수 있는 파일에 쓸 수 있는 권한입니다. pgexecuteserver_program 권한: 데이터베이스 서버에서 프로그램을 실행할 수 있는 권한을 부여합니다. pg_checkpoint 권한: CHECKPOINT 명령을 실행할 수 있는 권한입니다. pgusereserved_connections 권한: 예약된 연결 슬롯을 사용할 수 있는 권한입니다. pgcreatesubscription 권한: CREATE 권한이 있는 데이터베이스에서 CREATE SUBSCRIPTION 명령을 실행할 수 있는 권한입니다. 권한 부여 관리자는 다음과 같은 SQL 명령을 사용하여 특정 사용자에게 이러한 역할을 부여할 수 있습니다: GRANT pg_signal_backend TO admin_user; 주의 사항 보안: 이러한 역할은 강력한 권한을 부여하므로, 필요한 사용자에게만 부여해야 하며, 그 사용에 대한 이해가 필요합니다. 모니터링: pg_monitor 와 같은 역할은 데이터베이스 서버의 모니터링을 용이하게 하며, 일반적으로 슈퍼유저만 접근할 수 있는 통계 및 설정 정보에 접근할 수 있도록 합니다. CREATE USER test_user WITH PASSWORD '1253' LOGIN; CREATE ROLE personal_group; GRANT personal_group TO test_user; CREATE DATABASE test_db OWNER test_user; REVOKE CONNECT ON DATABASE test_db FROM PUBLIC; REVOKE TEMPORARY ON DATABASE test_db FROM PUBLIC; GRANT CONNECT ON DATABASE test_db TO test_user; -- 필요 없을 수 있음 GRANT CONNECT ON DATABASE test_db TO postgres; -- 필요 없을 수 있음",
      "frontmatter": {
        "date": "2024-09-16T04:11:00+09:00",
        "lastmod": "2024-09-16T04:11:00+09:00"
      }
    },
    "Rest Full API": {
      "path": "/temp/rest-full-api/",
      "filename": "Rest Full API",
      "content": "RESTful API는 현대 웹 애플리케이션의 필수 요소로 자리잡았습니다. 그러나 RESTful API를 설계할 때는 몇 가지 원칙을 따르는 것이 중요합니다. 이번 글에서는 RESTful API 설계의 핵심 원칙과 그 원칙을 예시를 통해 자세히 살펴보겠습니다. 이 원칙들을 준수하면 API의 일관성과 확장성을 높이고, 개발자들에게 편의성을 제공할 수 있습니다. 자원(리소스) 기반 URI 자원(리소스) 기반 URI는 RESTful API의 핵심입니다. 각 자원은 고유한 URI(Uniform Resource Identifier)를 가지며, 이를 통해 자원을 식별합니다. 자원 기반 URI는 설계의 일관성을 유지하고 의미를 명확하게 전달하는 데 도움을 줍니다. 예를 들어, 다음과 같은 URI를 사용하여 각각의 리소스를 다룰 수 있습니다. 사용자 리소스를 다루는 API 모든 사용자 정보를 가져옴: GET /users 새로운 사용자를 생성함: POST /users 특정 사용자의 정보를 가져옴: GET /users/{id} 특정 사용자의 정보를 업데이트함: PUT /users/{id} 특정 사용자를 삭제함: DELETE /users/{id} 특정 사용자의 모든 포스트 정보를 가져옴: GET /users/{id}/posts 특정 사용자에게 새로운 포스트를 생성함: POST /users/{id}/posts 특정 사용자의 특정 포스트 정보를 가져옴: GET /users/{id}/posts/{post_id} 특정 사용자의 특정 포스트 정보를 업데이트함: PUT /users/{id}/posts/{post_id} 특정 사용자의 특정 포스트를 삭제함: DELETE /users/{id}/posts/{post_id} 특정 사용자의 정보 일부를 업데이트함: PATCH /users/{id} 특정 사용자의 특정 포스트 정보 일부를 업데이트함: PATCH /users/{id}/posts/{post_id} 도서 리소스를 다루는 API 모든 도서 정보를 가져옴: GET /books 새로운 도서를 생성함: POST /books 특정 도서의 정보를 가져옴: GET /books/{id} 특정 도서의 정보를 업데이트함: PUT /books/{id} 특정 도서를 삭제함: DELETE /books/{id} 특정 도서의 모든 리뷰 정보를 가져옴: GET /books/{id}/reviews 특정 도서에 새로운 리뷰를 생성함: POST /books/{id}/reviews 특정 도서의 특정 리뷰 정보를 가져옴: GET /books/{id}/reviews/{review_id} 특정 도서의 특정 리뷰 정보를 업데이트함: PUT /books/{id}/reviews/{review_id} 특정 도서의 특정 리뷰를 삭제함: DELETE /books/{id}/reviews/{review_id} 이벤트 리소스를 다루는 API 모든 이벤트 정보를 가져옴: GET /events 새로운 이벤트를 생성함: POST /events 특정 이벤트의 정보를 가져옴: GET /events/{id} 특정 이벤트의 정보를 업데이트함: PUT /events/{id} 특정 이벤트를 삭제함: DELETE /events/{id} 특정 이벤트의 모든 참가자 정보를 가져옴: GET /events/{id}/participants 특정 이벤트에 새로운 참가자를 추가함: POST /events/{id}/participants 특정 이벤트의 특정 참가자 정보를 가져옴: GET /events/{id}/participants/{participant_id} 특정 이벤트의 특정 참가자 정보를 업데이트함: PUT /events/{id}/participants/{participant_id} 특정 이벤트의 특정 참가자를 삭제함: DELETE /events/{id}/participants/{participant_id} 게시물 리소스를 다루는 API 모든 게시물 정보를 가져옴: GET /posts 새로운 게시물을 생성함: POST /posts 특정 게시물의 정보를 가져옴: GET /posts/{id} 특정 게시물의 정보를 업데이트함: PUT /posts/{id} 특정 게시물을 삭제함: DELETE /posts/{id} 상품 리소스를 다루는 API 모든 상품 정보를 가져옴: GET /products 새로운 상품을 생성함: POST /products 특정 상품의 정보를 가져옴: GET /products/{id} 특정 상품의 정보를 업데이트함: PUT /products/{id} 특정 상품을 삭제함: DELETE /products/{id} 주문 리소스를 다루는 API 모든 주문 정보를 가져옴: GET /orders 새로운 주문을 생성함: POST /orders 특정 주문의 정보를 가져옴: GET /orders/{id} 특정 주문의 정보를 업데이트함: PUT /orders/{id} 특정 주문을 삭제함: DELETE /orders/{id} 프로필 리소스를 다루는 API 모든 프로필 정보를 가져옴: GET /profiles 새로운 프로필을 생성함: POST /profiles 특정 프로필의 정보를 가져옴: GET /profiles/{id} 특정 프로필의 정보를 업데이트함: PUT /profiles/{id} 특정 프로필을 삭제함: DELETE /profiles/{id} 뉴스 리소스를 다루는 API 모든 뉴스 정보를 가져옴: GET /news 새로운 뉴스를 생성함: POST /news 특정 뉴스의 정보를 가져옴: GET /news/{id} 특정 뉴스의 정보를 업데이트함: PUT /news/{id} 특정 뉴스를 삭제함: DELETE /news/{id} HTTP 동사 활용 HTTP 프로토콜은 다양한 동사(메서드)를 제공합니다. RESTful API 설계에서는 이러한 동사를 적절하게 활용하여 API의 의도를 명확하게 전달해야 합니다. GET: 리소스의 정보를 조회하기 위해 사용합니다. POST: 리소스를 생성하기 위해 사용합니다. PUT: 리소스의 정보를 업데이트하기 위해 사용합니다. DELETE: 리소스를 삭제하기 위해 사용합니다. PATCH: 리소스의 일부분을 업데이트하기 위해 사용합니다. 적절한 상태 코드 반환 HTTP 상태 코드는 API의 응답에 포함되어 클라이언트에게 작업 결과를 알려줍니다. RESTful API 설계에서는 적절한 상태 코드를 반환하여 클라이언트가 요청에 대해 올바르게 대응할 수 있도록 해야 합니다. 일반적으로 사용되는 몇 가지 상태 코드는 다음과 같습니다: 1xx - Informational (정보) 100 - Continue (계속): 요청이 계속될 수 있음을 나타냄 101 - Switching Protocols (프로토콜 변경): 프로토콜 전환을 요청한 경우 사용 2xx - Successful (성공) 200 - OK: 성공적인 요청에 대한 응답 201 - Created (생성됨): 새 리소스가 성공적으로 생성됨 202 - Accepted (수락됨): 요청이 받아들여졌지만 아직 처리되지 않음 204 - No Content (콘텐츠 없음): 응답 본문이 없음을 나타냄 206 - Partial Content (부분 콘텐츠): 부분 콘텐츠를 반환하는 경우 사용 3xx - Redirection (리다이렉션) 300 - Multiple Choices (다중 선택): 리소스에 여러 가지 선택지가 있음 301 - Moved Permanently (영구적으로 이동): 리소스가 새로운 URI로 이동함 302 - Found (찾음): 리소스가 일시적으로 다른 URI로 이동함 304 - Not Modified (수정되지 않음): 리소스가 변경되지 않았으므로 클라이언트가 캐시 사용 4xx - Client Error (클라이언트 오류) 400 - Bad Request (잘못된 요청): 잘못된 요청으로 인해 서버가 요청을 이해하지 못함 401 - Unauthorized (권한 없음): 인증이 필요한 리소스에 대한 접근 권한 없음 403 - Forbidden (금지됨): 접근이 거부됨 404 - Not Found (찾을 수 없음): 요청한 리소스가 서버에서 찾을 수 없음 405 - Method Not Allowed (허용되지 않은 메서드): 지원되지 않는 HTTP 메서드를 사용했을 때 사용 429 - Too Many Requests (요청 너무 많음): 클라이언트가 요청 제한을 초과함 5xx - Server Error (서버 오류) 500 - Internal Server Error (내부 서버 오류): 서버 내부 오류로 인해 요청을 처리할 수 없음 502 - Bad Gateway (게이트웨이 오류): 게이트웨이 서버에서 업스트림 서버로의 잘못된 응답 수신 503 - Service Unavailable (서비스 이용 불가): 서버가 일시적으로 서비스 이용 불가 상태임 504 - Gateway Timeout (게이트웨이 시간 초과): 게이트웨이 서버가 업스트림 서버로부터 응답을 기다리는 동안 시간 초과 발생 적절한 데이터 포맷 RESTful API에서는 주로 JSON(JavaScript Object Notation)이나 XML(Extensible Markup Language)과 같은 데이터 포맷을 사용합니다. JSON은 경량이며 다양한 프로그래밍 언어에서 지원되기 때문에 널리 사용됩니다. 응답 데이터의 형식은 Accept 헤더를 통해 클라이언트가 지정할 수 있으며, 요청 데이터는 Content-Type 헤더를 통해 서버에게 전달됩니다. 적절한 인증과 권한 부여 보안은 모든 API 설계에서 중요한 고려 사항입니다. RESTful API에서는 적절한 인증(Authentication)과 권한 부여(Authorization) 메커니즘을 구현해야 합니다. 대표적인 방법으로는 API 키, OAuth, JWT(Json Web Token) 등이 있습니다. 이를 통해 사용자 인증 및 권한 부여를 제어하고 API의 안전성을 강화할 수 있습니다. 마치며 RESTful API 설계는 일관성과 확장성을 고려하여 자원 기반 URI, HTTP 동사 활용, 적절한 상태 코드 반환, 적절한 데이터 포맷, 적절한 인증과 권한 부여 등의 원칙을 따라야 합니다. 이러한 원칙을 준수하면 API의 사용성과 유지보수성이 향상되며, 개발자와 클라이언트 간의 협업을 원활하게 할 수 있습니다. RESTful API 설계는 웹 애플리케이션의 핵심 부분이므로, 항상 최선의 노력을 기울여야 합니다.",
      "frontmatter": {
        "date": "2025-03-14T03:00:00+09:00",
        "lastmod": "2025-10-23T02:44:19+09:00"
      }
    },
    "SRG(그래픽 랜더링 선택) escape sequence": {
      "path": "/temp/srg그래픽-랜더링-선택-escape-sequence/",
      "filename": "SRG(그래픽 랜더링 선택) escape sequence",
      "content": "SGR(Select Graphic Rendition) 이것을 이해하기 위해 wiki 에는 이렇게 나와 있다 ANSI 이스케이프 시퀀스는 비디오 텍스트 터미널 과 터미널 에뮬레이터 에서 커서 위치, 색상, 글꼴 스타일 및 기타 옵션을 제어하기 위한 인 밴드 신호 표준 즉 cli 환경에서 커서 위치, 색상, 글꼴 스타일 등등의 옵션을 제어하기 위한 표준이라고 나와있다",
      "frontmatter": {
        "date": "2024-07-30T19:36:00+09:00",
        "lastmod": "2025-08-11T00:45:48+09:00"
      }
    },
    "Transaction 의 ACID": {
      "path": "/temp/transaction-의-acid/",
      "filename": "Transaction 의 ACID",
      "content": "ACID는 데이터베이스 트랜잭션의 신뢰성을 보장하는 네 가지 주요 속성입니다. 각각의 속성을 한 문장으로 요약하고 예시를 들어 설명하겠습니다. 원자성 (Atomicity) 요약: 트랜잭션 내의 모든 작업이 성공적으로 완료되거나, 전혀 수행되지 않아야 한다. 예시: 은행에서 10,000원을 A 계좌에서 B 계좌로 이체하는 경우, 두 계좌의 잔액이 모두 변경되거나, 하나라도 실패하면 모든 변경이 취소된다. 일관성 (Consistency) 요약: 트랜잭션이 완료되면 데이터베이스는 항상 일관된 상태를 유지해야 한다. 예시: 학생 등록 시스템에서 학생의 성적을 업데이트할 때, 성적이 0 이상 100 이하의 범위를 벗어나지 않도록 보장해야 한다. ACID 중 C consistency 에는 2가지가 있다 explicitly specified ingegrity constraint (pk, fk 조건들 ) implicit ingegrity constraints (비즈니스 로직이나 수학적 관계에 의해서만 존재하는 개념) 직원 table(employeeid, 입사일) 프로젝트 참여 table(projectid,employee_id, 참여일) 일반적으로는 참여일이 입사일보다 빠를 수는 없다 <- implicit ingegrity constraints 고립성 (Isolation) 요약: 동시에 실행되는 트랜잭션은 서로 영향을 주지 않도록 격리되어야 한다. 예시: 두 사용자가 동시에 같은 자원을 수정할 때, 한 사용자의 변경 사항이 다른 사용자에게 영향을 미치지 않도록 보장해야 한다. 지속성 (Durability) 요약: 트랜잭션이 성공적으로 완료되면, 그 결과는 시스템 오류가 발생하더라도 영구적으로 유지되어야 한다. 예시: 고객이 온라인 쇼핑몰에서 주문을 완료한 후, 서버가 다운되더라도 주문 정보는 데이터베이스에 안전하게 저장되어야 한다.",
      "frontmatter": {
        "date": "2025-06-03T06:05:16+09:00",
        "lastmod": "2025-09-27T15:04:00+09:00"
      }
    },
    "Untitled 4": {
      "path": "/temp/untitled-4/",
      "filename": "Untitled 4",
      "content": "SQL 표준에서는 dirty read: commit 되지 않은 데이터를 읽었을 때 발생하며, 데이터를 읽은 이후에 이 데이터를 rollback하게 되는 경우 문제가 발생하는 현상 non-repeatable read: 한 트랜잭션에서 같은 데이터를 두 번 읽었는데 두 데이터의 결과가 다른 현상 phantom read: 한 트랜잭션에서 같은 조건으로 데이터를 두 번 읽었는데 두 데이터의 결과가 다른 현상 이렇게 세 가지 이상 현상 개념에 대해서 다루고 이에 대한 isolation level을 아래와 같이 정의했습니다. read uncommited: 세 가지 이상 현상을 다 허용 read commited: dirty read가 발생하는 schedule은 허용 X repeatable read: non-repeatable read까지 발생하는 scehdule은 허용 X serializable: 가장 엄격한 isolation level로 모든 이상 현상이 발생하는 schedule 허용 X 하지만 1995년 이 SQL 표준을 비판하는 논문이 등장하였고 이 논문에서는 SQL 표준에서 언급한 이상 현상 외에 더 많은 이상 현상이 존재하며, 다룬 이상 현상도 더 넓은 개념에서 바라보아야 한다고 비판하였습니다. 그렇게 소개한 새로운 이상 현상은 다음과 같습니다. read skew: 한 트랜잭션에서 관련 있는 서로 다른 데이터를 읽었는데 데이터 일관성이 깨지는 현상 write skew: 두 트랜잭션에서 관련 있는 서로 다른 데이터에 쓰기 작업을 진행했는데 데이터 일관성이 깨지는 현상 lost update: 한 트랜잭션이 abort혹은 commit되면서 다른 트랜잭션이 commit하여 업데이트한 결과가 사라지는 현상 dirty write: 한 트랜잭션이 commit 되지 않은 다른 트랜잭션이 write한 데이터를 write 작업을 하면서 나타나는 이상 현상 그리고 더 넓은 개념으로 바라보아야 하는 이상 현상 dirty read의 확장: commit 되지 않은 데이터를 읽었을 때, 그 데이터가 rollback 되는 경우에만 발생하지 않음 phantom read의 확장: 같은 조건으로 데이터를 여러 번 읽는 경우가 아니더라도 phantom read가 발생 이렇게 두 가지를 비판하였습니다.",
      "frontmatter": {}
    },
    "Untitled 5": {
      "path": "/temp/untitled-5/",
      "filename": "Untitled 5",
      "content": "https://www.postgresql.org/docs/current/sql-createrole.html ( CREATE USER is the same as CREATE ROLE except that it implies LOGIN .)",
      "frontmatter": {}
    },
    "Untitled 8": {
      "path": "/temp/untitled-8/",
      "filename": "Untitled 8",
      "content": "%20image%2020241220070853.png)",
      "frontmatter": {}
    },
    "arm cpu": {
      "path": "/temp/arm-cpu/",
      "filename": "arm cpu",
      "content": "CPU 설계에서 비순차 실행 및 슈퍼스칼라 아키텍처는 성능 향상을 위한 여러 기법들을 포함하고 있습니다. RISC(축소 명령 집합 컴퓨터) 진영에서 사용되는 주요 성능 향상 기법들은 다음과 같습니다: 비순차 실행 (Out-of-Order Execution) 개념: 명령어가 프로그램의 순서와 관계없이 실행될 수 있도록 하는 기법입니다. 이는 데이터 종속성이나 자원 대기 시간으로 인해 발생하는 지연을 최소화합니다. 장점: 실행 가능한 명령어가 있다면, 지연되는 명령어를 건너뛰고 다른 명령어를 실행하여 CPU의 자원을 효율적으로 사용할 수 있습니다. 슈퍼스칼라 아키텍처 (Superscalar Architecture) 개념: 한 사이클 내에 여러 개의 명령어를 동시에 실행할 수 있는 구조입니다. 여러 개의 실행 유닛을 통해 명령어의 병렬 처리를 가능하게 합니다. 장점: 명령어 레벨 병렬성을 활용하여 성능을 극대화할 수 있습니다. 파이프라이닝 (Pipelining) 개념: 명령어를 여러 단계로 나누어 각 단계를 동시에 처리하는 기법입니다. 각 단계는 명령어의 서로 다른 부분을 처리합니다. 장점: 명령어 처리 속도를 높이고 CPU 자원의 활용도를 증가시킬 수 있습니다. 명령어 예측 (Instruction Prediction) 개념: 분기 명령어의 결과를 예측하여 분기 예측을 통해 파이프라인의 중단을 최소화합니다. 장점: 잘못된 예측을 줄이면 파이프라인의 비효율성을 줄이고 성능을 향상시킬 수 있습니다. 레지스터 재배치 (Register Renaming) 개념: 물리적 레지스터 수를 늘려서 명령어 간의 데이터 종속성을 해결하는 기법입니다. 가상의 레지스터를 사용해 실제 레지스터와의 매핑을 관리합니다. 장점: 명령어 간의 충돌을 줄여 비순차 실행의 이점을 극대화합니다. 캐시 메모리 (Cache Memory) 개념: CPU와 메모리 사이에 위치하여 자주 사용하는 데이터를 빠르게 접근할 수 있도록 하는 메모리입니다. 장점: 데이터 접근 시간을 줄여 CPU의 성능을 향상시킵니다. 멀티스레딩 (Multithreading) 개념: 하나의 CPU 코어가 여러 스레드를 동시에 실행할 수 있도록 하는 기술입니다. 각 스레드는 독립적으로 실행되며, 자원을 공유합니다. 장점: CPU의 자원 활용도를 높이고, 대기 시간 동안 다른 스레드를 실행하여 성능을 향상시킬 수 있습니다. SIMD (Single Instruction, Multiple Data) 개념: 하나의 명령어로 여러 데이터에 동시에 작업을 수행하는 방식입니다. 장점: 벡터 연산이나 이미지 처리와 같은 데이터 집약적인 작업에서 성능을 크게 향상시킬 수 있습니다. 이러한 기법들은 RISC 아키텍처에서 성능을 극대화하기 위해 사용되며, 현대 CPU 설계의 핵심 요소로 자리 잡고 있습니다. 추가적인 질문이 있다면 언제든지 말씀해 주세요!",
      "frontmatter": {}
    },
    "bash": {
      "path": "/temp/bash/",
      "filename": "bash",
      "content": "\\.\\/ vs source \\=\\= \\. 도트 및 소스 연산자 공식문서 source 와 . 은 완벽하게 동일 test.sh 파일 ps -ejH # 플로세스를 tree 형태로 보여주는 명령 실행별 차이 User@HostName:~/test$ . test.sh # . or source PID PGID SID TTY TIME CMD 3996 3996 3996 ? 00:00:00 SessionLeader 3998 3996 3996 ? 00:00:00 Relay(4003) 4003 4003 4003 pts/6 00:00:00 bash 39032 39032 4003 pts/6 00:00:00 ps User@HostName:~/test$ ./test.sh # ./ PID PGID SID TTY TIME CMD 3996 3996 3996 ? 00:00:00 SessionLeader 3998 3996 3996 ? 00:00:00 Relay(4003) 4003 4003 4003 pts/6 00:00:00 bash 39255 39255 4003 pts/6 00:00:00 test.sh # !!!!!!!!! 39256 39255 4003 pts/6 00:00:00 ps ./test.sh의 경우 다른 명령(gcc)와 마찬가지로 명령을 셸에서 해석하여 시스템에게 전달하며 새로운 프로세스 fork() 형태로 하위 프로세스로 생성한다? #ModificationRequired 하지만 . test.sh 또는 source test.sh 의 경우는 현재 실행되고 있는 셸이 직접 실행하는 것이다 그로므로 test.sh 에 선언된 전역변수가 현재 셸에 적용된다 bash 파일 실행시 interactive shell 인가 non interactive shell 인가의 차이 일반 파일 실행시 이 코드는 Bash 스크립트로, 깃 상태 정보를 비동기적으로 반환하는 데몬 프로세스인 gitstatusd 를 시작하는 데 사용됩니다. 스크립트의 각 부분을 하나씩 분석해 보겠습니다. Bash 버전 확인 if [ \"$BASH_VERSION\" < 4 ](%20\"$BASH_VERSION\"%20<%204%20); then # 에러 메시지를 표준 에러(stderr)에 출력하고 함수를 종료합니다. return 1 fi 이 부분은 현재 실행 중인 Bash의 버전이 4.0 이상인지 확인합니다. 그렇지 않다면, 사용자에게 메시지를 출력하고 함수를 종료합니다. 옵션 파싱 unset OPTIND local opt timeout=5 max_dirty=-1 ttl=3600 extra_flags=... while getopts \"t:s:u:c:d:m:r:eUWD\" opt; do case \"$opt\" in ... esac done (( OPTIND == $# + 1 )) || { echo \"usage: gitstatus_start [OPTION]...\" >&2; return 1; } [ -z \"${GITSTATUS_DAEMON_PID:-}\" ](%20-z%20\"${GITSTATUS_DAEMON_PID:-}\"%20) || return 0 스크립트 실행 시 전달된 명령줄 옵션들을 처리합니다. getopts 빌트인을 사용하여 각각의 옵션에 대한 값을 지역 변수에 할당합니다. 플러그인 디렉터리 설정 if [ \"${BASH_SOURCE[0](%20\"${BASH_SOURCE[0); then ... fi gitstatus_plugin_dir 변수는 스크립트 파일이 있는 디렉터리 경로를 설정합니다. 이 경로는 후속 명령어에서 사용됩니다. gitstatus_start_impl 함수 이 함수는 데몬 프로세스를 시작하고, 필요한 파일 FIFO (First-In First-Out) 특수 파일을 생성 및 구성하고, 데몬과 통신을 설정합니다. function gitstatus_start_impl() { ... } tmpdir 설정 및 FIFO 파일 생성 tmpdir=\"$(command mktemp -d \"$tmpdir\"/gitstatus.bash.$$.XXXXXXXXXX)\" || return command mkfifo -- \"$req_fifo\" \"$resp_fifo\" || return 임시 디렉터리를 생성하고, 데몬에 요청을 보내고 응답을 받는데 사용될 FIFO 파일을 만듭니다. 데몬 프로세스 시작 source \"$gitstatus_plugin_dir\"/install || return \"$_gitstatus_bash_daemon\" ... <&\"$fd_in\" >&\"$fd_out\" & install 스크립트를 소스화 함으로써 데몬 프로세스를 가져오고 구동합니다. 파일 디스크립터 할당 및 정리 exec {_GITSTATUS_REQ_FD}>>\"$req_fifo\" {_GITSTATUS_RESP_FD}<\"$resp_fifo\" || return command rm -f -- \"$req_fifo\" \"$resp_fifo\" || return 읽기 및 쓰기 작업을 위해 파일 디스크립터를 FIFO 파일에 할당하고, 더 이상 필요하지 않은 파일은 제거합니다. 데몬 초기화 확인 IFS='' read -r -u $_GITSTATUS_RESP_FD GITSTATUS_DAEMON_PID || return [ \"$GITSTATUS_DAEMON_PID\" == [1-9](%20\"$GITSTATUS_DAEMON_PID\"%20==%20[1-9) || return 데몬 프로세스가 성공적으로 시작되었는지 확인하고, 그 PID를 저장합니다. 청소 unset -f gitstatus_start_impl 필요하지 않은 함수를 삭제하고, 청소 작업을 수행합니다. 환경 변수 내보내기 export _GITSTATUS_CLIENT_PID _GITSTATUS_REQ_FD _GITSTATUS_RESP_FD GITSTATUS_DAEMON_PID 데몬 관련 변수를 내보내어 다른 스크립트에서 사용할 수 있게 합니다. 전체적인 요약 이 스크립트는 gitstatusd 데몬을 시작하고, 해당 데몬과 통신할 수 있게 준비하는 일련의 과정을 수행합니다. 이를 통해 git 저장소의 상태 정보를 빠르게 제공받을 수 있으며, 쉘 프롬프트를 더 효율적으로 만들어 주는 기능을 제공합니다. ======================= 이 코드는 gitstatus_start 라는 함수를 정의하고 있습니다. 이 함수는 git 상태를 빠르게 가져오는데 사용되는 gitstatusd 데몬을 시작하는 역할을 합니다. 각 부분의 목적과 문법은 다음과 같습니다: BASH 버전 확인: 이 코드는 BASH 버전이 4.0 이상인지 확인합니다. 이는 gitstatusd 가 BASH 4.0 이상에서만 작동하기 때문입니다. if [ \"$BASH_VERSION\" < 4 ](%20\"$BASH_VERSION\"%20<%204%20); then &2 printf 'gitstatus_start: need bash version >= 4.0, found %s\\n' \"$BASH_VERSION\" return 1 fi 옵션 파싱: getopts 를 사용하여 함수에 전달된 옵션을 파싱합니다. 각 옵션은 gitstatusd 의 동작을 조정하는데 사용됩니다. unset OPTIND local opt timeout=5 max_dirty=-1 ttl=3600 extra_flags= local max_num_staged=1 max_num_unstaged=1 max_num_conflicted=1 max_num_untracked=1 while getopts \"t:s:u:c:d:m:r:eUWD\" opt; do case \"$opt\" in t) timeout=$OPTARG;; s) max_num_staged=$OPTARG;; u) max_num_unstaged=$OPTARG;; c) max_num_conflicted=$OPTARG;; d) max_num_untracked=$OPTARG;; m) max_dirty=$OPTARG;; r) ttl=$OPTARG;; e) extra_flags+='--recurse-untracked-dirs ';; U) extra_flags+='--ignore-status-show-untracked-files ';; W) extra_flags+='--ignore-bash-show-untracked-files ';; D) extra_flags+='--ignore-bash-show-dirty-state ';; *) return 1;; esac done 데몬 시작: gitstatusd 데몬을 시작합니다. 데몬은 백그라운드에서 실행되며, git 상태 정보를 빠르게 가져오는데 사용됩니다. if [ -z \"${GITSTATUS_DAEMON_PID:-}\" ](%20-z%20\"${GITSTATUS_DAEMON_PID:-}\"%20); then # 데몬 시작 코드... fi 데몬 초기화 실패 처리: 만약 gitstatusd 데몬이 제대로 시작되지 않았다면, 오류 메시지를 출력하고 함수를 종료합니다. if ! gitstatus_start_impl; then &2 printf '[\\033[31mERROR\\033[0m]: gitstatus failed to initialize.\\n' gitstatus_stop return 1 fi 이 함수는 gitstatusd 데몬을 효율적으로 관리하고, git 상태 정보를 빠르게 가져오는 기능을 제공합니다. 이는 git 저장소가 매우 큰 경우에 유용하며, git 상태 정보를 표시하는 프롬프트를 빠르게 업데이트하는 데 사용될 수 있습니다. 이 함수는 gitstatus 플러그인의 일부입니다. 이 플러그인은 Zsh 및 Bash 쉘에서 사용할 수 있습니다.",
      "frontmatter": {
        "tags": [
          "shell"
        ],
        "date": "2024-02-21T16:50:00+09:00",
        "lastmod": "2024-02-21T16:50:00+09:00"
      }
    },
    "c 에서 오류를 발생시키는 방법": {
      "path": "/temp/c-에서-오류를-발생시키는-방법/",
      "filename": "c 에서 오류를 발생시키는 방법",
      "content": "반환 값 사용: 함수에서 특정 값을 반환하여 오류를 나타냅니다. return -1; // 오류를 나타내는 음수 값 return NULL; // 포인터 함수에서 오류 표시 exit() 함수: 프로그램을 즉시 종료하고 운영 체제에 상태 코드를 반환합니다. #include <stdlib.h> exit(1); // 비정상 종료를 나타내는 0이 아닌 값 abort() 함수: 프로그램을 비정상적으로 종료하고 코어 덤프를 생성합니다. #include <stdlib.h> abort(); assert() 매크로: 조건이 거짓일 때 프로그램을 중단하고 오류 메시지를 출력합니다. #include <assert.h> assert(condition); // condition이 거짓이면 오류 발생 perror() 함수: 시스템 오류 메시지를 표준 오류로 출력합니다. #include <stdio.h> perror(\"오류 발생\"); fprintf()를 이용한 오류 메시지 출력: 오류 메시지를 표준 오류 스트림으로 출력합니다. #include <stdio.h> fprintf(stderr, \"오류: %s\\n\", \"오류 메시지\"); setjmp()와 longjmp() 함수: 예외 처리와 유사한 기능을 구현할 수 있습니다. #include <setjmp.h> jmp_buf env; if (setjmp(env) == 0) { // 정상 실행 코드 } else { // 오류 처리 코드 } // 오류 발생 시 longjmp(env, 1); errno 전역 변수: 시스템 호출이나 라이브러리 함수의 오류 코드를 저장합니다. #include <errno.h> #include <string.h> if (errno != 0) { fprintf(stderr, \"오류: %s\\n\", strerror(errno)); } 이러한 방법들을 상황에 맞게 적절히 조합하여 사용하면 C 프로그램에서 효과적으로 오류를 처리할 수 있습니다.",
      "frontmatter": {
        "tags": [
          "c"
        ],
        "date": "2024-09-27T13:07:00+09:00",
        "lastmod": "2024-09-27T13:07:00+09:00"
      }
    },
    "c++ 입력함수": {
      "path": "/temp/c++-입력함수/",
      "filename": "c++ 입력함수",
      "content": "// 정수 int n; cin >> n; // 문자열 string str; cin >> str; <iostrem> 에 포함 표준 입력 버퍼에서 개행문자를 제외한 값을 가져옴 공백이나 개행입력시 공백 이전까지의 값만 결과로 받아드린다 개행문자를 입력 버퍼에 남겨둔다 getline()함수는 두 가지가 존재하는데 각가 다른 라이브러리에 존재한다. istream 라이브러리에 속한 cin.getline()함수와 string 라이브러리에 속하는 getline()함수가 있다. istream 라이브러리의 cin.getline() 문자 배열이며 마지막 글자가 ‘\\0’(terminator)인 c-string을 입력 받는데 사용 n-1개의 문자 개수만큼 읽어와 str에 저장 (n번째 문자는 NULL(‘\\0’)로 바꾼다.) 세 번째 인자인 delim은 별도로 지정해주지 않으면 엔터(‘\\n’)로 인식 delim을 지정해주면 그 제한자(delim)문자 직전까지 읽어서 str에 저장 cin.getline(char* str, streamsize n); cin.getline(char* str, streamsize n, char dlim); cin.getline(변수 주소, 최대 입력 가능 문자수, 종결 문자); ex) cin.getline(str, 100); string 라이브러리의 getline() 최대 문자 수를 입력하지 않아도 됨. 원하는 구분자(delimiter)를 만날 때 까지 모든 문자열을 입력 받아 하나의 string 객체에 저장 getline(istream& is, string str); getline(istream& is, string str, char dlim); getline(입력스트림 오브젝트, 문자열을 저장할 string객체, 종결 문자); ex) getline(cin, str); [](https://kyu9341.github.io/C-C/2020/01/17/C++getline()/#%EC%A3%BC%EC%9D%98 \"주의\")주의[](https://kyu9341.github.io/C-C/2020/01/17/C++getline()/#%EC%A3%BC%EC%9D%98) getline() 함수를 사용할 때 주의할 점이 있다. 1 2 3 4 int n; string str; cin >> n; getline(cin, str); 위와 같은 상황을 보자. 위 코드대로 실행을 하면 n을 입력 받은 후 문자열을 입력받지 않고 바로 다음 코드로 넘어가게 된다. 이유는 버퍼에 정수 값을 입력한 뒤 누른 엔터(‘\\n’)가 그대로 남아있어 getline()에 들어가기 때문이다. 이를 해결하기 위해 cin.ignore() 라는 함수를 사용할 수 있다. 1 2 3 4 5 int n; string str; cin >> n; cin.ignore(); getline(cin, str); 위와 같이 변경하면 cin.ingore()가 입력 버퍼의 모든 내용을 제거해주어 getline()이 정상적으로 동작할 수 있다. 추가적으로 cin.ignore() 함수에 대해 알아보자면 cin.ignore(int n, char dlim); cin.ignore(읽어들일 문자의 개수, 종결 문자); 와 같은 형태로도 사용이 가능하다. 표준 입력 버퍼에서 문자를 하나만 가져온다. 문자 하나만 입력이 가능하며 공백과 개행도 입력으로 포함한다. 1 2 3 char ch1, ch2; ch1 = cin.get(); ch2 = cin.get();",
      "frontmatter": {
        "date": "2023-12-24T08:43:00+09:00",
        "lastmod": "2025-06-05T06:16:41+09:00"
      }
    },
    "composite pattern": {
      "path": "/temp/composite-pattern/",
      "filename": "composite pattern",
      "content": "복합 객체와 단일 객체를 동일하게 취급하고 싶다 과일과 과일박스를 동일한 과일박스 안에 넣고 싶다 트리 형태로 퍼져나간다 class animal: def __init__(self, name): self.name = name def speak(self): pass class dog(animal): def speak(self): print(self.name) class cat(animal): def speak(self): print(self.name) class animal_group(animal): def __init__(self): self.animals = [] def add(self, animal): self.animals.append(animal) def speak(self): print(\"group speak\") for animal in self.animals: animal.speak() group1 = animal_group() group1.add(dog(\"dog1\")) group1.add(dog(\"dog2\")) group1.add(cat(\"cat1\")) group2 = animal_group() group2.add(cat(\"cat2\")) group2.add(group1) group2.speak() ==== group speak cat2 group speak dog1 dog2 cat1",
      "frontmatter": {
        "tags": [
          "design_patterns"
        ],
        "date": "2024-02-06T04:27:00+09:00",
        "lastmod": "2024-02-06T04:27:00+09:00"
      }
    },
    "css hidden 대신 transition": {
      "path": "/temp/css-hidden-대신-transition/",
      "filename": "css hidden 대신 transition",
      "content": "Tailwind CSS를 사용한다고 가정하고, 좌우 사이드바(왼쪽 사이드바와 오른쪽 사이드바)가 각각 화면 왼쪽/오른쪽 밖에 숨어 있다가, 필요할 때 애니메이션과 함께 나타나도록 설정하는 과정입니다. 🔧 요구사항 요약 좌우 사이드바에 다음 클래스를 추가: transition-transform : transform 속성에 대한 트랜지션(애니메이션)을 활성화 duration-300 : 트랜지션 지속 시간을 300ms로 설정 초기 상태에서: 왼쪽 사이드바는 화면 왼쪽 밖에 위치 → -translate-x-full 오른쪽 사이드바는 화면 오른쪽 밖에 위치 → translate-x-full 기존의 hidden 클래스는 제거 → hidden 은 display: none 을 적용하므로, 애니메이션이 작동하지 않음 📄 HTML 구조 예시 (수정 전) 먼저, 일반적인 사이드바 구조를 예로 들어보겠습니다: <!-- 왼쪽 사이드바 --> <aside id=\"left-sidebar\" class=\"hidden\"> <!-- 콘텐츠 --> </aside> <!-- 오른쪽 사이드바 --> <aside id=\"right-sidebar\" class=\"hidden\"> <!-- 콘텐츠 --> </aside> 이 상태에서는 hidden 클래스 때문에 두 사이드바가 완전히 DOM에서 사라지며, CSS 트랜지션(애니메이션)이 작동하지 않습니다. ✅ 수정된 HTML 구조 (수정 후) <!-- 왼쪽 사이드바 --> <aside id=\"left-sidebar\" class=\"fixed inset-y-0 left-0 z-50 w-64 bg-white shadow-lg transform -translate-x-full transition-transform duration-300\"> <!-- 사이드바 콘텐츠 --> </aside> <!-- 오른쪽 사이드바 --> <aside id=\"right-sidebar\" class=\"fixed inset-y-0 right-0 z-50 w-64 bg-white shadow-lg transform translate-x-full transition-transform duration-300\"> <!-- 사이드바 콘텐츠 --> </aside> 📌 각 클래스의 역할 상세 설명 fixed inset-y-0 left-0 / right-0 fixed : 뷰포트 기준으로 고정 위치 inset-y-0 : 상단과 하단을 0으로 고정 → 전체 높이 차지 left-0 / right-0 : 왼쪽 또는 오른쪽 끝에 붙임 w-64 사이드바 너비를 16rem (256px)로 설정 (Tailwind 기본값) bg-white shadow-lg 시각적 스타일 (배경색 + 그림자) — 실제 디자인에 따라 달라질 수 있음 transform 필수 클래스: Tailwind에서 translate-* 클래스를 사용하려면 반드시 transform 클래스가 있어야 실제 CSS transform 속성이 적용됩니다. 이 클래스가 없으면 -translate-x-full 등이 무시될 수 있습니다. -translate-x-full (왼쪽 사이드바) 요소를 자신의 너비만큼 왼쪽으로 이동 → 화면 밖으로 사라짐 예: 너비가 256px이면, 왼쪽으로 256px 이동 → 화면 왼쪽 끝에서 완전히 숨김 translate-x-full (오른쪽 사이드바) 요소를 자신의 너비만큼 오른쪽으로 이동 → 화면 오른쪽 밖으로 사라짐 transition-transform transform 속성(예: translateX )에 대해 애니메이션을 적용하도록 지시 이 클래스가 없으면 이동이 즉시 일어나서 애니메이션이 없음 duration-300 트랜지션 지속 시간을 300ms로 설정 → 부드러운 슬라이드 인/아웃 효과 hidden 클래스 제거 hidden 은 display: none 을 적용 → 요소가 렌더링되지 않음 display: none 상태에서는 어떤 CSS 트랜지션도 작동하지 않음 대신, 위치만 이동시켜 시각적으로 숨기는 방식을 사용 → 애니메이션 가능 🎯 사이드바를 보이게 하려면? 나중에 JavaScript나 Alpine.js, Vue 등으로 사이드바를 열고 싶을 때는: 왼쪽 사이드바: -translate-x-full → 제거 (또는 translate-x-0 추가) 오른쪽 사이드바: translate-x-full → 제거 (또는 translate-x-0 추가) 예시 (JavaScript로 왼쪽 사이드바 열기): document.getElementById('left-sidebar').classList.remove('-translate-x-full'); // 또는 document.getElementById('left-sidebar').classList.add('translate-x-0'); 닫을 때는 다시 -translate-x-full 를 추가하면 됩니다. 💡 요약 변경 사항 설명 hidden 제거 애니메이션을 위해 요소는 항상 DOM에 존재해야 함 transform 추가 Tailwind에서 translate 계열 클래스를 동작시키기 위함 -translate-x-full / translate-x-full 초기에 화면 밖으로 이동 transition-transform duration-300 부드러운 슬라이드 애니메이션 활성화 이렇게 설정하면, 사이드바는 처음에는 보이지 않지만, 필요할 때 애니메이션과 함께 부드럽게 슬라이드 인/아웃할 수 있습니다. Tailwind의 유틸리티 기반 클래스 시스템을 최대한 활용한 접근입니다.",
      "frontmatter": {
        "tags": [
          "ai-content"
        ],
        "date": "2025-09-27T06:18:15+09:00",
        "lastmod": "2025-09-27T06:18:29+09:00"
      }
    },
    "dbms 잡 정보": {
      "path": "/temp/dbms-잡-정보/",
      "filename": "dbms 잡 정보",
      "content": "데이터베이스 별 기본 파일 저장 위치 /opt/oracle/oradata : oracle database /var/lib/mysql : mysql /var/lib/mysql : mariadb /var/lib/postgresql/<version>/main : postgresql",
      "frontmatter": {
        "tags": [
          "database",
          "잡지식"
        ],
        "date": "2024-04-20T01:32:00+09:00",
        "lastmod": "2024-04-20T01:32:00+09:00"
      }
    },
    "docker 명령 모음": {
      "path": "/temp/docker-명령-모음/",
      "filename": "docker 명령 모음",
      "content": "docker run (<옵션>) <이미지 식별자> (<명령어>) (<인자>) d : detach => background it : interactive, tty docker exec -it <컨테이너 식별자> (<명령어>)",
      "frontmatter": {
        "tags": [
          "command"
        ],
        "date": "2024-04-08T13:33:00+09:00",
        "lastmod": "2024-04-08T13:33:00+09:00"
      }
    },
    "docker 없이 컨테이너": {
      "path": "/temp/docker-없이-컨테이너/",
      "filename": "docker 없이 컨테이너",
      "content": "관련 리포지토리 유튜브 영상 아래의 패지지 의존 sudo apt-get update && sudo apt-get -y install gcc && sudo apt-get -y install make && sudo apt-get -y install pkg-config && sudo apt-get -y install libseccomp-dev && sudo apt-get -y install tree && sudo apt-get -y install jq && sudo apt-get -y install bridge-utils 전제 루트 파일 시스템 container => / 는 overlay 로 마운트 host => / 는 일반적인 곳으로 마운트 프로세스 container => 1번이 shell host => 일반 네트워크 container => eth0@~~ host => 일반 유저 및 호스트 container => root (host 의 root 와 동일할까??), 이상한 문자열 호스트네임 host => chroot 프로세스를 가두자 => 사용자 프로세스를 fake 루트를 통해 막는다 (프로세스를 가둔다는 말이 무었일까 #ModificationRequired ) chroot myroot /dirname docker export $(docker create nginx) | tar -C nginx-root -xvf -; 네임스페이스 chroot 의 경우 #ModificationRequired 이유로 프로세스를 탈옥이 가능했다 하지만 마운트의 원리를 사용하여 루트 파일시스템을 pivot_root 명령으로 바꿀 수 있고 이러면 원래 루트파일 시스템으로 접근 할 수 없다 또한 이러한 접근 권한을 폴더 접근 뿐만 아니라 네트워크 ipc 마운트 등등으로 적용하여 접근을 제한 할 수 있다 이러한 제한에 사용되는 용어가 바로 네임스페이스이다 lsns 명령으로 네임스페이스 확인가능 inode 로 확인 ushare 명령으로 네임스페이스를 변경 가능 mount chroot_pivot 이란 루트 파일 시스템을 변경 overlay 마운트 개념 오버레이를 통해 프로그램이 필요한 위존성을 따로 hub 에 올릴 수 있다 네트워크 /var/run/netns 위치에 저장",
      "frontmatter": {
        "tags": [
          "linux"
        ],
        "date": "2024-04-10T03:21:00+09:00",
        "lastmod": "2024-04-10T03:21:00+09:00"
      }
    },
    "execute run code": {
      "path": "/temp/execute-run-code/",
      "filename": "execute run code",
      "content": "int main() { printf(\"hello world\"); return 0; } int main() { std::cout << \"Hello, World!\\n\"; return 0; } print(\"hello world\") class Main { static public void main(String []args) { System.out.println(\"Hello, World!\"); } }",
      "frontmatter": {}
    },
    "find 명확하게 이해하자": {
      "path": "/temp/find-명확하게-이해하자/",
      "filename": "find 명확하게 이해하자",
      "content": "어떤 폴더를 제외하고 파일을 찾고자 -not -path 옵션을 사용했지만 접근 불가능한 파일(proc 내부의 파일들)에 접근하려고 해서 오류(Permission denied)를 뿜어내서 찾아보게 되었다 하지만 생각보다 조금 내용이 복잡한 것 같아 완벽하게 정리해 보고자 한다 find 명령이 하는 일이 단순하게 2가지를 한다고 이해하면 찾는 것 ~= 탐색하는 것 탐색의 나열중 조건에 맞는 것을 출력하는 것 2가지 이다 find 기본 문법 find {option} {starting pointing} {expression} ... option starting pointing : find 명령의 시작 위치 expression: 조건 여러가지 종류가 존재 테스트(Tests) : 파일의 속성에 기반한 true false 값 반환 ex) -name 'stdio.h' => 이름이 stdio.h 인 경우 true 동작(Actions) : T/F 반환 이외의 std 출력과 같은 부가적이 동작이 있음 그리고 그 부가적인 동작의 성공 여부에 따라 T/F 를 반환 ex) -path '/include' -prune => '/include' 폴더의 내부는 탐색하지 않는다 또한 전체의 결과는 true 이다 전역 옵션(Global options) 전역 옵션은 명령 줄의 어느 부분에서도 지정된 테스트 및 동작의 작동에 영향을 줍니다. 전역 옵션은 항상 true를 반환합니다. 예를 들어 -depth 옵션은 find가 파일 시스템을 깊이 우선 순서로 탐색하도록 만듭니다. 위치 옵션(Positional options) 위치 옵션은 그들 뒤에 따라오는 테스트 또는 동작에만 영향을 미칩니다. 위치 옵션은 항상 true를 반환합니다. 예를 들어 -regextype 옵션은 위치 지정이며, 명령 줄에서 나중에 발생하는 정규 표현식에 대해 정규 표현식 방언을 지정합니다. 연산자(Operators) 연산자는 표현 내의 다른 항목들을 결합합니다. 예를 들어 -o(논리 OR를 의미함)와 -a(논리 AND를 의미함)가 있습니다. 연산자가 누락된 경우 -a가 가정됩니다. /usr 폴더에서 /usr/bin 출력하지도 탐색하지도 않고 이름이 stdio.h 인 파일을 찾는다 find /usr -not \\( -path '/usr/bin' -prune \\) -name 'stdio.h' / 에서 시작한다 path 옵션을 통해 /usr/bin 와 일치하는 것을 찾는다 prune 옵션을 통해 찾은 파일이 디렉토리인 경우 더이상 탐색하지 않는다 그러므로 /usr/bin 폴더 만이 찾아진다 not 옵션을 통해 -path 옵션을 통해 찾은 폴더는 false 이다 name 옵션을 통해 이름이 'stdio.h' 인 파일을 찾는다 이 코드는 표현식 트리를 구성하는 함수입니다. 주요 작업은 다음과 같습니다: 표현식 파싱: 입력된 인수들을 파싱하여 표현식을 구성합니다. 각각의 인수는 특정 조건을 나타내는 'predicate’로 처리됩니다. 이 과정에서 find_parser 함수를 사용하여 각 인수에 대한 파서를 찾고, 해당 파서의 함수를 호출하여 표현식을 구성합니다. 표현식 트리 구성: 파싱된 표현식들을 트리 형태로 구성합니다. 이 트리는 각 노드가 하나의 조건을 나타내며, 트리를 순회하면서 각 조건을 평가하여 전체 표현식의 결과를 도출합니다. 트리 최적화: 표현식 트리를 최적화하여 평가 시간을 줄입니다. 트리의 노드를 재배치하여 평가 순서를 변경하고, 가능한 경우 일부 조건의 평가를 생략하도록 합니다. 디버깅 정보 출력: 디버깅 옵션이 활성화된 경우, 표현식 트리와 최적화된 명령 줄을 출력합니다. 이 정보는 표현식의 구조와 최적화 과정을 이해하는 데 도움이 됩니다. 이 함수의 결과는 최적화된 표현식 트리입니다. 이 트리는 후속 처리에서 사용되어, 각 파일이 주어진 조건에 맞는지 평가하는 데 사용됩니다. 이 함수는 주어진 인수들을 통해 표현식을 구성하고 최적화하는 역할을 합니다. 이 과정에서 여러 가지 오류 상황을 체크하고, 오류가 발생하면 적절한 에러 메시지를 출력하고 프로그램을 종료합니다. 이는 입력된 인수들이 올바른 표현식을 형성하지 않는 경우에 발생합니다. 이러한 방식으로, 이 함수는 표현식의 구성과 검증을 담당하며, 이를 통해 파일 검색 조건을 효과적으로 처리합니다.",
      "frontmatter": {
        "aliases": [
          "find"
        ],
        "tags": [
          "shell",
          "command"
        ],
        "date": "2024-03-02T14:44:00+09:00",
        "lastmod": "2024-03-02T14:44:00+09:00"
      }
    },
    "gdb 사용법": {
      "path": "/temp/gdb-사용법/",
      "filename": "gdb 사용법",
      "content": "기본 명령어 명령어 설명 list 현재 디버깅 중인 소스 코드의 일부를 출력합니다. run 프로그램을 실행합니다. (명령 뒤에 인자를 추가하여 프로그램 실행 시 인자를 전달할 수 있습니다.) break (또는 b ) 브레이크포인트를 설정합니다. (예: b main 또는 b 파일명:줄번호 ) clear 특정 위치에 설정된 브레이크포인트를 삭제합니다. delete 설정된 브레이크포인트를 삭제합니다. (예: delete 1 으로 특정 브레이크포인트 삭제) next (또는 n ) 한 줄씩 코드 실행을 진행합니다(함수 호출은 건너뜀). step (또는 s ) 한 줄씩 코드 실행을 진행하며, 함수 호출 내부로 진입합니다. print (또는 p ) 변수나 표현식의 값을 출력합니다. (예: p 변수명 ) display 특정 변수의 값을 계속 표시합니다. (예: display 변수명 ) bt (또는 backtrace ) 호출 스택(traceback)을 출력합니다. kill 실행 중인 프로그램을 강제 종료합니다. cout GDB에서 직접 제공하지 않는 명령어로, 일반적으로 C++에서 표준 출력 스트림을 의미합니다. help GDB 명령어 도움말을 제공합니다. (예: help break ) quit GDB를 종료합니다. watch varname 특정 변수( varname )의 값이 변경될 때 중단합니다. info locals 현재 스코프의 지역 변수 값을 표시합니다. info variables 모든 전역 변수와 정적 변수를 표시합니다. info break 설정된 모든 브레이크포인트를 나열합니다. info func 디버깅 대상의 모든 함수 이름을 나열합니다. set 특정 변수나 환경 설정을 변경합니다. (예: set var 변수명 = 값 ) finish 현재 함수의 실행을 끝내고 호출한 함수로 복귀합니다. 추가 명령어 명령어 설명 continue (또는 c ) 브레이크포인트까지 계속 실행합니다. stepi 기계어 수준에서 한 단계 실행합니다. disassemble 기계어 수준에서 디스어셈블리된 코드를 출력합니다. info threads 현재 실행 중인 모든 스레드의 정보를 표시합니다. thread apply 특정 스레드에 대해 명령을 적용합니다. (예: thread apply all bt 는 모든 스레드의 백트레이스를 출력) set args 실행 시 전달할 프로그램의 인자를 설정합니다. info registers 레지스터의 상태를 출력합니다. x 메모리를 검사합니다. (예: x/10x 변수 는 변수부터 10개의 16진수를 출력) attach 실행 중인 프로세스에 연결하여 디버깅합니다. detach 연결된 프로세스에서 디버깅을 종료하고 분리합니다. 추가로 궁금한 명령어나 세부적인 사용 예시가 필요하다면 알려주세요! 😊",
      "frontmatter": {
        "tags": [
          "c",
          "cpp",
          "gdb"
        ],
        "date": "2024-12-30T23:48:00+09:00",
        "lastmod": "2024-12-30T23:48:00+09:00"
      }
    },
    "interactive shell vs non-interactive shell": {
      "path": "/temp/interactive-shell-vs-non-interactive-shell/",
      "filename": "interactive shell vs non-interactive shell",
      "content": "ex) 터미널 vs #!/bin/bash 로 시작하는 파일 파일의 셸은 $BASH_ENV 환경변수를 로드한다",
      "frontmatter": {
        "tags": [
          "shell"
        ],
        "date": "2024-02-21T17:21:00+09:00",
        "lastmod": "2024-02-21T17:21:00+09:00"
      }
    },
    "java cli compile": {
      "path": "/temp/java-cli-compile/",
      "filename": "java cli compile",
      "content": "",
      "frontmatter": {
        "tags": [
          "java",
          "language"
        ],
        "date": "2024-01-10T00:09:00+09:00",
        "lastmod": "2024-01-10T00:09:00+09:00"
      }
    },
    "java 쓰레드(thread)": {
      "path": "/temp/java-쓰레드thread/",
      "filename": "java 쓰레드(thread)",
      "content": "java 메모리(memory)).md) 참고 Pasted image 2024!210440.png) java는 thread의 쓰레드 구현 방법 Thread 상속 Runnable 인터페이스 구현 Thread thread = new Thread(){ // Thread 상속 및 익명 클래스로 사용 @Override public void run() { 여기에 사용 } }; thread.start(); Thread thread = new Thread(new Runnable(){ // Runnable 구현 및 생성자의 인자 @Override public void run() { 여기에 사용 } }); thread.start(); Thread 를 상속한 객체는 그 자체로 하나의 쓰레드 객체를 의미하지만 Runnable 인터페이스를 구현한 객체는 쓰레드로 들어가 일거리라는 의미이다 그러므로 Thread 객체를 new Thread 할 때 생성자에 넣어주어야 한다 스레드 상태 Pasted image 20240218213931 NEW : Thread 객체만 생성된 상태 RUNNABLE : thread.start() 이후 실행중인 상태 또는 cpu 스케줄링 순서에 따라 대기 상태 일시정지 BLOCKED : 특정 조건시 풀린다 ex) Scanner 사용자 입력 기다림 WAITING : TIMED_WAITING : 시간 지난후 풀린다 ex) sleep() TERMINATED; [NEW: 쓰레드가 생성되었지만 아직 시작되지 않은 상태입니다1. 이 상태에서는 쓰레드가 아직 실행되지 않았으므로 시스템 자원이 할당되지 않습니다2. RUNNABLE: 쓰레드가 실행 중이거나 언제든지 실행할 준비가 된 상태입니다3. 이 상태에서 쓰레드는 실제로 실행 중일 수도 있고, 운영 체제로부터 다른 리소스를 기다리고 있을 수도 있습니다1. BLOCKED: 쓰레드가 모니터 잠금을 획득하려고 시도하지만 현재 다른 쓰레드가 잠금을 보유하고 있는 상태입니다3. 쓰레드는 잠금을 획득할 때까지 BLOCKED 상태에 머무릅니다4. WAITING: 쓰레드가 무기한으로 다른 쓰레드가 특정 작업을 수행하기를 기다리는 상태입니다3. 이 상태는 쓰레드가 Object.wait() 또는 Thread.join() 메서드를 호출했을 때 발생합니다 또는 Thread.join() Thread.sleep() , Object.wait(timeout) , Thread.join(timeout) 등의 메서드를 호출했을 때 발생합니다]() , Object.wait(timeout) , Thread.join(timeout) 등의%20메서드를%20호출했을%20때%20발생합니다)6. TERMINATED: 쓰레드가 종료된 상태입니다3. 쓰레드는 작업을 성공적으로 완료하거나 오류로 인해 종료되거나 강제로 종료될 때 이 상태에 들어갑니다7. Pasted image 20240218220357 wait(), notify(), notifyAll() 은 Object 메서드 나머지는 Thread 메서드 쓰레드 그룹 실제로는 쓰레드는 계층이라는 것이 없지만 java 에서는 쓰레드를 계층 관계 그룹으로 관리 할 수 있다 prioity 우선순위 설정 기능을 활용 할 수 있다 쓰레드 풀 쓰레드의 무한 증가를 막기위해 제한된 개수만큼 정해 놓고 작업 큐에 들어오는 작업들을 쓰레드가 하나씩 맡아 처리하는 방식",
      "frontmatter": {
        "tags": [
          "java"
        ],
        "date": "2024-02-18T19:19:00+09:00",
        "lastmod": "2024-02-18T19:19:00+09:00"
      }
    },
    "join 연산 크기 추청": {
      "path": "/temp/join-연산-크기-추청/",
      "filename": "join 연산 크기 추청",
      "content": "이 문제는 관계형 데이터베이스에서 조인 연산의 크기 추정에 대한 설명입니다. 주어진 내용을 기반으로 예제를 들어 설명하겠습니다. 문제 설명: $R \\bowtie S$: 릴레이션 $R$과 $S$를 조인합니다. $A$는 두 릴레이션 $R$과 $S$ 사이의 조인 속성입니다. $n_R$: 릴레이션 $R$의 튜플 수 $n_S$: 릴레이션 $S$의 튜플 수 $V(A, S)$: 속성 $A$의 $S$에서의 고유 값 수 (카디널리티) $V(A, R)$: 속성 $A$의 $R$에서의 고유 값 수 조인 결과의 크기는 아래와 같이 두 가지 방식으로 추정됩니다: $nR \\times \\frac{nS}{V(A, S)}$ $nS \\times \\frac{nR}{V(A, R)}$ 이 두 값 중 더 작은 값을 선택하는 것이 더 정확한 추정을 # join 연산 크기 추청 제공한다고 설명합니다. 예제: 데이터: 릴레이션 $R$: A B 1 X 2 Y 3 Z $n_R = 3$, $V(A, R) = 3$ (속성 $A$의 값 1, 2, 3은 모두 고유함) 릴레이션 $S$: A C 1 P 1 Q 2 R 2 S 3 T $n_S = 5$, $V(A, S) = 3$ (속성 $A$의 값 1, 2, 3은 모두 고유함) 조인 크기 추정: 조인 $R \\bowtie S$는 $A$ 속성을 기준으로 이루어집니다. 첫 번째 추정: $nR \\times \\frac{nS}{V(A, S)} = 3 \\times \\frac{5}{3} = 5$ 두 번째 추정: $nS \\times \\frac{nR}{V(A, R)} = 5 \\times \\frac{3}{3} = 5$ 결론: 이 경우 두 추정 값이 동일하므로 조인 결과 크기를 5로 예측합니다. 조인을 실행해 보면 실제로 조인된 튜플 수가 5임을 확인할 수 있습니다. 추가 개선: 만약 속성 $A$의 히스토그램(각 값이 얼마나 자주 등장하는지에 대한 분포 정보)이 있다면, 각 값별로 보다 정확한 추정을 할 수 있습니다. 예를 들어, 값 \"1\"이 $R$에서 1번, $S$에서 2번 등장한다면 해당 값의 조인 크기는 $1 \\times 2 = 2$로 세부적으로 계산 가능합니다.",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "date": "2024-12-04T14:29:00+09:00",
        "lastmod": "2024-12-04T14:29:00+09:00"
      }
    },
    "mac java 버전 변경": {
      "path": "/temp/mac-java-버전-변경/",
      "filename": "mac java 버전 변경",
      "content": "JDKS_DIR=\"/Library/Java/JavaVirtualMachines\" JDKS=( $(ls ${JDKS_DIR}) ) JDKS_STATES=() for (( i = 0; i < ${#JDKS[@]}; i++ )); do if [ -f \"${JDKS_DIR}/${JDKS[$i](%20-f%20\"${JDKS_DIR}/${JDKS[$i); then JDKS_STATES[${i}]=enable else JDKS_STATES[${i}]=disable fi echo \"${i} ${JDKS[$i]} ${JDKS_STATES[$i]}\" done DEFAULT_JDK_DIR=\"\" DEFAULT_JDK=\"\" OPTION=\"\" while [](%20!%20\"$OPTION\"%20=~%20^[0-9]+$%20||%20OPTION%20-ge%20\"${#JDKS[@]}\"%20); do read -p \"Enter Default JDK: \" OPTION if [ ! \"$OPTION\" =~ ^[0-9](%20!%20\"$OPTION\"%20=~%20^[0-9); then echo \"Sorry integers only\" fi if [](%20OPTION%20-ge%20\"${#JDKS[@]}\"%20); then echo \"Out of index\" fi done DEFAULT_JDK_DIR=\"${JDKS_DIR}/${JDKS[$OPTION]}\" DEFAULT_JDK=\"${JDKS[$OPTION]}\" for (( i = 0; i < ${#JDKS[@]}; i++ )); do if [ -f \"${JDKS_DIR}/${JDKS[$i](%20-f%20\"${JDKS_DIR}/${JDKS[$i); then sudo mv \"${JDKS_DIR}/${JDKS[$i]}/Contents/Info.plist\" \"${JDKS_DIR}/${JDKS[$i]}/Contents/Info.plist.disable\" fi done if [ -f \"${DEFAULT_JDK_DIR}/Contents/Info.plist.disable\" ](%20-f%20\"${DEFAULT_JDK_DIR}/Contents/Info.plist.disable\"%20); then sudo mv \"${DEFAULT_JDK_DIR}/Contents/Info.plist.disable\" \"${DEFAULT_JDK_DIR}/Contents/Info.plist\" echo \"Enable ${DEFAULT_JDK} as default JDK\" fi",
      "frontmatter": {}
    },
    "nextInt vs nextLine": {
      "path": "/temp/nextint-vs-nextline/",
      "filename": "nextInt vs nextLine",
      "content": "완벽히 이해했습니다. 아래는 기존의 틀과 구조를 100% 유지하면서, 각 단계에서 nextInt() 와 nextLine() 의 동작 차이를 병렬로 설명한 버전입니다. → 동일한 입력( 25\\n )이 주어졌을 때, 두 메서드가 어떻게 다른 방식으로 버퍼를 소비하고 커서를 이동시키는지를 계층별로 명확히 대조합니다. 🧩 완전한 데이터 흐름: 사용자 입력 → Scanner 내부 버퍼 (시스템 프로그래밍 + 터미널 관점 — nextInt() vs nextLine() 비교 포함) ✍️ 목표: 사용자가 키보드로 25 를 치고 Enter 를 누른 순간부터, Java의 Scanner.nextInt() 또는 Scanner.nextLine() 이 어떻게 다른 방식으로 25\\n 을 처리하는지 모든 계층을 투명하게 추적 + 비교. 🌐 전체 흐름 다이어그램 (계층별 분리 — 두 메서드 병렬 비교) [User] → [Keyboard Hardware] → [OS TTY Driver (Line Buffering)] → [Kernel stdin Buffer] → [JVM System.in (FileInputStream)] → [InputStreamReader (decode)] → [Scanner CharBuffer + Tokenizer] ├→ nextInt() → \"25\" 반환, \\n은 버퍼에 남김 └→ nextLine() → \"25\" 반환, \\n까지 소비 🔍 1. 사용자 입력 — 키보드 인터럽트 사용자가 키보드로 2 , 5 , Enter 입력 하드웨어 인터럽트 발생 → CPU → OS 커널로 전달 커널은 TTY 드라이버(또는 PTY, 가상 터미널)에 입력을 전달 💡 여기서 중요한 개념: “터미널은 기본적으로 라인 버퍼링 모드” → 이 동작은 nextInt() 든 nextLine() 이든 공통 전제 조건입니다. ⚙️ 2. 터미널(TTY) 동작 — Canonical Mode (Cooked Mode) ✅ 기본 동작: “라인 단위 입력” — 사용자가 Enter 칠 때까지 커널이 버퍼링 터미널은 기본적으로 Canonical Mode (Cooked Mode) 로 동작 사용자가 입력하는 모든 문자는 커널의 TTY 버퍼에 쌓임 Enter ( \\n ) 또는 Ctrl+D (EOF)를 칠 때까지 아무것도 애플리케이션(JVM)에 전달되지 않음 🖥️ 즉, 25 만 치고 있으면 — JVM은 아무것도 읽지 못함. Enter 를 쳐야 비로소 커널이 25\\n 을 stdin으로 푸시. → nextInt() 든 nextLine() 이든, 이 시점까지는 동일하게 25\\n 을 받습니다. 📜 TTY 버퍼링 예시 (두 메서드 공통 출발점) User types: 2 → 5 → Enter TTY Buffer: [ '2', '5', '\\n' ] ← Enter 전까지 여기에 쌓임 ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ JVM은 아직 아무것도 못 읽음! User presses Enter → TTY sends \"25\\n\" to stdin → 이제 JVM이 읽을 수 있음. ✅ 핵심 공통점: Scanner 버퍼에는 항상 25\\n 이 들어감. ✅ 핵심 차이점: nextInt() 는 \\n 을 버퍼에 남기고, nextLine() 은 \\n 을 소비함. 📥 3. OS 커널 → JVM: read() 시스템 콜 JVM의 System.in 은 FileInputStream 기반 내부적으로 FileDescriptor.in (stdin, fd=0)을 감싸고 있음 Scanner 가 데이터를 요청하면 → source.read(buf) → 결국 read(0, buffer, len) 시스템 콜 발생 // 실제 리눅스 시스템 콜 — 두 메서드 모두 동일하게 호출 ssize_t n = read(STDIN_FILENO, buffer, sizeof(buffer)); // buffer에 \"25\\n\" 복사됨 → 이 시점에서 커널 TTY 버퍼의 25\\n 이 JVM의 바이트 버퍼로 복사됨 → 이후부터 Scanner 내부에서 두 메서드의 동작이 갈림. 🔄 4. JVM 내부: 바이트 → 문자 디코딩 (InputStreamReader) System.in 은 바이트 스트림 → Scanner는 문자 기반 중간에 InputStreamReader 가 존재 → CharsetDecoder 로 UTF-8 → char[] 변환 // Scanner 생성 시 내부적으로 이렇게 연결됨 — 두 메서드 동일 Readable source = new InputStreamReader(System.in, charset); → \"25\\n\" (UTF-8 바이트) → char[] { '2', '5', '\\n' } 로 디코딩 → Scanner의 CharBuffer 에 적재 → 이제 nextInt() 와 nextLine() 이 이 버퍼를 다르게 해석합니다. 🧠 5. Scanner 내부: 토큰 파싱 vs 라인 파싱 — 핵심 분기점 🔹 nextInt() 호출 시 int age = scanner.nextInt(); 🔁 동작 단계: 버퍼 확인: CharBuffer 에 ['2','5','\\n'] 존재 정규식 매칭: Pattern.compile(\"-?\\\\d+\") → lookingAt() → \"25\" 매칭 성공 토큰 반환: Integer.parseInt(\"25\") → age = 25 커서 이동: 버퍼의 읽기 위치를 '2','5' 다음으로 이동 → 즉, \\n 바로 앞으로 이동 → \\n 은 버퍼에 그대로 남아 있음! 🚫 nextInt() 는 토큰만 소비 — 구분자(whitespace, including \\n )는 소비하지 않음 🔸 nextLine() 호출 시 String line = scanner.nextLine(); 🔁 동작 단계: 버퍼 확인: CharBuffer 에 ['2','5','\\n'] 존재 줄 단위 읽기: 현재 위치에서 다음 \\n 까지의 모든 문자를 읽음 → \"25\" \\n 소비: \\n 도 포함해 완전히 소비 커서 이동: \\n 다음 위치(다음 줄 시작)로 이동 → 버퍼는 완전히 비워짐 (또는 다음 입력 대기 상태) ✅ nextLine() 은 줄 전체 + 줄바꿈 문자까지 소비 — 항상 커서를 다음 줄로 이동 📝 6. 혼합 사용 시 문제 — nextInt() → nextLine() int age = scanner.nextInt(); // → \"25\" 반환, \\n 버퍼에 남김 String name = scanner.nextLine(); // → 남은 \\n을 읽어 \"\" 반환 → 버그! 🔄 버퍼 상태 변화: 초기: [ '2', '5', '\\n' ] nextInt() 후: [ '\\n' ] ← 커서는 \\n 앞 nextLine() 후: [ ] ← \\n 소비, 커서는 다음 줄 → name = \"\" → 사용자는 이름을 입력할 기회조차 얻지 못함 — 버그 발생! 🆚 만약 처음부터 nextLine() 만 사용했다면? String ageStr = scanner.nextLine(); // → \"25\" 반환, \\n 소비 완료 String name = scanner.nextLine(); // → 다음 입력 정상 대기 🔄 버퍼 상태 변화: 초기: [ '2', '5', '\\n' ] 첫 nextLine() 후: [ ] ← \"25\" 반환, \\n 소비 두 번째 nextLine() → 사용자에게 입력 대기 → 정상 입력 가능 → 문제 없음! 🧱 계층별 정리: 각 레이어의 책임과 데이터 상태 + 메서드별 차이 레이어 책임 nextInt() 동작 후 상태 nextLine() 동작 후 상태 시스템 프로그래밍 키워드 Keyboard + Hardware 전기 신호 → 인터럽트 2 , 5 , Enter 신호 발생 (공통) 동일 IRQ, PS/2, USB HID OS TTY Driver 라인 버퍼링 25\\n 전달 (공통) 동일 termios , ICANON Kernel stdin Buffer read()로 전달 25\\n 전달 (공통) 동일 fd 0 , read(2) JVM System.in 바이트 스트림 byte[]{50,53,10} (공통) 동일 FileInputStream , JNI InputStreamReader 바이트 → 문자 char[]{'2','5','\\n'} (공통) 동일 UTF-8 , StreamDecoder Scanner CharBuffer 토큰/라인 파싱 커서: \\n 앞, \\n 잔류 커서: \\n 다음, 버퍼 비움 Pattern , Matcher , readLine() Your Code 비즈니스 로직 age=25 , 이후 nextLine() 이 빈 문자열 line=\"25\" , 이후 정상 입력 가능 nextInt() , nextLine() 💡 왜 이렇게 복잡한가? — 추상화와 트레이드오프 터미널 라인 버퍼링: 사용자 편의 → 공통 전제 Scanner 토큰 기반 vs 라인 기반: → nextInt() 는 유연한 토큰 파싱을 위해 구분자 보존 → nextLine() 은 줄 단위 처리를 위해 구분자 소비 버퍼링 계층: 시스템 콜 최소화 → 성능 향상 → 두 메서드는 같은 버퍼를 공유하지만, “어디까지 소비할 것인가”에 대한 철학이 다름 → 혼용 시 레이어 간 데이터 흐름이 깨져 버그 발생 🛠️ 해결책 — 시스템 프로그래머의 시각에서 ✅ 방법 1: nextInt() 후 nextLine() 으로 버퍼 정리 int age = scanner.nextInt(); scanner.nextLine(); // ← \\n 소비 — 커서를 다음 줄로 강제 이동 String name = scanner.nextLine(); // 정상 입력 → Scanner 레이어에서 수동으로 커서를 다음 줄로 이동 ✅ 방법 2: 항상 nextLine() 사용 → 파싱은 수동으로 int age = Integer.parseInt(scanner.nextLine()); // \\n까지 소비 String name = scanner.nextLine(); // 다음 줄 정상 입력 → 토큰/라인 혼용 문제를 원천 차단 — 권장! ✅ 방법 3: 터미널 모드 변경 (Advanced) — Raw Mode stty raw java YourProgram stty cooked → 권장하지 않음 — Scanner의 파서 설계와 충돌 가능성 ↑ 📊 시각화: 입력 데이터의 생명 주기 — 두 메서드 비교 [User] │ ▼ [Keyboard] → \"2\" → \"5\" → \"\\n\" (Enter) — 공통 시작점 │ ▼ [TTY Driver] → Canonical Mode → \"25\\n\" 전달 — 공통 │ ▼ [Kernel stdin] → read() → \"25\\n\" 전달 — 공통 │ ▼ [JVM System.in] → byte[]{50,53,10} — 공통 │ ▼ [InputStreamReader] → char[]{'2','5','\\n'} — 공통 │ ▼ [Scanner CharBuffer] — 여기서 분기! ├─ nextInt() → \"25\" 반환 → 커서: \\n 앞 → \\n 잔류 │ │ │ ▼ │ [nextLine()] → 즉시 \\n 소비 → \"\" 반환 → 버그! │ └─ nextLine() → \"25\" 반환 → \\n 소비 → 커서: 다음 줄 → 정상 ✅ 최종 결론 — 시스템 프로그래머라면 이렇게 말해야 한다 “사용자 입력은 터미널의 Canonical Mode에 의해 라인 단위로 커널 버퍼에 쌓이다가 Enter 와 함께 25\\n 형태로 JVM에 전달된다. Scanner는 이 문자열을 내부 CharBuffer에 적재한 후, nextInt() 는 정수 토큰 ‘25’만 추출하고 줄바꿈 문자 \\n 은 구분자로 간주해 소비하지 않아 버퍼에 남긴다. 반면 nextLine() 은 줄 전체를 읽고 \\n 까지 소비해 커서를 다음 줄로 이동시킨다. 따라서 nextInt() 직후 nextLine() 을 호출하면, 남아 있던 \\n 을 ‘빈 줄’로 해석해 즉시 반환함으로써 입력 누락이라는 치명적 버그를 유발한다. 이는 각 계층(OS TTY, JVM InputStream, Scanner Tokenizer)이 자신의 책임에 충실하게 동작한 결과이며, 해결하려면 레이어 간의 데이터 흐름을 명시적으로 제어해야 한다 — 특히, 토큰 기반 입력과 라인 기반 입력을 혼용할 때는 반드시 버퍼 정리( nextLine() 호출)가 필요하다.” 📚 추천 학습 — 터미널 + I/O 마스터를 위한 길 man termios — TTY 설정, Canonical vs Raw Mode stty -a — 현재 터미널 설정 확인 Linux strace -e read,write java YourApp — 실제 시스템 콜 추적 OpenJDK 소스: Scanner.java — next() , nextLine() 메서드 직접 비교 《Advanced Programming in the UNIX Environment》 — Richard Stevens — TTY 장(chapter 18) 필독 필요하시면 리눅스에서 strace로 실제로 read() 시스템 콜 추적 + Scanner 내부 리플렉션으로 버퍼 상태 실시간 확인하는 실습 코드도 제공해 드립니다. 이제 진짜 “시스템이 어떻게 돌아가는지 + Java API가 왜 이렇게 설계되었는지” 를 동시에 아는 프로그래머가 되셨습니다. 🚀",
      "frontmatter": {
        "tags": [
          "ai-content"
        ],
        "date": "2025-09-19T16:08:18+09:00",
        "lastmod": "2025-09-19T16:09:18+09:00",
        "share_link": "https://share.note.sx/5qh8oyi0#jWUxnNYJuELWLPrXroZeBBPL8TUMvHS+yYMx3QmSFjo",
        "share_updated": "2025-09-19T17:49:56+09:00"
      }
    },
    "python login": {
      "path": "/temp/python-login/",
      "filename": "python login",
      "content": "refresh_token 은 JWT 인증 시스템에서 중요한 역할을 합니다. 일반적으로 access_token 은 짧은 유효 기간을 가지며, 클라이언트가 보호된 리소스에 접근할 때 사용됩니다. 반면, refresh_token 은 긴 유효 기간을 가지며, 만료된 access_token 을 갱신하는 데 사용됩니다. 이를 통해 사용자는 매번 로그인하지 않고도 안전하게 세션을 유지할 수 있습니다. 현재 코드에서는 access_token 만 발행하고 있으며, refresh_token 이 누락되어 있습니다. 이를 추가하여 완전한 JWT 인증 시스템을 구현해보겠습니다. refresh_token 추가를 위한 설정 Flask-JWT-Extended는 create_refresh_token() 함수를 제공하며, 이를 사용하여 refresh_token 을 생성할 수 있습니다. 또한, @jwt_required(refresh=True) 데코레이터를 사용하여 refresh_token 을 검증하고 새로운 access_token 을 발급할 수 있습니다. 최종 코드 (Refresh Token 포함) from flask import Flask, request, jsonify from flask_jwt_extended import ( JWTManager, create_access_token, create_refresh_token, jwt_required, get_jwt_identity, verify_jwt_in_request, verify_jwt_in_request_optional, ) import requests app = Flask(__name__) # JWT 설정 app.config['JWT_SECRET_KEY'] = 'your_jwt_secret_key' # 실제 서비스에서는 보안적으로 안전한 값을 사용하세요. jwt = JWTManager(app) # 로그인 API 엔드포인트 @app.route('/login', methods=['POST']) def login(): # 클라이언트로부터 ID와 비밀번호 수신 user_id = request.json.get('id', None) user_password = request.json.get('password', None) if not user_id or not user_password: return jsonify({\"msg\": \"ID와 비밀번호를 입력해주세요.\"}), 400 # 외부 API로 인증 요청 try: response = requests.post( 'https://sso1.mju.ac.kr/mju/userCheck.do', data={ 'id': user_id, 'passwrd': user_password } ) # JSON 응답 파싱 api_response = response.json() # 로그인 성공 여부 확인 if api_response.get('error') == '0000': # JWT 토큰 생성 access_token = create_access_token(identity=user_id) refresh_token = create_refresh_token(identity=user_id) return jsonify(access_token=access_token, refresh_token=refresh_token), 200 else: # 로그인 실패 메시지 반환 error_message = api_response.get('error_message', '로그인 실패: 알 수 없는 오류') return jsonify({\"msg\": error_message}), 401 except Exception as e: return jsonify({\"msg\": f\"외부 API 요청 중 오류 발생: {str(e)}\"}), 500 # Refresh Token으로 Access Token 갱신 @app.route('/refresh', methods=['POST']) @jwt_required(refresh=True) def refresh(): current_user = get_jwt_identity() new_access_token = create_access_token(identity=current_user) return jsonify(access_token=new_access_token), 200 # before_request 훅을 사용하여 전역 보호 설정 @app.before_request def protect_routes(): # 로그인 경로와 Refresh Token 경로는 예외 처리 if request.path in ['/login', '/refresh'] and request.method == 'POST': return # JWT 토큰 검증 from flask_jwt_extended import verify_jwt_in_request try: verify_jwt_in_request() except Exception as e: return jsonify({\"msg\": \"토큰이 유효하지 않거나 누락되었습니다.\"}), 401 # 테스트용 보호된 엔드포인트 @app.route('/protected', methods=['GET']) @jwt_required() def protected(): current_user = get_jwt_identity() return jsonify(logged_in_as=current_user), 200 if __name__ == '__main__': app.run(debug=True) 코드 설명 1) refresh_token 발행 /login 엔드포인트에서 create_refresh_token(identity=user_id) 를 사용하여 refresh_token 을 생성합니다. 클라이언트에게 access_token 과 함께 refresh_token 을 반환합니다. 2) refresh_token 으로 access_token 갱신 /refresh 엔드포인트를 추가하여 refresh_token 을 검증하고 새로운 access_token 을 발급합니다. @jwt_required(refresh=True) 데코레이터를 사용하여 refresh_token 의 유효성을 검사합니다. 3) 예외 처리 /login 과 /refresh 경로는 @app.before_request 훅에서 예외 처리하여 JWT 토큰 없이 접근할 수 있도록 합니다. 실행 방법 Flask 애플리케이션을 실행합니다. python app.py /login 으로 POST 요청을 보내어 JWT 토큰을 발급받습니다. curl -X POST http://127.0.0.1:5000/login \\ -H \"Content-Type: application/json\" \\ -d '{\"id\": \"rightId\", \"password\": \"rightPasswd\"}' 응답 예시 { \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\", \"refresh_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\" } refresh_token 을 사용하여 새로운 access_token 을 발급받습니다. curl -X POST http://127.0.0.1:5000/refresh \\ -H \"Authorization: Bearer <발급받은_refresh_token>\" 응답 예시 { \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\" } 보호된 엔드포인트( /protected )에 접근합니다. curl -X GET http://127.0.0.1:5000/protected \\ -H \"Authorization: Bearer <발급받은_access_token>\" 응답 예시 { \"logged_in_as\": \"rightId\" } 주요 장점 세션 유지: refresh_token 을 사용하여 사용자가 매번 로그인하지 않고도 안전하게 세션을 유지할 수 있습니다. 보안 강화: access_token 의 유효 기간을 짧게 설정하여 보안성을 높이고, refresh_token 을 통해 필요한 경우만 새로운 access_token 을 발급합니다. 유연성: refresh_token 을 관리하여 사용자 경험을 개선할 수 있습니다. 주의사항 유효 기간 설정: access_token 과 refresh_token 의 유효 기간을 적절히 설정해야 합니다. Flask-JWT-Extended의 JWT_ACCESS_TOKEN_EXPIRES 와 JWT_REFRESH_TOKEN_EXPIRES 설정을 사용하여 조정할 수 있습니다. app.config['JWT_ACCESS_TOKEN_EXPIRES'] = timedelta(minutes=15) # 15분 app.config['JWT_REFRESH_TOKEN_EXPIRES'] = timedelta(days=30) # 30일 HTTPS 사용: refresh_token 은 민감한 정보이므로 HTTPS를 통해 통신해야 합니다. 토큰 저장 및 관리: 클라이언트 측에서 refresh_token 을 안전하게 저장하고 관리해야 합니다. 일반적으로 HTTP-only 쿠키를 사용하는 것이 좋습니다. 이제 refresh_token 을 포함한 완전한 JWT 인증 시스템을 구현했습니다. 이를 통해 사용자는 더 안전하고 편리하게 인증을 유지할 수 있습니다. Flask 애플리케이션을 적절히 분리하고 구조화하는 것은 프로젝트의 유지보수성과 확장성을 높이는 데 매우 중요합니다. 아래에서는 Flask 애플리케이션을 모듈화하고, 데이터베이스 초기화 및 파일 구조를 설명하겠습니다. 파일 분리의 목적 Flask 프로젝트를 분리하는 주요 목적은 다음과 같습니다: 가독성: 코드가 길어질수록 한 파일에 모든 코드를 작성하면 관리하기 어렵습니다. 재사용성: 특정 기능(예: 데이터베이스 관리, 인증)을 별도의 모듈로 분리하면 다른 프로젝트에서도 쉽게 재사용할 수 있습니다. 유지보수성: 코드를 기능별로 나누면 수정이나 디버깅이 용이해집니다. 기본적인 파일 구조 아래는 Flask 프로젝트를 분리한 기본적인 파일 구조입니다: mju_ecs_test/ ├── app/ │ ├── __init__.py # Flask 앱 초기화 │ ├── models.py # 데이터베이스 모델 정의 │ ├── routes.py # API 엔드포인트 정의 │ ├── auth.py # 인증 관련 로직 (JWT 포함) │ └── db.py # 데이터베이스 초기화 및 연결 ├── migrations/ # 데이터베이스 마이그레이션 파일 (옵션) ├── config.py # 환경 설정 ├── requirements.txt # 의존성 목록 └── run.py # 애플리케이션 실행 스크립트 각 파일의 역할 1) run.py (애플리케이션 실행 스크립트) Flask 애플리케이션을 실행하는 진입점입니다. 데이터베이스 초기화를 여기서 수행할 수 있습니다. # run.py from app import create_app from app.db import init_db app = create_app() if __name__ == '__main__': # 데이터베이스 초기화 init_db() app.run(debug=True) 2) app/__init__.py (Flask 앱 초기화) Flask 애플리케이션 객체를 생성하고, 라우트와 확장을 등록합니다. # app/__init__.py from flask import Flask from flask_jwt_extended import JWTManager from .routes import register_routes def create_app(): app = Flask(__name__) # 환경 설정 로드 app.config.from_object('config') # JWT 초기화 jwt = JWTManager(app) # 라우트 등록 register_routes(app) return app 3) config.py (환경 설정) JWT 비밀 키, 데이터베이스 경로 등의 환경 설정을 관리합니다. # config.py import os class Config: SECRET_KEY = os.getenv('SECRET_KEY', 'your_jwt_secret_key') DATABASE_PATH = os.getenv('DATABASE_PATH', 'users.db') JWT_ACCESS_TOKEN_EXPIRES = int(os.getenv('JWT_ACCESS_TOKEN_EXPIRES', 900)) # 15분 JWT_REFRESH_TOKEN_EXPIRES = int(os.getenv('JWT_REFRESH_TOKEN_EXPIRES', 2592000)) # 30일 4) app/db.py (데이터베이스 초기화 및 연결) 데이터베이스 초기화 및 연결을 담당합니다. 처음 실행 시 데이터베이스가 없으면 자동으로 생성합니다. # app/db.py import sqlite3 from config import Config def init_db(): conn = sqlite3.connect(Config.DATABASE_PATH) cursor = conn.cursor() cursor.execute(''' CREATE TABLE IF NOT EXISTS USERS ( id TEXT PRIMARY KEY, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP last_login TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) ''') conn.commit() conn.close() def get_db_connection(): conn = sqlite3.connect(Config.DATABASE_PATH) conn.row_factory = sqlite3.Row # 딕셔너리 형태로 결과 반환 return conn 5) app/models.py (데이터베이스 모델) 데이터베이스와 상호작용하는 함수를 정의합니다. # app/models.py from .db import get_db_connection def add_user_to_db(user_id): conn = get_db_connection() cursor = conn.cursor() cursor.execute('INSERT OR IGNORE INTO USERS (id) VALUES (?)', (user_id,)) conn.commit() conn.close() def is_user_in_db(user_id): conn = get_db_connection() cursor = conn.cursor() cursor.execute('SELECT id FROM USER WHERE id = ?', (user_id,)) result = cursor.fetchone() conn.close() return result is not None 6) app/auth.py (인증 관련 로직) JWT 토큰 발행 및 갱신 로직을 포함합니다. # app/auth.py from flask_jwt_extended import create_access_token, create_refresh_token, jwt_required, get_jwt_identity def generate_tokens(user_id): access_token = create_access_token(identity=user_id) refresh_token = create_refresh_token(identity=user_id) return access_token, refresh_token 7) app/routes.py (API 엔드포인트 정의) /login , /refresh , /protected 같은 API 엔드포인트를 정의합니다. # app/routes.py from flask import request, jsonify from .auth import generate_tokens from .models import add_user_to_db, is_user_in_db import requests def register_routes(app): @app.route('/login', methods=['POST']) def login(): user_id = request.json.get('id', None) user_password = request.json.get('password', None) if not user_id or not user_password: return jsonify({\"msg\": \"ID와 비밀번호를 입력해주세요.\"}), 400 try: response = requests.post( 'https://sso1.mju.ac.kr/mju/userCheck.do', data={'id': user_id, 'passwrd': user_password} ) api_response = response.json() if api_response.get('error') == '0000': if not is_user_in_db(user_id): add_user_to_db(user_id) access_token, refresh_token = generate_tokens(user_id) return jsonify(access_token=access_token, refresh_token=refresh_token), 200 else: return jsonify({\"msg\": api_response.get('error_message', '로그인 실패')}), 401 except Exception as e: return jsonify({\"msg\": f\"외부 API 요청 중 오류 발생: {str(e)}\"}), 500 @app.route('/refresh', methods=['POST']) @jwt_required(refresh=True) def refresh(): current_user = get_jwt_identity() new_access_token = create_access_token(identity=current_user) return jsonify(access_token=new_access_token), 200 @app.route('/protected', methods=['GET']) @jwt_required() def protected(): current_user = get_jwt_identity() return jsonify(logged_in_as=current_user), 200 설명 요약 모듈화: 각 기능을 별도의 파일로 분리하여 관리합니다. 예: 데이터베이스( db.py ), 인증( auth.py ), 라우트( routes.py ) 등. 초기화: run.py 에서 애플리케이션을 실행하며, 데이터베이스 초기화를 수행합니다. 데이터베이스가 없는 경우 자동으로 생성됩니다. 환경 설정: config.py 에서 환경 변수를 통해 설정을 관리합니다. 추가 고려사항 테스트: 단위 테스트를 위해 각 모듈을 독립적으로 테스트할 수 있도록 설계합니다. 확장성: SQLite 대신 MySQL 또는 PostgreSQL을 사용하려면 db.py 를 수정하면 됩니다. 마이그레이션 도구: 데이터베이스 스키마 변경을 관리하기 위해 Alembic 같은 마이그레이션 도구를 사용할 수 있습니다. 이렇게 파일을 분리하고 구조화하면 프로젝트가 더 체계적이고 유지보수가 쉬워집니다. 필요한 경우 추가적인 설명이나 코드를 요청해주세요!",
      "frontmatter": {}
    },
    "python 모듈(module)": {
      "path": "/temp/python-모듈module/",
      "filename": "python 모듈(module)",
      "content": "",
      "frontmatter": {
        "tags": [
          "python",
          "language"
        ],
        "date": "2023-12-24T09:18:00+09:00",
        "lastmod": "2023-12-24T09:18:00+09:00"
      }
    },
    "python 입력함수": {
      "path": "/temp/python-입력함수/",
      "filename": "python 입력함수",
      "content": "(function) def input( __prompt: object = \"\", / ) -> str",
      "frontmatter": {
        "tags": [
          "python",
          "language"
        ],
        "date": "2023-12-25T04:06:00+09:00",
        "lastmod": "2023-12-25T04:06:00+09:00"
      }
    },
    "python 클래스 및 인스턴스 변수": {
      "path": "/temp/python-클래스-및-인스턴스-변수/",
      "filename": "python 클래스 및 인스턴스 변수",
      "content": "",
      "frontmatter": {
        "aliases": [
          "클래스 인스턴스 변수 함수 차이"
        ],
        "tags": [
          "python",
          "language"
        ],
        "date": "2023-12-24T14:28:00+09:00",
        "lastmod": "2023-12-24T14:28:00+09:00"
      }
    },
    "python 클래스(class)": {
      "path": "/temp/python-클래스class/",
      "filename": "python 클래스(class)",
      "content": "",
      "frontmatter": {
        "aliases": [
          "class"
        ],
        "tags": [
          "python",
          "language"
        ],
        "date": "2023-12-24T09:12:00+09:00",
        "lastmod": "2023-12-24T09:12:00+09:00"
      }
    },
    "servlet": {
      "path": "/temp/servlet/",
      "filename": "servlet",
      "content": "%20image%2020240214041835.png) 일반적으로 java 진영에서 많이 사용됨 소켓 연결 http 파싱후 읽기 등 일반적으로 동일한 방식의 http 방식의 처리를 객체화 하여 쉽게 처리하게 해줌 사용자 입장에서 비지니스 로직 파트만 실행될 수 있게 한다 HttpServletRequest 요청된 http 정보를 객체화 오버라딩해서 사용 HttpServletResponse 응답용 http 정보를 객체화됨",
      "frontmatter": {
        "tags": [
          "java"
        ],
        "date": "2024-02-14T04:16:00+09:00",
        "lastmod": "2024-02-14T04:16:00+09:00"
      }
    },
    "shell 의 로그인 과정": {
      "path": "/temp/shell-의-로그인-과정/",
      "filename": "shell 의 로그인 과정",
      "content": "linux bash 기준으로 설명 login shell vs non-login shell-20231223130204 Login Shell vs non Login Shell Login Shell : ssh, x 윈도우 접속시 userid passwd 입력해서 들어가는 방법 Non Longin Shell : 이미 다른 로그인 된 shell 에서 shell 을 fork 형태로 불러내는 방법 Interactive Shell vs non Interactive Shell 타 프로그래밍 언어와의 큰 차이이다 interactive Shell 은 python 명령어 입력시 나타나는 것과 비슷한 것으로 사용자 입력을 순차적으로 입력 받을 수 있는 방식이다 이에 반해 non interactive shell 은 python hello.py 같이 실행한다 특수 메개 변수 $1 , $2 , $3 , ...는 위치 매개변수 입니다 . \"$@\" 모든 위치 매개변수의 배열과 유사한 구성입니다 {$1, $2, $3 ...} . \"$*\" 는 모든 위치 매개변수의 IFS 확장입니다 $1 $2 $3 ... . $# 위치 매개변수의 수입니다. $- 쉘에 설정된 현재 옵션. $$ 현재 쉘의 pid(서브쉘 아님) $_ 가장 최근 매개변수(또는 시작 후 즉시 현재 쉘을 시작하는 명령의 절대 경로). $IFS (입력) 필드 구분 기호입니다. $? 가장 최근의 포그라운드 파이프라인 종료 상태입니다. $! 가장 최근 백그라운드 명령의 PID입니다. $0 쉘 또는 쉘 스크립트의 이름입니다",
      "frontmatter": {
        "tags": [
          "shell"
        ],
        "date": "2024-02-21T16:53:00+09:00",
        "lastmod": "2024-02-21T16:53:00+09:00"
      }
    },
    "sizeof": {
      "path": "/temp/sizeof/",
      "filename": "sizeof",
      "content": "sizeof 연산자는 C 언어에서 객체나 데이터 타입의 크기를 바이트 단위로 반환하는 데 사용됩니다. sizeof 의 사용법과 규칙은 다음과 같습니다. 기본 사용법 변수나 데이터 타입의 크기를 알아낼 수 있습니다. int a; printf(\"%zu\\n\", sizeof(a)); // int의 크기 출력 printf(\"%zu\\n\", sizeof(int)); // int 타입의 크기 출력 배열의 크기 배열에 대해서는 전체 배열의 크기를 반환합니다. char text[] = \"hello\"; printf(\"%zu\\n\", sizeof(text)); // 6 (5 characters + 1 for '\\0') 그러나 포인터에 대해서는 포인터의 크기만 반환합니다. char *ptr = \"hello\"; printf(\"%zu\\n\", sizeof(ptr)); // 보통 4 또는 8 (32비트 또는 64비트 시스템에서의 포인터 크기) 구조체의 크기 구조체의 크기는 그 안에 포함된 멤버들의 크기의 합과 메모리 정렬 패딩을 고려하여 계산됩니다. struct MyStruct { char a; int b; }; printf(\"%zu\\n\", sizeof(struct MyStruct)); // 메모리 정렬에 따라 8 또는 12 반환 함수의 크기 함수에 대해서는 sizeof 를 사용할 수 없습니다. 함수 이름은 포인터로 해석되기 때문에, 컴파일러는 오류를 발생시킵니다. int myFunction() { return 0; } // printf(\"%zu\\n\", sizeof(myFunction)); // 오류 발생 연산자의 크기 연산자에 대해서도 sizeof 를 사용할 수 있습니다. 이 경우, 연산자의 결과 타입에 따라 크기가 결정됩니다. printf(\"%zu\\n\", sizeof(3.14)); // double의 크기 출력 타입의 크기 typedef와 같은 사용자 정의 타입에 대해서도 사용할 수 있습니다. typedef unsigned long ulong; printf(\"%zu\\n\", sizeof(ulong)); // unsigned long의 크기 출력 sizeof 의 평가 시기 sizeof 는 컴파일 타임에 평가됩니다. 즉, 런타임에 실행되지 않습니다. 따라서, sizeof 는 변수를 인자로 전달할 때 그 변수가 배열인지 포인터인지 구분할 수 있습니다. 조건부 연산 sizeof 는 조건부 표현식의 피연산자에 대해서도 사용할 수 있습니다. int a; printf(\"%zu\\n\", sizeof(a > 0 ? a : 0)); // int의 크기 출력 요약 sizeof 는 다양한 데이터 타입과 객체의 크기를 구하는 데 유용한 연산자입니다. 배열과 포인터, 구조체, 기본 데이터 타입 등 각각의 크기 계산에 대한 규칙을 이해하는 것이 중요합니다. 추가 질문이 있으시면 언제든지 말씀해 주세요! 이런 자료를 참고했어요. [1] Microsoft Learn - sizeof 연산자 (https://learn.microsoft.com/ko-kr/cpp/cpp/sizeof-operator?view=msvc-170) [2] Microsoft Learn - sizeof 연산자 - 형식에 대한 스토리지 요구 사항 결정 - C# ... (https://learn.microsoft.com/ko-kr/dotnet/csharp/language-reference/operators/sizeof) [3] NAVER - c언어 - sizeof의 개념 (https://blog.naver.com/PostView.naver?blogId=jsky10503&logNo=220584768026) [4] 티스토리 - Sizeof, 쉼표 연산자, 조건부 연산자 - Hyonii - 티스토리 (https://hyoniidaaa.tistory.com/6) 뤼튼 사용하러 가기 > https://agent.wrtn.ai/5xb91l",
      "frontmatter": {}
    },
    "spring @Autowired 의존관계 주입시 중복 문제": {
      "path": "/temp/spring-@autowired-의존관계-주입시-중복-문제/",
      "filename": "spring @Autowired 의존관계 주입시 중복 문제",
      "content": "Autowired 어노테이션을 통해 어떠한 객체를 생성할 spring에서 의존 관계를 자동으로 주입해 준다 이때 조회되는 빈이 2개 이상이라면 즉 동일한 부모타입의 객체가 2개가 중복으로 등록되었다면 다음의 3가지 방법으로 해결한다 조회 대상 빈이 2개 이상일 때 해결 방법 @Autowired 필드 명 매칭 @Qualifier -> @Qualifier끼리 매칭 빈 이름 매칭 @Primary 사용",
      "frontmatter": {
        "tags": [
          "spring"
        ],
        "date": "2024-02-01T08:31:00+09:00",
        "lastmod": "2024-02-01T08:31:00+09:00"
      }
    },
    "spring bean scope": {
      "path": "/temp/spring-bean-scope/",
      "filename": "spring bean scope",
      "content": "스프링은 다음과 같은 다양한 스코프를 지원한다. 싱글톤: 기본 스코프, 스프링 컨테이너의 시작과 종료까지 유지되는 가장 넓은 범위의 스코프이다. 프로토타입: 스프링 컨테이너는 프로토타입 빈의 생성과 의존관계 주입까지만 관여하고 더는 관리하지 않는 매우 짧은 범위의 스코프이다. 웹 관련 스코프 request: 웹 요청이 들어오고 나갈때 까지 유지되는 스코프이다. session: 웹 세션이 생성되고 종료될 때 까지 유지되는 스코프이다. application: 웹의 서블릿 컨텍스트와 같은 범위로 유지되는 스코프이다 프로토타입 초기화 메서드 실행되지만 종료 메서드 호출 안됨 사용자가 직접 해야함 싱글톤 내부 의존관계로 프로토타입 스코프의 bean 을 가질때 싱글톤 객체 내부에 필드로 프로토타입을 가지고 있는경우 서로 다른 scope 성질로 인해 문제가 발생한다 두가지 방식으로 해결 ObjectProvider 로 생성시점을 조절 JSR-330 javax.injectProvider ObjectFatory 부모 ObjectProvider 자식 객체를 생성하는 시기를 getObject() 통해 조절 가능 DL dependency Lookup 스프링 의존적 java 표준을 사용 jakarta.inject.Provider JSR-330 자바 표준 import jakarta.inject.Provider; implementation 'jakarta.inject:jakarta.inject-api:2.0.1' 웹 스코프 웹 스코프의 특징 웹 스코프는 웹 환경에서만 동작한다. 웹 스코프는 프로토타입과 다르게 스프링이 해당 스코프의 종료시점까지 관리한다. 따라서 종료 메서드가 호출된 다. 웹 스코프 종류 request: HTTP 요청 하나가 들어오고 나갈 때 까지 유지되는 스코프, 각각의 HTTP 요청마다 별도의 빈 인스턴 스가 생성되고, 관리된다. session: HTTP Session과 동일한 생명주기를 가지는 스코프 application: 서블릿 컨텍스트( ServletContext )와 동일한 생명주기를 가지는 스코프 websocket: 웹 소켓과 동일한 생명주기를 가지는 스코프",
      "frontmatter": {
        "aliases": [
          "범위"
        ],
        "tags": [
          "spring"
        ],
        "date": "2024-02-02T09:26:00+09:00",
        "lastmod": "2024-02-02T09:26:00+09:00"
      }
    },
    "spring bean 조회": {
      "path": "/temp/spring-bean-조회/",
      "filename": "spring bean 조회",
      "content": "최상위 BeanFactory 인터페이스를 구현한 모든 클래스들은 bean 들을 관리하는 컨테이너이다 HierarchicalBeanFactory: 빈 계층구조 관리 ListableBeanFactory : 빈 여러개 조회 가능 ApplicationContext : 컨테이너 + 기능추가 AnnotationConfigApplicationContext : 어노테이션을 설정 정보로 하는 컨테이너 이때 컨테이너에 존재하는 bean 들을 조회하는 방법이다 class BeanFactory: //최상위 컨테이너 T getBean(Class<T> requiredType); // 타입으로 조회 Object getBean(String name); // 이름으로 조회 T getBean(Class<T> requiredType, Object... args); //이름과 타입으로 조회 ============= class ListableBeanFactory: // 여러개 조회가능 컨테이너 int getBeanDefinitionCount() //팩토리에 정의된 빈 개수를 반환합니다. String getBeanDefinitionNames() //이 팩토리에 정의된 모든 Bean의 이름을 반환합니다. <T> Map<String,T> getBeansOfType(Class <T> type) //타입으로 모든 Bean 조회 추가적으로 bean들의 설정 정보를 담고 있는 BeanDefinition 클래스가 있다 getRole(), 등등의 설정 정보를 담고 있는 클래스이다 getBeanDefinition() 메서드를 통해 얻을 수 있다",
      "frontmatter": {
        "date": "2025-06-27T19:39:35+09:00",
        "lastmod": "2025-09-05T17:38:41+09:00"
      }
    },
    "spring intro": {
      "path": "/temp/spring-intro/",
      "filename": "spring intro",
      "content": "Spring Boot Devtools 사용법 controller란 무엇인가? 사용자의 요청이 진입하는 지점(entry point)이며 요청에 따라 어떤 처리를 할지 결정해주며 단, controller는 단지 결정만 해주고 실질적인 처리는 서비스(Layered Architecture)에서 담당한다. 사용자에게 View(또는 서버에서 처리된 데이터를 포함하는 View)를 응답으로 보내준다. mvc model view controller model : view : controller : ss 명령어 유닉스 ss -ltn 명령어를 통해 listen 되고 있는 포트를 본다 netstat -ano -p tcp 윈도우 jsp <% 스크립틀릿 scriptlet java코드 작성영역 %> 서버 생성시 초기화 jspinit() <%! 선언문 멤버필드 멤버 메서드 정의 %> 객체 생성시 초기화 <%= 출력부 %> http 프로토콜 http + ssl => https http method 총 9개 get head post put delete connect options trace patch",
      "frontmatter": {}
    },
    "spring life cycle": {
      "path": "/temp/spring-life-cycle/",
      "filename": "spring life cycle",
      "content": "스프링 빈의 이벤트 라이프사이클 스프링 컨테이너 생성 스프링 빈 생성 의존관계 주입 초기화 콜백 사용 소멸전 콜백 스프링 종료 스프링빈 생성시 생성자 의존관계일떄는 같이 설정정보(xml, java etc)를 바탕으로 빈을 관리하는 컨테이너를 생성 스프링 빈 생성 & 의존관계 주입 수동 @Configuration // 중복되는 객체생성을 막아준다 public class AppConfig { @Bean public MemberService memberService(){ return new MemberServiceImpl(memberRepository());} @Bean public OrderService orderService(){ return new OrderServiceImpl(memberRepository(), discountPolicy()); //필드 주입을 한다면 필요 없음} @Bean public DiscountPolicy discountPolicy() { return new RateDiscountPolicy();} @Bean public MemberRepository memberRepository() { return new MemoryMemberRepository();} } @Configration 바이트 조작 기술을 통해 중복되는 객체(memberRepository) 생성을 1개만 생성해 공유해 준다 @Bean 메서드 수준의 어노테이션을 통해 return 되는 값을 객체로 만들어준다 자동 @Component public class OrderServiceImpl implements OrderService{ private final MemberRepository memberRepository; private final DiscountPolicy discountPolicy; @Autowired public OrderServiceImpl(MemberRepository memberRepository, DiscountPolicy discountPolicy){ this.memberRepository = memberRepository; this.discountPolicy = discountPolicy; } ... } @Configuration @ComponentScan public class AutoAppconfig { } 실제 객체를 만들고 싶은 클래스에 @Component 어노테이션을 통해 이 클래스로부터 생성되는 객체를 spring bean 으로 등록하고 싶음을 알린다 @Autowired 어노테이션을 통해 의존관계를 주입 @Component 어노테이션을 통해 @Component 를 탐색 초기화 콜백 & 소멸전 콜백 스프링은 크게 3가지 방법으로 빈 생명주기 콜백을 지원한다. 인터페이스(InitializingBean, DisposableBean) 설정 정보에 초기화 메서드, 종료 메서드 지정 @PostConstruct, @PreDestroy 애노테이션 지원 인터페이스 스프링 의존 설계 InitializingBean -> afterPropertiesSet 구현 DisposableBean -> destory 구현 설정정보 초기화 메서드 수동 등록에서만 가능 @Bean (initMethod = \"init\", destoryMethod = \"close\") // 메서드 이름을 자신이 설정 일반적으로 라이브러리들의 종료 메서드는 close, shutdown destoryMethod 의 default 값은 (infered) 추론으로 자동으로 라이브러리의 종료 메서드를 불러준다 어노테이션 @PostConstruct @PreDestroy 자동 등록 컴포넌트 스캔과 잘 어울림",
      "frontmatter": {
        "aliases": [
          "주기"
        ],
        "tags": [
          "spring",
          "java"
        ],
        "date": "2024-02-01T14:18:00+09:00",
        "lastmod": "2024-02-01T14:18:00+09:00"
      }
    },
    "sql date 관련 정리": {
      "path": "/temp/sql-date-관련-정리/",
      "filename": "sql date 관련 정리",
      "content": "sql 에서 제공하는 date 에 관련한 정보를 기술한다 자료형 DATE 타입 DATE 타입은 날짜는 포함하지만 시간은 포함하지 않을 때 사용하는 타입입니다. DATE 타입 YYYY-MM-DD 형식 입력가능하며, '1000-01-01' 부터 '9999-12-31' 까지만 입력가능합니다. DATETIME 타입 DATETIME 타입은 날짜와 시간을 모두 포함할 때 사용하는 타입입니다. YYYY-MM-DD HH:MM:SS의 형태로 사용되며 '1001-01-01 00:00:00'부터 '9999-12-31 23:59:59'까지 입력이 가능하다 TIME 타입 TIME은 HH:MM:SS의 형태를 지닌다.(HHH:MM:SS의 형태를 띄기도 한다) 838:59:59 부터 838:59:59 까지의 범위를 가진다. 이때 TIME type의 시간이 크다고 느낄수도 있다. TIME은 현재의 시간을 표현할때만 쓰는것이 아니라 이미 지나버린 시간이나, 특정 이벤트끼리의 간극을 표현하는데 사용되기 때문에 이처럼 쓰인다. TIMESTAMP 타입 TIMESTAMP 역시 날짜와 시간을 포함한다. TIMESTAMP는 1970-01-01 00:00:01 UTC 부터 2038-01-19 03:14:07UTC 까지가 그 범위이다. 관련 함수 dayofweek(date) 날짜를 한 주의 몇 번째 요일인지를 나타내는 숫자로 리턴한다. (1 = 일요일, 2 = 월요일, ... 7 = 토요일) mysql> select dayofweek('1998-02-03'); -> 3 weekday(date) 날짜를 한 주의 몇 번째 요일인지를 나타내는 숫자로 리턴한다. (0 = 월요일, 1=화요일 ... 6 = 일요일) mysql> select weekday('1997-10-04 22:23:00'); -> 5 mysql> select weekday('1997-11-05'); -> 2 dayofmonth(date) 그 달의 몇 번째 날인지를 알려준다. 리턴 값은 1에서 31 사이이다. mysql> select dayofmonth('1998-02-03'); -> 3 dayofyear(date) 한 해의 몇 번째 날인지를 알려준다. 리턴 값은 1에서 366 사이이다. mysql> select dayofyear('1998-02-03'); -> 34 month(date) 해당 날짜가 몇 월인지 알려준다. 리턴 값은 1에서 12 사이이다. mysql> select month('1998-02-03'); -> 2 dayname(date) 해당 날짜의 영어식 요일이름을 리턴한다. mysql> select dayname(\"1998-02-05\"); -> thursday monthname(date) 해당 날짜의 영어식 월 이름을 리턴한다. mysql> select monthname(\"1998-02-05\"); -> february quarter(date) 분기를 리턴한다 (1~ 4) mysql> select quarter('98-04-01'); -> 2 week(date) week(date,first) 인수가 하나일 때는 해달 날짜가 몇 번째 주일인지(0 ~ 52)를 리턴하고 2개일 때는 주어진 인수로 한 주의 시작일을 정해 줄 수 있다. 0이면 일요일을 1이면 월요일을 한 주의 시작일로 계산해 몇 번째 주인가 알려준다. mysql> select week('1998-02-20'); -> 7 mysql> select week('1998-02-20',0); -> 7 mysql> select week('1998-02-20',1); -> 8 year(date) 년도를 리턴한다.(1000 ~ 9999) mysql> select year('98-02-03'); -> 1998 hour(time) 시간을 알려준다.(0 ~ 23) mysql> select hour('10:05:03'); -> 10 minute(time) 분을 알려준다(0 ~ 59) mysql> select minute('98-02-03 10:05:03'); -> 5 second(time) 초를 알려준다(0 ~ 59) mysql> select second('10:05:03'); -> 3 period_add(p,n) yymm 또는 yyyymm 형식으로 주어진 달에 n개월을 더한다. 리턴 값은 yyyymm의 형식이다. mysql> select period_add(9801,2); -> 199803 period_diff(p1,p2) yymm 또는 yyyymm 형식으로 주어진 두 기간사이의 개월을 구한다 mysql> select period_diff(9802,199703); -> 11 date_add(date,interval expr type) date_sub(date,interval expr type) adddate(date,interval expr type) subdate(date,interval expr type) 위의 함수들은 날자 연산을 한다. 잘 만 사용하면 꽤나 편리한 함수 들이다. 모두 mysql 3.22 버전에서 새롭게 추가되었다. adddate() 과 subdate() 는 dateadd() 와 datesub()의 또 다른 이름이다. 인수로 사용되는 date 는 시작일을 나타내는 datetime 또는date 타입이다. expr 는 시작일에 가감하는 일수 또는 시간을 나타내는 표현식이다. type 값의 의미 사용 예 second, seconds 초 minute, minutes 분 hour, hours 시간 day, days 일 month, months 월 year, years 년 minute_second, \"minutes:seconds\" 분:초 hour_minute, \"hours:minutes\" 시:분 day_hour, \"days hours\" 일 시 year_month, \"years-months\" 년 월 hour_second, \"hours:minutes:seconds\" 시 분 day_minute, \"days hours:minutes\" 일, 시, 분 day_second, \"days hours:minutes:seconds\" 일, 시, 분, 초 [예제] mysql> select date_add(\"1997-12-31 23:59:59\",interval 1 second); -> 1998-01-01 00:00:00 mysql> select date_add(\"1997-12-31 23:59:59\",interval 1 day); -> 1998-01-01 23:59:59 mysql> select dateadd(\"1997-12-31 23:59:59\",interval \"1:1\" minutesecond); -> 1998-01-01 00:01:00 mysql> select datesub(\"1998-01-01 00:00:00\",interval \"1 1:1:1\" daysecond); -> 1997-12-30 22:58:59 mysql> select dateadd(\"1998-01-01 00:00:00\",interval \"-1 10\" dayhour); -> 1997-12-30 14:00:00 mysql> select date_sub(\"1998-01-02\", interval 31 day); -> 1997-12-02 to_days(date) 주어진 날짜를 0000년부터의 일수로 바꾼다. mysql> select to_days(950501); -> 728779 mysql> select to_days('1997-10-07'); -> 729669 from_days(n) 주어진 일수 n로부터 날짜를 구한다 mysql> select from_days(729669); -> '1997-10-07' date_format(date,format) format 의 정의에 따라 날자 혹은 시간을 출력한다. 매우 빈번히 쓰이는 함수 이다. format 에 사용되는 문자는 다음과 같다. %m 월이름 (january..december) %w 요일명 (sunday..saturday) %d 영어식 접미사를 붙인 일(1st, 2nd, 3rd, etc.) %y 4자리 년도 %y 2자리 년도 %a 짧은 요일명(sun..sat) %d 일(00..31) %e 일(0..31) %m 월(01..12) %c 월(1..12) %b 짧은 월이름 (jan..dec) %j 한해의 몇 번째 요일인가 (001..366) %h 24시 형식의 시간 (00..23) %k 24시 형식의 시간 (0..23) %h 12시 형식의 시간 (01..12) %i 12시 형식의 시간 (01..12) %l 시간 (1..12) %i 분 (00..59) %r 시분초12시 형식 (hh:mm:ss [ap]m) %t 시분초 24시 형식 (hh:mm:ss) %s 초 (00..59) %s 초 (00..59) %p am 또는 pm 문자 %w 일주일의 몇 번째 요일인가(0=sunday..6=saturday) %U 한해의 몇 번째 주인가(0..52). 일요일이 시작일 %u 한해의 몇 번째 주인가(0..52). 월요일이 시작일 %% 위 표에 나와 있는 것들을 제외한 모든 문자는 그냥 그대로 출력된다. mysql> select date_format('1997-10-04 22:23:00', '%w %m %y'); -> 'saturday october 1997' mysql> select date_format('1997-10-04 22:23:00', '%h:%i:%s'); -> '22:23:00' mysql> select date_format('1997-10-04 22:23:00','%d %y %a %d %m %b %j'); -> '4th 97 sat 04 10 oct 277' mysql> select date_format('1997-10-04 22:23:00','%h %k %i %r %t %s %w'); -> '22 22 10 10:23:00 pm 22:23:00 00 6' 주의! : mysql 3.23 버전부터 % 기호가 각 형식문자 앞에 필요하게 되었다 그 이전 버전에서는 선택 사항이다. **time_format(time,format)** 이 함수는 date_format()와 비슷한 역할을 하지만 단지 시,분,초 만을 나타낼 수 있다는 점이다. **curdate()** **current_date()** 오늘 날짜를 'yyyy-mm-dd' 또는 yyyymmdd 형식으로 리턴한다, 리턴 값은 이 함수가 문자열로 쓰이느냐 숫자로 쓰이느냐에 따라 달라진다. mysql> select curdate(); -> '1997-12-15' mysql> select curdate() + 0; -> 19971215 **curtime()** **current_time()** 'hh:mm:ss' 또는 hhmmss 형식으로 현재시간을 나타낸다. 리턴 값은 이 함수가 문자열로 쓰이느냐 숫자로 쓰이느냐에 따라 달라진다. mysql> select curtime(); -> '23:50:26' mysql> select curtime() + 0; -> 235026 **now()** **sysdate()** **current_timestamp()** 오늘 날자와 현재 시간을 'yyyy-mm-dd hh:mm:ss' 또는 yyyymmddhhmmss 형식으로 리턴 한다, 역시 리턴 값은 이 함수가 문자열로 쓰이느냐 숫자로 쓰이느냐에 따라 달라진다. 실제 개발 시 사용자의 등록일시 등을 나타낼 때 유용하게 쓰이는 함수다. 뒷부분의 실전예제에서 보게 될 것이다. mysql> select now(); -> '1997-12-15 23:50:26' mysql> select now() + 0; -> 19971215235026 **unix_timestamp()** **unix_timestamp(date)** 인수가 없이 사용될 경우 현재 시간의 유닉스 타임스탬프를 리턴하고 만일 날짜형식의 date 가 인수로 주어진 경우에는 주어진 날짜의 유닉스 타임스탬프를 리턴한다 유닉스 타임스탬프 란 그리니치 표준시로 1970 년 1월 1일 00:00:00 이 후의 시간경과를 초단위로 나타낸 것이다. mysql> select unix_timestamp(); -> 882226357 mysql> select unix_timestamp('1997-10-04 22:23:00'); -> 875996580 주의 : 만일 unix_timestamp함수가 timestamp 컬럼 에서 사용될 경우에는 주어진 시간이 타임스탬프로 바뀌지 않고 그대로 저장된다. **from_unixtime(unix_timestamp)** 주어진 유닉스 타임스탬프 값으로부터 'yyyy-mm-dd hh:mm:ss' 또는 yyyymmddhhmmss 형식의 날짜를 리턴한다. mysql> select from_unixtime(875996580); -> '1997-10-04 22:23:00' mysql> select from_unixtime(875996580) + 0; -> 19971004222300 **from_unixtime(unix_timestamp,format)** 주어진 유닉스 타임스탬프 값을 주어진 날짜 형식에 맞게 바꿔서 보여준다. 여기서 사용되는 형식문자는 date_format() 함수에서 사용된 것과 같다. 아래 예에서 %x 는 형식문자가 아니므로 그냥 x 가 표시됨에 유의하기 바란다. mysql> select from_unixtime(unix_timestamp(), '%y %d %m %h:%i:%s %x'); -> '1997 23rd december 03:43:30 x' **sec_to_time(seconds)** 주어진 초를 'hh:mm:ss' 또는 hhmmss 형식의 시간단위로 바꿔준다. mysql> select sec_to_time(2378); -> '00:39:38' mysql> select sec_to_time(2378) + 0; -> 3938 **time_to_sec(time)** 주어진 시간을 초 단위로 바꿔준다. mysql> select time_to_sec('22:23:00'); -> 80580 mysql> select time_to_sec('00:39:38'); -> 2378 **[예제]** 쿼리문으로 날짜계산 $query = \"SELECT (now() - interval ′1 month′)::timestamp\"; // 현재 부터 한 달 전 날짜 $query = \"SELECT (now() + interval ′6 month′)::timestamp\"; // 현재 부터 6 달 후 날짜 ... **[Q/A]** mysql에서 타임스탬프값을 날짜값으로 바꿔주는 함수 있나요? select from_unixtime(날짜필드) ... 하시면 우리가 보는 시간으로 보일거예요~ **[MySQL에서 제공하는 날자 관련 함수]** **DAYOFMONTH(date)** : 날짜만 리턴해주는 함수. (1-31) 한달을 단위로. **DAYOFYEAR(date)** : 이역시 날짜만 리턴. (1-366) 1년을 단위로. **TO_DAYS(date)** : 연도와 달을 모두 날짜화 시켜서 리턴해줍니다. (1999-01-01 = (1999 * 365) + (01 * 31) + 1) **MONTH(date)** : 달을 리턴해주는 함수. **DAYNAME(date)** : 요일을 문자로 리턴. (ex :'Thursday') **MONTHNAME(date)** : 달을 문자로 리턴. (ex :'February') **WEEK(date)** : 해당 연도에 몇번째 주인지를 리턴 (0-52) **YEAR(date)** : 연도를 리턴 (1000-9999) **HOUR(time)** : 시간 리턴 **MINUTE(time)** : 분 리턴 **SECOND(time)** : 초 리턴 **DATE_FORMAT(date,format)** %W' Weekday name ( Sunday'.. Saturday') %D' Day of the month with english suffix ( 1st', 2nd', 3rd', etc.) %y' Year, numeric, 2 digits %a' Abbreviated weekday name ( Sun'.. %d' Day of the month, numeric ( 00'.. 31') %e' Day of the month, numeric ( 0'.. %m' Month, numeric ( 01'.. 12') %c' Month, numeric ( 1'.. %b' Abbreviated month name ( Jan'.. Dec') %j' Day of year ( 001'.. %H' Hour ( 00'.. 23') %k' Hour ( 0'.. %h' Hour ( 01'.. 12') %I' Hour ( 01'.. %l' Hour ( 1'.. 12') %i' Minutes, numeric ( 00'.. %r' Time, 12-hour ( %T' Time, 24-hour ( %S' Seconds ( 00'.. 59') %s' Seconds ( 00'.. %p' AM' or PM' %w' Day of the week ( 0'=Sunday.. %U' Week ( 0'.. 52'), Sunday is the first day of the week. %u' Week ( 0'.. %%' Single %' characters are ignored. Use %%' to produce a literal `%' (for future extensions).",
      "frontmatter": {
        "tags": [
          "sql"
        ],
        "date": "2024-04-12T09:58:00+09:00",
        "lastmod": "2024-04-12T09:58:00+09:00"
      }
    },
    "sql 틀린문제": {
      "path": "/temp/sql-틀린문제/",
      "filename": "sql 틀린문제",
      "content": "null 의 비교연산 null where referee_id!=2 or referee_id = null -- 틀린답 이 아닌 where referee_id!=2 or referee_id is null -- 이것이 맞는 답 SQL에서 NULL 값은 특별하게 취급되며, NULL은 '값이 없음'을 의미합니다. 따라서, NULL 값은 다른 값들과 일반적인 비교 연산자(=, !=, , 등)로 비교할 수 없습니다. NULL과의 비교는 IS NULL 또는 IS NOT NULL 연산자를 사용해야만 합니다. not in 문제 visits table and transactions table 이 존재 방문했지만 거래를 하지 않은 고객을 식별하려면 방문한 모든 고객의 목록에서 거래를 한 고객의 기록을 제거해야 합니다. 이를 통해 이 문제를 일반적인 \"NOT IN\" 문제로 변환합니다. \"NOT IN\" 문제를 해결하는 방법은 크게 두 가지입니다: 1) NOT IN/EX와 유사한 기능을 직접 사용하거나 2) 오른쪽 테이블이 NULL로 설정된 LEFT OUTER JOIN 입니다. -- left outer join select customer_id, count(visit_id) as count_no_trans from visits left join transactions using(visit_id) where transaction_id is null group by customer_id -- not in select customer_id, count(visit_id) as count_no_trans from visits where visits.visit_id not in (select visit_id from transactions) group by customer_id leetcode 1661 leetcode 원본 문제 링크 group by 에서 새로운 relation 이 생성된다고 생각하고 참여하지 않은 attribute 는 사용할 수 없다고 생각한다면 아래와 같은답을 내였다 select d.machine_id, round(sum(d.timesub)/count(d.timesub),3) as processing_time from ( select a.machine_id, a.process_id, round(b.timestamp - a.timestamp, 3) as timesub from Activity as a join Activity as b where a.machine_id = b.machine_id and a.process_id = b.process_id and a.activity_type = 'start' and b.activity_type = 'end' ) as d group by d.machine_id 하지만 group by 에서 참여하지 않은 attribute 도 사용할 수 있다면 또한 문제에 제시된 답은 아래답이다 select a.machine_id, round(avg(b.timestamp - a.timestamp),3) as processing_time from Activity as a join Activity as b where a.machine_id = b.machine_id and a.process_id = b.process_id and a.activity_type = 'start' and b.activity_type = 'end' group by a.machine_id 하지만 위의 2개 접근 (사실은 동일한 접근) 은 두개의 table을 조인 연산하여 select 문에서 산술연산을 통해 processing_time 을 산출 하는 것이 주된 접근으로 보이는데 사실은 두개의 table 을 join 하지 않고도 case 문 일종의 if 문으로 계산이 가능하다",
      "frontmatter": {
        "tags": [
          "sql",
          "문제풀이"
        ],
        "date": "2024-04-11T13:04:00+09:00",
        "lastmod": "2024-04-11T13:04:00+09:00"
      }
    },
    "stateful 과 stateless 프로토콜": {
      "path": "/temp/stateful-과-stateless-프로토콜/",
      "filename": "stateful 과 stateless 프로토콜",
      "content": "stateful : TCP, FTP stateless : udp http ... ==stateful 구조는 server와 client 세션의 state(상태)에 기반하여 client 에 response 를 보낸다== 즉 server 측 client 모두 state 를 저장한다 (참조 네트워크 스텍) stateless : 보내고 아몰랑?? 궁금증 http(stateless) 응용계층의 프로토콜은 하위계층(4계층) TCP(stateful) 를 사용한다?? statless http 프로토콜은 연결된 상태를 어떻게 판단 구현하는가?? 2가지에 대해 궁금증이 생겨서 글을 작성하게됨 TCP 프로토콜은 stateful 을 어떻게 구현하는가 stateful 을 구현하고자 하는 [](../02.inbox/따라IT/네트워크%204계층.md#TCP) 는 syn.. ack... 두가지 정보의 교환으로 서로의 연결이 되었다는 것을 논리적으로 확인하게 된다 이때 상태 정보가 운영체제에 스텍으로 메모리에 저장된다 [](https://elixir.bootlin.com/linux/latest/source/net/ipv4/tcp_ipv4.c#L2161) statless http 프로토콜의 연결 상태 구현 cookie 를 통해 클라이언트 측에만 상태를 강제로 구현함 보안문제 session : 연결되었다는 사람이 인지하는 일종의 추상적 개념 ex) 로그인 성공 상태 결론 tcp 를 통해 http PDU(massage) 가 전부 전달 되었다는 것만 보장 나머지 로그인 인증 등 과정은 http 에서 확인해야함 http 버전 RFC2068(1997) RFC2616(1999) RFC7230~7235(2014)",
      "frontmatter": {
        "tags": [
          "network"
        ],
        "date": "2024-02-13T02:02:00+09:00",
        "lastmod": "2024-02-13T02:02:00+09:00"
      }
    },
    "sudo vs su": {
      "path": "/temp/sudo-vs-su/",
      "filename": "sudo vs su",
      "content": "sudo, sudoedit — 명령을 다른 사용자로 실행합니다 su - 대체 사용자 및 그룹 ID를 사용하여 명령을 실행합니다 3308 3308 3308 pts/1 00:00:00 bash 3519 3519 3308 pts/1 00:00:00 sudo 3520 3520 3520 pts/3 00:00:00 sudo 3521 3521 3520 pts/3 00:00:00 su 3522 3522 3520 pts/3 00:00:00 bash 3388 3388 3388 pts/2 00:00:00 bash 3539 3539 3388 pts/2 00:00:00 sudo 3540 3540 3540 pts/4 00:00:00 sudo 3541 3541 3540 pts/4 00:00:00 bash",
      "frontmatter": {
        "tags": [
          "command",
          "shell"
        ],
        "date": "2024-02-28T09:13:00+09:00",
        "lastmod": "2024-02-28T09:13:00+09:00"
      }
    },
    "test": {
      "path": "/temp/test/",
      "filename": "test",
      "content": "파일 테이블 파일 포인터 파일 상태 플래그 파일 (File) 파일은 데이터를 저장하는 기본 단위로, 디스크와 같은 저장 매체에 존재하는 정보의 집합입니다. 파일은 텍스트, 이미지, 비디오 등 다양한 형태로 존재할 수 있으며, 파일 시스템에 의해 관리됩니다. 각 파일은 고유한 이름과 경로를 가지며, 파일의 내용과 메타데이터(예: 크기, 생성 날짜, 수정 날짜 등)가 포함됩니다. 파일 테이블 (File Table) 파일 테이블은 운영체제가 시스템에서 열린 파일에 대한 정보를 관리하는 데이터 구조입니다. 시스템 전체에서 열린 파일에 대한 상태를 추적하며, 각 파일에 대한 메타 정보를 포함합니다. 파일 테이블은 모든 프로세스가 공유할 수 있으며, 파일의 참조 카운터와 접근 정보 등이 포함됩니다. 파일 디스크립터 (File Descriptor) 파일 디스크립터는 프로세스가 열린 파일을 식별하는 정수 값입니다. 각 프로세스는 자신의 파일 디스크립터 테이블을 가지고 있으며, 이 테이블에서 파일 디스크립터는 특정 파일에 대한 핸들 역할을 합니다. 파일 디스크립터는 파일에 대한 읽기 및 쓰기 작업을 수행할 때 사용됩니다. 파일 디스크립터 테이블 (File Descriptor Table) 파일 디스크립터 테이블은 각 프로세스가 가진 열린 파일에 대한 파일 디스크립터를 저장하는 데이터 구조입니다. 각 프로세스는 고유한 파일 디스크립터 테이블을 가지며, 열린 파일에 대한 파일 디스크립터와 그에 해당하는 파일 테이블 엔트리를 참조합니다. 테이블의 인덱스는 파일 디스크립터의 값과 일치하며, 프로세스가 열린 파일에 접근할 때 사용됩니다. 파일 테이블 엔트리 (File Table Entry) 파일 테이블 엔트리는 파일 테이블의 각 항목으로, 특정 열린 파일에 대한 정보를 저장합니다. 여기에는 다음과 같은 정보가 포함됩니다: 파일 오프셋: 파일에서 현재 읽거나 쓸 위치. 파일 상태 플래그: 파일이 읽기 전용인지, 쓰기 전용인지 등의 정보. 참조 카운터: 해당 파일 테이블 엔트리를 참조하는 파일 디스크립터의 수. 파일 접근 권한: 파일에 대한 접근 권한 정보. 파일 시스템 정보: 파일이 저장된 위치나 파일 시스템 타입 정보. 요약 파일: 데이터를 저장하는 기본 단위. 파일 테이블: 열린 파일에 대한 정보를 관리하는 시스템 데이터 구조. 파일 디스크립터: 프로세스가 열린 파일을 식별하는 정수 값. 파일 디스크립터 테이블: 프로세스가 가진 열린 파일의 파일 디스크립터를 저장하는 구조. 파일 테이블 엔트리: 파일 테이블의 각 항목으로, 특정 파일에 대한 메타 정보를 포함. 이런 자료를 참고했어요. [1] 네이버 블로그 - fd(File Descriptor)가 할당되는 과정 - 네이버 블로그 (https://m.blog.naver.com/songblue61/221391888403) [2] 티스토리 - [Linux] 파일 디스크립터 (File Descriptor) - Icarus - 티스토리 (https://icarus8050.tistory.com/122) [3] 티스토리 - [운영체제 공부] 파일디스크립터(file descriptor) 공부하기 (https://cinnamonc.tistory.com/m/343) [4] 티스토리 - [Lecture 2] Unix File Descriptors and Pipes - CS 노트 - 티스토리 (https://devjourney7.tistory.com/27) 뤼튼 사용하러 가기 > https://agent.wrtn.ai/5xb91l",
      "frontmatter": {}
    },
    "tmux": {
      "path": "/temp/tmux/",
      "filename": "tmux",
      "content": "Pasted image 20250117105337 Pasted image 20250117105406 Pasted image 20250117105309 tmux 는 터미널 멀티플렉서로, 하나의 터미널 창에서 여러 세션을 관리할 수 있게 해줍니다. 아래에 tmux 의 기본 사용법(생성, 접근, 종료 등)을 정리해 드릴게요. 🔹 1. tmux 설치 확인 (리눅스 기준) tmux -V # 버전 확인 설치가 필요하다면: 우분투/데비안: sudo apt install tmux CentOS: sudo yum install tmux macOS (Homebrew): brew install tmux 🔹 2. 세션 생성 tmux new -s [세션이름] 예시: tmux new -s mysession 이렇게 하면 mysession 이라는 이름의 새로운 tmux 세션이 시작됩니다. 🔹 3. tmux에서 나가기 (detach) tmux 세션 안에서 나와서도 계속 실행되도록 하려면: Ctrl + b 를 누른 후 d 를 누릅니다. 즉, Ctrl+b → d 순서로 입력하면 현재 세션에서 분리(detach)됩니다. 🔹 4. 실행 중인 세션 보기 tmux ls 또는 tmux list-sessions 출력 예시: mysession: 1 windows (created Mon Jan 1 00:00:00 2024) anothersession: 2 windows (created Mon Jan 1 00:05:00 2024) 🔹 5. 세션에 다시 접속 (attach) tmux attach -t [세션이름] 예시: tmux attach -t mysession 🔹 6. 세션 종료 (kill) 특정 세션 종료: tmux kill-session -t [세션이름] 예시: tmux kill-session -t mysession 모든 세션 종료: tmux kill-server 🔹 7. 윈도우 및 패널 관리 (부가 정보) 기능 명령어 새 윈도우 만들기 Ctrl + b → c 윈도우 전환 Ctrl + b → n (다음), p (이전) 화면 분할 (수평) Ctrl + b → \" 화면 분할 (수직) Ctrl + b → % 패널 전환 Ctrl + b → o 패널 크기 조절 Ctrl + b → Alt + 방향키 🔹 8. 기본 세션 없이 tmux 시작 tmux 이렇게 하면 자동으로 0 번 이름의 세션이 생성되고 들어갑니다. 필요하시면 .tmux.conf 설정 파일 예제나 고급 사용법도 알려드릴 수 있어요! 😊 추가 질문 있으신가요?",
      "frontmatter": {
        "tags": [
          "잡지식"
        ],
        "date": "2025-10-22T11:37:30+09:00",
        "lastmod": "2025-10-23T02:43:19+09:00"
      }
    },
    "unix 도메인 소켓": {
      "path": "/temp/unix-도메인-소켓/",
      "filename": "unix 도메인 소켓",
      "content": "PF_UNIX (또는 AF_UNIX )는 로컬 시스템 내에서 프로세스 간 통신(IPC)을 위한 프로토콜로, 네트워크를 거치지 않고 소켓을 사용해 데이터를 교환합니다. 아래는 PF_UNIX 와 datagram socket 을 사용한 클라이언트와 서버 예제입니다. 서버 코드 (Datagram Socket with PF_UNIX) int main() { int server_sock; struct sockaddr_un server_addr, client_addr; char buffer[BUFFER_SIZE]; socklen_t client_addr_len; // 1. 소켓 생성 if ((server_sock = socket(PF_UNIX, SOCK_DGRAM, 0)) == -1) { perror(\"socket\"); exit(EXIT_FAILURE); } // 2. 주소 구조체 초기화 memset(&server_addr, 0, sizeof(server_addr)); server_addr.sun_family = AF_UNIX; strncpy(server_addr.sun_path, SOCKET_PATH, sizeof(server_addr.sun_path) - 1); // 3. 소켓 주소 바인딩 unlink(SOCKET_PATH); // 기존 소켓 파일 삭제 if (bind(server_sock, (struct sockaddr *)&server_addr, sizeof(server_addr)) == -1) { perror(\"bind\"); close(server_sock); exit(EXIT_FAILURE); } printf(\"Server is running and waiting for messages...\\n\"); // 4. 데이터 수신 client_addr_len = sizeof(client_addr); while (1) { ssize_t received = recvfrom(server_sock, buffer, BUFFER_SIZE, 0, (struct sockaddr *)&client_addr, &client_addr_len); if (received == -1) { perror(\"recvfrom\"); break; } buffer[received] = '\\0'; printf(\"Received: %s\\n\", buffer); // 5. 클라이언트로 응답 전송 const char *response = \"Message received!\"; if (sendto(server_sock, response, strlen(response), 0, (struct sockaddr *)&client_addr, client_addr_len) == -1) { perror(\"sendto\"); break; } } // 6. 소켓 종료 close(server_sock); unlink(SOCKET_PATH); // 소켓 파일 삭제 return 0; } 클라이언트 코드 (Datagram Socket with PF_UNIX) int main() { int client_sock; struct sockaddr_un server_addr, client_addr; char buffer[BUFFER_SIZE]; // 1. 소켓 생성 if ((client_sock = socket(PF_UNIX, SOCK_DGRAM, 0)) == -1) { perror(\"socket\"); exit(EXIT_FAILURE); } // 2. 클라이언트 주소 구조체 초기화 memset(&client_addr, 0, sizeof(client_addr)); client_addr.sun_family = AF_UNIX; strncpy(client_addr.sun_path, CLIENT_SOCKET_PATH, sizeof(client_addr.sun_path) - 1); // 3. 소켓 주소 바인딩 unlink(CLIENT_SOCKET_PATH); // 기존 소켓 파일 삭제 if (bind(client_sock, (struct sockaddr *)&client_addr, sizeof(client_addr)) == -1) { perror(\"bind\"); close(client_sock); exit(EXIT_FAILURE); } // 4. 서버 주소 구조체 초기화 memset(&server_addr, 0, sizeof(server_addr)); server_addr.sun_family = AF_UNIX; strncpy(server_addr.sun_path, SERVER_SOCKET_PATH, sizeof(server_addr.sun_path) - 1); // 5. 서버로 메시지 전송 const char *message = \"Hello from client!\"; if (sendto(client_sock, message, strlen(message), 0, (struct sockaddr *)&server_addr, sizeof(server_addr)) == -1) { perror(\"sendto\"); close(client_sock); exit(EXIT_FAILURE); } printf(\"Message sent to server.\\n\"); // 6. 서버로부터 응답 수신 ssize_t received = recvfrom(client_sock, buffer, BUFFER_SIZE, 0, NULL, NULL); if (received == -1) { perror(\"recvfrom\"); close(client_sock); exit(EXIT_FAILURE); } buffer[received] = '\\0'; printf(\"Received from server: %s\\n\", buffer); // 7. 소켓 종료 close(client_sock); unlink(CLIENT_SOCKET_PATH); // 소켓 파일 삭제 return 0; } 실행 방법 서버를 먼저 실행: gcc -o unix_dgram_server server.c ./unix_dgram_server 클라이언트를 실행: gcc -o unix_dgram_client client.c ./unix_dgram_client 서버는 클라이언트로부터 메시지를 받고, 클라이언트로 응답을 보냅니다. 설명 서버: bind() 를 통해 지정된 파일 경로( /tmp/unix_dgram_server.sock )에 소켓을 바인딩. recvfrom() 로 메시지를 받고, 클라이언트 주소로 응답을 보냄. 클라이언트: 서버로 데이터를 전송하고, recvfrom() 를 통해 응답을 수신. 클라이언트 소켓 경로( /tmp/unix_dgram_client.sock )를 사용하여 독립적인 소켓 설정. 이 코드는 PF_UNIX 및 datagram socket 을 활용한 간단한 클라이언트-서버 모델을 보여줍니다.",
      "frontmatter": {}
    },
    "vscode 설정": {
      "path": "/temp/vscode-설정/",
      "filename": "vscode 설정",
      "content": "keybindings.json // Toggle between terminal and editor focus { \"key\": \"ctrl+'\", \"command\": \"workbench.action.terminal.focus\"}, { \"key\": \"ctrl+'\", \"command\": \"workbench.action.focusActiveEditorGroup\", \"when\": \"terminalFocus\"} 터미널 왔다리 갔다리 Ctrl + Space 기능: 트리거 제안(Trigger Suggestions) 설명: 현재 커서 위치에서 사용 가능한 코드 제안(Suggestions)을 수동으로 호출합니다. 이는 자동 완성(IntelliSense) 기능을 강제로 활성화하는 역할을 합니다. 예를 들어, 변수 이름이나 함수 이름을 입력 중일 때, VSCode가 자동으로 제안을 보여주지 않는 경우에 Ctrl + Space 를 눌러 명시적으로 제안 목록을 열 수 있습니다. 제안 목록에는 변수, 함수, 클래스, 메서드 등이 포함됩니다. 단축키: Windows/Linux: Ctrl + Space macOS: Control + Space Tab 기능: 자동 완성 선택 또는 들여쓰기(Indentation) 설명: 자동 완성 선택: IntelliSense 제안 목록이 열려 있는 상태에서 Tab 키를 누르면 현재 선택된 제안을 확정하고 코드를 자동으로 완성합니다. 예: pri 를 입력한 후 제안 목록에서 print() 가 선택된 상태라면 Tab 을 누르면 print() 로 자동 완성됩니다. 들여쓰기: 코드 블록에서 Tab 키를 누르면 줄을 오른쪽으로 들여씁니다(indent). 반대로 Shift + Tab 은 줄을 왼쪽으로 밀어냅니다(unindent). 단축키: Windows/Linux/macOS: Tab 인수 목록(Parameter Hints) 기능: 함수 또는 메서드의 매개변수 정보 표시 설명: 함수 또는 메서드를 호출할 때, 인수 목록과 각 매개변수의 타입 및 설명을 툴팁 형태로 표시합니다. 예를 들어, Python에서 print( 를 입력하면 print(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False) 와 같은 매개변수 정보가 표시됩니다. 이를 통해 함수 호출 시 필요한 인수와 그 순서를 쉽게 파악할 수 있습니다. 단축키: 기본적으로 매개변수 힌트는 함수 호출 시 자동으로 표시됩니다. 만약 자동으로 표시되지 않는다면, Ctrl + Shift + Space 를 눌러 수동으로 매개변수 힌트를 열 수 있습니다. 단축키: Windows/Linux: Ctrl + Shift + Space macOS: Command + Shift + Space 함수 설명(Function Documentation) 기능: 함수 또는 메서드에 대한 문서(Documentation) 표시 설명: 특정 함수나 메서드에 대한 상세 설명을 확인할 수 있습니다. 이 설명에는 함수의 목적, 매개변수, 반환값, 예제 코드 등이 포함될 수 있습니다. 예를 들어, Python에서 len( 를 입력하면 len(object) -> int 와 같은 설명이 표시되며, 추가적으로 \"Return the number of items in a container.\"와 같은 문서 내용을 볼 수 있습니다. 단축키: 기본적으로 함수 이름 위에 마우스를 올리거나, 함수 이름을 입력한 후 잠시 대기하면 자동으로 문서 힌트가 표시됩니다. 수동으로 문서 힌트를 열고 싶다면 Ctrl + K, Ctrl + I 를 사용합니다. 단속키: Windows/Linux: Ctrl + K, Ctrl + I (두 키를 순차적으로 누름) macOS: Command + K, Command + I 요약 기능 설명 단축키 (Windows/Linux) 단축키 (macOS) 트리거 제안 IntelliSense 제안 목록을 수동으로 호출 Ctrl + Space Control + Space 자동 완성/들여쓰기 제안 목록에서 선택하거나 코드를 들여쓰기 Tab Tab 인수 목록 함수 호출 시 매개변수 정보 표시 Ctrl + Shift + Space Command + Shift + Space 함수 설명 함수 또는 메서드의 문서 및 설명 표시 Ctrl + K, Ctrl + I Command + K, Command + I 위 기능들은 VSCode의 기본 설정을 기준으로 설명되었습니다. 필요에 따라 단축키를 사용자 정의할 수 있으며, File > Preferences > Keyboard Shortcuts (또는 Code > Preferences > Keyboard Shortcuts on macOS)에서 단축키를 변경할 수 있습니다.",
      "frontmatter": {
        "tags": [
          "vscode"
        ],
        "date": "2024-10-06T03:00:00+09:00",
        "lastmod": "2024-10-06T03:00:00+09:00"
      }
    },
    "wordpress 과제": {
      "path": "/temp/wordpress-과제/",
      "filename": "wordpress 과제",
      "content": "wordpress docker 이미지를 사용하되 컴포즈를 통해 내부 네트워크에서 db 컨테이너와 함께 사용하자 맥 +docker compose 조합 compose 문서 먼저 wordpress 폴더를 만들어서 들어가자 여기에 db 가 사용하는 data 폴더 docker compose yml 파일등이 들어간다 mkdir wordpress cd wordpress 내부 네트워크에서 db와 wordpress 가 통신한다 오직 services.wordpress.port 가 현재 10202:80 으로 들어가 있는데 {접속포트}:80 이므로 접속할 포트(일반적으로 80) 로 설정한다 networks: wordpress: external: false services: # 데이터베이스 db: # We use a mariadb image which supports both amd64 & arm64 architecture image: mariadb:10.6.4-focal # If you really want to use MySQL, uncomment the following line #image: mysql:8.0.27 container_name: mariadb_for_wordpress command: '--default-authentication-plugin=mysql_native_password' environment: - MYSQL_ROOT_PASSWORD=somewordpress - MYSQL_DATABASE=wordpress - MYSQL_USER=wordpress - MYSQL_PASSWORD=wordpress restart: always networks: - wordpress volumes: - ./db_data:/var/lib/mysql # 워드프레스 wordpress: image: wordpress:latest container_name: wordpress ports: - 10202:80 restart: always environment: - WORDPRESS_DB_HOST=db - WORDPRESS_DB_USER=wordpress - WORDPRESS_DB_PASSWORD=wordpress - WORDPRESS_DB_NAME=wordpress networks: - wordpress depends_on: - db 컨테이너 실행 docker-compose -f wordpress-compose.yml up -d 브라우저에서 localhost:{설정한 포트} 접속 삭제할때 data 파일 까지 완전 삭제 docker-conpose -f wordpress-compose.yml down --rmi all",
      "frontmatter": {}
    },
    "x86-64 CPU 레지스터(Register)의 개념 및 종류": {
      "path": "/temp/x86-64-cpu-레지스터register의-개념-및-종류/",
      "filename": "x86-64 CPU 레지스터(Register)의 개념 및 종류",
      "content": "32bit, 64bit 운영체제에서 32bit, 64bit 는 레지스터 및 데이터 경로의 크기 를 의미한다. 위 예시에서 AH 는 8bit 운영체제와 호환되는 레지스터라고 이해하면 된다. 운영체제의 발전에 따라, 수행해야할 기능이 많아지면서 많은 정보를 다룰 수 있도록 새로운 레지스터가 추가되고, 크기도 점점 커졌다. \\* E 는 Extended 의 약자. R은 왜 R인지 모르겠다.. \\* CPU의 아키텍쳐에 따라 레지스터의 종류가 다를 수 있다. _ 범용 레지스터 ==== 범용 레지스터는 연산 결과의 임시 저장, 산술 및 논리 연산, 주소 색인 등 다양한 용도로 사용되는 다목적 레지스터이다. 종류는 EAX, EBX, ECX, EDX, ESI, EDI, ESP, EBP, EIP 가 있다. 하지만 이는 관례적으로 사용되는 용도별로 나눠놓은 것으로 범용 레지스터라는 이름과 같이 프로그래머의 의도, 또는 규약(stdcall, Thiscall...) 따라 다르게 사용될 수 있다. EAX/RAX (Accumulator Register, 누산기 레지스터) 산술, 논리 연산을 담당하는 레지스터로, 함수의 반환값이 이 레지스터에 저장된다. EAX 레지스터의 종류 RAX: 64비트 (x86-64 아키텍처에서의 확장된 EAX) EAX: 32비트 누산기 레지스터 AX: 16비트의 EAX 레지스터 하위 부분 AH: AX의 상위 8비트 AL: AX의 하위 8비트 EBX/RBX (Base Register, 베이스 레지스터)메모리 주소를 저장하기 위해 사용되는 레지스터. 종종 배열이나 문자열과 같은 데이터 구조에 접근하기 위한 기준 포인터로 사용된다. EBX 레지스터의 종류 RBX: 64비트 (x86-64 아키텍처에서의 확장된 EBX) EBX: 32비트 베이스 레지스터 BX: 16비트의 EBX 레지스터 하위 부분 BH: BX의 상위 8비트 BL: BX의 하위 8비트 ECX/RCX (Count Register, 카운트 레지스터) 반복 작업에서 카운터 역할을 수행하는 레지스터이다. loop 명령어 사용시 레지스터의 값을 하나씩 감소시키며, 0이 될때까지 반복 작업을 수행한다. ECX 레지스터의 종류 RCX: 64비트 (x86-64 아키텍처에서의 확장된 ECX) ECX: 32비트 카운트 레지스터 CX: 16비트의 ECX 레지스터 하위 부분 CH: CX의 상위 8비트 CL: CX의 하위 8비트 EDX/RDX (Data Register, 데이터 레지스터) EAX 레지스터와 함께 사용하여 큰 수를 연산을 하거나, 그 결과를 저장할 수 있는 레지스터이다. 64bit 더블워드 연산을 수행할 때에도 사용 가능하다. (div, mul) EDX 레지스터의 종류 RDX: 64비트 (x86-64 아키텍처에서의 확장된 EDX) EDX: 32비트 데이터 레지스터 DX: 16비트의 EDX 레지스터 하위 부분 DH: DX의 상위 8비트 DL: DX의 하위 8비트 _ 인덱스 레지스터 메모리 내의 데이터 접근 및 조작에 특화된 레지스터로, SI, DI 는 x86 아키텍쳐에서 범용 레지스터로 분류되기도 한다. ESI/RSI (Source Index, 소스 인덱스 레지스터) 데이터 복사, 문자열 연산, 입력/출력 처리 등의 작업에서 소스 데이터의 주소를 가리키는 데 사용된다. ESI 레지스터의 종류 RSI: 64비트 (x86-64 아키텍처에서의 확장된 ESI) ESI: 32비트 소스 인덱스 레지스터 SI: 16비트의 ESI 레지스터 하위 부분 EDI/RDI (Destination Index, 목적지 인덱스 레지스터) 데이터 복사, 문자열 처리, 배열조작 등의 작업에서 목적지 데이터의 메모리 주소를 가리키는데 사용된다. EDI 레지스터의 종류 RDI: 64비트 (x86-64 아키텍처에서의 확장된 EDI) EDI: 32비트 목적지 인덱스 레지스터 DI: 16비트의 EDI 레지스터 하위 부분 _ 포인터 레지스터 스택과 프로그램의 실행 흐름 관리에 사용되는 레지스터로, 메모리 주소가 저장된다. ESP/RSP (Stack Pointer, 스택 포인터 레지스터) 프로그램의 스택 메모리 내에서, 현재 스택 최상단 주소를 저장하는 레지스터이다. 함수 호출, 지역 변수 관리, 함수 내 데이터 저장 및 복구 등의 작업에서 필수적으로 사용된다. ESP 레지스터의 종류 RSP: 64비트 (x86-64 아키텍처에서의 확장된 ESP) ESP: 32비트 스택 포인터 레지스터. SP: 16비트의 ESP 레지스터 하위 부분. EBP/RBP (Base Pointer, 베이스 포인터 레지스터)함수내의 지역 변수와 인자에 일관되고, 쉽게 접근하기 위해 사용되는 포인터 역할 레지스터이다. 스택 내에서 접근할 부분의 메모리 주소를 저장한다. EBP 레지스터의 종류 RBP: 64비트 (x86-64 아키텍처에서의 확장된 EBP). EBP: 32비트 베이스 포인터 레지스터. BP: 16비트의 EBP 레지스터 하위 부분. EIP/RIP (Instruction Pointer, 명령 포인터) 다음에 실행될 명령의 메모리 주소를 저장한다. EIP 레지스터의 종류 RIP: 64비트 (x86-64 아키텍처) EIP: 32비트 _ 세그먼트 레지스터 메모리를 다른 세그먼트로 나누어 관리하고, 각 세그먼트에 대한 기준 주소를 저장한다. 세그먼트를 이용함으로써, 물리 메모리를 효율적으로 사용하고 프로그램 간 메모리 격리를 가능하게 한다. CS (Code Segment, 코드 세그먼트): 현재 프로그램의 코드가 포함된 세그먼트의 주소를 저장하는 레지스터 DS (Data Segment, 데이터 세그먼트): 데이터가 포함된 세그먼트의 주소를 저장하는 레지스터 SS (Stack Segment, 스택 세그먼트): 스택이 포함된 세그먼트의 주소를 저장하는 레지스터 ES, FS, GS: 추가적인 데이터 세그먼트 주소를 저장하는 레지스터 _ 플래그 레지스터 연산의 결과를 나타내는 플래그들을 포함하고, 특정 프로세서 연산을 제어한다. RFLAGS: 64비트 (x86-64 아키텍처) EFLAGS: 32비트 Zero Flag(ZF) 목적: 작업 결과가 0일 경우 설정한다. 사용: JZ(Jump-if-zero) 또는 JNZ(Jump-if-not-zero)와 같은 조건부 분기 지시에 일반적으로 사용된다. Sign Flag(SF) 목적: 작업 결과가 음수일 경우 설정한다. 사용: 부호화된 산술 연산에서 결과의 부호를 나타낸다. Carry Flag(CF) 목적: 산술 연산에서 가장 중요한 비트의 수행이 발생할 경우 설정한다(부호가 없는 연산에서 유용함). 사용: 최소 비트에서 다음 비트로의 오버플로를 나타내기 위해 다중 정밀도 산술에서 사용된다. Overflow Flag(OF) 목적: 산술 연산으로 인해 부호화된 오버플로가 발생할 경우 설정한다. 즉, 결과가 너무 커서 지정된 비트 수로 표시할 수 없다. 사용: 서명된 산술 연산에 중요하다. Parity Flag(PF) 목적: 결과에 설정된 비트 수가 짝수인지 설정한다. 사용: 오류 검사 및 단순 패리티 검사에 자주 사용된다. Auxiliary Carry Flag(AC) 목적: BCD(Binary Coded Decimal) 산술에서 사용되는 결과의 하위 절반부터 수행되는 수행이 있는지 설정한다. 용도: BCD 계산에서 특화된 산술 연산 및 특정 유형의 보정에 유용하다. Interrupt Enable/Disable Flag(IF) 목적: 하드웨어 인터럽트를 활성화하거나 비활성화하는 데 사용된다. 사용: CPU가 인터럽트 처리를 제어할 수 있다. Direction Flag(DF) 목적: 문자열 작업에서 처리 방향(증가 또는 감소)을 제어하기 위해 사용된다. 사용: 문자열 및 메모리 작업에서 데이터 블록을 통한 이동 방향을 제어한다. _ 8-bit(예:&nbsp;Intel&nbsp;8080,&nbsp;Zilog&nbsp;Z80)A (Accumulator), B, C, D, E, H, L, SP (Stack Pointer), PC (Program Counter)16-bit(예:&nbsp;Intel&nbsp;8086/8088)AX, BX, CX, DX, SI, DI, BP, SP, CS, DS, ES, SS, IP (Instruction Pointer), Flags32-bit(예:&nbsp;Intel&nbsp;80386)EAX, EBX, ECX, EDX, ESI, EDI, EBP, ESP, CS, DS, ES, SS, FS, GS, EIP (Extended IP), EFlags64-bit(예:&nbsp;x86-64&nbsp;아키텍처)RAX, RBX, RCX, RDX, RSI, RDI, RBP, RSP, CS, DS, ES, SS, FS, GS, RIP (Register IP), RFlags, R8-R15 \\* 틀린 부분이 있으면 알려주세요",
      "frontmatter": {
        "tags": [
          "cpu",
          "register"
        ],
        "date": "2024-03-20T14:24:00+09:00",
        "lastmod": "2024-03-20T14:24:00+09:00"
      }
    },
    "zsh 성능 측정": {
      "path": "/temp/zsh-성능-측정/",
      "filename": "zsh 성능 측정",
      "content": "zsh는 측정용 프로파일링 모듈을 가지고 있다. [](https://zsh.sourceforge.io/Doc/Release/Zsh-Modules.html#The-zsh_002fzprof-Module)라는 모듈인데, .zshrc 파일에 설정만 해두면 사용할 수 있다. ~/.zshrc 파일 가장 상단에 zmodload zsh/zprof 를 적는다. (import라고 생각하자) 그리고 가장 하단에 zprof 라고 적어두자. (이는 세션이 시작될 때 zprof 명령어를 실행한다는 의미와 같다.) 세션 로드가 완료되면 time zsh -i -c echo 명령어를 사용해 측정 결과를 얻자.",
      "frontmatter": {
        "tags": [
          "zsh"
        ],
        "date": "2024-11-18T05:09:00+09:00",
        "lastmod": "2024-11-18T05:09:00+09:00"
      }
    },
    "교차검증": {
      "path": "/temp/교차검증/",
      "filename": "교차검증",
      "content": "교차 검증(Cross Validation)은 머신러닝 모델의 성능을 평가하는 데 널리 사용되는 기법입니다. 이를 통해 모델이 새로운 데이터에 대해 얼마나 잘 일반화될 수 있는지를 더 정확하게 평가할 수 있습니다. 아래에서 교차 검증의 개념과 예시를 단계별로 설명하겠습니다. 교차 검증의 기본 아이디어 데이터셋을 훈련셋과 검증셋으로 나누는 경우, 훈련 데이터와 검증 데이터의 선택이 성능 평가 결과에 영향을 미칠 수 있습니다. 따라서 전체 데이터를 여러 개의 작은 부분(fold)으로 나눈 후, 각 부분을 검증셋으로 사용하고 나머지를 훈련셋으로 사용하여 여러 번 실험을 진행합니다. 이렇게 얻은 여러 성능 평가 결과의 평균값을 최종 성능 평가 결과로 사용합니다. k-겹 교차 검증(K-Fold Cross Validation)의 과정 데이터를 k개의 서브셋(fold)으로 나눈다. 데이터셋을 동일한 크기의 k개로 나눕니다. 예를 들어, 데이터셋이 100개이고 $ k=5 $라면, 각 fold는 20개의 데이터를 포함합니다. 각 fold를 검증셋으로 사용하며 모델 학습 및 평가를 반복한다. Fold 1을 검증셋으로 사용하고, 나머지 Fold 2~5를 훈련셋으로 사용해 모델을 학습한 후 검증셋(Fold 1)에서 성능을 평가합니다. 이 과정을 Fold 2, Fold 3, ..., Fold k까지 반복합니다. k번의 성능 평가 결과를 평균하여 최종 성능을 산출한다. 각 fold에서 얻은 성능(예: 정확도, F1 스코어 등)을 평균하여 최종 성능으로 간주합니다. 예시 상황: 데이터셋: 100개의 샘플 $ k = 5 $ (5-fold cross validation) 성능 평가 지표: 정확도(Accuracy) Step-by-step 설명: 데이터 분할: 데이터셋을 5개의 fold로 나눕니다. 각 fold는 20개의 샘플을 포함합니다. Fold 1: 1~20번 샘플 Fold 2: 21~40번 샘플 Fold 3: 41~60번 샘플 Fold 4: 61~80번 샘플 Fold 5: 81~100번 샘플 1번째 반복: Fold 1을 검증셋, Fold 2~5를 훈련셋으로 사용합니다. Fold 2~5(80개 샘플)로 모델을 학습하고, Fold 1(20개 샘플)에서 성능을 평가합니다. 예를 들어, Fold 1에서 정확도가 85%라고 가정합니다. 2번째 반복: Fold 2를 검증셋, Fold 1, 3~5를 훈련셋으로 사용합니다. Fold 1, 3~5(80개 샘플)로 모델을 학습하고, Fold 2(20개 샘플)에서 성능을 평가합니다. Fold 2에서 정확도가 88%라고 가정합니다. 3~5번째 반복: 위 과정을 Fold 3, Fold 4, Fold 5에서도 동일하게 수행합니다. 각 fold에서의 정확도 결과: Fold 1: 85% Fold 2: 88% Fold 3: 90% Fold 4: 87% Fold 5: 89% 최종 성능 평가: 5개 fold에서 얻은 정확도의 평균을 계산합니다. $$ \\text{평균 정확도} = \\frac{85 + 88 + 90 + 87 + 89}{5} = 87.8\\% $$ 장점 데이터 활용 효율성: 모든 데이터가 훈련과 검증 과정에 고르게 사용됩니다. 특히 데이터가 적을 때 유용합니다. 더 신뢰할 수 있는 성능 평가: 단순히 한 번의 훈련/검증만으로 성능을 평가하는 것보다, 여러 번의 평가를 통해 모델의 일반화 성능을 더 정확하게 파악할 수 있습니다. 단점 시간과 비용 소모: $ k $가 클수록 모델 학습과 평가 횟수가 늘어나므로 시간과 계산 비용이 증가합니다. 예를 들어, $ k=10 $인 경우 10번의 학습과 평가가 필요합니다. 데이터 분포의 불균형 문제: 데이터가 클래스별로 불균형할 경우, 각 fold의 분포가 원래 데이터셋의 분포와 달라질 수 있습니다. 이를 해결하기 위해 층화 K-Fold(Stratified K-Fold)를 사용할 수 있습니다. 층화 K-Fold(Stratified K-Fold) 클래스 비율을 유지하면서 데이터를 분할하는 방법입니다. 예를 들어, 데이터셋에서 클래스 A가 70%, 클래스 B가 30%라면 각 fold에도 같은 비율로 클래스 A와 B가 포함되도록 합니다. 결론 교차 검증은 모델의 성능을 안정적으로 평가하기 위한 강력한 도구입니다. 특히 데이터가 제한적인 상황에서 유용하며, 다양한 모델의 성능을 비교하거나 하이퍼파라미터 튜닝을 할 때 자주 사용됩니다. $$ \\boxed{\\text{k-겹 교차 검증은 데이터를 k개의 fold로 나눠 여러 번 학습과 평가를 반복하는 방식으로, 모델의 일반화 성능을 신뢰성 있게 평가합니다.}} $$",
      "frontmatter": {
        "date": "2025-03-17T14:53:00+09:00",
        "lastmod": "2025-11-02T02:43:25+09:00"
      }
    },
    "나의 웹서버 패킷 분석 이야기": {
      "path": "/temp/나의-웹서버-패킷-분석-이야기/",
      "filename": "나의 웹서버 패킷 분석 이야기",
      "content": "내가 만든 웹서버 shinnk.iptime.org html과 css 로만 만들어진 단순한 index.html 파일을 요청하는 상황이다 chrome 프로세스가 port 50837를 사용한다 웹서버는 80 포트로 listen 중이다 http wire shark 로 확인하였다 (https 의 경우 ssl 암호화 과정때문에 프로토콜이 일부 보이지 않는다 그래서 http 로 진행) Pasted image 20240207203522ng) 총 7개의 통신 흔적이 보인다 http 통신은 tcp 를 사용한다 그러므로 3way handshake 과정이 선행된다 no 24 no 26 no 27 no 30 no 31 no 32 no 33 24 26 27 가 3 way handshake 과정이다 Pasted image 20240207204116",
      "frontmatter": {
        "tags": [
          "network"
        ],
        "date": "2024-02-07T20:34:00+09:00",
        "lastmod": "2024-02-07T20:34:00+09:00"
      }
    },
    "데이터 베이스 비이진 관계 스키마 생성시": {
      "path": "/temp/데이터-베이스-비이진-관계-스키마-생성시/",
      "filename": "데이터 베이스 비이진 관계 스키마 생성시",
      "content": "비이진 관계에서의 기본 키 선택 비이진 관계에서 카디널리티 제약이 없는 경우, 앞서 설명한 대로 형성된 슈퍼키가 유일한 후보 키가 되며, 이를 기본 키로 선택합니다. 하지만 카디널리티 제약이 있을 경우 기본 키 선택이 더 복잡해집니다. 우리가 6.4절에서 언급한 바와 같이, 비이진 관계 집합에서 최대 한 개의 화살표만 허용합니다. 이는 비이진 관계 집합에서 두 개 이상의 화살표가 있을 경우 해석이 두 가지로 나뉘어질 수 있기 때문입니다. 예시 관계 집합 ( R )이 엔티티 집합 ( E1, E2, E3, E4 ) 간의 관계를 나타내고, 화살표가 ( E3 )와 ( E4 )로만 향해 있다고 가정합니다. 이 경우 두 가지 가능한 해석이 있습니다: 첫 번째 해석: ( E1 )과 ( E2 )의 특정 조합이 ( E3 )와 ( E4 )의 조합과 최대 한 번만 연결될 수 있습니다. 이 경우 관계 ( R )의 기본 키는 ( E1 )과 ( E2 )의 기본 키의 합집합으로 구성됩니다. 두 번째 해석: ( E1 ), ( E2 ), ( E3 )의 특정 조합이 ( E4 )의 조합과 최대 한 번만 연결될 수 있으며, 또한 ( E1 ), ( E2 ), ( E4 )의 조합이 ( E3 )의 조합과 최대 한 번만 연결될 수 있습니다. 이 경우 ( E1 ), ( E2 ), ( E3 )의 기본 키의 합집합과 ( E1 ), ( E2 ), ( E4 )의 기본 키의 합집합이 모두 후보 키가 됩니다. 이 두 해석은 모두 실제로 사용되며, 특정 모델링을 위해서도 올바른 해석입니다. 따라서 혼란을 피하기 위해 비이진 관계 집합에서 오직 하나의 화살표만 허용하며, 이 경우 두 해석이 동등하게 적용됩니다. 비이진 관계의 대체 여러 화살표가 필요한 상황을 표현하기 위해 E-R 설계를 수정하여 비이진 관계 집합을 엔티티 집합으로 대체할 수 있습니다. 즉, 비이진 관계 집합의 각 인스턴스를 엔티티로 간주하고, 이 엔티티를 ( E1, E2, E4 )의 인스턴스와 별도의 관계 집합을 통해 연결할 수 있습니다. 함수적 종속성 더 간단한 접근법은 함수적 종속성을 사용하는 것입니다. 이는 이후 장에서 다룰 내용으로, 이러한 해석을 명확하게 지정할 수 있게 해줍니다. 최종 기본 키 선택 관계 집합 ( R )의 기본 키는 관계 집합 ( R )로부터의 화살표가 없는 참여 엔티티 집합 ( E_i )의 기본 키의 합집합으로 정의됩니다. 이러한 내용은 비이진 관계에서 기본 키를 정의하고 선택하는 데 중요한 역할을 합니다.",
      "frontmatter": {
        "date": "2024-10-12T08:49:00+09:00",
        "lastmod": "2024-10-12T08:49:00+09:00"
      }
    },
    "데이터베이스 전공책 chapter6, 7 정리": {
      "path": "/temp/데이터베이스-전공책-chapter6-7-정리/",
      "filename": "데이터베이스 전공책 chapter6, 7 정리",
      "content": "ER diagram 설명 및 스키마 변경 스키마의 좋은 나쁜 설계 판별 요구사항 명세서 -> 개념적 설계(ERD ER diagram) -> 기능적 요구사항 명세서 -> 논리적 설계(ddl) -> 물리적 설계(인덱싱 파일 구성) redundancy : database 에서 attribute 는 1개만 있는 것이 좋다 왜래키로 지정하자 dept_name 은 departname 을 pk 로 가져가고 나머지는 fk 로 만들자 entity 개체 : 사람A : 속성의 집합 entity set 개체 집합 : instructor : 개체의 집합 attribute 속성 : 구성원들이 소유하는 설명 특성 value 값 : 속성의 특정 값 value set = domain : 속성이 가질 수 있는 값 집합 relationship 관계 : 개채 사이의 연관성 : 교수와 학갱의 지도교수 관계 relationship set : 관계 집합 relationship instance : 실제 조직내의 명명된 개체들 사이에 연관성 participation 참여 : entity set (개체 집합) 사이의 연관 mapping cardinality cardinality ratio 대응 키디널리티 또는 카니널리티 비율 : 대응수+ total vs partial role 역할 : 관계에서 개체가 행하는 기능 재귀 관계집합 : course - prereq : role 적는 것이 특히 재귀관계집합 이해에 도움이 된다 descriptive attribute 설명속성 : student - takes - sections 에서 task 는 학생의 성적 기록을 위해 grade 속성을 저장하기를 원한다 multivalued attribute : 다중값 속성 : student - takes - sections 에서 성적은 분반 1개에 1개의 성적을 받을 수 있는데 여러개의 성적이 들어간다면 grade 속성이 많은 것을 저장해야 한다 관계집합 차수 : 관계집합에 참여하는 개체집합의 수 2 이상 paricipation 표기?? %20image%2020241012082751.png) 식별관계는 설명속성을 가질수없다 약한 개체 집합과 연관될수 있기 때문 복합속성을 지닌 강한 개체 집합의 표현 만약 전번이 2개 이상 가진 교수의 경우 어케할까 instructorphone(ID, phonenumber) 이렇게 해서 표현한다 ID 는 instructor 의 fk 인 id 로 여기서는 왜래키 이자 pk 이다 식별관계 에서의 브릿지 table 가능성 weak entity set",
      "frontmatter": {
        "series": "Database System Concepts\\r Book by Abraham Silberschatz, Henry F. Korth, and S. Sudarshan",
        "series_weight": "6",
        "date": "2024-10-11T04:31:00+09:00",
        "lastmod": "2024-10-11T04:31:00+09:00"
      }
    },
    "동작원리": {
      "path": "/temp/동작원리/",
      "filename": "동작원리",
      "content": "src/app.py: Flask 애플리케이션 기본 및 oracle 드라이버 설정 src/views.py: db 쿼리 및 사용자에게 보낼 html 을 정함 여기서 기능을 추가하면 됩니다!!! src/test_views.py: 예시 db 쿼리 입니다 src/templates/base.html 기본적인 html 양식입니다 다른 html 에서 {% extends \"base.html\" %} 를 통해 가져와서 사용하게 됩니다 src/templates/home.html 제일 처음에 보이는 양식입니다 로그인 화면을 생각중입니다 src/templates/test/html 들 예시를 위해 만든 html 입니다 복사해서 기능 추가할 때 가져가면 됩니다 기능을 추가할 때 view.py 와 실제 사용자에게 전해줄 templates 폴더에 html 문서만 작성해주시면 됩니다 test_views.py @test.route('/test/employee', methods=['GET', 'POST']) def employee(): 여기서 모든 부서 관련 쿼리를 처리합니다 if request.method == 'POST': # 사용자 요청 처리 post 요청이 아니므로 일단은 무시 됩니다 if not employees: # 직원 목록이 비어있으면 모든 직원 조회 직원목록이 비어 있으므로 모든 직원이 조회됩니다 SELECT employee_id, employee_name, department_name, employee_address, employee_phone_number, employee_email FROM employee JOIN department ON employee.department_id = department.department_id ORDER BY employee_id 사용자에게 보낼 html 을 정하고 전달할 인수를 설정합니다 모든 직원 목록을 보냅니다(다른 기능 추가시 salary.html) render_template('test/employee.html', employees=employee ,employee_to_edit=employee_to_edit) 전달된 모든 db 내용을 표시합니다 <table> <thead> <tr> <th>직원 ID</th> <th>직원 이름</th> <th>부서 이름</th> <th>주소</th> <th>전화번호</th> <th>이메일</th> </tr> </thead> <tbody> {% for employee in employees %} <tr> <td>{{ employee[0] }}</td> <td>{{ employee[1] }}</td> <td>{{ employee[2] }}</td> <td>{{ employee[3] }}</td> <td>{{ employee[4] }}</td> <td>{{ employee[5] }}</td> </tr> {% endfor %} </tbody> </table> 사용자가 id 를 넣고 삭제 버튼을 누른다고 가정 post 요청으로 delete_employee 으로 요청합니다 <div class=\"section delete-employee\" id=\"section\"> <h3>직원 삭제</h3> <form method=\"POST\"> <label for=\"delete_employee\">직원 ID:</label> <input type=\"text\" id=\"delete_employee_id\" name=\"employee_id\" required> <br><br> <input type=\"submit\" name=\"delete_employee\" value=\"직원 삭제\"> </form> </div> 삭제 쿼리를 진행 if request.method == 'POST': # 사용자 요청 처리 여기서 POST 요청이르로 통과 case 'delete_employee': # 직원 삭제 요청 employee_id = request.form['employee_id'] cursor.execute(\"\"\" DELETE FROM employee WHERE employee_id = :employee_id \"\"\", {'employee_id': employee_id}) print(f\"Deleted employee: {employee_id}\") connection.commit() 다시 모든 직원을 조회 cursor.execute(\"\"\" SELECT employee_id, employee_name, department_name, employee_address, employee_phone_number, employee_email FROM employee JOIN department ON employee.department_id = department.department_id ORDER BY employee_id \"\"\")",
      "frontmatter": {}
    },
    "레지스터": {
      "path": "/temp/레지스터/",
      "filename": "레지스터",
      "content": "x86-64, AMD64 CPU rax, rbx, rcx, rdx: 64비트 범용 레지스터입니다. rsi, rdi: 소스 인덱스와 목적지 인덱스 레지스터입니다. rbp, rsp: 베이스 포인터와 스택 포인터 레지스터입니다. r8에서 r15까지: 추가적인 64비트 범용 레지스터입니다. rip: 명령어 포인터 레지스터입니다. eflags: 프로세서의 현재 상태를 나타내는 플래그 레지스터입니다. eax, ebx, ecx, edx: 32비트 범용 레지스터입니다. esi, edi: 32비트 소스 인덱스와 목적지 인덱스 레지스터입니다. ebp, esp: 32비트 베이스 포인터와 스택 포인터 레지스터입니다. Segs (세그먼트 레지스터): cs: 코드 세그먼트 ss: 스택 세그먼트 ds: 데이터 세그먼트 es: 추가 데이터 세그먼트 fs: 추가 데이터 세그먼트 gs: 추가 데이터 세그먼트 FPU (부동 소수점 유닛): st0에서 st7까지: FPU 스택 레지스터 fctrl: FPU 제어 레지스터 fstat: FPU 상태 레지스터 ftag: FPU 태그 워드 fiseg: FPU 명령 포인터 세그먼트 fioff: FPU 명령 포인터 오프셋 foseg: FPU 데이터 포인터 세그먼트 fooff: FPU 데이터 포인터 오프셋 fop: FPU 연산 코드 SSE 섹션: xmm0부터 xmm31까지: 128비트 XMM 레지스터입니다. 각 레지스터는 8개의 16비트 부동소수점 값(bfloat16)을 저장할 수 있습니다. mxcsr: SSE 제어 및 상태 레지스터로, 부동소수점 연산 모드와 예외 처리를 제어합니다. YMM 레지스터: ymm0h , ymm1h , ... ymm31h 는 AVX512의 상위 256비트 부분을 나타내며, ymm0 , ymm1 , ... ymm31 는 하위 256비트 부분을 나타냅니다. ymm 는 256비트 크기의 AVX 레지스터를 의미하며, AVX512에서는 zmm 레지스터가 512비트로 확장됩니다. 각 YMM 레지스터는 벡터 연산을 위해 사용됩니다. 벡터 연산은 한 번에 여러 데이터를 처리할 수 있어 병렬 처리를 효율적으로 수행할 수 있습니다. ymm0h , ymm1h 등: 이들은 AVX 레지스터의 상위 비트 (하위 256비트를 제외한 부분)입니다. ymm0h 는 ymm0 레지스터의 상위 256비트입니다. 각 레지스터가 0x0 값을 가지는 것은 이 레지스터가 현재 비어 있거나, 아직 연산에 사용되지 않았음을 의미할 수 있습니다. ymm0 , ymm1 등: 이들은 AVX 레지스터의 하위 비트이며, 주로 연산에 사용되는 실제 데이터가 이곳에 저장됩니다. 예를 들어, ymm8 = {v16_bfloat16 = {0x0 <repeats 16 times>}} 는 ymm8 레지스터가 bfloat16 형식의 데이터 16개로 채워져 있고, 그 값이 모두 0x0 임을 나타냅니다. Other Register k0-k7: AVX-512 마스크 레지스터입니다. zmm8h, zmm9h: 512비트 AVX-512 벡터 레지스터의 상위 부분입니다. fsbase, gsbase: FS와 GS 세그먼트의 베이스 주소입니다. orig_rax: 시스템 콜 진입 시 원본 RAX 값입니다. al, bl, cl, dl, sil, dil, bpl, spl: 8비트 하위 레지스터들입니다. r8l-r15l: 확장 레지스터의 8비트 하위 부분입니다. ah, bh, ch, dh: 8비트 상위 레지스터들입니다. ax, bx, cx, dx, si, di, bp: 16비트 레지스터들입니다. r8w-r15w: 확장 레지스터의 16비트 부분입니다. r8d-r15d: 확장 레지스터의 32비트 부분입니다. zmm8, zmm9: 512비트 AVX-512 벡터 레지스터입니다. MMX 섹션: zmm0h부터 zmm29h: 512비트 ZMM 레지스터의 상위 256비트 부분입니다. zmm0부터 zmm29: 전체 512비트 ZMM 레지스터입니다. AMD3DNow 섹션: zmm10h부터 zmm31h: ZMM 레지스터의 상위 256비트 부분입니다. zmm10부터 zmm31: 전체 512비트 ZMM 레지스터입니다.",
      "frontmatter": {
        "date": "2024-08-11T12:06:00+09:00",
        "lastmod": "2024-08-11T12:06:00+09:00"
      }
    },
    "리눅스 파일 권한": {
      "path": "/temp/리눅스-파일-권한/",
      "filename": "리눅스 파일 권한",
      "content": "인터넷에 적절하지 못한 파일 권한 설명 그중에 특히 sticky 비트에 대해 오류가 많아 작성하게 됨 OS 의 경우 linux 를 가정하고 Mac OS 의 경우 도 조금 서술 FILE TYPE expression nomal directory read r 읽기 내부 파일 이름 읽기 write w 쓰기 내부에 파일 생성 삭제 변경(execute 의존) execute x 실행 내부 파일 메타데이터 읽기(read 의존) setUID s 소유자 권한으로 실행 소유자 권한으로 실행 setGIDbit s 소유 그룹 권한으로 실행 소유한 그룹의 권한으로 실행 sticky bit t linux : 무시 쓰기 권한이 존재해도 다른 사용자가 소유한 파일을 건드리지 못한다 (ex /tmp) s 와 t 가 대문자인 경우 소유한 유저 또는 소유한 그룹이 실행 권한이 없어 비트가 설정되어 있어도 실제로는 동작시키지 못한다 실행별 결과 rwx 디렉토리 동작과정 shinnk@DESKTOP-KRSG68U:~/test/permission$ ls -la total 36 drwxrwxr-x 9 shinnk shinnk 4096 Oct 7 10:57 . drwxr-xr-x 10 shinnk shinnk 4096 Oct 7 10:33 .. d--x------ 2 shinnk shinnk 4096 Oct 7 10:56 100 d-w------- 2 shinnk shinnk 4096 Oct 7 10:56 200 d-wx------ 2 shinnk shinnk 4096 Oct 7 11:01 300 dr-------- 2 shinnk shinnk 4096 Oct 7 10:56 400 dr-x------ 2 shinnk shinnk 4096 Oct 7 10:56 500 drw------- 2 shinnk shinnk 4096 Oct 7 10:56 600 drwx------ 2 shinnk shinnk 4096 Oct 7 10:56 700 ===================================================== shinnk@DESKTOP-KRSG68U:~/test/permission$ ls {1,2,3,4,5,6,7}00 ls: cannot open directory '100': Permission denied ls: cannot open directory '200': Permission denied ls: cannot open directory '300': Permission denied 400: 500: 600: 700: tempfile =================================================== shinnk@DESKTOP-KRSG68U:~/test/permission$ ls -la {1,2,3,4,5,6,7}00 ls: cannot open directory '100': Permission denied ls: cannot open directory '200': Permission denied ls: cannot open directory '300': Permission denied 400: ls: cannot access '400/..': Permission denied ls: cannot access '400/.': Permission denied total 0 d????????? ? ? ? ? ? . d????????? ? ? ? ? ? .. 500: total 8 dr-x------ 2 shinnk shinnk 4096 Oct 7 10:56 . drwxrwxr-x 9 shinnk shinnk 4096 Oct 7 10:57 .. 600: ls: cannot access '600/..': Permission denied ls: cannot access '600/.': Permission denied total 0 d????????? ? ? ? ? ? . d????????? ? ? ? ? ? .. 700: total 8 drwx------ 2 shinnk shinnk 4096 Oct 7 10:56 . drwxrwxr-x 9 shinnk shinnk 4096 Oct 7 10:57 .. -rw-r--r-- 1 shinnk shinnk 0 Oct 7 10:56 tempfile shinnk@DESKTOP-KRSG68U:~/test/permission$ touch {1,2,3,4,5,6,7}00/temp touch: cannot touch '100/temp': Permission denied touch: cannot touch '200/temp': Permission denied touch: cannot touch '400/temp': Permission denied touch: cannot touch '500/temp': Permission denied touch: cannot touch '600/temp': Permission denied",
      "frontmatter": {
        "tags": [
          "linux"
        ],
        "date": "2024-10-07T04:56:00+09:00",
        "lastmod": "2024-10-07T04:56:00+09:00"
      }
    },
    "메모리 기반 database": {
      "path": "/temp/메모리-기반-database/",
      "filename": "메모리 기반 database",
      "content": "in-memory database 종류 radis 데이터 저장 방식 인메모리 저장: 인메모리 DBMS는 데이터를 RAM에 저장하여 빠른 접근을 가능하게 합니다. 데이터는 일반적으로 테이블 형식으로 구성되며, 각 테이블은 행(row)과 열(column)로 이루어져 있습니다. 데이터 구조 행 기반 vs. 열 기반: 일부 DBMS는 행 기반 저장 방식을 사용하고, 다른 DBMS는 열 기반 저장 방식을 사용합니다. 예를 들어, SAP HANA는 열 기반 저장 방식을 사용하여 분석 쿼리에 최적화되어 있습니다. 트랜잭션 처리 ACID 준수: 많은 인메모리 DBMS는 ACID(원자성, 일관성, 고립성, 지속성) 트랜잭션을 지원합니다. 이들은 메모리에 저장된 데이터의 일관성을 보장하는 메커니즘을 갖추고 있습니다. 쿼리 처리 SQL 지원: 대부분의 인메모리 DBMS는 SQL 쿼리를 지원합니다. 사용자는 SQL을 사용하여 데이터를 삽입, 선택, 업데이트 및 삭제할 수 있습니다.",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "date": "2024-09-08T21:32:00+09:00",
        "lastmod": "2024-09-08T21:32:00+09:00"
      }
    },
    "문자 찾기": {
      "path": "/temp/문자-찾기/",
      "filename": "문자 찾기",
      "content": "일반적인 이중 반복 int index = 0; // 인덱스 초기화 while (index < n) { // 특정 작업 수행 doSomething(index); // 조건에 맞는 동안 반복하는 작업 수행 while (someCondition(index) && index < n) { doSomethingElse(index); index++; } // 조건이 충족되지 않았을 때도 index 증가 if (index < n) index++; } 재귀 방식 void processIndex(int index, int n) { if (index >= n) return; // 종료 조건 // 특정 작업 수행 doSomething(index); // 조건에 따라 작업 수행 반복 if (someCondition(index)) { doSomethingElse(index); processIndex(index + 1, n); // 다음 인덱스 재귀 호출 } else { // 조건이 충족되지 않았을 때 처리 doSomethingElse(index); processIndex(index + 1, n); // 다음 인덱스 재귀 호출 } }",
      "frontmatter": {
        "date": "2024-12-31T22:23:00+09:00",
        "lastmod": "2024-12-31T22:23:00+09:00"
      }
    },
    "산업경영": {
      "path": "/temp/산업경영/",
      "filename": "산업경영",
      "content": "1장 가내수공업 공장재 수공업 공장제 기계공업 산업 분류 1차 2차 3차 산업 clark의 법칙 %20image%2020240912024542.png) 초록선 1차 빨간선 2차 파란선 3차 주황선 4차 표준 산업 분리 KSCIC 21개의 대분류 소분류 세분류 세세분류 4단계의 분리 1197개 2024기준 의로용품업체를 지원사업이 실행되면 사업자 등록증에 등록된 산업에 해당되는 산업만 수혜를 얻는다 분업 과학적 관리 (작업관리) 테일러 시간관점 길브레스 동작관점 서블릭 (마이크로노미터)(cycle graph) 간트 간트차트 이동조립 시스템에 의한 대량생산 이동조립법(포드 시스템) modern times 찰리 채플린(영화) 품질관리 연구 품질 관리도 작업심리 연구 호손 실험 : 물리적 조건 1 :가격 탄력적 : 특정 모델 스마트폰이 비싸질 수록 특정 모델 스마트폰 수요가 떨어진다 e 0 : 대체제 : 아반떼가 못생기면 k5 가 잘 팔린다 e 어떤 2가지 재화가 동일한 산업인지 판단할 때 2가지 수요가 대채성 성향을 보일때 수요의 대채성 관점으로는 같은 산업이라고 판단한다 캐스퍼 가격이 조금 오른다고 g90 이 더 잘팔리지는 않는다 즉 대채성 성향을 보이지 않고 오히려 독립재적 특징을 보인다 하지만 수요의 대채성을 기준으로 산업을 정의한다면 g90 과 캐스퍼는 같은 산업이 아니라는 이상한 결론이 나온다 그래서 다른 방향으로도 접근해볼 필요가 있다 핵심기준2 가격의 상관성 두 재화가 같은 산업에 속한다면 수요의 변화로 받는 가격 영향의 크기나 방향도 유사 이것을 보자 Pasted image 20240922134762 공학적 산업 공학 생산중심 기술 중심 산업 구조와 시스템 접근 산업 조직과 시스템 접근 주요기준 1 : 산업 집중도 산업을 지배하는 힘이 소수 기업에 집중되어 있는지 아니면 많은 기업에 분산되어 있는지에 대한 지표 주요기준 2 : 시장의 경쟁 형태 경쟁 형태 기업의 수 제품 차별성 진입/퇴출장벽 완전경쟁 많음 낮음 낮음 독점적 경쟁 많음 부분 시장 지배 낮음 과점 소수 낮음 or 높음 높음 독점 한개 - 높음 정량적 지표 : 집중도 지수 CR CRN 상위 N 개 기업의 집중도 CR1 >= 50 : 독점 CR2 >= 75 : 복점 CR3 >= 75 : 과점 CR4 < 40 : 경쟁 장점: 이해 및 측정이 용이함 단점: 측정 대상으로 포함된 N개 기업 전체의 집중도 값은 알 수 있으나 개별기업 각각의 차이는 알 수 없음 허핀달 인덱스 HI 분산과 일정 부분 비슷Pasted image 20240927154222 Pasted image 20240927154311 산업 분석과 시스템 접근 Five-forces 모형(Porter, 1979): 전략적 목적에서의 산업시스템 분석 기본 가정: 기업의 수익성, 경쟁력, 매력도의 핵심은 5가지 구조적 요인(force) 기존경쟁자 – 경쟁수준 경쟁 기업의 수 시장의 크기 시장의 성장률 제품의 차별성 수준 고정 비용 철수 장벽의 수준(높이) 공급자 – 교섭력 (원자재 남품 대상 기업) 경쟁 기업의 수공급자의 수 공급 제품(서비스)의 차별성 공급 제품(서비스)의 대체재 수 공급자 변경 용이성 공급자의 대형화 가능성 구매자 - 교섭력 (구매자의 힘) 구매자의 수 구매량 대체재 수 구매자의 집중도(소수 구매자의 비중) (구매자 입장에서) 공급자 교체 비용 제품(서비스)의 차별성 브랜드 인지도 잠재적 경쟁자 – 진입장벽 초기 자본금 법률적 제도적 규제 수준 규모의 경제 도달 가능성 제품(서비스)의 차별화 브랜드 충성도 유통 채널의 이용 용이성 철수 장벽의 수준(높이) 대체재 – 등장 가능성 향후 대체재의 수 대체재의 질적 수준 대체재의 가격 구매자의 대체재 구매 가능성 4장 기업조직과 시스템 접근 기업의 규모 (scale) 정량적 기준 : 매출액, 종업원수, 자본금 등 → 대기업, 중견기업, 중소기업 분류 전략적 기준 : “규모의 경제” 개념과 연계 규모의 경제 (economies of scale) 기업의 규모가 커지면서 얻을 수 있는 비용의 절감효과나 생산의 확대효과 범위의 경제 (economies of scope) 여러 재화를 따로 생산하는데 드는 비용과 묶어서 함께 생산하는데 드는 비용의 크기를 비교Pasted image 20240927165783 기업의 성장 옵션Pasted image 20240927170300 경영전략과 시스템 접근 Pasted image 20240927212137 기본분석 - SWOT 분석 기업 내부의 강점(S: Strength) 약점(W: Weakness) 기업 외부의 기회(O: Opportunity) 위협(T: Threat) 요인을 종합적으로 분석 SWOT 분석의 절차 1단계: 기업(사업) 프로필 분석 – 업종, 시장 영역, 경쟁 상황, 최고경영층 능력과 비전 등 2단계: 외부환경 분석 – 시장요인, 경쟁요인, 경제요인, 기술 요인, 사회 요인 등 3단계: 기회-위협 요인 도출 4단계: 내부조건 분석 – 마케팅 능력, 재무 능력, 연구개발 능력, 생산/물류능력, 관리 능력 5단계: 강점-약점 요인 도출 6단계: SWOT 매트릭스 도출 7단계: 전략 방향 제시 Pasted image 20240927212844 포지션 분석 (위상 분석) 분석과 조사의 대상이 되는 중요 요인들에 대해서, 우리 기업 또는 제품이 다른 기업과 비교하여 상대적으로 어떤 위상을 차지하는지 살펴보는 분석 Pasted image 20240927213214 포트폴리오 분석 (분포 분석) 기업이 현재 수행하고 경영 과제와 미래에 수행할 과제들이 전체적으로 어떻게 분포되어 있는가를 살펴보는 분석 목적과 용도 분포도 작성 : 기업의 보유 제품라인 / 비즈니스의 균형 정도 파악 전략방향 조정 : 포트폴리오를 개선/조정할 수 있는 전략적 방향 및 비율 결정 Pasted image 20240927213850 동태 분석 (Dynamic Analysis) 분석대상이 되는 요인의 상태가 시간의 흐름에 따라 어떻게 변화하고 있는지를 살펴보는 분석 분석대상 요인의 변화방향과 변화율 분석대상 요인의 중장기적 성장성이나 경쟁력 원가 우위 전략 차별화 전략 집중화 전략 경영조직과 시스템 접근 5장 제품의 유형 계획생산(MST) 주문 생산 (MTO) 주문 조립 (ATO) 부품 제고 있음 주문 설계 (DTO) 소품종 대량생산 전략 vs 다품종 소량생산 전략 대량맞춤 전략 (mass customization) 제조공정 분류 공정별 배치 – Job Shop  유사한 기능을 가지는 설비들을 모아 한 지역에 배치  유연성은 높으나, 효율성은 낮음 제품별 배치 – Flow Shop  특정 제품(부품)의 생산에 필요한 설비만을 뽑아 한 장소(라인)에 배치  자동차 조립라인, 자동차 차체성형 라인, TV 조립라인, 맥주 생산라인  효율성은 높으나, 유연성은 낮음 고정형 배치  생산 제품이 하나의 장소에 고정되어 있고, 인력과 물자가 이동하는 형태  대형선박 조선, 항공기 생산, 건축  인력과 자재의 이동 비용이 크고, 설비 활용도가 낮음 제품과 공정의 조합 설비입지",
      "frontmatter": {
        "date": "2024-09-12T03:59:00+09:00",
        "lastmod": "2024-09-12T03:59:00+09:00"
      }
    },
    "서브쿼리 종류": {
      "path": "/temp/서브쿼리-종류/",
      "filename": "서브쿼리 종류",
      "content": "",
      "frontmatter": {
        "tags": [
          "dbms"
        ],
        "date": "2024-12-04T15:06:00+09:00",
        "lastmod": "2024-12-04T15:06:00+09:00"
      }
    },
    "선형대수학": {
      "path": "/temp/선형대수학/",
      "filename": "선형대수학",
      "content": "선형변환은 선형 결합을 보존하는, 두 벡터 공간 사이의 함수이다. 선형결합 선형결합 = 좌표평면위의 (3,2) 백터를 정의한다면 실제로는 기저가 (1,0) , (0,1) 를 기저로 한다는 것을 정의한다 즉 좌표평면위의 (3,2) 백터는 (1,0) , (0,1) 과 (3,2) 의 선형 결합이다 두 벡터 공간 사이의 함수이다 $R2 의\\ 기저는 \\{ u1 = (1,3), u2 = (2,2)\\}$ $이를\\ 통해\\ 직교기저 \\{v1, v2\\} 를\\ 구하여라\\ \\&\\ 정규직교기저 \\{p1, p_2\\} 를 구하라$ $문제\\ 2\\ 그램\\ 슈미트\\ 직교화\\ 과정을\\ 통해\\ 직교기저가\\ 아닌\\ 기저에서\\ 직교기저를\\ 구한다$ $u1 = v1 = (1,3)$ $u_2 = (2,2)$ $v2 = u2 + \\frac{u2 \\cdot v1}{ v1 ^2} v1$ $v_2 = (2,2) + \\frac{4}{10}(1,-3)$ $v_2 = (\\frac{12}{5}, \\frac{4}{5})$ $즉\\ 직교기저\\ (v1,v2) = \\{(1,-3),(\\frac{12}{5},\\frac{4}{5})\\}$ $문제\\ 2\\ 직교\\ 기저에\\ 정규화를\\ 진행한다$ $(q1,q2) = \\{(k1,-3k1),(\\frac{12}{5}k2,\\frac{4}{5}k2)\\}$ $ q1 = \\sqrt{{k1}^2 + 9k_1^2} = 1$ $ q2 = \\sqrt{\\frac{144}{25}{{k2}^2 + \\frac{16}{25}k_2^2}} = 1$ $k_1 = \\frac{1}{\\sqrt{10}}$ $k_2 = \\frac{5}{4\\sqrt{10}}$ $즉\\ 정규직교기저\\ (q1,q2) = \\{(\\frac{1}{\\sqrt{10}},\\frac{-3}{\\sqrt{10}}),(\\frac{3}{\\sqrt{10}},\\frac{1}{\\sqrt{10}})\\}$ 물론입니다. 선형변환이 행렬식을 어떻게 바꾸는지에 대한 예를 들어 설명하겠습니다. 예시 1: 2차원 공간에서의 선형변환 기본 사각형: 원점을 기준으로 (1, 0)과 (0, 1) 벡터로 이루어진 단위 사각형을 생각해 봅시다. 이 사각형의 부피(면적)는 1입니다. 왜냐하면 가로 1, 세로 1인 사각형이기 때문입니다. 행렬 $A$: 이제 이 단위 사각형에 행렬 $A = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}$를 적용해 봅시다. 이 행렬은 x축 방향으로 2배, y축 방향으로 3배 늘리는 변환을 의미합니다. 이 변환을 적용하면, 단위 사각형은 가로 2, 세로 3인 사각형이 됩니다. 이 새로운 사각형의 면적은 $2 \\times 3 = 6$입니다. 따라서, $det(A) = 6$입니다. 행렬 $B$: 이번에는 행렬 $B = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$를 생각해 봅시다. 이 행렬은 (x, y) 좌표를 (x + y, y)로 변환합니다. 단위 사각형에 이 변환을 적용하면, 새로운 사각형의 좌표는 (1, 1), (1, 0), (0, 1), (0, 0)이 됩니다. 이 사각형의 면적은 여전히 1입니다. 따라서, $det(B) = 1$입니다. 행렬 $A$와 $B$의 곱: 이제 $AB$를 계산해 봅시다: $AB = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 2 \\\\ 0 & 3 \\end{pmatrix}$ 이 새로운 행렬 $AB$는 원래 사각형을 먼저 $B$로 변환한 후 $A$로 변환한 결과입니다. $AB$의 행렬식을 구해보면, $det(AB) = 2 \\times 3 - 2 \\times 0 = 6$입니다. 따라서, $det(AB) = det(A) \\times det(B) = 6 \\times 1 = 6$이 성립합니다. 요약 행렬 $A$는 단위 사각형을 가로 2배, 세로 3배로 늘려서 면적을 6으로 만듭니다. 행렬 $B$는 단위 사각형의 모양을 바꾸지만 면적은 그대로 유지합니다. 행렬 $A$와 $B$를 순차적으로 적용하면, 최종적으로 면적은 $det(A) \\times det(B)$가 됩니다. 이렇게 선형변환을 통해 행렬식이 부피(면적)에 어떤 영향을 미치는지 쉽게 이해할 수 있습니다. ] 네, 맞습니다. 선형변환은 행렬 곱셈을 통해 표현될 수 있습니다. 선형변환을 통해 도형이나 벡터를 변환할 때, 그 변환은 행렬과 벡터의 곱셈으로 나타낼 수 있습니다. 선형변환과 행렬 곱셈의 관계 선형변환의 정의: 선형변환 $T$는 벡터 공간의 벡터를 다른 벡터로 변환하는 함수입니다. 예를 들어, 2차원 공간에서의 선형변환 $T$는 벡터 $\\mathbf{v} = \\begin{pmatrix} v1 \\\\ v2 \\end{pmatrix}$를 $\\mathbf{w} = T(\\mathbf{v}) = \\begin{pmatrix} w1 \\\\ w2 \\end{pmatrix}$로 변환할 수 있습니다. 행렬로 표현된 선형변환: 선형변환 $T$는 행렬 $A$로 표현될 수 있습니다. 예를 들어, $T(\\mathbf{v}) = A\\mathbf{v}$라 할 때, $A$는 다음과 같은 2x2 행렬일 수 있습니다: $A = \\begin{pmatrix} a{11} & a{12} \\\\ a{21} & a{22} \\end{pmatrix}$ 이 행렬 $A$를 사용하여 벡터 $\\mathbf{v}$를 변환하면, 다음과 같은 새로운 벡터 $\\mathbf{w}$를 얻습니다: $\\mathbf{w} = A\\mathbf{v} = \\begin{pmatrix} a{11} & a{12} \\\\ a{21} & a{22} \\end{pmatrix} \\begin{pmatrix} v1 \\\\ v2 \\end{pmatrix} = \\begin{pmatrix} a{11}v1 + a{12}v2 \\\\ a{21}v1 + a{22}v2 \\end{pmatrix}$ 두 선형변환의 합성: 선형변환 $T1$과 $T2$가 각각 행렬 $A$와 $B$로 표현될 때, 두 변환의 합성 $T2(T1(\\mathbf{v}))$는 행렬 곱 $AB$로 표현됩니다. $T2(T1(\\mathbf{v})) = B(A\\mathbf{v}) = (BA)\\mathbf{v}$ 예를 들어, $A$와 $B$가 다음과 같다면: $A = \\begin{pmatrix} a{11} & a{12} \\\\ a{21} & a{22} \\end{pmatrix}, \\quad B = \\begin{pmatrix} b{11} & b{12} \\\\ b{21} & b{22} \\end{pmatrix}$ 합성된 변환은 다음과 같은 행렬 곱으로 표현됩니다: $BA = \\begin{pmatrix} b{11} & b{12} \\\\ b{21} & b{22} \\end{pmatrix} \\begin{pmatrix} a{11} & a{12} \\\\ a{21} & a{22} \\end{pmatrix} = \\begin{pmatrix} b{11}a{11} + b{12}a{21} & b{11}a{12} + b{12}a{22} \\\\ b{21}a{11} + b{22}a{21} & b{21}a{12} + b{22}a{22} \\end{pmatrix}$ 따라서, 선형변환은 행렬 곱셈을 통해 표현될 수 있으며, 여러 선형변환의 합성도 행렬 곱셈으로 나타낼 수 있습니다. 이는 선형변환의 특징과 행렬 연산의 특성이 밀접하게 연결되어 있음을 보여줍니다. 이런 자료를 참고했어요. [1] 네이버 블로그 - 선형변환(Linear Transformation) - 네이버 블로그 (https://m.blog.naver.com/spin898/221139853857) [2] TISTORY - 7-3. 선형변환의 행렬 벡터 곱 - SInce 20180106 (https://blockchainstudy.tistory.com/100) [3] SASA Math - 선형변환과 행렬의 관계 (\\(K^n\\) 공간) (https://sasamath.com/blog/articles/relation-between-linear-transformations-and-matrices/) [4] 티스토리 - 선형대수학, 그 열다섯 번째 이야기 선형 변환의 합성과 행렬곱 (https://chocobear.tistory.com/120) 뤼튼 사용하러 가기 > https://agent.wrtn.ai/5xb91l",
      "frontmatter": {}
    },
    "쓰레드의 공유 자원과 분리자원": {
      "path": "/temp/쓰레드의-공유-자원과-분리자원/",
      "filename": "쓰레드의 공유 자원과 분리자원",
      "content": "void *child_routine(void *param) { int id = *(int*)param; printf(\"Detach thread %d\\n\", id); pthread_detach(pthread_self()); } int main() { pthread_t thread[10]; void *return_value[10]; for (int i = 0; i < 10; i++) { pthread_create(&thread[i], 0, &child_routine, (void *)&i); } } 위의 코드는 오류가 발생한다 왜 발생할까",
      "frontmatter": {
        "date": "2024-12-03T15:56:00+09:00",
        "lastmod": "2024-12-03T15:56:00+09:00"
      }
    },
    "알고리즘 변수명": {
      "path": "/temp/알고리즘-변수명/",
      "filename": "알고리즘 변수명",
      "content": "인덱스 i(ndex) index pos(ition) count left <-> right start <-> end [ ]: brackets { } : braces ( ) : parentheses",
      "frontmatter": {
        "date": "2025-01-29T21:17:00+09:00",
        "lastmod": "2025-01-29T21:17:00+09:00"
      }
    },
    "알고리즘 의존성": {
      "path": "/temp/알고리즘-의존성/",
      "filename": "알고리즘 의존성",
      "content": "https://neetcode.io/roadmap %20image%2020240406092231.png)",
      "frontmatter": {}
    },
    "알르레기 반응 중 건강검진 (피검사)": {
      "path": "/temp/알르레기-반응-중-건강검진-피검사/",
      "filename": "알르레기 반응 중 건강검진 (피검사)",
      "content": "의뢰기관명 2222 미금성모의원 3333 의뢰기관코드 4 66006 5 수진자명 6 신년기 7777 접수번호 8 20250826-01865 999 Chart No. 10101010 접수일자 11 2025/08/26 18:47 121212 성별/나이 13131313 M/24 14141414 검체종류 15 NaF, Serum, Urine, WB 161616 채취일자 17171717 2025-08-26 18181818 검사일자 19 2025/08/26 20:03 202020 담당의사 21212121 김중연 22222222 보고일자 23 2025/08/27 03:50 242424 진료과/병동 25252525 담당자 26 609 27 주민번호 28282828 기타 29 보험코드 3030 검사명 3131 결과 3232 판정 3333 임상참고치/단위 3434 ■ 간담도, 췌장질환 검사 D186000HZ 35 AST(GOT) 36 19 37 M: 0-40 U/LF: 0-35 38 D185000HZ 39 ALT(GPT) 40 9 41 M: 0-45 U/LF: 0-35 42 D189000HZ 43 r-GTP 44 11 45 M: 11-60 U/LF: 8-39 46 ■ 신장질환 검사 D230000HZ 47 BUN 48 15.8 49 8.00-20.00 mg/dL 50 D228000HZ 51 Creatinine 52 1.06 53 M: 0.67-1.17 mg/dLF: 0.51-0.95 54 eGFR(CKD-EPI) 55 97.74 56 > 60 mL/min/1.73m² 57 ■ 당뇨관련 검사 D302200HZ 58 Glucose(AC) 59 80 60 60.00-100.00 mg/dL 61 ■ 전해질 및 대사관련 검사 D231000HZ 62 Uric acid 63 4.5 64 M: 3.5-7.2 mg/dLF: 2.6-6.0 65 D280002HZ 66 Sodium(Na) 67 136 68 135.00-146.00 mmol/L 69 D280006HZ 70 Potassium(K) 71 4.6 72 3.50- 5.50 mmol/L 73 ■ 혈액 검사 D000201HZ 74 WBC(백혈구) 75 10.9 76 3.50-11.00 K/uL 77 D000203HZ 78 RBC(적혈구) 79 4.79 80 M: 4.30-5.80 M/uLF: 3.68-4.88 81 D000205HZ 82 Hemoglobin(혈색소) 83 14.7 84 M: 12.4-17.1 g/dLF: 11.5-15.0 85 D000204HZ 86 Hematocrit 87 43.1 88 M: 38-52 %F: 35-45 89 MCV 90 89.9 91 80.00-105.00 fL 92 MCH 93 30.6 94 27.00- 33.00 pg 95 MCHC 96 34.1 97 31.00- 36.00 g/dL 98 D000202HZ 99 RDW 100 12.4 101 10.50-14.50 % 102 D000206HZ 103 PDW 104 15.9 105 10.50- 23.50 fL 106 D000207HZ 107 Platelets(혈소판) 108 262 109 150.00-440.00 K/uL 110 D001300HZ 111 N.Segment 112 76 113 H 114 35.00-70.00 % 115 N.Band 116 0.00-7.00 % 117 Lymphocyte 118 18 119 L 120 25.00-55.00 % 121 Monocyte 122 6 123 0.00-11.00 % 124 Eosinophil 125 0 126 0.00-7.00 % 127 Basophil 128 0.00-1.00 % 129 ■ 소변 검사 D225300HZ 130 U-Occult blood 131 음성 132 Negative 133 U-Leukocyte 134 음성 135 Negative 136 U-Bilirubin 137 음성 138 Negative 139 U-Urobilinogen 140 +/- 141 +/- 142 U-Ketone 143 음성 144 Negative 145 U-Protein 146 음성 147 Negative 148 U-Nitrite 149 음성 150 Negative 151 U-Glucose 152 음성 153 Negative 154 U-pH 155 6.0 156 5.00- 8.00 157 U-S.G. 158 1.005 159 1.005-1.030 160 D220101HZ 161 U-WBC 162 0-3 163 0-3/HPF 164 U-RBC 165 Not Found 166 0-2/HPF 167 ■ 갑상선질환 검사 D323006HZ 168 T3 (Triiodothyronine) 169 0.71 170 0.60- 1.81 ng/mL 171 D323005HZ 172 Free T4 173 1.11 174 0.89- 1.76 ng/dL 175 D325001HZ 176 TSH 177 1.18 178 0.55- 4.78 µIU/mL 179 검사자: 전미순 17005 180180180180 전문의: 김완 217 181181181181 케이씨엘의료재단 한국임상의학연구소 182182182182182182182182182 본 검사실은 CAP 인증과 대한진단검사의학회/진단검사의학재단의 우수검사실 신임 인증을 받은 검사실입니다. 183183183183 (의)케이씨엘의료재단 한국의원 서울특별시 강동구 성내로 71 184184184184 TEL: 02-559-2300 FAX: 02-517-7965 www.kcllab.com 185185185185 검사기관기호: 12340570 186186186186 검사 분류 검사 항목 결과 임상 참고치 검사 항목의 의미 간/담관질환 AST (GOT) 19 0-40 U/L 간, 심장, 근육 세포 손상 시 증가하는 효소. ALT (GPT) 9 0-45 U/L 주로 간세포에 존재하는 효소로 간 손상 시 증가. r-GTP (감마지티피) 11 11-60 U/L 간과 담도 기능의 민감한 지표, 알코올 섭취와 관련. 신장질환 BUN (혈액요소질소) 15.8 8.00-20.00 mg/dL 신장 기능, 단백질 섭취, 체내 수분 상태를 반영. Creatinine (크레아티닌) 1.06 0.67-1.17 mg/dL 근육에서 생성되는 노폐물로 신장 기능의 주요 지표. eGFR (사구체여과율) 97.74 > 60 mL/min/1.73m² 신장이 혈액을 얼마나 잘 걸러내는지를 나타내는 수치. 당뇨 Glucose (공복혈당) 80 60.00-100.00 mg/dL 혈액 속 포도당 농도로 당뇨병의 기본 지표. 전해질/대사 Uric acid (요산) 4.5 3.5-7.2 mg/dL 단백질 대사 산물로, 높으면 통풍의 원인이 됨. Sodium (나트륨) 136 135.00-146.00 mmol/L 체내 수분 균형과 신경 기능에 중요한 전해질. Potassium (칼륨) 4.6 3.50- 5.50 mmol/L 세포 기능, 특히 심장과 근육에 필수적인 전해질. 혈액 WBC (백혈구) 10.9 3.50-11.00 K/uL 면역 체계를 담당, 감염이나 염증 시 증가. RBC (적혈구) 4.79 4.30-5.80 M/uL 산소를 운반하는 혈액 세포. Hemoglobin (혈색소) 14.7 12.4-17.1 g/dL 적혈구 내 단백질로 산소 운반 능력의 지표, 빈혈 진단. Hematocrit (적혈구용적률) 43.1 38-52 % 전체 혈액에서 적혈구가 차지하는 비율. N.Segment (분절호중구) 76 35.00-70.00 % 백혈구의 일종으로 세균 감염 시 증가. (정상치 초과) Lymphocyte (림프구) 18 25.00-55.00 % 백혈구의 일종으로 바이러스 감염 시 증가. (정상치 미만) Platelets (혈소판) 262 150.00-440.00 K/uL 혈액 응고 및 지혈 작용을 담당. 갑상선질환 T3 0.71 0.60- 1.81 ng/mL 갑상선 호르몬 중 하나. Free T4 1.11 0.89-1.76 ng/dL 갑상선에서 분비되는 주된 호르몬. TSH 1.18 0.55-4.78 µIU/mL 갑상선 자극 호르몬으로 갑상선 기능 조절. 소변 U-Urobilinogen (우로빌리노겐) +/- +/- 정상적인 소변에서도 소량 발견될 수 있음. U-pH (산도) 6 5.00-8.00 소변의 산성 또는 알칼리성 정도. U-S.G. (비중) 1.005 1.005-1.030 소변의 농축 정도를 나타내며 수분 섭취량과 관련. 해당 검사결과를 정확하게 알려줘 네, 보내주신 건강검진 결과를 바탕으로 정확하고 알기 쉽게 분석해 드리겠습니다. 종합 소견 (Summary) 전반적으로 대부분의 검사 결과는 매우 양호하며, 간, 신장, 갑상선 기능 및 당뇨, 전해질 수치 등 주요 항목들이 모두 정상 범위 안에 있습니다. 다만, 혈액 검사 중 백혈구 수치에서 약간의 변화가 관찰됩니다. 이는 현재 몸에 가벼운 염증 반응이나 감염이 있을 가능성을 시사하지만, 다른 수치들이 모두 정상적이므로 크게 우려할 상황은 아닐 수 있습니다. 항목별 상세 분석 정상 수치 항목 (Excellent Results) 간/담관 기능 (AST, ALT, r-GTP): 모든 수치가 정상 범위 내에 있어 간과 담관 기능이 건강함을 의미합니다. 신장 기능 (BUN, Creatinine, eGFR): 신장의 노폐물 여과 기능이 매우 원활하며, 신장 기능이 정상적으로 유지되고 있습니다. 당뇨 (공복혈당): 공복혈당 수치가 80mg/dL로 매우 이상적입니다. 당뇨 걱정은 안 하셔도 됩니다. 전해질/대사 (요산, 나트륨, 칼륨): 통풍 위험도 낮고, 체내 수분 및 전해질 균형이 잘 유지되고 있습니다. 갑상선 기능 (T3, Free T4, TSH): 갑상선 호르몬 수치가 모두 정상으로, 갑상선 기능은 정상입니다. 소변 검사: 비중, 산도 등 모든 항목이 정상입니다. 주의가 필요한 항목 (Items to Note) 결과에서 유일하게 참고치를 벗어난 부분은 백혈구의 세부 항목입니다. N.Segment (분절호중구): 76 % (정상: 35-70%) - 높음 의미: 호중구는 우리 몸의 면역세포 중 가장 많은 수를 차지하며, 주로 세균 감염에 대한 1차 방어를 담당합니다. 이 수치가 높다는 것은 몸이 세균 감염이나 염증과 싸우고 있을 가능성을 의미합니다. 감기, 편도염, 상처 등 경미한 염증이 있을 때도 일시적으로 상승할 수 있습니다. Lymphocyte (림프구): 18 % (정상: 25-55%) - 낮음 의미: 림프구는 주로 바이러스 감염 방어와 면역 기억을 담당합니다. 호중구 수치가 상대적으로 증가하면, 백혈구 전체 비율(100%)에서 림프구의 비율은 상대적으로 감소하게 됩니다. 따라서 호중구 증가에 따른 상대적인 감소일 가능성이 높습니다. WBC (총 백혈구): 10.9 K/uL (정상: 3.5-11.0 K/uL) 의미: 총 백혈구 수치가 정상 범위의 최상한치에 가깝습니다. 이는 위에서 언급한 호중구 증가와 일맥상통하는 결과로, 면역 체계가 무언가에 반응하고 있음을 보여줍니다. 결론 및 권장사항 현재 상태: 다른 모든 검사 결과가 매우 건강하므로 심각한 질병이 있을 가능성은 낮습니다. 현재 몸에 가벼운 감기, 피로, 스트레스, 또는 눈에 띄지 않는 염증(예: 잇몸 염증, 피부 상처 등)이 있을 수 있습니다. 권장사항: 최근 감기 증상이나 몸살, 피로감, 상처 등이 있었는지 확인해 보세요. 충분한 휴식을 취하고 수분 섭취를 늘리는 것이 좋습니다. 만약 발열, 통증 등 다른 불편한 증상이 동반된다면 병원을 방문하여 의사의 진료를 받아보시는 것이 좋습니다. 특별한 증상이 없다면, 보통 이런 수치는 시간이 지나면서 자연스럽게 정상으로 돌아옵니다. 다음 건강검진 시에 추적 관찰해 보시는 것으로 충분할 수 있습니다. ※ 매우 중요한 안내: 이 분석은 제공된 수치를 기반으로 한 정보 제공이며, 의사의 진단이 아닙니다. 정확한 건강 상태 평가와 진단은 반드시 의사와 상담하시기 바랍니다. 건강한 지표도 각각 설명해줘 어느정도 낮은 지 등등 물론입니다. 정상 범위에 있는 건강한 지표들에 대해서도 각각의 수치가 어느 정도로 좋은 상태인지, 그 의미는 무엇인지 상세하게 설명해 드리겠습니다. 정상 수치 항목 상세 분석 간/담관 기능: \"매우 깨끗하고 건강한 간\" AST (19 U/L) / ALT (9 U/L) 수준 분석: 정상치(0-40, 0-45)의 하위 20~40%에 위치하는 매우 안정적이고 낮은 수치입니다. 의미: 간세포 손상이 거의 없음을 의미합니다. AST와 ALT는 간세포가 파괴될 때 혈액으로 나오는 효소인데, 이 수치가 낮다는 것은 알코올, 지방간, 약물, 바이러스 등으로 인한 간의 부담이나 손상이 거의 없는 매우 깨끗한 상태라는 긍정적인 신호입니다. r-GTP (감마지티피) (11 U/L) 수준 분석: 정상치(11-60)의 최하한치에 해당하는 아주 이상적인 수치입니다. 의미: 이 수치는 특히 알코올 섭취나 담도의 문제에 민감하게 반응합니다. 수치가 최하한치에 있다는 것은 최근 음주를 거의 하지 않았거나, 알코올에 의한 간 손상이 전혀 없으며, 담즙이 배출되는 담관의 기능도 매우 원활하다는 것을 강력하게 시사합니다. 신장 기능: \"최상의 효율을 보이는 신장\" BUN (15.8 mg/dL) / Creatinine (1.06 mg/dL) 수준 분석: 두 수치 모두 정상 범위의 가장 이상적인 중간 지점에 위치합니다. 의미: 신장이 혈액 속 노폐물을 과하지도, 부족하지도 않게 아주 효율적으로 걸러내고 있음을 의미합니다. 단백질 섭취나 체내 수분 상태 또한 매우 균형 잡혀 있다는 뜻으로, 신장 기능이 쌩쌩하게 잘 작동하고 있습니다. eGFR (사구체여과율) (97.74 mL/min/1.73m²) 수준 분석: 60 이상이 정상이지만, 보통 90 이상을 최적의 신장 기능으로 봅니다. 97.74는 거의 100에 가까운 점수로, 신장이 97.74%의 효율로 일하고 있다고 이해하시면 됩니다. 매우 우수한 결과입니다. 당뇨: \"당뇨와는 거리가 먼 안정적인 혈당\" Glucose (공복혈당) (80 mg/dL) 수준 분석: 정상치(60-100) 중에서도 당뇨병 전단계(100 이상)와는 거리가 멀고, 저혈당(60 미만)도 아닌 가장 이상적인 구간(70-90)에 위치합니다. 의미: 췌장에서 인슐린이 매우 효과적으로 분비되고 있으며, 혈당 조절 시스템이 훌륭하게 작동하고 있다는 증거입니다. 식습관과 신체 대사가 건강하게 유지되고 있습니다. 전해질/대사: \"완벽한 신체 밸런스\" Uric acid (요산) (4.5 mg/dL) 수준 분석: 정상치(3.5-7.2)의 낮은 쪽에 해당하는 좋은 수치입니다. 의미: 요산 수치가 높으면 통풍의 위험이 커지는데, 이렇게 낮은 쪽에 위치한다는 것은 단백질 대사가 원활하고 통풍의 위험이 매우 낮다는 것을 의미합니다. Sodium (나트륨) (136 mmol/L) / Potassium (칼륨) (4.6 mmol/L) 수준 분석: 두 항목 모두 정상 범위의 정중앙에 위치한 안정적인 수치입니다. 의미: 우리 몸은 나트륨과 칼륨 농도를 매우 정밀하게 조절합니다. 이 수치가 중앙에 있다는 것은 체내 수분량이 적절하고, 세포와 신경, 근육 기능이 최적의 상태로 유지되고 있음을 보여줍니다. 혈액 (빈혈 관련): \"산소 운반 능력이 충분한 건강한 피\" RBC (적혈구) (4.79 M/uL), Hemoglobin (혈색소) (14.7 g/dL), Hematocrit (적혈구용적률) (43.1 %) 수준 분석: 세 가지 수치 모두 정상 범위 내에서 안정적으로 유지되고 있습니다. 빈혈을 걱정할 필요가 전혀 없는 충분한 양입니다. 의미: 혈액의 산소 운반 능력이 충분하며, 신체 각 조직에 산소를 원활하게 공급하고 있음을 의미합니다. 어지럼증이나 만성 피로를 유발할 수 있는 빈혈과는 거리가 멉니다. Platelets (혈소판) (262 K/uL) 수준 분석: 정상치(150-440) 내에서 충분하고 안정적인 수치입니다. 의미: 상처가 났을 때 피를 멎게 하는 지혈 기능이 정상적으로 잘 작동하고 있음을 의미합니다. 갑상선 기능: \"매우 효율적인 갑상선\" T3, Free T4, TSH (모두 정상) 수준 분석: 갑상선 호르몬(T3, Free T4)과 이를 조절하는 갑상선 자극 호르몬(TSH)이 모두 이상적인 정상 범위에 있습니다. 특히 TSH 수치(1.18 µIU/mL)가 낮은 쪽에 가까운데, 이는 뇌하수체에서 갑상선을 \"자극\"할 필요가 거의 없을 만큼 갑상선 스스로 일을 잘하고 있다는 긍정적인 신호입니다. 의미: 신진대사 조절, 체온 유지 등 갑상선의 모든 기능이 과하지도(항진증), 부족하지도(저하증) 않게 매우 균형 잡힌 상태로 잘 이루어지고 있습니다. 소변 검사: \"깨끗한 요로계와 좋은 수분 섭취\" U-S.G. (비중) (1.005) 수준 분석: 정상치(1.005-1.030)의 가장 낮은 최하한치입니다. 의미: 이는 질병 상태가 아니라, 검사 직전에 물을 충분히 마셔서 소변이 매우 묽어진 상태를 의미합니다. 신장이 수분을 잘 배출하고 있다는 건강한 신호이며, 평소 수분 섭취를 잘하고 계신 것으로 보입니다. 최종 요약 백혈구 일부 항목의 경미한 변화를 제외하면, 나머지 모든 지표는 단순히 '정상'을 넘어 '매우 건강하고 이상적인' 수준에 가깝습니다. 특히 간 기능, 신장 기능, 혈당 조절 능력은 훌륭한 상태를 보여주고 있습니다. 지금처럼 건강 관리를 잘 유지하시면 되겠습니다.",
      "frontmatter": {
        "date": "2025-08-21T19:48:31+09:00",
        "lastmod": "2025-08-27T22:04:33+09:00"
      }
    },
    "원격지 서버 시간 확인하기": {
      "path": "/temp/원격지-서버-시간-확인하기/",
      "filename": "원격지 서버 시간 확인하기",
      "content": "NTP 와 같은 프로토콜을 통해 시간을 동기화 할 수 있지만 원격지 서버의 허용을 대부분 안해주기 때문에 웹서버의 요청시 http 응답의 Date 항목을 통해 원격지 서버의 시간을 예측해 본다 현재 서버의 시간을 확인하는 예시 int main() { struct timespec ts; while (1) { // 현재 시간을 가져옵니다. clock_gettime(CLOCK_REALTIME, &ts); // 시간 출력 printf(\"\\r%04ld-%02ld-%02ld %02ld:%02ld:%02ld.%09ld\", ts.tv_sec / 31557600 + 1970, // 연도 계산 (ts.tv_sec % 31557600) / 2629743 + 1, // 월 계산 (ts.tv_sec % 2629743) / 86400 + 1, // 일 계산 (ts.tv_sec % 86400) / 3600, // 시 계산 (ts.tv_sec % 3600) / 60, // 분 계산 ts.tv_sec % 60, // 초 계산 ts.tv_nsec); // 나노초 fflush(stdout); // 출력 버퍼를 플러시하여 즉시 출력 usleep(1000); // 10,000 마이크로초 (1 밀리초) 대기 } return 0; } 위와 같은 인터페이스로 구현하고자 한다 프로그램 시작부터 요청 전까지 원격지 서버 요청부터 도착시점 원격지 서버 처리 시간 원격지 서버 부터 client 까지 Date 항목 처리 출력 1 1 1 프로그램 시작 원격지 서버 요청 도착 원격지 서버 처리 시간 원격지 서버 출발 부터 client 까지 시간 추출 및 처리 출력 원격지 서버 처리시간은 정확히 그시간을 확인할 길이 없음 4 원격지 서버 출발 부터 client 까지 delay 의 경우 실시간으로 변경됨 5 시간 추출 및 처리의 경우 일반적으로 비슷한 시간이 걸림 6 출력 시간의 경우 비슷한 시간이 걸림 네트워크 지연시간 curl -s -o /dev/null -w %{time_total}\\\\n class.mju.ac.kr 서버 데이터 갱신시간의 규칙 예측하기",
      "frontmatter": {}
    },
    "의존관계 주입(dependency injection)": {
      "path": "/temp/의존관계-주입dependency-injection/",
      "filename": "의존관계 주입(dependency injection)",
      "content": "생성자 주입 설정자 수정자 주입(setter) 필드 주입 일반 메서드 주입 생성자 주입 생성자 주입(Constructor Injection) 이 방법 객체 생성 시점 의존성 부여 불변,필수 public class ExampleClass { private SomeDependency dependency; public ExampleClass(SomeDependency dependency) { this.dependency = dependency; } } spring 에서는 @Autowired 를 통해 의존성을 주입하는데 생성자가 1개 라면 생략 가능하다 설정자 주입 설정자 주입(Setter Injection) 이 방법은 객체 생성 이후에도 의존성 변경 가능 public class ExampleClass { private SomeDependency dependency; public void setDependency(SomeDependency dependency) { this.dependency = dependency; } } 필드 주입 필드 주입(Field Injection) public class ExampleClass { @Inject public SomeDependency dependency; } public 접근제어자를 사용하지 않고 private 을 사용하고도 @Autowired 를 사용하여 의존성 주입을 할 수 있다 하지만 권장하지 않음 일반 메서드 주입 일반 메서드 주입(Method Injection) public class ExampleClass { private SomeDependency dependency; public void anyMethodName(SomeDependency dependency) { this.dependency = dependency; } }",
      "frontmatter": {
        "aliases": [
          "의존성 주입",
          "DI",
          "di"
        ],
        "tags": [
          "java",
          "spring",
          "oop"
        ],
        "date": "2024-01-31T20:25:00+09:00",
        "lastmod": "2025-08-19T22:14:39+09:00"
      }
    },
    "익명 클래스(Anonymous Class)": {
      "path": "/temp/익명-클래스anonymous-class/",
      "filename": "익명 클래스(Anonymous Class)",
      "content": "내부 클래스의 일종으로 이름이 없는 클래스 java 인라인으로 한방에 사용 새로 정의한 메소드 사용 불가 // 부모 클래스 class Animal { public String bark() { return \"동물이 웁니다\"; } } public class Main { public static void main(String[] args) { Animal dog = new Animal() { // @Override 메소드 public String bark() { return \"개가 짖습니다\"; } // 새로 정의한 메소드 public String run() { return \"달리기 ㄱㄱ싱\"; } }; dog.bark(); dog.run(); // ! Error - 외부에서 호출 불가능 } } 클래스 필드로 이용 특정 클래스 내부에서 여러 메소드에서 이용될때 고려해볼 만 하다 class Animal { ... } class Creature { // 필드에 익명자식 객체를 생성 하여 이용 Animal dog = new Animal() { public String bark() { return \"멍멍\"; } }; public void method() { dog.bark(); } public void method2() { dog.bark(); } } 지역 변수로서 이용 메소드에서 일회용으로 사용하고 버려질 클래스라면 적당하다 class Animal { ... } class Creature { // ... public void method() { // 지역 변수같이 클래스를 선언하여 일회용으로 사용 Animal dog = new Animal() { public String bark() { return \"멍멍\"; } }; dog.bark(); } } 메소드 아규먼트로 이용 만일 메소드 매개변수로서 클래스 자료형이 이용된다고 할때 일회성으로만 사용한다면 아규먼트로 익명 객체를 넘겨주면 된다. class Animal { ... } class Creature { // ... public void method(Animal dog) { // 익명 객체 매개변수로 받아 사용 dog.bark(); } } public class Main { public static void main(String[] args) { Creature monster = new Creature(); // 메소드 아규먼트에 익명 클래스 자체를 입력값으로 할당 monster.method(new Animal() { public String bark() { return \"멍멍\"; } }); } }",
      "frontmatter": {
        "tags": [
          "language"
        ],
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2023-12-20T07:12:00+09:00"
      }
    },
    "임베디드 os 개발 프로젝트": {
      "path": "/temp/임베디드-os-개발-프로젝트/",
      "filename": "임베디드 os 개발 프로젝트",
      "content": "abi 참조 realview-pb-a8 을 사용 컴퓨터의 전원이 들어오면 가장 먼저 시작하는 명령이 0000.. 번지의 리셋 벡터 32(word)를 읽어서 실행 어셈블 바이너리 덤프 hexdump arm-none-eabi-as -march=armv7-a -mcpu=cortex-a8 -o Entry.o ./Entry.S arm-none-eabi-objcopy -O binary Entry.o Entry.bin hexdump Entry.bin .text .code 32 .global vector_start .global vector_end vector_start: MOV R0, R1 vector_end: .space 1024, 0 .end .text : text 섹션임을 알림 섹션 종료 지시자인 .end 까지 .global : c 언어의 extern 과 일치 .code : 명령어의 크기가 32 임을 알림 vector_start , vector_end : 레이블 설정 MOV R0, R1 : r1 레지스터의 내용을 r0 레지스터로 .space 1024, 0 : 현재 위치부터 1024 바이트를 0으로 채우라는 명령 ELF 구조 ELF wikipidia ELF 나무 위키 $ arm-none-eabi-readelf -a Entry.o ENTRY(vector_start) SECTIONS { . = 0x0; .text : { *(vector_start) *(.text .rodata) } .data : { *(.data) } .bss : { *(.bss) } }",
      "frontmatter": {
        "date": "2025-01-23T08:42:00+09:00",
        "lastmod": "2025-01-23T08:42:00+09:00"
      }
    },
    "자산 관리": {
      "path": "/temp/자산-관리/",
      "filename": "자산 관리",
      "content": "학자금 대출 3000 만원의 대출금의 경우 1.7% 기준 1년에 510,000 의 이자가 발생하고 한달 42500원의 이자금이 발생되게 된다 2000만원의 대출금의 경우 1.7% 기준 1년에 28,333 의 이자가 발생하고 한달 28333원의 이자금이 발생되게 된다 세대주 관련 주민등록법상 세법상 주택청약시 주민등록법상과 관련되 세대주 의 경우에는 단순히 전입신고를 통해 세대주가 인정되게 된다 세법상의 경우에는 전입신고와 (30세 이상 또는 결혼 또는 중위소득 40 이상(2024 기준 891,378원)(지난 1년이상)) 을 받으면 인정되게 된다 주택청약시 세대주 요건의 경우에는 위의 경우 세법상 경우를 따라가지만 무주택 기간산정에(보통은 만 30세 이상부터 카운트가 되고, 만 30세 전이라도 혼인시에는 혼인신고 시점부터 무주택 기간이 산정됩니다) 소득기준 %20image%2020240826145746.png) 중위소득 50% 1인 가구 기준 1,114,223 중위 소득 40% 1인 가구 기준 891,378 cb300 임페리알레400 메테오350 아퀼라300",
      "frontmatter": {}
    },
    "정규 표현식": {
      "path": "/temp/정규-표현식/",
      "filename": "정규 표현식",
      "content": "문자열의 일정한 패턴을 표현하는 일종의 형식 언어",
      "frontmatter": {
        "tags": [
          "잡지식"
        ],
        "date": "2024-03-01T05:45:00+09:00",
        "lastmod": "2024-03-01T05:45:00+09:00"
      }
    },
    "정렬": {
      "path": "/temp/정렬/",
      "filename": "정렬",
      "content": "==처리되지 않은 데이터 중에서 가장 작은 데이터를 선택해 맨 앞의 처리되지 않은 데이터과 교체한다== ==처리되지 않은 데이터를 하나씩 골라 처리된 데이트의 적절한 위치에 삽입한다== 새로운 배열을 만들 것인가 ==기준 데이터를 설정하고 기준보다 큰 데이터와 작은 데이트의 위치를 변경==",
      "frontmatter": {
        "tags": [
          "algorithm"
        ],
        "series": "이것이 코딩테스트다 with python",
        "series_weight": "21",
        "date": "2024-01-08T04:24:00+09:00",
        "lastmod": "2024-01-08T04:24:00+09:00"
      }
    },
    "클라우드 컴퓨팅 과제": {
      "path": "/temp/클라우드-컴퓨팅-과제/",
      "filename": "클라우드 컴퓨팅 과제",
      "content": "하둡 컨테이너를 구성하고자 하는 환경(가상머신, 리눅스 서버 등) 설명 초기 arm 기반 mac 에서 실행되지 않아 홈서버를 사용하여 x86-64 cpu, linux ubuntu server 버전에서 ssh 로 접근하여 사용하였음 -p 50070:50070 -p 50030:50030 등을 사용하여 실행시 적절하게 외부에서 hdfs mapreduce 웹 인터페이스 또한 접근가능 WordCount 실행 결과 Pasted image 20241105134407 psql manual 파일을 사용하였다 psqlmanualfile.txt 를 하둡파일시스템으로 input Pasted image 20241105134994Pasted image 20241105132250 실행 및 웹 인터페이스로 접근한 완료된 word count 작업 Pasted image 20241105135038 k2,2nr: 두 번째 필드(빈도수)를 기준으로 내림차순(nr)으로 정렬 Pasted image 20241105135129 결과예시",
      "frontmatter": {}
    },
    "클컴": {
      "path": "/temp/클컴/",
      "filename": "클컴",
      "content": "하둡은 2개자로 hadoop HDFS hadoop file system hdfs 좋은점 일반적 하드웨어를 사용해서 비싸지 않다 노드 죽어도 계속된다 hdfs 나쁜점 접근의 낮은시간 => 지연시간의 희생 높은 처리량 많은 작은 파일 => 데이터 수정, 많은 작성자 64MB large block size => SEEK 상승 RAID 금지 namenode (Master) : namespace, 파일시스템에 정보는 local disk 에도 저장 블락 위치는 ram 에만 저장 켜질때 보고 받고 재구성 3개의 복제 datanodes (Slave) : 원본 data block’s metadata including checksums for the block data and the generation stamp Block report : 처음 block 는 바로 보내고 2번째 부터는 1시간 마다 보고 heartbeats : 3초 마다 total storage capacity, fraction of storage in use, the number of data transfers currently in progress Piggybacking : replicate blocks to other nodes remove local block replicas re-register or shut down the node send an immediate block report HDFS Client Reading a file asks the NameNode for the list of DataNodes that host replicas of blocks of the file contacts a DataNode directly and requests the transfer of the desired block Writing a file asks the NameNode to choose DataNodes to host replicas of the first block of the file organizes a pipeline from node-to-node and sends the data when the first block is filled, the client requests new DataNodes to be chosen to host replicas of the next block",
      "frontmatter": {
        "date": "2024-10-25T18:37:00+09:00",
        "lastmod": "2024-10-25T18:37:00+09:00"
      }
    },
    "키보드(하드웨어) 이벤트 인식 과정": {
      "path": "/temp/키보드하드웨어-이벤트-인식-과정/",
      "filename": "키보드(하드웨어) 이벤트 인식 과정",
      "content": "리눅스에서 키보드를 인식하고 입력을 처리하는 과정은 여러 단계로 나뉘며, 하드웨어와 소프트웨어 간의 상호작용을 포함합니다. 아래는 리눅스 시스템에서 키보드가 인식되는 과정을 단계별로 설명한 것입니다. 하드웨드 초기화 및 드라이버 로드 키보드는 USB 또는 PS/2 포트를 통해 연결됩니다. 리눅스 커널은 부팅 과정에서 하드웨어 장치를 스캔하며, 연결된 키보드를 감지합니다. USB 키보드: USB 장치는 usbhid 드라이버에 의해 관리됩니다. PS/2 키보드: PS/2 장치는 atkbd 드라이버에 의해 관리됩니다. 커널은 적절한 드라이버를 자동으로 로드하여 키보드와 통신할 수 있도록 준비합니다. 입력 이벤트 생성 (Input Subsystem) 리눅스 커널에는 입력 장치를 추상화하는 Input Subsystem이라는 프레임워크가 있습니다. 이 시스템은 다양한 입력 장치(키보드, 마우스 등)를 일관된 방식으로 처리합니다. 키보드 드라이버는 사용자가 키를 누르거나 뗄 때마다 스캔 코드(Scan Code)라는 신호를 생성합니다. 이 스캔 코드는 커널 내부에서 처리되어 키 코드(Key Code)로 변환됩니다. 변환된 키 코드는 /dev/input/eventX 파일로 전달됩니다. 여기서 X 는 특정 입력 장치에 할당된 번호입니다. 예를 들어, 키보드가 /dev/input/event3 에 매핑되었다면, 이 파일을 읽으면 키보드 이벤트를 확인할 수 있습니다. 사용자 공간에서의 처리 커널이 생성한 이벤트는 사용자 공간(User Space)으로 전달되어 응용 프로그램에서 사용될 수 있습니다. (1) udev 및 장치 노드 생성 리눅스의 udev 시스템은 새로운 입력 장치가 연결될 때 이를 감지하고 /dev/input/ 디렉토리에 해당 장치 노드를 생성합니다. 예: /dev/input/event3 , /dev/input/by-id/usb-Keyboard_XXXX (2) X Server 또는 Wayland GUI 환경에서는 X Server 또는 Wayland가 키보드 이벤트를 처리합니다. X Server: X 서버는 /dev/input/eventX 파일에서 이벤트를 읽고, 이를 GUI 애플리케이션에 전달합니다. Wayland: Wayland 컴포지터(compositor)가 직접 입력 이벤트를 처리하고 클라이언트에 전달합니다. (3) TTY(Teletype) 모드 GUI가 없는 상태에서 텍스트 콘솔(TTY)을 사용하는 경우, 커널의 VT(Virtual Terminal) 서브시스템이 직접 키보드 이벤트를 처리합니다. 이 경우 키보드 입력은 현재 활성화된 TTY로 전달됩니다. 응용 프로그램에서의 처리 응용 프로그램은 키보드 이벤트를 다음과 같은 방법으로 처리할 수 있습니다. 직접 /dev/input/eventX 파일 읽기: 낮은 수준에서 직접 이벤트를 읽습니다. libinput 또는 evdev 라이브러리 사용: 고수준 API를 통해 입력 이벤트를 처리합니다. GUI 프레임워크 사용: Qt, GTK 등의 GUI 프레임워크가 내부적으로 입력 이벤트를 처리합니다. 키맵과 문자 매핑 키보드의 물리적인 키는 스캔 코드와 키 코드로 식별되지만, 실제 문자(예: 'A', 'B')로 변환되려면 키맵(Keymap)이 필요합니다. keymaps: 키 코드를 문자나 명령으로 변환하는 규칙입니다. loadkeys 또는 setxkbmap 명령어를 사용하여 키맵을 설정할 수 있습니다. 예: # 콘솔에서 키맵 설정 sudo loadkeys us # X 서버에서 키맵 설정 setxkbmap us 요약 리눅스에서 키보드가 인식되고 처리되는 과정은 다음과 같습니다: 하드웨어 감지: 커널이 키보드를 감지하고 드라이버를 로드합니다. 입력 이벤트 생성: Input Subsystem이 키보드 이벤트를 처리하고 /dev/input/eventX 파일로 전달합니다. 사용자 공간 처리: udev, X Server, Wayland, 또는 TTY가 이벤트를 처리합니다. 응용 프로그램 처리: 응용 프로그램이 이벤트를 받아 사용자 입력을 처리합니다. 키맵 적용: 키 코드를 문자로 변환하여 출력합니다. 이 과정은 리눅스의 모듈화된 설계 덕분에 매우 유연하게 동작하며, 다양한 환경에서 동일한 하드웨어를 활용할 수 있도록 합니다.",
      "frontmatter": {}
    },
    "터미널 command line shell": {
      "path": "/temp/터미널-command-line-shell/",
      "filename": "터미널 command line shell",
      "content": "터미널 command line 이라고 한정하여 shell 이라고 말한다면 일반적으로 /dev 파일 내부에 터미널 드라이브 장치를 통해 사용되는 shell 을 말한다 즉![[../08.media/20231223130204.png login shell vs non-login shell-20231223130204]]2020231223130204.png) 분류 login vs non-login : shell 이 다른 셸의 하위 프로세스로 실행되는가 interactive vs non-interactive : 사용자와 상호작용하는가 login shell vs non-login shell [!요약] Login Shell : userid passwd 입력해서 들어가는 방법 Non Longin Shell : 이미 다른 로그인 된 shell 에서 shell 을 fork 형태로 불러내는 방법 초기에는 자원의 효과적인 사용을 위해서 사용된 개념이다 로그인 셸에서 할 수 있는 최대한의 환경 구성을 미리 해두면 이후 비로그인 셸에서 적게 환경구성을 할 수 있다 로그인 셸은 대화형 세션에서 로그인 할 때 사용자 id 로 실행되는 첫번째 프로세스 이다 Interactive Shell vs non Interactive Shell 타 프로그래밍 언어와의 큰 차이이다 interactive Shell 은 python 명령어 입력시 나타나는 것과 비슷한 것으로 사용자 입력을 순차적으로 입력 받을 수 있는 방식이다 이에 반해 non interactive shell 은 python hello.py 같이 실행한다 로그인 셸 비로그인 셸 확인하기 prompt> echo $0 # 로그인 셸 -bash # \"-\" is the first character. Therefore, this is a login shell. prompt> echo $0 # 비 로그인 셸 bash # \"-\" is NOT the first character. This is a non-login shell. 실행순서 bash -lx : -l 로그인 셸 -x 디버깅 : 로그인 셸이 실행한 모든 코드를 볼 수 있다 로그인 셸 1) /etc/profile 2) ~/.bash_profile or ~/.bash_login or ~/.profile 비로그인 셸 로그인 셸의 속성중 상속을 받을 수 있는 속성만 상속후 3) /etc/bashrc (우분투는 bash.bashrc) 4) ~/.bashrc 하지만 현실은... (우분투 기준) if [ -f /etc/bashrc ]; then . /etc/bashrc fi 구문으로 인해 1) /etc/profile 2) ~/.bash_profile or ~/.bash_login or ~/.profile 3) /etc/bashrc (우분투는 bash.bashrc) 4) ~/.bashrc 순으로 실행된다 \"로그인 셸의 속성중 상속을 받을 수 있는 속성만 상속후\" 의 의미 ex) root 로 로그인 후 shinnk 로 비로그인 하면 root의 .profile 설정과 shinnk의 .bashrc 설정을 가지게 된다 사용자 이동시에는 로그인 셸을 사용하자 그렇다면 상속을 받을 수 있는 속성이란 특수 매게 변수 $1 , $2 , $3 , ...는 위치 매개변수 입니다 . \"$@\" 모든 위치 매개변수의 배열과 유사한 구성입니다 {$1, $2, $3 ...} . \"$*\" 는 모든 위치 매개변수의 IFS 확장입니다 $1 $2 $3 ... . $# 위치 매개변수의 수입니다. $- 쉘에 설정된 현재 옵션. $$ 현재 쉘의 pid(서브쉘 아님) $_ 가장 최근 매개변수(또는 시작 후 즉시 현재 쉘을 시작하는 명령의 절대 경로). $IFS (입력) 필드 구분 기호입니다. $? 가장 최근의 포그라운드 파이프라인 종료 상태입니다. $! 가장 최근 백그라운드 명령의 PID입니다. $0 쉘 또는 쉘 스크립트의 이름입니다 셸 옵션 변경 set 명령과 shopt 명령을 통해 셸의 옵션을 변경할 수 있는데 set 보다 shopt 가 조금더 고급의 추상화된 작업을 제공한다 셸 확장 Brace Expansion Brace expansion은 중괄호를 사용하여 표현식을 확장하여 여러 아이템을 생성합니다. 예시: echo a{1,2,3}b 는 a1b a2b a3b 로 확장됩니다. Tilde Expansion Tilde expansion은 홈 디렉토리 경로를 나타내는 데 사용됩니다. 예시: cd ~ 는 사용자의 홈 디렉토리로 이동합니다. echo ~user 는 user 의 홈 디렉토리 경로를 출력합니다. Parameter and Variable Expansion 변수나 파라미터의 값을 대체합니다. 예시: name=\"World\"; echo Hello, $name! 는 Hello, World! 로 확장됩니다. Command Substitution 명령어의 실행 결과를 대체합니다. 예시: echo \"Today is $(date)\" 는 Today is 와 date 명령어의 실행 결과로 확장됩니다. Arithmetic Expansion 산술 연산의 결과를 계산하여 대체합니다. 예시: echo $((2 + 3)) 는 5 로 확장됩니다. Word Splitting 변수의 값을 IFS(Internal Field Separator)에 따라 단어로 나눕니다. 예시: name=\"one two three\"; echo $name 는 one , two , three 로 나누어 출력합니다. 이는 IFS의 기본값이 공백, 탭, 개행 문자임을 반영합니다. Filename Expansion 와일드카드를 사용하여 파일 이름을 확장합니다. 예시: echo *.txt 는 현재 디렉토리의 모든 .txt 파일 명을 출력합니다. 각 확장 방식은 스크립트를 보다 유연하고 강력하게 만들어주는 도구입니다. 적절히 사용하면 복잡한 작업을 간단하게 수행할 수 있습니다. Process Substitution (프로세스 치환) 프로세스의 입력이나 출력을 파일명을 사용하여 참조합니다. 예를 들어, diff <(ls folder1) <(ls folder2) 는 두 디렉토리의 리스트를 출력하여 비교합니다. Quote Removal (인용 부호 제거) 이스케이프 문자, 인용 부호, 백슬래시의 비인용 인스턴스를 제거합니다. 예를 들어, echo \"Hello, \\\"World\\\"!\" 는 Hello, \"World\"! 를 결과로 나타냅니다.",
      "frontmatter": {
        "tags": [
          "shell",
          "linux"
        ],
        "description": "리눅스 콘솔 환경에서 shell 의 관한 정보",
        "date": "2024-04-28T08:46:00+09:00",
        "lastmod": "2025-08-11T00:45:47+09:00"
      }
    },
    "프로그래밍 특성 분류": {
      "path": "/temp/프로그래밍-특성-분류/",
      "filename": "프로그래밍 특성 분류",
      "content": "정적 타이핑 언어 동적 타입핑 언어 코드상의 타입의 명시 유무 정적 타이핑 언어 c, c++, java, typescript 동적 타입핑 언어 python, javascript runtime 타입 정보 유지 실행시간에 타입정보를 유지하는가 실행 시간에 타입 정보를 유지하는 언어의 특성은 \"런타임 타입 정보\" (Runtime Type Information, 줄여서 RTTI) 또는 \"타입 인트로스펙션\" (Type Introspection)이라고 부릅니다. 유지 https://dataonair.or.kr/db-tech-reference/d-lounge/technical-data/?mod=document&uid=235810 타입정보를 유지하므로 인한 기술 reflection",
      "frontmatter": {
        "date": "2024-02-18T17:07:00+09:00",
        "lastmod": "2024-02-18T17:07:00+09:00"
      }
    },
    "프로젝트 금요일 10-1 이후 변경점": {
      "path": "/temp/프로젝트-금요일-10-1-이후-변경점/",
      "filename": "프로젝트 금요일 10-1 이후 변경점",
      "content": "",
      "frontmatter": {}
    },
    "프로젝트 로그인 기능 구현": {
      "path": "/temp/프로젝트-로그인-기능-구현/",
      "filename": "프로젝트 로그인 기능 구현",
      "content": "/test/employee 뷰를 특정 역할을 가진 로그인된 사용자만 접근할 수 있도록 하려면 인증 및 권한 부여를 구현해야 합니다. 이를 위해 Flask-Login과 역할 기반 액세스 제어(RBAC)를 사용할 수 있습니다. 단계별 가이드: Flask-Login 설치 및 설정 사용자 모델 생성 및 역할 추가 로그인 및 로그아웃 기능 구현 접근 제한 데코레이터 작성 employee 뷰 보호 Flask-Login 설치 및 설정 먼저 Flask-Login을 설치합니다: pip install flask-login app.py 에서 Flask-Login을 설정합니다: from flask import Flask from flask_login import LoginManager def create_app(): app = Flask(__name__) app.secret_key = os.urandom(24) # Initialize the database init_db(app) # Initialize Flask-Login login_manager = LoginManager() login_manager.login_view = 'auth.login' login_manager.init_app(app) # Register blueprints from auth import auth as auth_blueprint app.register_blueprint(auth_blueprint) import views import test_views app.register_blueprint(views.main) app.register_blueprint(test_views.test) return app 사용자 모델 생성 및 역할 추가 사용자 정보를 저장하고 역할을 관리하기 위한 모델을 생성합니다. from flask_login import UserMixin class User(UserMixin): def __init__(self, id, username, role): self.id = id self.username = username self.role = role 사용자 정보를 데이터베이스에서 가져오는 함수도 필요합니다: from flask_login import LoginManager from models import User login_manager = LoginManager() @login_manager.user_loader def load_user(user_id): # 데이터베이스에서 사용자 정보 가져오기 cursor = g.db.cursor() cursor.execute(\"SELECT id, username, role FROM users WHERE id = :id\", {'id': user_id}) result = cursor.fetchone() if result: return User(id=result[0], username=result[1], role=result[2]) return None 로그인 및 로그아웃 기능 구현 auth 블루프린트를 생성하여 인증 관련 뷰를 관리합니다. from flask import Blueprint, render_template, redirect, url_for, request, flash from flask_login import login_user, logout_user from models import User auth = Blueprint('auth', __name__) @auth.route('/login', methods=['GET', 'POST']) def login(): if request.method == 'POST': username = request.form['username'] password = request.form['password'] # 데이터베이스에서 사용자 인증 cursor = g.db.cursor() cursor.execute(\"SELECT id, username, role FROM users WHERE username = :username AND password = :password\", {'username': username, 'password': password}) result = cursor.fetchone() if result: user = User(id=result[0], username=result[1], role=result[2]) login_user(user) return redirect(url_for('test.employee')) else: flash('Invalid credentials', 'error') return render_template('login.html') @auth.route('/logout') def logout(): logout_user() return redirect(url_for('auth.login')) 접근 제한 데코레이터 작성 특정 역할을 가진 사용자만 접근할 수 있도록 데코레이터를 작성합니다. from flask import redirect, url_for, flash from flask_login import current_user from functools import wraps def role_required(role): def wrapper(f): @wraps(f) def decorated_function(*args, **kwargs): if not current_user.is_authenticated: flash('You need to be logged in to access this page.', 'error') return redirect(url_for('auth.login')) if current_user.role != role: flash('You do not have permission to access this page.', 'error') return redirect(url_for('auth.login')) return f(*args, **kwargs) return decorated_function return wrapper employee 뷰 보호 employee 뷰에 데코레이터를 적용하여 역할 기반 접근을 제어합니다. from decorators import role_required from flask_login import login_required @test.route('/test/employee', methods=['GET', 'POST']) @login_required @role_required('2') # '2'가 경영관리 직원의 역할 코드라고 가정 def employee(): # 기존 코드 그대로 ... 추가 사항 데이터베이스에 사용자 및 역할 테이블 추가: 사용자 정보를 저장하기 위한 테이블을 데이터베이스에 생성해야 합니다. 비밀번호 보안 처리: 실제로는 비밀번호를 평문으로 저장하지 않고 해싱하여 저장해야 합니다. 로그인 템플릿 작성: login.html 템플릿을 생성하여 로그인 폼을 제공합니다. 이렇게 하면 /test/employee 뷰는 로그인된 사용자 중에서 역할이 '2' 인 사용자만 접근할 수 있게 됩니다. 필요한 경우 역할 비교 부분을 수정하여 정확한 역할 이름이나 코드를 사용하세요.",
      "frontmatter": {
        "date": "2024-11-25T10:56:00+09:00",
        "lastmod": "2024-11-25T10:56:00+09:00"
      }
    },
    "환경변수 목록": {
      "path": "/temp/환경변수-목록/",
      "filename": "환경변수 목록",
      "content": "아래는 env 명령어를 통해 출력된 환경 변수들에 대한 자세한 설명입니다. 각 변수는 시스템, 사용자, 프로그램 등에서 특정 정보를 저장하거나 설정하기 위해 사용됩니다. 기본 시스템 및 셸 관련 변수 SHELL=/bin/bash 현재 사용 중인 셸(Shell)의 경로를 나타냅니다. 여기서는 Bash가 사용 중임을 의미합니다. TERM=xterm-256color 터미널 유형을 나타냅니다. xterm-256color 는 256색을 지원하는 터미널임을 의미합니다. USER=shinnk 현재 로그인한 사용자의 이름입니다. HOME=/home/shinnk 사용자의 홈 디렉토리 경로입니다. PWD=/home/shinnk 현재 작업 디렉토리(Working Directory)의 경로입니다. LOGNAME=shinnk 로그인한 사용자의 이름입니다. 일반적으로 USER 와 동일합니다. SHLVL=1 셸의 중첩 레벨을 나타냅니다. 1 은 최초 실행된 셸임을 의미합니다. OLDPWD=/etc/profile.d 이전 작업 디렉토리의 경로입니다. HOSTTYPE=x86_64 시스템의 아키텍처를 나타냅니다. 여기서는 64비트(x86_64) 아키텍처를 사용 중입니다. WSL 관련 변수 WSL_DISTRO_NAME=Ubuntu-22.04 현재 실행 중인 WSL 배포판의 이름입니다. 여기서는 Ubuntu 22.04를 사용 중입니다. WSLENV=WT_SESSION:WT_PROFILE_ID: WSL과 Windows 간에 공유되는 환경 변수 목록입니다. 여기서는 WT_SESSION 과 WT_PROFILE_ID 가 공유되고 있습니다. WSL_INTEROP=/run/WSL/588_interop WSL과 Windows 간의 상호 운용성을 위한 소켓 파일 경로입니다. WSL2_GUI_APPS_ENABLED=1 WSLg(WSL GUI 애플리케이션 지원)가 활성화되어 있음을 나타냅니다. 값이 1 이면 GUI 애플리케이션을 실행할 수 있습니다. GUI 및 디스플레이 관련 변수 DISPLAY=:0 X 서버의 디스플레이 번호를 나타냅니다. WSLg에서는 기본적으로 :0 을 사용합니다. WAYLAND_DISPLAY=wayland-0 Wayland 디스플레이 서버의 이름입니다. WSLg에서는 Wayland를 지원합니다. XDG_RUNTIME_DIR=/run/user/1000/ 사용자별 런타임 데이터를 저장하는 디렉토리입니다. GUI 애플리케이션에서 사용됩니다. PULSE_SERVER=unix:/mnt/wslg/PulseServer 오디오 서버(PulseAudio)의 소켓 경로입니다. WSLg에서는 /mnt/wslg/PulseServer 를 사용합니다. DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus D-Bus 세션 버스의 주소입니다. GUI 애플리케이션에서 프로세스 간 통신(IPC)에 사용됩니다. 언어 및 지역 관련 변수 LANG=C.UTF-8 시스템의 기본 언어 및 문자 인코딩을 나타냅니다. 여기서는 UTF-8 인코딩을 사용 중입니다. LS_COLORS=... ls 명령어에서 파일 유형별로 색상을 지정하는 규칙입니다. 예를 들어, 디렉토리는 파란색( di=01;34 ), 실행 파일은 초록색( ex=01;32 )으로 표시됩니다. 패스 및 실행 파일 관련 변수 PATH=... 실행 파일을 검색할 디렉토리 경로 목록입니다. 여러 디렉토리가 : 로 구분되어 있으며, 여기에는 시스템 바이너리, 사용자 정의 스크립트, Windows 드라이브 경로 등이 포함됩니다. FNM_DIR=/home/shinnk/.local/share/fnm FNM(Fast Node Manager)의 설치 디렉토리입니다. Node.js 버전 관리 도구로 사용됩니다. FNM_MULTISHELL_PATH=/run/user/1000/fnm_multishells/635_1739414833737 FNM의 멀티셸 설정 경로입니다. FNM_NODE_DIST_MIRROR=https://nodejs.org/dist Node.js 바이너리 다운로드 미러 URL입니다. Windows Terminal 관련 변수 WT_SESSION=4baee21f-a1b1-47d9-bc9f-038c67796e20 Windows Terminal 세션의 고유 식별자입니다. WT_PROFILE_ID={17bf3de4-5353-5709-bcf9-835bd952a95e} Windows Terminal에서 사용 중인 프로필의 고유 식별자입니다. 기타 도구 및 설정 관련 변수 FNM_ARCH=x64 FNM이 실행 중인 아키텍처입니다. 여기서는 x64(64비트)입니다. FNM_COREPACK_ENABLED=false Corepack(Node.js 패키지 관리 도구)이 비활성화되어 있음을 나타냅니다. FNM_LOGLEVEL=info FNM의 로그 레벨입니다. 여기서는 info 로 설정되어 있습니다. FNM_RESOLVE_ENGINES=true FNM이 엔진(Node.js) 버전을 자동으로 확인하도록 설정되었음을 나타냅니다. DOTNET_BUNDLE_EXTRACT_BASE_DIR=/home/shinnk/.cache/dotnet_bundle_extract .NET 관련 파일의 추출 경로입니다. LESSCLOSE=/usr/bin/lesspipe %s %s less 명령어 종료 시 호출되는 후처리 스크립트입니다. LESSOPEN=| /usr/bin/lesspipe %s less 명령어 실행 시 호출되는 전처리 스크립트입니다. XDG 관련 변수 XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop XDG 데이터 디렉토리 목록입니다. GUI 애플리케이션에서 사용되는 리소스(아이콘, 메뉴 등)를 저장하는 경로입니다. 임시 및 내부 변수 _=/usr/bin/env 마지막으로 실행된 명령어의 경로입니다. 여기서는 env 명령어를 실행했으므로 /usr/bin/env 가 저장됩니다. NAME=DESKTOP-KRSG68U 시스템의 호스트 이름입니다. 위의 환경 변수들은 WSL 환경에서 다양한 기능을 지원하고, GUI 애플리케이션 실행, 언어 설정, 경로 관리 등을 가능하게 합니다. 필요한 경우 특정 변수를 수정하거나 추가하여 시스템 동작을 조정할 수 있습니다.",
      "frontmatter": {}
    },
    "환경변수와 셸변수의 차이": {
      "path": "/temp/환경변수와-셸변수의-차이/",
      "filename": "환경변수와 셸변수의 차이",
      "content": "이둘의 차이를 알려면 먼저 셀변수라는 것을 알아야 한다 터미널에 들어가면 보이는 창 이것이 바로 interactive 하게 작용하는 shell 이 실행중이다",
      "frontmatter": {
        "tags": [
          "shell",
          "잡지식"
        ],
        "date": "2024-02-26T02:58:00+09:00",
        "lastmod": "2024-02-26T02:58:00+09:00"
      }
    },
    "2의 보수를 음수 표현법으로 정한 이유": {
      "path": "/02.inbox/2의-보수를-음수-표현법으로-정한-이유/",
      "filename": "2의 보수를 음수 표현법으로 정한 이유",
      "content": "첫자리 a0 부터 a3 이라고 가정 맨 앞자리를 음수를 나타내는 비트라고 정하는 방법과 2의 보수를 음수를 나타내는 비트라고 정하는 방법 2가지를 비교 이진수 초기 구현 2의 보수로 구현한 음수 더하기 식 0000 0 0 0*a3 + 0*a2 + 0*a1 + 0*a0 0001 1 1 1*a3 + 0*a2 + 0*a1 + 1*a0 0010 2 2 0*a3 + 0*a2 + 1*a1 + 0*a0 0011 3 3 0*a3 + 0*a2 + 1*a1 + 1*a0 0100 4 4 0*a3 + 1*a2 + 0*a1 + 0*a0 0101 5 5 0*a3 + 1*a2 + 0*a1 + 1*a0 0110 6 6 0*a3 + 1*a2 + 1*a1 + 0*a0 0111 7 7 0*a3 + 1*a2 + 1*a1 + 1*a0 1000 -0 -8 -1*a3 + 0*a2 + 0*a1 + 0*a0 1001 -1 -7 -1*a3 + 0*a2 + 0*a1 + 1*a0 1010 -2 -6 -1*a3 + 0*a2 + 1*a1 + 0*a0 1011 -3 -5 -1*a3 + 0*a2 + 1*a1 + 1*a0 1100 -4 -4 -1*a3 + 1*a2 + 0*a1 + 0*a0 1101 -5 -3 -1*a3 + 1*a2 + 0*a1 + 1*a0 1110 -6 -2 -1*a3 + 1*a2 + 1*a1 + 0*a0 1111 -7 -1 -1*a3 + 1*a2 + 1*a1 + 1*a0 이진수 덧셈 방식에 따를 때 (회로의 구현방식중 ) 2의 보수가 가장 적절한 음수 표현 방법이다 2는 0으로 부터 아래로 2칸 -4는 0으로 부터 위로 4칸 음수를 2의 보수로 표현 가능하다면 덧셈 회로 만으로도 뺄셈 방식이 구현 가능하다 unsigned 와 signed 는 오버 플로우 발생 비트가 다르다 (1000 에서 발생 vs 1111 에서 발생) 결국 이러한 방식의 남은 마지막 문제는 0111 = 7 에서 양수가 더해질 때 즉 7보다 큰 수 -8 보다 작은수를 표현하려고 할 때 오버플로가 일어나 의도하지 않은 숫자가 나오게 된다 Pasted image 20240304161938 위에는 일반적인 자료형의 음수 표현 비트 사용이다 하지만 unsigned 자료형의 경우 위의 것을 고민할 필요가 없다",
      "frontmatter": {
        "tags": [
          "operating-system"
        ],
        "series": "컴퓨터 하드웨어",
        "series_weight": "304",
        "date": "2024-03-04T16:13:00+09:00",
        "lastmod": "2025-10-26T04:45:51+09:00"
      }
    },
    "AT&T 문법 과 Intel Assemble 문법 차이": {
      "path": "/02.inbox/att-문법-과-intel-assemble-문법-차이/",
      "filename": "AT&T 문법 과 Intel Assemble 문법 차이",
      "content": "x86 아키텍처에서 어셈블리 언어는 주로 Intel과 AT&T 두 가지 구문 형식으로 나뉘어 사용된다 이 두 형식은 문법과 명령어의 표현 방식에서 차이가 있다 Intel 구문 형식: 피연산자는 보통 목적지(대상) 먼저, 원천(소스) 다음으로 나열 예시: MOV EAX, EBX ; EBX의 값을 EAX로 이동 ADD EAX, 5 ; EAX에 5를 더함 AT&T 구문 형식: 소스가 먼저, 목적지가 뒤 레지스터와 즉시 값 앞에 '%'와 '$'를 붙입니다. 예시: mov %ebx, %eax ; EBX의 값을 EAX로 이동 add $5, %eax ; EAX에 5를 더함",
      "frontmatter": {
        "tags": [
          "assembler"
        ],
        "date": "2024-08-26T07:37:00+09:00",
        "lastmod": "2024-08-26T07:37:00+09:00"
      }
    },
    "CDN 링크 찾기": {
      "path": "/02.inbox/cdn-링크-찾기/",
      "filename": "CDN 링크 찾기",
      "content": "🔹 jsDelivr GitHub, npm 패키지를 CDN으로 제공 검색창에 라이브러리 이름 입력 → 버전 선택 → 자동 생성된 링크 복사 예: https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js 🔹 cdnjs 수천 개의 오픈소스 라이브러리 제공 검색 → 원하는 버전 클릭 → Copy 버튼으로 링크 복사 예: https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js 🔹 UNPKG npm 패키지를 CDN으로 제공 https://unpkg.com/[패키지명]@[버전]/[파일경로] 예: https://unpkg.com/react@18/umd/react.production.min.js",
      "frontmatter": {
        "tags": [
          "잡지식"
        ],
        "date": "2025-09-08T17:23:17+09:00",
        "lastmod": "2025-09-09T17:16:27+09:00"
      }
    },
    "Cloud Computing 과제2 보고서": {
      "path": "/02.inbox/cloud-computing-과제2-보고서/",
      "filename": "Cloud Computing 과제2 보고서",
      "content": "Pasted image 20241124053028 기존 WordCount 프로그램에 추가하거나 개선하고자 하는 각 기능 설명 문장 부호(!?,.'\"{}()) 및 소 중괄호 제거 대소문자 구분 여부를 확인하기 위해 --case-sensitive true false 인수를 받음 입력 데이터 설명 - 데이터 전체를 보고서에 포함하거나 별도 파일로 제출 bashmenualfile 를 사용하여 입력데이터로 사용하였음 각 기능에 대한 구현 결과물 - 소스 코드 전체를 보고서에 포함하거나 별도 파일로 제출 (jar 파일 X) WordCount.java 파일 각 기능에 대한 실행 결과물 - 실행 화면 전체를 보고서에 포함하거나 별도 파일로 제출] 대소문자 구분 기능 Pasted image 20241124043621 대소문자 구분 안 하는 버전 Pasted image 20241124043955 대소문자 구분하지 않은 control 단어는 114개 산출됨 Pasted image 20241124044070 대소문자 구분 버전 Pasted image 20241124044113 Control : 30 개 control : 84개 각각 분리됨 문장 부호 !?,.'\"{}() 및 소 중괄호 제거 기능 Pasted image 20241124044790 제대로 동작확인함 응용 분야에 대한 설명 및 적합도(Why?) 실시간으로 업데이트 되는 주식관련 기사를 분석하여 현재 시장에서 관심있는 키워드 분석을 통해 여론을 실시간으로 파악하는 것이 목표 RSS 피드를 통해 원하는 뉴스 출처를 구독하고 대규모 데이터 처리에 적합한 MapReduce 모델을 활용하여 대량의 기사 데이터를 효과적으로 분석 가능 입출력 데이터 정보 - 입력 데이터의 특징 및 수집 방법 - 출력 데이터 Format 및 이를 통해 도출하고자 하는 정보 입력 데이터: RSS 피드를 통해 수집된 주식관련 뉴스 기사. 각 기사는 제목, 본문, 날짜 등으로 구성됩니다. 수집 방법: Python의 feedparser 라이브러리 등을 사용하여 RSS 피드에서 데이터를 추출하고, 해당 데이터를 HDFS에 저장합니다. 출력데이터 : 실시간 뉴스 기다들의 키워드 별 빈도수를 계산한다 MapReduce 적용 방법론 - Input Key/Value Pairs, Input Parameters, Map 함수 로직, Reduce 함수 로직, Output Key/Value Pairs 등을 기술 Input Key/Value Pairs: Key: PATH (뉴스 기사저장 위치) Value: Text (뉴스 기사 내용) Input Parameters: 입력 경로: HDFS에 저장된 RSS 피드 데이터 경로 출력 경로: 분석 결과를 저장할 HDFS 경로 대소문자 구분 여부: --case-sensitive true false Map 함수 로직: 기사를 입력받아 문장 부호를 제거하고, 대소문자를 처리한 후 단어를 토큰화 각 단어의 빈도를 카운트하여 (word, 1) 형식으로 출력 Reduce 함수 로직: 같은 단어에 대해 Map 단계에서 생성된 값을 합산하여 각 단어의 총 빈도를 계산합니다. 최종적으로 각 단어와 그 빈도를 출력합니다. Output Key/Value Pairs: Key: Text (단어) Value: IntWritable (단어 빈도수)",
      "frontmatter": {
        "tags": [
          "university"
        ],
        "description": "대학 수업 cloud computing 보고서",
        "date": "2024-11-24T05:40:00+09:00",
        "lastmod": "2024-11-24T05:40:00+09:00"
      }
    },
    "Doxyzen 키워드": {
      "path": "/02.inbox/doxyzen-키워드/",
      "filename": "Doxyzen 키워드",
      "content": "키워드 용도 @brief 짧은 설명 @param 매개변수 설명 @return 반환값 설명 @pre 함수를 호출하기 전에 만족되어야 할 조건 (호출자 책임) @post 함수 실행 후 보장되는 상태 (구현자 책임) @throws 예외 명시 @note 부가 정보 @warning 경고 @see / @ref 관련 항목 참조 @deprecated 사용 중단 알림 @todo 향후 작업 @invariant 불변 조건 (클래스용) @author , @date 메타 정보 @brief 용도: 함수, 클래스, 변수 등의 간결한 요약 설명을 제공합니다. 자동 생성 문서에서 제목처럼 사용되며, 첫 문장으로 쓰는 것이 일반적입니다. 길이 제한은 없지만 한 문장이 이상적입니다. 예시: /** * @brief 정수 배열을 힙 정렬로 오름차순 정렬합니다. */ void heapSort(std::vector<int>& arr); @param 용도: 함수의 매개변수에 대한 설명을 제공합니다. 매개변수의 이름, 의미, 제약 조건 등을 명시하며, 여러 매개변수가 있을 경우 각각에 대해 기술합니다. 예시: /** * @param arr 정렬할 정수 배열 (참조로 전달됨) * @param ascending true면 오름차순, false면 내림차순 */ void sortArray(std::vector<int>& arr, bool ascending); @return 용도: 함수의 반환값이 의미하는 바를 설명합니다. void 함수에는 사용하지 않으며, 반환 타입이 복잡하거나 의미가 명확하지 않을 때 특히 중요합니다. 예시: /** * @brief 두 수의 최대공약수를 반환합니다. * @param a 양의 정수 * @param b 양의 정수 * @return a와 b의 최대공약수 (항상 ≥ 1) */ int gcd(int a, int b); @pre 용도: 함수를 호출하기 전에 반드시 만족되어야 하는 조건을 명시합니다. 이는 호출자의 책임이며, 조건을 위반할 경우 정의되지 않은 동작(undefined behavior)이 발생할 수 있습니다. 예시: /** * @brief 배열의 중앙값을 반환합니다. * @param arr 비어 있지 않은 정렬된 배열 * @pre arr.size() > 0 * @return 중앙값 */ int median(const std::vector<int>& arr); @post 용도: 함수 실행이 완료된 후 보장되는 상태를 설명합니다. 이는 구현자의 책임이며, 출력, 부작용, 객체 상태 변화 등을 명확히 할 때 사용됩니다. 예시: /** * @brief 힙에 새 요소를 삽입합니다. * @param value 삽입할 값 * @post 힙 속성이 유지되며, size()가 1 증가함 */ void push(int value); @throws (또는 @exception ) 용도: 함수가 던질 수 있는 예외의 종류와 그 조건을 명시합니다. 예외 안전성(exception safety)을 이해하고 안정적인 코드를 작성하는 데 중요합니다. 예시: /** * @brief 인덱스로 요소에 접근합니다. * @param i 접근할 인덱스 * @throws std::out_of_range i가 [0, size()) 범위를 벗어날 경우 * @return 참조된 요소 */ int& at(size_t i); @note 용도: 부가 정보, 알고리즘 특성, 시간/공간 복잡도, 사용 팁 등을 기술합니다. @warning 보다 덜 강조적이며, 참고용 정보를 담을 때 적합합니다. 예시: /** * @note 시간 복잡도: O(n log n), 공간 복잡도: O(1) * @note 불안정 정렬입니다 (같은 값의 순서가 바뀔 수 있음) */ void heapSort(std::vector<int>& arr); @warning 용도: 심각한 부작용, 데이터 손실, 성능 문제 등 사용자가 꼭 알아야 할 경고를 강조합니다. 주의를 강하게 요구할 때 사용합니다. 예시: /** * @warning 이 함수는 원본 배열을 파괴적으로 수정합니다. * 정렬 전 백업이 필요하면 복사본을 사용하세요. */ void destructiveSort(std::vector<int>& arr); @see / @ref 용도: 관련된 함수, 클래스, 문서 등을 참조하도록 안내합니다. @see : 일반 텍스트로 참조 @ref : Doxygen이 자동으로 하이퍼링크를 생성 (HTML 문서에서 클릭 가능) 예시: /** * @brief 힙을 구성합니다. * @see heapSort * @ref pushDown 함수도 참조하세요. */ void buildHeap(std::vector<int>& arr); @deprecated 용도: 더 이상 사용하지 말아야 할(구식) API임을 명시합니다. 대체 수단을 함께 제시하는 것이 좋습니다. 예시: /** * @deprecated * @brief 구버전 정렬 함수 (비효율적) * @see heapSort 대신 사용하세요. */ void oldSort(int* arr, int n); @todo 용도: 향후 구현하거나 개선해야 할 사항을 기록합니다. 개발 중인 기능, 리팩토링 계획, 확장 포인트 등을 문서화할 때 유용합니다. 예시: /** * @todo C++20 ranges 지원 추가 * @todo 병렬화 (std::execution::par) */ void advancedSort(std::vector<int>& arr); @invariant 용도: 클래스 전체에서 항상 유지되어야 하는 불변 조건(invariant)을 명시합니다. 생성자, 모든 public 메서드 호출 전후에 이 조건이 항상 참이어야 합니다. 예시: /** * @class MaxHeap * @brief 최대 힙 자료구조 * @invariant 모든 i에 대해: arr[i] >= arr[2*i+1] 및 arr[i] >= arr[2*i+2] * @invariant size() == arr.size() */ class MaxHeap { /* ... */ }; @author , @date , @version 용도: 파일 또는 프로젝트 수준의 메타 정보를 제공합니다. 주로 파일 상단의 헤더 주석에 사용되며, 유지보수 및 버전 관리에 도움이 됩니다. 예시: /** * @file heap_sort.cpp * @author 신년기 * @date 2025-10-19 * @version 1.2 * @brief 힙 정렬 알고리즘 구현 (Doxygen 문서화 완료) */ 이 스타일로 주석을 작성하면 코드의 가독성, 유지보수성, 협업 효율성이 크게 향상됩니다.",
      "frontmatter": {
        "aliases": [
          "주석 키워드"
        ],
        "tags": [
          "reference",
          "잡지식"
        ],
        "date": "2025-10-19T21:54:56+09:00",
        "lastmod": "2025-10-19T22:15:15+09:00"
      }
    },
    "HTTPS와 비대칭 키 초기 설정부터 데이터 전송까지의 모든 과정 simulation": {
      "path": "/02.inbox/https와-비대칭-키-초기-설정부터-데이터-전송까지의-모든-과정-simulation/",
      "filename": "HTTPS와 비대칭 키 초기 설정부터 데이터 전송까지의 모든 과정 simulation",
      "content": "클라이언트가 서버로부터 HTTPS 프로토콜을 통해 index.html 을 요청하는 상황에서 TCP 핸드셰이크가 완료된 후 진행되는 TLS 과정은 크게 세 단계로 나눌 수 있습니다: 핸드셰이크 (Handshake Phase) 키 도출 (Key Derivation Phase) 데이터 전송 (Data Transfer Phase) TLS 과정 상세 설명 TLS 핸드셰이크 (Handshake Phase) TLS 핸드셰이크 단계는 클라이언트와 서버가 안전한 통신을 위해 필요한 매개변수와 키를 설정하는 과정입니다. 이 단계에서는 다음의 중요한 과정들이 진행됩니다: 클라이언트 \"Hello\" 메시지 전송: 클라이언트(Bob)는 서버(Alice)에게 지원하는 암호화 알고리즘 목록을 보냅니다. 이 목록에는 대칭키 알고리즘 (예: AES), 공개키 알고리즘 (예: RSA), HMAC 알고리즘 (예: MD5 또는 SHA-1) 등이 포함될 수 있습니다. 또한, 클라이언트 논스(Client Nonce)를 함께 전송합니다. 논스는 프로토콜에서 한 번만 사용되는 고유한 숫자이며, 재생 공격(playback attack)을 방지하고 상대방이 '살아있는(live)' 상태임을 확인하는 데 사용됩니다. 서버 \"Hello\" 및 인증서 응답: 서버(Alice)는 클라이언트가 보낸 목록에서 선호하는 암호화 알고리즘들을 선택하여 클라이언트에게 다시 보냅니다. 서버는 자신의 디지털 인증서(Certificate)와 서버 논스(Server Nonce)를 함께 전송합니다. 인증서(Certificate)는 서버의 공개키(Public Key)와 서버의 신원 정보를 포함하고 있으며, 인증 기관(Certification Authority, CA)에 의해 디지털 서명되어 있습니다. 클라이언트의 서버 인증 및 Pre-Master Secret 전송: 클라이언트(Bob)는 수신한 서버의 인증서를 인증 기관(CA)의 공개키를 사용하여 검증합니다. 이 과정을 통해 클라이언트는 인증서에 포함된 서버의 공개키가 실제로 해당 서버(Alice)의 것임을 신뢰하게 됩니다. 이는 Trudy와 같은 침입자가 Bob에게 Alice로 가장하여 가짜 공개키를 전달하는 것을 방지합니다. 클라이언트는 Pre-Master Secret (PMS)이라는 무작위 값을 생성합니다. 이 PMS를 서버의 공개키(Server's Public Key)로 암호화한 후, 암호화된 PMS (Encrypted PMS)를 서버로 전송합니다. 서버의 PMS 복호화 및 Master Secret 계산: 서버(Alice)는 클라이언트로부터 수신한 암호화된 PMS를 자신의 개인키(Private Key)를 사용하여 복호화하여 원래의 PMS를 얻습니다. 이제 클라이언트와 서버 모두 PMS, 클라이언트 논스, 서버 논스를 공유하게 됩니다. 이 세 가지 정보를 바탕으로 양측은 동일한 키 도출 함수(Key Derivation Function)를 사용하여 마스터 시크릿(Master Secret, MS)을 독립적으로 계산합니다. 핸드셰이크 메시지 무결성 확인: 클라이언트는 지금까지 주고받은 모든 핸드셰이크 메시지들의 HMAC(Hash-based Message Authentication Code)을 계산하여 서버에 전송합니다. 서버도 동일하게 지금까지 주고받은 모든 핸드셰이크 메시지들의 HMAC을 계산하여 클라이언트에 전송합니다. 이 HMAC 값들은 핸드셰이크 과정에서 메시지가 조작되지 않았음을 상호 검증하는 데 사용됩니다. 예를 들어, Trudy가 클라이언트가 제안한 강력한 알고리즘을 목록에서 삭제하려 했다면, 이 HMAC 검증 단계에서 감지되어 연결이 종료될 수 있습니다. 이 단계에서 사용된 논스들은 \"연결 재생 공격(connection replay attack)\"을 방어하는 데 핵심적인 역할을 합니다. 키 도출 (Key Derivation Phase) 핸드셰이크 단계에서 공유된 마스터 시크릿(MS)을 사용하여, 클라이언트와 서버는 실제로 데이터 암호화 및 무결성 검증에 사용될 4개의 세션 키를 도출합니다. 이는 보안 강화를 위한 것으로, 보통 동일한 키를 모든 암호화와 무결성 검증에 사용하지 않습니다. 마스터 시크릿(MS)으로부터 세션 키 생성: EB : 클라이언트(Bob)가 서버(Alice)에게 데이터를 보낼 때 사용할 세션 암호화 키. MB : 클라이언트(Bob)가 서버(Alice)에게 데이터를 보낼 때 사용할 세션 HMAC 키. EA : 서버(Alice)가 클라이언트(Bob)에게 데이터를 보낼 때 사용할 세션 암호화 키. MA : 서버(Alice)가 클라이언트(Bob)에게 데이터를 보낼 때 사용할 세션 HMAC 키. Initialization Vector (IV) 생성 (필요시): 만약 선택된 대칭 암호화 방식이 CBC(Cipher Block Chaining) 모드를 사용하는 경우 (예: 3DES 또는 AES), 각 통신 방향(클라이언트->서버, 서버->클라이언트)에 대한 Initialization Vector (IV)도 마스터 시크릿(MS)으로부터 도출됩니다. IV는 동일한 평문 블록이 항상 다른 암호문 블록을 생성하도록 무작위성을 부여하는 데 사용됩니다. 이 단계가 완료되면, 클라이언트와 서버는 이제 향후 모든 메시지를 암호화하고 인증하는 데 필요한 모든 세션 키를 공유하게 됩니다. 데이터 전송 (Data Transfer Phase) 모든 세션 키가 설정된 후, 클라이언트와 서버는 안전한 TCP 연결을 통해 실제 애플리케이션 데이터 (예: index.html 파일의 내용)를 주고받기 시작합니다. 데이터 스트림을 레코드(Record)로 분할: TLS는 긴 데이터 스트림을 레코드(record)라는 작은 단위로 분할합니다. index.html 파일의 내용도 여러 TLS 레코드로 나뉘어 전송됩니다. HMAC 추가 및 암호화: 각 레코드에 대해 발신자(클라이언트 또는 서버)는 해당 레코드 데이터, 해당 방향의 HMAC 세션 키 (클라이언트의 경우 MB , 서버의 경우 MA ), 그리고 TLS 시퀀스 번호를 입력으로 사용하여 HMAC을 계산합니다. 이 HMAC은 메시지 무결성을 보장합니다. 계산된 HMAC은 레코드 데이터 뒤에 추가됩니다. 이 \"레코드 + HMAC\" 패키지 전체는 해당 방향의 암호화 세션 키 (클라이언트의 경우 EB , 서버의 경우 EA )를 사용하여 암호화됩니다. TCP로 전송: 암호화된 레코드 패키지는 하위 TCP 계층으로 전달되어 인터넷을 통해 전송됩니다. 수신자의 처리: 수신자(예: index.html 을 받는 클라이언트)는 암호화된 레코드를 받으면 먼저 해당 세션 암호화 키로 복호화합니다. 복호화된 레코드와 TLS 시퀀스 번호를 사용하여 HMAC을 다시 계산하고, 수신된 HMAC과 비교하여 데이터 무결성을 확인합니다. HMAC이 일치하면 데이터가 변경되지 않았음을 확인합니다. TLS 시퀀스 번호의 사용은 Trudy가 TCP 세그먼트를 재정렬하거나 재생하는 등의 공격(\"woman-in-the-middle\" attack)을 막아줍니다. 무결성이 확인된 복호화된 데이터는 애플리케이션 계층으로 전달됩니다. 연결 종료 (Connection Closure): TLS 세션은 더 이상 필요하지 않을 때 종료됩니다. TLS는 단순히 하위 TCP 연결을 종료하는 대신, 레코드의 type 필드를 사용하여 TLS 세션 종료를 명시적으로 알립니다. 이는 Trudy가 TCP FIN 세그먼트를 주입하여 세션을 조기에 종료(truncation attack)시키고 수신자가 모든 데이터를 받지 못했다고 오인하게 만드는 것을 방지합니다. TLS 과정 요약 표 단계 클라이언트 (요청자) 서버 (응답자) 주요 목적 및 포함 정보 핸드셰이크 1. 지원 암호화 알고리즘 목록 및 클라이언트 논스 전송. 2. 암호화 알고리즘 선택, 서버 인증서, 서버 논스 전송. 안전한 통신을 위한 매개변수 및 키 설정. 서버 인증서 검증 (CA를 통해 서버 공개키 신뢰). 4. Pre-Master Secret (PMS) 복호화. CA 역할: 서버 공개키가 진짜임을 검증. PMS 생성 및 서버 공개키로 암호화 후 전송. 6. PMS와 논스로부터 Master Secret (MS) 계산. 논스: 재생 공격 방지 및 '살아있음' 증명. 핸드셰이크 메시지 HMAC 전송. 8. 핸드셰이크 메시지 HMAC 전송. HMAC: 핸드셰이크 메시지 무결성 검증. 키 도출 MS로부터 4개의 세션 키 ( EB , MB , EA , MA ) 도출. MS로부터 4개의 세션 키 ( EB , MB , EA , MA ) 도출. 실제 데이터 암호화 및 무결성 검증에 사용될 대칭 세션 키 생성. (CBC 사용 시) IV 도출. (CBC 사용 시) IV 도출. 효율적이고 안전한 데이터 전송을 위한 준비. 데이터 전송 index.html 데이터 스트림을 레코드로 분할. 수신한 레코드 복호화 및 HMAC 검증. index.html 과 같은 애플리케이션 데이터의 기밀성 및 무결성 보장. 레코드에 HMAC 추가 (시퀀스 번호 포함). 무결성 확인된 데이터를 애플리케이션 계층에 전달. HMAC: 메시지 무결성 보장. 레코드+HMAC을 세션 암호화 키로 암호화 후 TCP로 전송. TLS 연결 종료 시 명시적 종료 레코드 사용. 시퀀스 번호: 재생 및 재정렬 공격 방지. 실제 요청시 TLS 과정 shinnk@DESKTOP-KRSG68U:~/source_main/university/network_박현민_4$ curl -v \"https://dns.google\" * Trying 8.8.4.4:443... * Connected to dns.google (8.8.4.4) port 443 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * CAfile: /etc/ssl/certs/ca-certificates.crt * CApath: /etc/ssl/certs * TLSv1.0 (OUT), TLS header, Certificate Status (22): * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.2 (IN), TLS header, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.2 (IN), TLS header, Finished (20): * TLSv1.2 (IN), TLS header, Supplemental data (23): * TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8): * TLSv1.3 (IN), TLS handshake, Certificate (11): * TLSv1.3 (IN), TLS handshake, CERT verify (15): * TLSv1.3 (IN), TLS handshake, Finished (20): * TLSv1.2 (OUT), TLS header, Finished (20): * TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1): * TLSv1.2 (OUT), TLS header, Supplemental data (23): * TLSv1.3 (OUT), TLS handshake, Finished (20): * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 * ALPN, server accepted to use h2 * Server certificate: * subject: CN=dns.google * start date: Jun 2 08:37:32 2025 GMT * expire date: Aug 25 08:37:31 2025 GMT * subjectAltName: host \"dns.google\" matched cert's \"dns.google\" * issuer: C=US; O=Google Trust Services; CN=WE2 * SSL certificate verify ok. * Using HTTP2, server supports multiplexing * Connection state changed (HTTP/2 confirmed) * Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0 * TLSv1.2 (OUT), TLS header, Supplemental data (23): * TLSv1.2 (OUT), TLS header, Supplemental data (23): * TLSv1.2 (OUT), TLS header, Supplemental data (23): * Using Stream ID: 1 (easy handle 0x55896f9109f0) * TLSv1.2 (OUT), TLS header, Supplemental data (23): GET / HTTP/2 Host: dns.google user-agent: curl/7.81.0 accept: */* * TLSv1.2 (IN), TLS header, Supplemental data (23): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * old SSL session ID is stale, removing * TLSv1.2 (IN), TLS header, Supplemental data (23): * TLSv1.2 (OUT), TLS header, Supplemental data (23): * TLSv1.2 (IN), TLS header, Supplemental data (23): * TLSv1.2 (IN), TLS header, Supplemental data (23): < HTTP/2 200 < content-security-policy: object-src 'none';base-uri 'self';script-src 'nonce-V0n3tKAxmea423-IDB1pAg' 'strict-dynamic' 'report-sample' 'unsafe-eval' 'unsafe-inline' https: http:;report-uri https://csp.withgoogle.com/csp/honest_dns/1_0;frame-ancestors 'none' < strict-transport-security: max-age=31536000; includeSubDomains; preload < x-content-type-options: nosniff < content-security-policy-report-only: script-src 'none'; form-action 'none'; frame-src 'none'; report-uri https://csp.withgoogle.com/csp/scaffolding/ntdsgswbsc:55:0 < cross-origin-opener-policy-report-only: same-origin; report-to=ntdsgswbsc:55:0 < report-to: {\"group\":\"ntdsgswbsc:55:0\",\"max_age\":2592000,\"endpoints\":[{\"url\":\"https://csp.withgoogle.com/csp/report-to/scaffolding/ntdsgswbsc:55:0\"}],} < server: scaffolding on HTTPServer2 < x-xss-protection: 0 < x-frame-options: SAMEORIGIN < date: Wed, 25 Jun 2025 16:24:52 GMT < expires: Wed, 25 Jun 2025 16:29:52 GMT < cache-control: public, max-age=300 < content-type: text/html; charset=UTF-8 < age: 90 < alt-svc: h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000 < accept-ranges: none < vary: Accept-Encoding < * TLSv1.2 (IN), TLS header, Supplemental data (23): 주어진 tls.txt 결과는 클라이언트(curl)가 dns.google 서버와 HTTPS (TLS) 연결을 수립하는 과정을 상세하게 보여줍니다. 이미 TCP 핸드셰이크가 완료되었다는 가정하에, 이 로그를 TLS 핸드셰이크, 키 도출, 그리고 데이터 전송의 세 가지 주요 TLS 단계에 맞춰 분석하고 해석해 드리겠습니다. 서버 측에서 TLS 1.3을 사용하고 있는 것이 확인됩니다. tls.txt 로그 해석: TLS 과정의 단계별 이해 tls.txt 로그는 클라이언트(여기서는 curl 명령어를 실행한 당신의 시스템)와 서버( dns.google ) 간의 TLS 통신을 보여줍니다. 이 로그는 output.pdf 에 설명된 TLS 프로토콜의 원리와 실제 구현이 어떻게 연결되는지 이해하는 데 도움이 됩니다. 시작 부분: TCP 연결 및 ALPN 협상 * Trying 8.8.4.4:443... * Connected to dns.google (8.8.4.4) port 443 (#0) 이 부분은 클라이언트가 서버의 IP 주소 8.8.4.4 의 443 포트로 TCP 연결을 시도하고 성공했음을 보여줍니다. 이는 TCP 핸드셰이크가 완료되었다는 가정을 충족합니다. * ALPN, offering h2 * ALPN, offering http/1.1 ALPN (Application-Layer Protocol Negotiation)은 TLS 핸드셰이크 과정 중 클라이언트가 서버에게 자신이 지원하는 애플리케이션 계층 프로토콜 목록을 제안하는 것입니다 [외부 정보: ALPN은 TLS 확장 기능으로, HTTPS에서 HTTP/1.1과 HTTP/2 중 무엇을 사용할지 협상하는 데 사용됩니다]. 여기서 클라이언트는 HTTP/2 ( h2 )와 HTTP/1.1을 제안하고 있습니다. * CAfile: /etc/ssl/certs/ca-certificates.crt * CApath: /etc/ssl/certs 이것은 클라이언트의 시스템이 인증서 (Certificate)를 검증하기 위해 사용할 인증 기관 (CA) 인증서의 경로를 나타냅니다. 클라이언트는 이 경로의 CA 인증서를 사용하여 서버가 전송한 인증서를 신뢰할 수 있는지 확인합니다. TLS 핸드셰이크 (Handshake Phase) TLS 핸드셰이크는 클라이언트와 서버가 안전한 통신을 위한 모든 매개변수를 협상하고, 서로의 신원을 확인하며, 세션 키를 생성하는 데 필요한 마스터 시크릿(Master Secret)을 안전하게 교환하는 과정입니다. * TLSv1.0 (OUT), TLS header, Certificate Status (22): * TLSv1.3 (OUT), TLS handshake, Client hello (1): 클라이언트 \"Hello\" 메시지 전송: 클라이언트는 TLS 1.3 버전으로 \"Client Hello\" 메시지를 전송하며 핸드셰이크를 시작합니다. 이 메시지에는 클라이언트가 지원하는 암호화 알고리즘 목록 (대칭키, 공개키, HMAC 알고리즘 등)과 함께 클라이언트 논스 (Client Nonce)가 포함됩니다. 논스는 재생 공격(playback attack)을 방지하고 상대방이 현재 \"살아있는\" 상태임을 확인하는 데 사용되는 \"한 번만 사용되는 값\"입니다. 앞서 보이는 TLSv1.0 이나 TLSv1.2 레코드는 curl 의 내부 로깅이나 초기 협상 시도일 수 있으며, 실제로는 더 높은 버전이 사용됩니다. * TLSv1.2 (IN), TLS header, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Server hello (2): 서버 \"Hello\" 및 인증서 응답: 서버는 클라이언트의 목록에서 암호화 알고리즘들을 선택하고, \"Server Hello\" 메시지를 보냅니다. 이 메시지에는 서버가 선택한 알고리즘과 함께 서버 논스 (Server Nonce)가 포함됩니다. * TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8): 암호화된 확장 (Encrypted Extensions): TLS 1.3의 특징 중 하나로, 이전 버전에서는 평문으로 전송되던 일부 핸드셰이크 확장(예: ALPN 협상 결과 등)이 이제 암호화되어 전송됩니다 [외부 정보: TLS 1.3의 주요 변경 사항 중 하나는 핸드셰이크 메시지의 더 많은 부분을 암호화하여 정보 노출을 줄이는 것입니다]. * TLSv1.3 (IN), TLS handshake, Certificate (11): 서버 인증서 전송: 서버는 자신의 디지털 인증서 (Certificate)를 클라이언트에게 전송합니다. 이 인증서는 서버의 공개키 (Public Key)와 서버의 신원 정보를 포함하고 있으며, 신뢰할 수 있는 인증 기관 (CA)에 의해 디지털 서명되어 있습니다. * TLSv1.3 (IN), TLS handshake, CERT verify (15): 인증서 검증 (Certificate Verify): 서버는 자신의 개인키 (Private Key)로 핸드셰이크 메시지들의 해시 값에 서명하여 클라이언트에게 보냅니다. 클라이언트는 서버의 공개키로 이 서명을 검증하여, 서버가 해당 공개키에 상응하는 개인키를 소유하고 있음을 확인하고, 핸드셰이크 메시지가 위변조되지 않았음을 검증합니다. * TLSv1.3 (IN), TLS handshake, Finished (20): * TLSv1.2 (OUT), TLS header, Finished (20): * TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1): * TLSv1.3 (OUT), TLS handshake, Finished (20): 핸드셰이크 완료 (Finished): 클라이언트와 서버는 지금까지 주고받은 모든 핸드셰이크 메시지들의 HMAC (Hash-based Message Authentication Code)을 계산하여 상호 전송합니다. 이 HMAC 값은 핸드셰이크 과정에서 메시지가 조작되지 않았음을 상호 검증하는 데 사용됩니다. Change Cipher Spec 메시지는 주로 하위 호환성을 위해 전송되며, 이제 암호화된 통신이 시작됨을 알리는 역할을 합니다 [외부 정보: TLS 1.3에서는 이 메시지가 TLS 1.2만큼 중요하지 않고 주로 하위 호환성을 위해 사용됩니다]. * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 이 줄은 TLS 핸드셰이크가 성공적으로 완료되었음을 명확히 보여줍니다. 협상된 프로토콜 버전은 TLSv1.3이며, 사용될 암호화 스위트는 TLSAES256GCMSHA384입니다. 이는 데이터 암호화에 AES-256 (GCM 모드)을 사용하고, 메시지 무결성 검증에 SHA384 기반의 HMAC를 사용하기로 결정되었음을 의미합니다. * ALPN, server accepted to use h2 서버가 클라이언트가 제안한 HTTP/2 (h2) 프로토콜 사용을 수락했음을 확인합니다. 서버 인증서 상세 정보 및 검증 결과: * Server certificate: ... * subject: CN=dns.google * start date: ... * expire date: ... * issuer: C=US; O=Google Trust Services; CN=WE2 * SSL certificate verify ok. 이 부분은 클라이언트가 서버로부터 받은 인증서의 세부 정보를 보여줍니다. 인증서의 주체 (Subject)가 dns.google 이며, 발급자 (Issuer)는 Google Trust Services 라는 CA임을 명확히 보여줍니다. 또한, SSL certificate verify ok. 는 클라이언트가 CA 공개키를 사용하여 서버 인증서의 유효성을 성공적으로 검증했으며, 서버의 신뢰성 (End-point authentication)이 확인되었음을 의미합니다. 키 도출 (Key Derivation Phase) TLS 1.3 핸드셰이크 과정에서 Pre-Master Secret (PMS)와 논스(Nonces)를 사용하여 마스터 시크릿 (Master Secret, MS)이 계산됩니다. 이 MS는 실제 데이터 암호화 및 무결성 검증에 사용될 세션 키들을 도출하는 데 사용됩니다. tls.txt 로그에는 이 과정이 명시적으로 나타나지 않지만, \"SSL connection using TLSv1.3 / TLSAES256GCMSHA384\"라는 결과는 키 도출이 완료되었음을 암시합니다. 세션 키 생성: 클라이언트와 서버는 MS와 논스를 바탕으로 동일한 키 도출 함수 (Key Derivation Function)를 사용하여 다음의 4가지 세션 키를 독립적으로 계산합니다: EB : 클라이언트가 서버로 데이터를 보낼 때 사용할 세션 암호화 키. MB : 클라이언트가 서버로 데이터를 보낼 때 사용할 세션 HMAC 키 (메시지 무결성 검증용). EA : 서버가 클라이언트로 데이터를 보낼 때 사용할 세션 암호화 키. MA : 서버가 클라이언트로 데이터를 보낼 때 사용할 세션 HMAC 키. Initialization Vector (IV) 생성: AES와 같은 블록 암호 방식이 CBC (Cipher Block Chaining) 모드를 사용하는 경우, 각 통신 방향에 대한 Initialization Vector (IV)도 MS로부터 도출됩니다. IV는 동일한 평문 블록이 항상 다른 암호문 블록을 생성하도록 무작위성을 부여하여 암호문의 패턴 노출을 방지합니다. (참고: AES-GCM은 IV/Nonce를 사용하지만, CBC와는 다른 방식으로 사용합니다. output.pdf 는 주로 CBC를 예로 설명합니다.) 이 단계가 완료되면, 클라이언트와 서버는 이후 모든 메시지를 암호화하고 인증하는 데 필요한 모든 대칭 세션 키를 공유하게 됩니다. 데이터 전송 (Data Transfer Phase) 모든 세션 키가 설정된 후, 클라이언트와 서버는 안전한 TLS 연결을 통해 실제 애플리케이션 데이터 (여기서는 HTTP/2 요청 및 응답)를 주고받기 시작합니다. * Using HTTP2, server supports multiplexing * Connection state changed (HTTP/2 confirmed) 클라이언트와 서버가 이제 HTTP/2 프로토콜을 통해 통신할 것임을 확인합니다. * TLSv1.2 (OUT), TLS header, Supplemental data (23): (여러 번 반복) > GET / HTTP/2 > Host: dns.google > user-agent: curl/7.81.0 > accept: */* 이 부분은 클라이언트가 서버로 실제 HTTP/2 요청을 전송하는 것을 보여줍니다. Supplemental data (23) 는 TLS 레코드 형식에서 애플리케이션 데이터를 나타냅니다. TLS는 긴 데이터 스트림 (예: HTTP 요청)을 레코드 (record)라는 작은 단위로 분할합니다. 각 레코드에 대해 발신자 (여기서는 클라이언트)는 해당 레코드 데이터, 클라이언트의 HMAC 세션 키 ( MB ), 그리고 TLS 시퀀스 번호를 사용하여 HMAC을 계산하고 이를 레코드 뒤에 추가합니다. 그런 다음, 이 \"레코드 + HMAC\" 패키지 전체는 클라이언트의 암호화 세션 키 ( EB )를 사용하여 암호화됩니다. 이 암호화된 패키지는 TCP 계층으로 전달되어 인터넷을 통해 전송됩니다. TLS 시퀀스 번호의 사용은 메시지 재정렬 또는 재생 공격 (woman-in-the-middle attack)을 막는 데 핵심적인 역할을 합니다. * TLSv1.2 (IN), TLS header, Supplemental data (23): (여러 번 반복) * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): (두 번) * old SSL session ID is stale, removing 서버는 암호화된 애플리케이션 데이터 ( Supplemental data (23) )와 함께 세션 재개 (Session Resumption)를 위한 New Session Ticket을 전송하고 있습니다 [외부 정보: TLS 1.3의 주요 기능 중 하나로, 다음 연결을 더 빠르게 설정할 수 있도록 암호화된 세션 키 정보를 클라이언트에게 제공합니다]. < HTTP/2 200 < content-security-policy: ... (이하 서버 응답 헤더) 이 부분은 서버로부터 수신된 HTTP/2 응답 헤더를 보여줍니다. HTTP/2 200 은 요청이 성공적으로 처리되었음을 나타냅니다. 클라이언트는 이 암호화된 레코드를 받으면 먼저 서버의 세션 암호화 키 ( EA )로 복호화하고, 수신된 HMAC과 자체 계산한 HMAC을 비교하여 데이터 무결성을 확인합니다. 무결성이 확인된 복호화된 데이터는 애플리케이션 계층으로 전달됩니다. 이 과정에서 index.html 파일의 내용 (또는 여기서는 dns.google 의 루트 페이지 내용)이 안전하게 클라이언트에게 전달됩니다. TLS 과정 요약 표 (TLS 1.3 중심) 단계 클라이언트 (시작) 서버 (응답) 주요 목적 및 tls.txt 로그 해석 핸드셰이크 1. Client hello (TLS 1.3) 전송: 지원 암호화 목록, 클라이언트 논스, ALPN 제안. 2. Server hello (TLS 1.3) 전송: 암호화 알고리즘 선택, 서버 논스, Encrypted Extensions 전송. 안전한 통신 매개변수 협상 및 키 교환 준비. 로그: * ALPN, offering h2 , * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): , * TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8): 서버 인증서 및 CERT verify 수신. CA 공개키로 서버 인증서 검증 (서버 인증). 4. Certificate 및 CERT verify 전송. 서버 신원 확인 및 핸드셰이크 메시지 무결성 보장. 로그: * TLSv1.3 (IN), TLS handshake, Certificate (11): , * TLSv1.3 (IN), TLS handshake, CERT verify (15): * SSL certificate verify ok. Pre-Master Secret (PMS) 생성, 서버 공개키로 암호화 후 전송. 6. PMS 복호화. 논스, PMS 이용 Master Secret (MS) 독립적 계산. 마스터 시크릿 교환: 세션 키 생성을 위한 공유 비밀 값 확립. 로그: 이 단계는 로그에 직접 표시되지 않지만, SSL connection using TLSv1.3 결과에 내포됨. output.pdf 에서 이 부분이 상세 설명됨. 모든 핸드셰이크 메시지의 HMAC 전송. 8. 모든 핸드셰이크 메시지의 HMAC 전송. 핸드셰이크 무결성 검증: 메시지 조작 방지. 로그: * TLSv1.3 (IN), TLS handshake, Finished (20): , * TLSv1.2 (OUT), TLS header, Finished (20): (클라이언트 측에서 TLSv1.2로 표시된 Finished 메시지도 이 역할을 수행) 키 도출 MS와 논스로부터 4개의 세션 키 ( EB , MB , EA , MA ) 및 IV(필요시) 도출. MS와 논스로부터 4개의 세션 키 ( EB , MB , EA , MA ) 및 IV(필요시) 도출. 실제 데이터 암호화 및 무결성 검증에 사용될 대칭 세션 키 생성. 로그: * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 이 줄이 키 도출이 성공했음을 최종적으로 확인. 데이터 전송 HTTP/2 요청 (예: GET / HTTP/2 )을 레코드로 분할. HMAC 추가, 세션 키로 암호화 후 전송. 수신한 레코드 복호화 및 HMAC 검증. 무결성 확인된 데이터를 애플리케이션에 전달. index.html 과 같은 애플리케이션 데이터의 기밀성 및 무결성 보장. 로그: * TLSv1.2 (OUT), TLS header, Supplemental data (23): (클라이언트 요청) * TLSv1.2 (IN), TLS header, Supplemental data (23): (서버 응답) < HTTP/2 200 (애플리케이션 데이터 수신 확인) (선택) 세션 재개를 위한 Newsession Ticket 수신 [외부 정보]. (선택) Newsession Ticket 전송 [외부 정보]. 다음 연결의 효율성 증대. 로그: * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): 추가적인 통찰: 보안 목표 달성: 이 TLS 과정을 통해 output.pdf 에서 강조하는 다음의 보안 목표들이 달성됩니다: 기밀성 (Confidentiality): index.html 내용과 같은 애플리케이션 데이터는 세션 암호화 키 ( EB , EA )로 암호화되어 전송되므로, Trudy (침입자)가 데이터를 스니핑하더라도 내용을 이해할 수 없습니다. 메시지 무결성 (Message integrity): 각 TLS 레코드에 HMAC이 포함되며, TLS 시퀀스 번호까지 HMAC 계산에 포함되므로, Trudy 가 메시지 내용을 변경하거나, 순서를 재정렬하거나, 재생하더라도 수신 측에서 이를 감지할 수 있습니다. 종단점 인증 (End-point authentication): 클라이언트는 서버의 디지털 인증서를 CA의 공개키로 검증하여 서버의 신원을 확인하고, 서버는 클라이언트의 Pre-Master Secret을 복호화하여 클라이언트가 공유 비밀을 알고 있음을 (선택적으로) 확인합니다 (이 예시에서는 서버 인증만 명확함). TLS 1.3의 효율성: output.pdf 는 Pre-Master Secret 을 서버 공개키로 암호화하여 보내고, 이를 통해 Master Secret 을 도출하는 과정을 설명합니다. TLS 1.3은 이 키 교환 과정을 더욱 최적화하여 대부분의 핸드셰이크가 단 한 번의 왕복으로 완료될 수 있도록 합니다 [외부 정보: 1-RTT 핸드셰이크]. 로그의 TLSv1.2 표기: 로그에 간간이 나타나는 TLSv1.2 표기는 실제 협상된 프로토콜 버전이 TLSv1.3 임에도 불구하고 curl 내부 로직이나 TLS 레코드 계층의 특정 상태 또는 하위 호환성 메커니즘을 나타내는 것일 수 있습니다. 핵심은 최종적으로 TLSv1.3 이 사용되었다는 점입니다. TLS 의 오해 바로잡기 [!question] 헨드세이크 과정중에 키 도출을 하는 거야 아니면 handshake 이후에 키 도출을 하는 거야? TLS 핸드셰이크 과정에서 키 도출은 핸드셰이크 단계 내에서 이루어집니다. 소스에 따르면, TLS 프로토콜은 핸드셰이크, 키 도출, 데이터 전송의 세 단계를 가집니다. 하지만 실제로 TLS 핸드셰이크 프로토콜을 더 자세히 살펴보면, 키 도출이 핸드셰이크 단계의 핵심 부분으로 통합되어 있음을 알 수 있습니다. 실제 TLS 핸드셰이크 과정에서 키 도출은 다음과 같이 진행됩니다: 클라이언트와 서버는 핸드셰이크 과정 중에 논스(nonces)를 서로 교환합니다. 논스는 재생 공격(replay attack)을 방지하고 통신 상대방의 \"생존성(liveness)\"을 확인하는 데 사용되는 고유한 숫자입니다. 클라이언트는 서버의 공개 키를 사용하여 사전 마스터 시크릿(Pre-Master Secret, PMS)을 생성하고 암호화하여 서버로 전송합니다. 클라이언트와 서버는 TLS 표준에 지정된 동일한 키 도출 함수를 사용하여 PMS와 논스로부터 마스터 시크릿(Master Secret, MS)을 독립적으로 계산합니다. 이 MS는 이후 세션 암호화 키(encryption keys)와 HMAC 키(HMAC keys)를 생성하는 데 사용됩니다. 만약 선택된 대칭 암호가 CBC(Cipher Block Chaining)를 사용한다면, 초기화 벡터(Initialization Vectors, IVs)도 MS에서 얻어집니다. 이 모든 과정, 즉 MS 계산 및 세션 키 생성은 TLS 핸드셰이크의 4단계에서 발생합니다. 이 단계가 완료되면 클라이언트와 서버 간의 모든 메시지는 생성된 키를 사용하여 암호화되고 인증(HMAC)됩니다. tls.txt 소스에서도 핸드셰이크 메시지 교환( Client hello , Server hello , Finished 등) 이후에 SSL 연결이 TLSv1.3 / TLS_AES_256_GCM_SHA384 와 같은 특정 암호화 방식을 사용하여 설정되었음을 보여주며, 이는 핸드셰이크 과정에서 키가 성공적으로 도출되었음을 시사합니다.",
      "frontmatter": {
        "tags": [
          "simulation",
          "encryption"
        ],
        "date": "2025-06-20T09:02:36+09:00",
        "lastmod": "2025-06-26T23:03:48+09:00"
      }
    },
    "IPC (massage passing)": {
      "path": "/02.inbox/ipc-massage-passing/",
      "filename": "IPC (massage passing)",
      "content": "아래는 Unix에서 사용되는 다양한 프로세스 간 통신(Inter-Process Communication, IPC) 방식들에 대한 자세한 설명입니다. 주요 내용은 다음과 같은 IPC 기법들을 포함합니다: Ordinary Pipe (일반 파이프) Named Pipe (FIFO) UNIX Domain Socket 🔹 1. Ordinary Pipe (일반 파이프) ✅ 개념 pipe() 시스템 콜을 통해 생성됨 두 개의 파일 디스크립터(file descriptor) 반환: 하나는 쓰기 전용(write-end) 하나는 읽기 전용(read-end) 데이터는 쓰기 쪽으로 넣고, 읽기 쪽에서 뺌 → Producer-Consumer 패턴 단방향(unidirectional) 통신만 지원 ✅ 특징 항목 내용 범위 부모와 자식 프로세스 간만 가능 (fork 이후 공유) 존속성 프로세스 종료 시 사라짐 방향성 단방향 (한쪽에서만 보내고 한쪽에서만 받음) 사용 예시 쉘 명령어 연결 ( ls \\| grep \"txt\" ) ✅ 예제 코드 #include <unistd.h> #include <stdio.h> #include <stdlib.h> int main() { int pipefd[2]; pid_t pid; char buf; if (pipe(pipefd) == -1) { perror(\"Pipe failed\"); exit(EXIT_FAILURE); } pid = fork(); if (pid < 0) { // Fork 실패 perror(\"Fork failed\"); exit(EXIT_FAILURE); } if (pid == 0) { // 자식 프로세스 (reader) close(pipefd[1]); // 쓰기 fd 닫기 while (read(pipefd[0], &buf, 1) > 0) write(STDOUT_FILENO, &buf, 1); close(pipefd[0]); } else { // 부모 프로세스 (writer) close(pipefd[0]); // 읽기 fd 닫기 write(pipefd[1], \"Hello from parent\", 17); close(pipefd[1]); wait(NULL); // 자식 기다리기 } return 0; } 이 코드는 부모가 메시지를 보내고 자식이 그것을 출력하는 간단한 파이프 예제입니다. 🔹 2. Named Pipe (FIFO: First In First Out) ✅ 개념 이름을 가진 파일 형태의 파이프 mkfifo() 함수로 생성되며, 실제 파일처럼 /tmp/myfifo 같은 경로에 존재함 여러 프로세스 간 통신 가능 (부모-자식 관계 필요 없음) ✅ 특징 항목 내용 범위 관련 없는 모든 프로세스 간 통신 가능 존속성 프로세스 종료 후에도 유지됨 (수동 삭제 필요) 방향성 기본 단방향, 하지만 양방향도 가능 (두 FIFO 사용하면 됨) 형태 파일 시스템 상에 물리적으로 존재하는 파일 ✅ 예제 코드 🧱 FIFO 생성자 #include <sys/types.h> #include <sys/stat.h> #include <fcntl.h> #include <unistd.h> int main() { mkfifo(\"myfifo\", 0666); // 권한 0666으로 myfifo 생성 return 0; } 📝 쓰기(writer) #include <fcntl.h> #include <unistd.h> #include <stdio.h> int main() { int fd = open(\"myfifo\", O_WRONLY); // Blocking 모드 write(fd, \"Hello from writer\", 17); close(fd); return 0; } 📖 읽기(reader) #include <fcntl.h> #include <unistd.h> #include <stdio.h> int main() { char buffer[80]; int fd = open(\"myfifo\", O_RDONLY); read(fd, buffer, sizeof(buffer)); printf(\"Received: %s\\n\", buffer); close(fd); return 0; } 위 코드를 실행하려면 세 개의 터미널 창에서 각각 순서대로 실행해야 합니다: $ ./creator $ ./reader # reader 먼저 실행하고 대기 $ ./writer # writer가 메시지 전송 🔹 3. UNIX Domain Socket (로컬 소켓) ✅ 개념 로컬 머신 내에서만 사용되는 소켓 파일 시스템 경로( /tmp/mysocket )를 주소로 사용함 TCP/IP 소켓과 비슷하지만, 커널 내부에서 처리되어 더 빠름 ✅ 특징 항목 내용 범위 동일 머신 내의 프로세스 간 존속성 프로세스 종료 후에도 파일 유지(삭제 필요) 방향성 양방향(full-duplex), 스트림 또는 데이터그램 방식 가능 성능 일반 파이프보다 느리지만 네트워크 소켓보다 빠름 ✅ 예제 코드 #include <sys/socket.h> #include <sys/un.h> #include <unistd.h> #include <stdio.h> #include <string.h> int main() { int sv[2]; // 소켓 페어 char buffer[100]; if (socketpair(AF_UNIX, SOCK_STREAM, 0, sv) == -1) { perror(\"Socketpair failed\"); return 1; } pid_t pid = fork(); if (pid < 0) { perror(\"Fork failed\"); return 1; } if (pid == 0) { // 자식 프로세스 (수신) close(sv[0]); // 부모 쪽 fd 닫기 read(sv[1], buffer, sizeof(buffer) - 1); printf(\"Child received: %s\\n\", buffer); close(sv[1]); } else { // 부모 프로세스 (송신) close(sv[1]); // 자식 쪽 fd 닫기 write(sv[0], \"Hello my child\", 15); close(sv[0]); wait(NULL); // 자식 종료 대기 } return 0; } 📊 전체 비교표 항목 Ordinary Pipe Named Pipe (FIFO) Unix Domain Socket 통신 범위 부모-자식 프로세스 모든 프로세스 모든 프로세스 존속성 일시적 영구적 (파일 시스템에 남음) 영구적 (경로로 지정된 파일) 방향성 단방향 단방향/양방향 가능 양방향(full-duplex) 방식 Direct Indirect Indirect 동기화 기본 blocking 기본 blocking 기본 blocking 비동기 지원 가능 가능 가능 성능 매우 빠름 중간 빠름 복잡성 낮음 중간 약간 복잡 사용 목적 간단한 parent-child 통신 여러 프로세스 간 안정적인 통신 고성능 로컬 멀티프로세스 통신 🚀 성능 비교 (내부 구현 기준) Ordinary Pipe > Unix Domain Socket > Named Pipe (FIFO) > Network Socket Ordinary Pipe : 가장 빠르고 간단 (커널 내부 버퍼 사용) Unix Domain Socket : 양방향, full-duplex, 네트워크 소켓처럼 사용 가능 Named Pipe (FIFO) : 파일 시스템 기반이므로 약간 느림 Network Socket : TCP/IP 오버헤드가 있어 가장 느림 💬 요약 Ordinary Pipe: 부모-자식 간 단방향 통신, 가장 간단하고 빠름 Named Pipe (FIFO): 파일처럼 생성하여 여러 프로세스 간 통신 가능 UNIX Domain Socket: 로컬 머신 내에서 네트워크 소켓처럼 사용되는 고성능 통신 수단",
      "frontmatter": {
        "tags": [
          "system-programing",
          "operating-system"
        ],
        "date": "2025-05-01T13:03:00+09:00",
        "lastmod": "2025-05-01T13:03:00+09:00"
      }
    },
    "Oracle database 권한": {
      "path": "/02.inbox/oracle-database-권한/",
      "filename": "Oracle database 권한",
      "content": "Oracle 데이터베이스에서 사용자의 권한을 설정할 때 사용할 수 있는 다양한 권한(privileges)과 역할(roles)의 예시는 아래와 같습니다. 권한은 크게 시스템 권한과 객체 권한으로 나뉩니다. 아래는 그 주요 예들입니다. 시스템 권한 (System Privileges) 시스템 권한은 데이터베이스 전체에서 특정 작업을 수행할 수 있도록 허용합니다. 권한 설명 CREATE SESSION 데이터베이스에 연결할 수 있는 권한. CREATE TABLE 새 테이블을 생성할 수 있는 권한. CREATE VIEW 새 뷰(View)를 생성할 수 있는 권한. CREATE MATERIALIZED VIEW 새 물리화된 뷰(Materialized View)를 생성할 수 있는 권한. CREATE PROCEDURE 프로시저, 함수 또는 패키지를 생성할 수 있는 권한. CREATE SEQUENCE 새 시퀀스를 생성할 수 있는 권한. CREATE TRIGGER 트리거(Trigger)를 생성할 수 있는 권한. CREATE USER 새로운 사용자를 생성할 수 있는 권한. CREATE ROLE 새로운 역할(Role)을 생성할 수 있는 권한. CREATE INDEX 새로운 인덱스를 생성할 수 있는 권한. CREATE SYNONYM 새로운 동의어(Synonym)를 생성할 수 있는 권한. CREATE PUBLIC SYNONYM 공용 동의어(Public Synonym)를 생성할 수 있는 권한. CREATE DATABASE LINK 데이터베이스 링크(Database Link)를 생성할 수 있는 권한. ALTER USER 사용자의 속성(예: 비밀번호, 테이블스페이스 등)을 변경할 수 있는 권한. DROP USER 사용자를 삭제할 수 있는 권한. DROP ANY TABLE 모든 테이블을 삭제할 수 있는 권한. SELECT ANY TABLE 모든 테이블에 대해 SELECT 쿼리를 실행할 수 있는 권한. UPDATE ANY TABLE 모든 테이블의 데이터를 업데이트할 수 있는 권한. DELETE ANY TABLE 모든 테이블의 데이터를 삭제할 수 있는 권한. INSERT ANY TABLE 모든 테이블에 데이터를 삽입할 수 있는 권한. EXECUTE ANY PROCEDURE 모든 프로시저와 함수를 실행할 수 있는 권한. MANAGE TABLESPACE 테이블스페이스를 관리할 수 있는 권한. 객체 권한 (Object Privileges) 객체 권한은 특정 객체(예: 테이블, 뷰, 시퀀스 등)에 대해 작업을 수행할 수 있는 권한입니다. 권한 설명 SELECT 테이블 또는 뷰에서 데이터를 조회할 수 있는 권한. INSERT 테이블에 데이터를 삽입할 수 있는 권한. UPDATE 테이블 데이터를 수정할 수 있는 권한. DELETE 테이블 데이터를 삭제할 수 있는 권한. REFERENCES 다른 테이블에서 외래 키 제약 조건을 생성할 때 참조할 수 있는 권한. INDEX 테이블의 인덱스를 생성할 수 있는 권한. EXECUTE 특정 프로시저, 함수, 또는 패키지를 실행할 수 있는 권한. ALTER 특정 객체(테이블, 뷰 등)를 수정할 수 있는 권한. GRANT 다른 사용자에게 객체 권한을 부여할 수 있는 권한. 역할 (Roles) 역할은 권한의 집합으로, 여러 권한을 하나의 그룹으로 묶어 효율적으로 관리할 수 있습니다. 역할(Role) 설명 CONNECT 기본 연결 권한. RESOURCE 데이터베이스 객체(테이블, 뷰, 시퀀스 등)를 생성할 수 있는 권한 세트. DBA 데이터베이스 관리자가 사용하는 모든 권한 세트. READ ONLY 데이터베이스를 읽기 전용으로 접근할 수 있는 역할. READ WRITE 읽기 및 쓰기 권한을 포함하는 역할. PUBLIC 모든 사용자에게 적용되는 기본 역할. 특정 권한 부여 예시 다양한 권한 부여 예시는 아래와 같습니다: -- 테이블 관련 권한 부여 GRANT SELECT, INSERT, UPDATE ON employees TO user_name; -- 프로시저 실행 권한 부여 GRANT EXECUTE ON my_procedure TO user_name; -- 테이블스페이스 관리 권한 부여 GRANT UNLIMITED TABLESPACE TO user_name; -- 모든 테이블에 대한 SELECT 권한 부여 GRANT SELECT ANY TABLE TO user_name; -- 특정 데이터베이스 링크 생성 권한 부여 GRANT CREATE DATABASE LINK TO user_name; -- 역할(Role) 부여 GRANT CONNECT TO user_name; GRANT RESOURCE TO user_name; GRANT DBA TO user_name; 고급 권한 관리 Oracle에서는 특정 작업에 대해 더 세부적으로 권한을 관리할 수 있습니다: 권한 회수: REVOKE 명령을 사용하여 권한을 회수할 수 있습니다. REVOKE CREATE TABLE FROM user_name; 권한 전달 허용: WITH GRANT OPTION 을 추가하면 사용자가 다른 사용자에게 권한을 다시 부여할 수 있습니다. GRANT SELECT ON employees TO user_name WITH GRANT OPTION; 이 예시들을 기반으로, 사용자의 역할과 요구 사항에 맞는 권한 구성을 설계할 수 있습니다.",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "date": "2024-11-29T13:13:00+09:00",
        "lastmod": "2024-11-29T13:13:00+09:00"
      }
    },
    "RAID": {
      "path": "/02.inbox/raid/",
      "filename": "RAID",
      "content": "하드웨어 RAID 소프트웨어 RAID 여기서는 소프트웨어 RAID 를 이야기 한다 2023-06-26150025 linear RAID : 디스크 여러개를 단순히 이어서 사용한다 RAID 0 : 순서대로 저장하는 것이 아니라 동시에 사용 속도를 나누어 사용하므로 속도가 N 개 만큼 빠르다 RAID 1 : 복사본 생성 RAID 5 : 1개 오류 허용 RAID 6 : 2023-06-26150727 2023-06-26150829 2023-06-26151558 2023-06-26151130 2023-06-26151329 2023-06-26151558 mdadm은 Linux에서 md 장치(일명 RAID 배열)를 구축, 관리 및 모니터링하기 위해 사용됩니다. 2023-06-26160341 파티션 생성 fdisk 명령 + 리눅스 raid raid outo { 16진수 : fd } 로 파일 시스템 mkfs.ext4 레이드 구성 --create [만들 /dev/md{숫자}] --level=[레이드버전] --raid-devices=[장치수] [레이드 만들 장치명1] [레이드 만들 장치명2]… or -C [만들 /dev/md{숫자}] -l [레이드 버전] -n [장치 수] [레이드 만들 장치명1] [레이드 만들 장치명2]…**** 개별 확인 --detail [/dev/md{숫자}] 전체 확인 \"/proc/mdstat\" 파일 mdadm —detail —scan 단일 장치 제거 [장치를 제거할 /dev/md{숫자}] --remove [제거할 장치명] 추가 [장치를 추가할 /dev/md{숫자}] --add [추가할 장치명] 정지 ( \"야 mdadm, 너 /dev/md0 계산 그만해\" ) stop [해제할 /dev/md{숫자}] 슈퍼블록 제거 ( 메타데이터 제거 ) zero-superblock [해제할 /dev/md{숫자}] 추가 zero-superblock 사용이유 RAID 0을 구성했었던, /dev/sdc1 이나 /dev/sdd1을 이용해 다른 RAID를 구성하려고 하면 \"appears to be part of a raid array\" 이렇게 다른 레이드의 구성요소로 보인다고 하며 뭐라 뜨는데 이건 파일시스템에서 배웠었던 메타데이터인 \"슈퍼블록\" 이 남아있어서 그럽니다. 즉 \"mdadm --create\"로 디스크들을 한번이라도 레이드로 묶으면 각 디스크에는 이 디스크는 RAID 0의 장치중 하나이고, 512KB씩 데이터를 분산저장해라 이런식의 \"사용설명서(슈퍼블록)\" 같은 것이 남있는것이죠 그래서 \"--zero-superblock\" 이란 명령어를 이용해 완전하게 슈퍼블록까지 지워준 다음 새로운 RAID를 만들어줘야 합니다",
      "frontmatter": {
        "aliases": [
          "레이드",
          "mdadm"
        ],
        "tags": [
          "linux"
        ],
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2023-12-20T07:12:00+09:00"
      }
    },
    "TCP TELNET 통신 과정 시뮬레이션": {
      "path": "/02.inbox/tcp-telnet-통신-과정-시뮬레이션/",
      "filename": "TCP TELNET 통신 과정 시뮬레이션",
      "content": "Telnet (TCP 기반) 패킷 이동 시뮬레이션: Sender/Receiver 윈도우 및 패킷 내용 Telnet은 애플리케이션 계층 프로토콜이며, 전송 계층에서는 TCP(Transmission Control Protocol)를 사용합니다. TCP는 신뢰성 있는 데이터 전송을 보장하기 위해 연결 설정(3-way handshake), 데이터 전송(슬라이딩 윈도우, 확인 응답), 연결 해제(4-way handshake) 과정을 거칩니다. 아래는 Telnet 클라이언트(Sender)가 Telnet 서버(Receiver)에 접속하여 간단한 데이터를 주고받는 과정을 가상으로 시뮬레이션한 결과입니다. 실제 환경에서는 윈도우 크기 변화, 패킷 손실 및 재전송 등 더 복잡한 상황이 발생할 수 있습니다. 용어 설명: Sender (송신자): Telnet 클라이언트 (사용자 측) Receiver (수신자): Telnet 서버 Seq: Sequence Number (순서 번호). 보내는 데이터의 바이트 순서를 나타냅니다. Ack: Acknowledgment Number (확인 응답 번호). 다음에 받아야 할 데이터의 시작 순서 번호를 나타냅니다. (즉, Ack-1 까지는 잘 받았다는 의미) Win: Window Size (윈도우 크기). 수신 버퍼의 남은 공간 크기를 알려주어 흐름 제어(Flow Control)에 사용됩니다. 송신자는 이 크기만큼만 확인 응답 없이 데이터를 보낼 수 있습니다. Flags: TCP 제어 플래그 (SYN, ACK, FIN, PSH 등) Payload: 실제 전송되는 데이터 (Telnet 명령어, 서버 응답 등) 시뮬레이션 시작 (가정): Sender의 초기 Sequence Number (ISN): X Receiver의 초기 Sequence Number (ISN): Y Sender의 초기 수신 윈도우 크기: Win_C Receiver의 초기 수신 윈도우 크기: Win_S 연결 설정 (3-Way Handshake) 단계 방향 TCP Flags Seq Ack Win Payload 설명 1 Sender -> Receiver [SYN] X 0 Win_C - 연결 요청. 자신의 초기 순서 번호(X)와 수신 윈도우 크기(Win_C) 전송. 2 Receiver -> Sender [SYN, ACK] Y X+1 Win_S - 연결 수락 및 요청 확인. 자신의 초기 순서 번호(Y), Sender의 요청 확인(X+1), 자신의 수신 윈도우 크기(Win_S) 전송. 3 Sender -> Receiver [ACK] X+1 Y+1 Win_C - Receiver의 연결 수락 확인(Y+1). 연결 성립 완료. 데이터 전송 (Telnet: 서버가 로그인 프롬프트 전송 -> 클라이언트가 사용자 이름 입력) 상황: 연결 후 Telnet 서버(Receiver)가 로그인 프롬프트를 보냅니다. (예: \"Login: \") 단계 방향 TCP Flags Seq Ack Win Payload 설명 4 Receiver -> Sender [PSH, ACK] Y+1 X+1 Win_S \"Login: \" (7 bytes) 서버가 데이터를 보냄 (Push 플래그 설정 가능). Seq=Y+1 (이전 ACK 다음 번호), Ack=X+1 (이전 Sender의 Seq+1). Payload 포함. 5 Sender -> Receiver [ACK] X+1 Y+8 Win_C - Sender가 서버 데이터 수신 확인. Ack=Y+1+7 = Y+8 (다음 받을 번호). 윈도우 크기는 Sender의 현재 버퍼 상황에 따라 업데이트될 수 있음. 상황: Telnet 클라이언트(Sender)가 사용자 이름 \"user1\\n\" (6 bytes)을 입력하여 서버로 전송합니다. 단계 방향 TCP Flags Seq Ack Win Payload 설명 6 Sender -> Receiver [PSH, ACK] X+1 Y+8 Win_C \"user1\\n\" (6 bytes) 클라이언트가 데이터를 보냄. Seq=X+1 (이전 ACK 이후 첫 데이터), Ack=Y+8 (이전 Receiver의 Seq+Payload 길이). Payload 포함. 7 Receiver -> Sender [ACK] Y+8 X+7 Win_S - Receiver가 클라이언트 데이터 수신 확인. Ack=X+1+6 = X+7 (다음 받을 번호). 윈도우 크기는 Receiver의 현재 버퍼 상황에 따라 업데이트될 수 있음. 연결 해제 (4-Way Handshake - 클라이언트가 먼저 종료 요청) 상황: Telnet 클라이언트(Sender)가 연결 종료를 원합니다. 단계 방향 TCP Flags Seq Ack Win Payload 설명 8 Sender -> Receiver [FIN, ACK] X+7 Y+8 Win_C - 연결 종료 요청 (FIN). 마지막으로 보낸 데이터의 다음 Seq 번호(X+7). 마지막으로 받은 데이터의 다음 번호(Y+8)를 Ack. 9 Receiver -> Sender [ACK] Y+8 X+8 Win_S - Sender의 종료 요청(FIN) 확인. Ack=X+7+1 = X+8. (서버는 아직 데이터를 보낼 수 있는 상태 - Half-Close) 상황: Telnet 서버(Receiver)도 데이터를 모두 보냈고, 연결 종료를 원합니다. 단계 방향 TCP Flags Seq Ack Win Payload 설명 10 Receiver -> Sender [FIN, ACK] Y+8 X+8 Win_S - 서버 측 연결 종료 요청 (FIN). 마지막으로 보낸 데이터의 다음 Seq 번호(Y+8). 마지막으로 받은 데이터의 다음 번호(X+8)를 Ack. 11 Sender -> Receiver [ACK] X+8 Y+9 Win_C - 서버의 종료 요청(FIN) 확인. Ack=Y+8+1 = Y+9. 연결 종료 완료. Sender는 TIME_WAIT 상태로 잠시 대기. 핵심 요약: Seq/Ack 번호: 데이터의 순서를 맞추고, 빠진 데이터 없이 잘 받았는지 확인하는 데 사용됩니다. Ack 번호는 '다음에 받을 것으로 예상되는 Seq 번호'를 의미합니다. 윈도우 크기 (Win): 수신 측의 버퍼 여유 공간을 송신 측에 알려주어, 송신 측이 과도하게 많은 데이터를 보내지 않도록 조절합니다 (흐름 제어). 수신 측의 처리 속도나 버퍼 상태에 따라 동적으로 변할 수 있습니다. TCP Flags: 연결 설정(SYN), 데이터 전송(PSH, URG), 확인 응답(ACK), 연결 종료(FIN, RST) 등 TCP 연결의 상태를 제어하는 데 사용됩니다. 이 시뮬레이션은 Telnet 통신 중 TCP 레벨에서 패킷이 어떻게 교환되는지 기본적인 흐름을 보여줍니다. 실제 네트워크 환경에서는 지연, 혼잡 제어(Congestion Control) 알고리즘, 재전송 등으로 인해 윈도우 크기 변화나 패킷 교환 순서가 더 복잡해질 수 있습니다.",
      "frontmatter": {
        "tags": [
          "network"
        ],
        "date": "2025-05-21T09:08:00+09:00",
        "lastmod": "2025-05-21T09:08:00+09:00"
      }
    },
    "UDP와 TCP Demultiplexing 의 이해": {
      "path": "/02.inbox/udp와-tcp-demultiplexing-의-이해/",
      "filename": "UDP와 TCP Demultiplexing 의 이해",
      "content": "디멀티플렉싱은 수신된 네트워크 패킷을 분석하여 해당 패킷이 어떤 소켓으로 전달되어야 하는지를 결정하는 과정 멀티플렉싱은 여러 응용 프로그램에서 생성된 데이터를 하나의 네트워크 인터페이스로 전송하는 과정 UDP와 TCP 디멀티플렉싱의 비교 분석 UDP는 디멀티플렉싱을 위해 목적지 IP 주소와 목적지 포트 번호라는 2-튜플을 사용하는 반면 8, TCP는 송신지 IP 주소, 송신지 포트 번호, 목적지 IP 주소, 목적지 포트 번호로 구성된 4-튜플을 사용합니다 이러한 차이는 각 프로토콜의 연결 지향성 여부에서 비롯됩니다. TCP는 연결 지향적인 특성상 각 연결의 상태를 추적해야 하며, 이는 고유한 4-튜플을 통해 이루어집니다 TCP 연결은 두 특정 호스트의 두 특정 프로세스 간의 전용 통신 채널을 의미하므로, 더 자세한 식별 정보가 필요합니다. TCP의 신뢰성 있는 데이터 전송과 순서 보장 기능은 이러한 연결 상태 유지에 의존합니다. 반면, UDP는 각 데이터그램을 독립적인 단위로 취급하는 비연결형 프로토콜이므로 데이터그램을 올바른 포트에서 수신 대기 중인 애플리케이션에 전달하는 데 목적지 정보만으로 충분합니다. UDP의 이러한 단순성은 디멀티플렉싱 과정을 더 빠르고 효율적으로 만들어줍니다 이는 속도가 중요하고 일부 데이터 손실이 허용되는 애플리케이션(예: 스트리밍, 온라인 게임)에 UDP가 적합한 이유입니다. 반대로, TCP의 복잡한 디멀티플렉싱 방식은 여러 동시 연결을 통해 데이터를 안정적이고 순서대로 전달하는 데 필수적이며 3, 웹 브라우징, 파일 전송, 이메일과 같이 데이터 무결성이 중요한 애플리케이션에 적합합니다. UDP와 TCP의 Demultiplexing 이해하기 UDP와 TCP의 demultiplexing 차이는 각 프로토콜의 연결 방식에서 비롯됩니다. UDP는 비연결형으로, 각 데이터그램을 독립적으로 처리하며 목적지 포트만으로 충분히 데이터를 전달할 수 있습니다. 반면, TCP는 연결 지향형으로, 두 호스트 간의 특정 프로세스 쌍을 식별하기 위해 더 많은 정보(4-튜플)가 필요합니다. 이를 실제로 느끼기 위해 Linux 명령어를 사용하거나 C 언어로 간단한 서버를 구현할 수 있습니다. 아래에서 단계별로 설명하겠습니다. Linux 명령어 사용하기 Linux에서 nc (netcat)와 ss 명령어를 사용해 차이를 관찰할 수 있습니다: UDP 관찰 UDP 서버 시작: nc -u -l 12345 다른 터미널에서 데이터 전송: echo \"Hello\" | nc -u 127.0.0.1 12345 UDP 소켓 확인: ss -u -a 포트 12345에서 듣고 있는 단일 UDP 소켓을 볼 수 있으며, 여러 출처에서 데이터를 받을 수 있습니다. TCP 관찰 TCP 서버 시작: nc -l 12345 다른 터미널에서 연결: nc 127.0.0.1 12345 후 데이터 입력(예: \"Hello\" 입력). TCP 소켓 확인: ss -t -a 듣기 중인 소켓과 특정 클라이언트 IP, 포트와 연결된 확립된 연결을 볼 수 있습니다. 이 과정에서 UDP는 단일 소켓으로 여러 클라이언트의 데이터를 처리하지만, TCP는 각 연결마다 별도의 소켓을 생성하는 것을 알 수 있습니다. ESTAB 0 0 127.0.0.1:37479 127.0.0.1:12345 # UDP LISTEN 0 1 0.0.0.0:12345 0.0.0.0:* # TCP ESTAB 0 0 127.0.0.1:12345 127.0.0.1:50972 # TCP C 언어 구현 C 언어로 간단한 서버를 작성해 차이를 체험할 수 있습니다. UDP 서버 예제 #include <stdio.h> #include <stdlib.h> #include <string.h> #include <sys/socket.h> #include <netinet/in.h> #include <arpa/inet.h> #define PORT 12345 #define BUFFER_SIZE 1024 int main() { int sockfd; struct sockaddr_in server_addr, client_addr; char buffer[BUFFER_SIZE]; socklen_t addr_len = sizeof(client_addr); sockfd = socket(AF_INET, SOCK_DGRAM, 0); if (sockfd < 0) { perror(\"socket creation failed\"); exit(EXIT_FAILURE); } memset(&server_addr, 0, sizeof(server_addr)); server_addr.sin_family = AF_INET; server_addr.sin_addr.s_addr = INADDR_ANY; server_addr.sin_port = htons(PORT); if (bind(sockfd, (struct sockaddr*)&server_addr, sizeof(server_addr)) < 0) { perror(\"bind failed\"); exit(EXIT_FAILURE); } printf(\"UDP server listening on port %d\\n\", PORT); while (1) { int n = recvfrom(sockfd, buffer, BUFFER_SIZE, 0, (struct sockaddr*)&client_addr, &addr_len); if (n < 0) { perror(\"recvfrom failed\"); continue; } buffer[n] = '\\0'; printf(\"Received from %s:%d: %s\\n\", inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port), buffer); } return 0; } 이 코드는 단일 소켓으로 모든 클라이언트의 데이터그램을 처리합니다. TCP 서버 예제 #include <stdio.h> #include <stdlib.h> #include <string.h> #include <sys/socket.h> #include <netinet/in.h> #include <arpa/inet.h> #define PORT 12345 #define BUFFER_SIZE 1024 int main() { int listenfd, connfd; struct sockaddr_in server_addr, client_addr; char buffer[BUFFER_SIZE]; socklen_t addr_len = sizeof(client_addr); listenfd = socket(AF_INET, SOCK_STREAM, 0); if (listenfd < 0) { perror(\"socket creation failed\"); exit(EXIT_FAILURE); } memset(&server_addr, 0, sizeof(server_addr)); server_addr.sin_family = AF_INET; server_addr.sin_addr.s_addr = INADDR_ANY; server_addr.sin_port = htons(PORT); if (bind(listenfd, (struct sockaddr*)&server_addr, sizeof(server_addr)) < 0) { perror(\"bind failed\"); exit(EXIT_FAILURE); } if (listen(listenfd, 5) < 0) { perror(\"listen failed\"); exit(EXIT_FAILURE); } printf(\"TCP server listening on port %d\\n\", PORT); while (1) { connfd = accept(listenfd, (struct sockaddr*)&client_addr, &addr_len); if (connfd < 0) { perror(\"accept failed\"); continue; } printf(\"Accepted connection from %s:%d\\n\", inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port)); int n = read(connfd, buffer, BUFFER_SIZE); if (n > 0) { buffer[n] = '\\0'; printf(\"Received: %s\\n\", buffer); } close(connfd); } return 0; } TCP 서버는 각 클라이언트 연결마다 새로운 소켓( connfd )을 생성하며, 이는 4-튜플로 식별됩니다. UDP와 TCP의 비교 항목 UDP TCP Demultiplexing Key 목적지 IP, 목적지 포트 (2-튜플) 송신지 IP, 송신지 포트, 목적지 IP, 목적지 포트 (4-튜플) 연결 모델 비연결형, 상태 없음 연결 지향형, 상태 유지 소켓 동작 모든 클라이언트가 하나의 소켓 사용 각 연결마다 별도 소켓 생성 적합한 애플리케이션 스트리밍, 온라인 게임 (속도 중요, 일부 데이터 손실 허용) 웹 브라우징, 파일 전송 (신뢰성, 순서 보장 중요) ss 명령어 출력 \"UNCONN\" 상태, 하나의 소켓만 표시 수신 대기 및 여러 개의 확립된 연결 표시 이 표에서 알 수 있듯이, UDP는 간단한 방식으로 데이터를 전달하지만 신뢰성이 낮고, TCP는 보다 복잡한 방식으로 데이터를 관리하여 신뢰성을 보장합니다. 추가 고려 사항 멀티 클라이언트 실험 UDP의 경우, 여러 개의 클라이언트에서 nc -u 를 사용해 데이터를 전송하면 하나의 소켓에서 모든 데이터를 받을 수 있습니다. TCP의 경우, 여러 개의 클라이언트가 연결하면 ss -t -a 명령어에서 각 클라이언트와 서버 간에 생성된 개별 연결을 확인할 수 있습니다. 패킷 분석 도구 사용 Wireshark와 같은 네트워크 패킷 분석기를 사용하면 UDP와 TCP의 패킷 헤더를 직접 확인할 수 있습니다. strace 명령어를 사용하여 시스템 호출을 분석하면, 각각의 프로토콜이 어떻게 소켓을 다루는지 더 깊이 이해할 수 있습니다. 이제 UDP와 TCP의 demultiplexing 방식이 어떻게 다른지, 그리고 그것이 실제 네트워크 애플리케이션에 어떤 영향을 미치는지 직접 체험할 수 있습니다.",
      "frontmatter": {
        "tags": [
          "network",
          "university"
        ],
        "date": "2025-03-28T09:53:00+09:00",
        "lastmod": "2025-03-28T09:53:00+09:00"
      }
    },
    "UTF-8": {
      "path": "/02.inbox/utf-8/",
      "filename": "UTF-8",
      "content": "GEEK News 꼭 읽어보기 바람 UTF-8-20231225115136 UTF-8-20231225115121 예시 1: 문자 A는 아스키 문자이며 유니코드 값은 65로, 이는 16진수 0x41(0100 0001)인데, 7비트 이내로 표현 가능하므로 UTF-8로도 0x41로 표현된다. 예시 2: 문자 π는 유니코드 값이 7비트를 벗어난다. 그러나 11비트 이내에 표현이 가능한 비교적 앞쪽에 위치한 문자며, 따라서 그림과 같이 2바이트에 표현이 가능 하다. 예시 3: 문자 한은 한글 문자로 16비트를 모두 사용한다. 마지막 16비트가 1이며 따라서 이를 표현하기 위해서는 그림과 같이 3바이트를 사용해야 한다. 참고로 유니코드에는 완성형 한글 11,172자뿐만 아니라 조합형 자모가 모두 포함되어 있으며, 이처럼 한글의 UTF-8 인코딩 값은 모두 각 문자당 3바이트를 차지한다. UTF-8-20231225115204 한글 조합형 자모는 [](https://namu.wiki/w/유니코드/1000~1FFF#s-3)(https://namu.wiki/w/UTF-8\\#s-3]] 영역에 위치한다. 한글 완성형 자모는 U+3130~318F 영역에 위치한다. 한글 완성형 글자는 U+AC00~D7A3(https://namu.wiki/w/UTF-8\\#fn-8) 영역에 위치한다. 예를 들어 '갑'의 유니코드값은 16진수 0xAC11(1010 1100 0001 0001)인데, 이는 총 16비트가 필요하므로 1110 1010 1011 0000 1001 0001 으로 표현한다. 완성형 글자 하나도 3바이트인데, 조합형은 자모 하나하나가 각각 3바이트씩(글자 하나당 6~9바이트) 사용한다. 따라서 일반적으로 완성형 글자를 사용하고, 조합형은 완성형에 없는 옛 한글 등을 쓰기 위해서 사용하는 것이 효율이 좋다. 유니코드 변환기 https://ko.rakko.tools/tools/89/",
      "frontmatter": {
        "tags": [
          "잡지식"
        ],
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2023-12-20T07:12:00+09:00"
      }
    },
    "Zone.Identifier 삭제": {
      "path": "/02.inbox/zone.identifier-삭제/",
      "filename": "Zone.Identifier 삭제",
      "content": ". 현재 폴더 아래의 \":Zone.Identifier\" 이 뒤에 붙은 모든 파일을 삭제 find . -name \"*:Zone.Identifier\" -type f -delete 어떤 기능을 하는 명령어인지 알아봅시다.",
      "frontmatter": {
        "tags": [
          "잡지식"
        ],
        "date": "2025-07-24T00:00:18+09:00",
        "lastmod": "2025-07-24T00:01:00+09:00"
      }
    },
    "abi": {
      "path": "/02.inbox/abi/",
      "filename": "abi",
      "content": "gcc-arm-[플렛폼]-[ABI 타입] gcc-arm-linux-gnueabi/jammy 4:11.2.0-1ubuntu1 amd64 GNU C compiler for the armel architecture gcc-arm-linux-gnueabihf/jammy 4:11.2.0-1ubuntu1 amd64 GNU C compiler for the armhf architecture gcc-arm-none-eabi/jammy 15:10.3-2021.07-4 amd64 GCC cross compiler for ARM Cortex-R/M processors gcc-arm-none-eabi-source/jammy 15:10.3-2021.07-4 all GCC cross compiler for ARM Cortex-R/M processors (source) arm-none-eabi-gcc ABI는 \"Application Binary Interface\"의 약자로, 소프트웨어와 하드웨어 간의 상호작용을 정의하는 규약입니다. ABI는 다음과 같은 요소를 포함합니다: 데이터 타입: 데이터의 크기, 정렬 방식, 표현 방식 등을 정의합니다. 함수 호출 규약: 함수에 인자를 전달하는 방법, 반환 값 처리, 스택 관리 등을 규정합니다. 레지스터 사용: CPU 레지스터의 사용 방식과 어떤 레지스터가 어떤 용도로 사용되는지를 정의합니다. 바이너리 형식: 실행 파일과 라이브러리의 구조와 형식에 대해 설명합니다.",
      "frontmatter": {
        "tags": [
          "operating-system",
          "c"
        ],
        "date": "2025-01-22T18:31:00+09:00",
        "lastmod": "2025-01-22T18:31:00+09:00"
      }
    },
    "android sdk tool list": {
      "path": "/02.inbox/android-sdk-tool-list/",
      "filename": "android sdk tool list",
      "content": "{sdk location}/platform-tools ⭐ 핵심 도구 (가장 중요하고 자주 사용) adb (Android Debug Bridge) 가장 중요하고 다재다능한 도구입니다. 실행 중인 안드로이드 기기(에뮬레이터 포함)와 통신하기 위한 클라이언트-서버 프로그램입니다. PC에서 실행하는 adb 는 클라이언트, 기기에서 실행되는 adbd 는 데몬(서버) 역할을 합니다. USB 케이블이나 Wi-Fi를 통해 연결됩니다. 주요 기능 및 사용 사례: 기기 연결 확인: adb devices : 현재 PC에 연결된 기기 목록을 확인합니다. 기기가 정상적으로 인식되었는지 확인할 때 가장 먼저 사용하는 명령어입니다. 앱 설치 및 삭제: adb install [앱이름].apk : PC에 있는 APK 파일을 기기에 설치합니다. adb uninstall [패키지이름] : 기기에 설치된 앱을 삭제합니다. 파일 전송: adb push [PC 경로] [기기 경로] : PC의 파일을 기기로 복사합니다. adb pull [기기 경로] [PC 경로] : 기기의 파일을 PC로 복사합니다. 쉘(Shell) 접근: adb shell : 기기의 리눅스 쉘에 원격으로 접속합니다. 이를 통해 기기 내부의 파일 시스템을 탐색하거나, 각종 시스템 명령어를 실행할 수 있습니다. (예: adb shell ls /sdcard/ ) 로그 확인 (디버깅): adb logcat : 기기에서 실시간으로 발생하는 시스템 로그를 출력합니다. 앱 개발 시 에러나 동작을 추적하는 데 필수적입니다. 기기 제어: adb reboot : 기기를 재부팅합니다. adb reboot recovery : 리커버리 모드로 재부팅합니다. adb reboot bootloader : 부트로더(패스트붓) 모드로 재부팅합니다. 기타 고급 기능: 포트 포워딩: adb forward tcp:[PC 포트] tcp:[기기 포트] - PC의 특정 포트로 들어오는 요청을 기기의 포트로 전달합니다. 디버깅 시 유용합니다. 스크린샷/녹화: adb shell screencap (스크린샷), adb shell screenrecord (화면 녹화). fastboot (Fastboot Protocol Tool) 부트로더(Bootloader) 상태의 기기와 통신하는 도구입니다. 운영체제(OS)가 부팅되기 전의 단계에서 기기의 파티션을 수정하거나 펌웨어(ROM)를 설치(플래싱)할 때 사용됩니다. adb 가 OS가 켜진 상태에서 통신하는 것과 달리, fastboot 는 더 낮은 수준(low-level)에서 기기를 제어합니다. 주요 기능 및 사용 사례: 부트로더 언락/락: fastboot flashing unlock 또는 fastboot oem unlock : 기기의 부트로더를 언락하여 커스텀 펌웨어를 설치할 수 있는 상태로 만듭니다. (※ 기기 데이터가 초기화됩니다!) fastboot flashing lock : 부트로더를 다시 잠급니다. 펌웨어 이미지 플래싱: fastboot flash [파티션 이름] [이미지 파일] : 특정 파티션에 이미지 파일을 덮어씌웁니다. 예를 들어, 시스템 파티션에 새로운 OS 이미지를 설치할 때 사용합니다. 예시: fastboot flash system system.img , fastboot flash boot boot.img 기기 정보 확인: fastboot getvar all : 기기의 시리얼 번호, 부트로더 버전 등 다양한 하드웨어 정보를 확인합니다. 재부팅: fastboot reboot : 기기를 일반 모드로 재부팅합니다. fastboot reboot bootloader : 현재의 부트로더 모드로 다시 재부팅합니다. 🛠️ 보조 및 전문 도구 sqlite3 안드로이드 앱이 데이터를 저장하는 데 널리 사용하는 SQLite 데이터베이스를 관리하기 위한 커맨드 라인 도구입니다. adb shell 을 통해 기기에 접속한 후, 이 도구를 사용하여 앱의 데이터베이스 파일(.db)을 직접 열고 SQL 쿼리를 실행할 수 있습니다. 사용 사례: 앱 개발 중 데이터가 올바르게 저장되고 있는지 확인하거나, 데이터베이스 스키마를 검사하고, 데이터를 직접 수정할 때 유용합니다. 예시: adb shell 로 접속 후, sqlite3 /data/data/[앱 패키지]/databases/mydatabase.db 명령어로 DB에 접근하여 .tables 로 테이블 목록을 보거나 SELECT * FROM my_table; 같은 쿼리를 실행할 수 있습니다. hprof-conv (HPROF Converter) 안드로이드 앱의 메모리 힙 덤프(Heap Dump) 파일을 변환하는 도구입니다. 안드로이드 스튜디오의 프로파일러에서 생성된 .hprof 파일은 안드로이드 고유의 형식을 사용합니다. 이 파일을 jhat 과 같은 표준 Java 힙 분석 도구에서 사용 가능한 형식으로 변환할 때 사용됩니다. 사용 사례: 메모리 누수(Memory Leak) 분석과 같은 고급 메모리 프로파일링 작업을 할 때 필요할 수 있습니다. etc1tool ETC1 형식의 텍스처 압축 및 해제를 위한 도구입니다. ETC1은 안드로이드가 지원하는 텍스처 압축 형식으로, GPU 메모리를 효율적으로 사용하는 데 도움을 줍니다. 사용 사례: 게임 개발이나 그래픽 집약적인 앱을 만들 때, PNG와 같은 일반 이미지 파일을 ETC1 형식(.pkm)으로 압축하거나, 압축된 파일을 다시 디코딩하여 확인하는 용도로 사용됩니다. 🔩 시스템 및 파일 시스템 관련 도구 (일반 개발자는 거의 사용하지 않음) 이 도구들은 안드로이드 OS 자체를 빌드하거나, 시스템 이미지를 생성하는 등 매우 낮은 수준의 작업을 할 때 사용됩니다. 일반적인 앱 개발자는 직접 사용할 일이 거의 없습니다. make_f2fs / make_f2fs_casefold F2FS(Flash-Friendly File System) 파일 시스템을 생성하는 도구입니다. F2FS는 플래시 메모리(SSD, eMMC 등)에 최적화된 파일 시스템으로, 최신 안드로이드 기기에서 널리 사용됩니다. make_f2fs_casefold : 파일 이름의 대소문자를 구분하지 않는(case-insensitive) F2FS 파일 시스템을 생성합니다. mke2fs / mke2fs.conf ext2, ext3, ext4 파일 시스템을 생성하는 도구입니다. 과거 안드로이드 기기들은 ext4를 주로 사용했습니다. mke2fs.conf : mke2fs 가 파일 시스템을 생성할 때 참조하는 설정 파일입니다. 📂 기타 파일 및 디렉토리 lib64 64비트 시스템에서 위 도구들이 실행되는 데 필요한 공유 라이브러리(Shared Libraries) 파일들이 들어있는 디렉토리입니다. (예: .dylib on macOS, .so on Linux, .dll on Windows) NOTICE.txt platform-tools 에 포함된 오픈소스 소프트웨어들의 라이선스 및 저작권 관련 고지 사항이 담긴 텍스트 파일입니다. package.xml / source.properties 안드로이드 SDK 매니저가 이 패키지를 관리하기 위해 사용하는 메타데이터 파일입니다. 패키지의 버전, 설명, 의존성 등의 정보가 기록되어 있습니다. SDK 매니저가 업데이트를 확인하거나 패키지를 설치/삭제할 때 이 파일들을 참조합니다. 요약 이름 종류 설명 adb 핵심 도구 실행 중인 안드로이드 기기와 통신 (앱 설치, 파일 전송, 디버깅 등) fastboot 핵심 도구 부트로더 상태의 기기와 통신 (펌웨어 플래싱, 부트로더 언락 등) sqlite3 전문 도구 안드로이드 기기 내의 SQLite 데이터베이스를 관리 hprof-conv 전문 도구 안드로이드 메모리 힙 덤프 파일을 표준 형식으로 변환 etc1tool 전문 도구 ETC1 텍스처 압축/해제 make_f2fs 등 시스템 도구 F2FS, ext4 등 안드로이드 파일 시스템 이미지를 생성 lib64 , NOTICE.txt 등 지원 파일 라이브러리, 라이선스, SDK 매니저용 메타데이터 결론적으로, 일반적인 안드로이드 개발이나 기기 관리를 위해서는 adb 와 fastboot 두 가지의 사용법만 확실히 알아두어도 대부분의 작업을 수행할 수 있습니다. 나머지 도구들은 특정 목적을 위한 보조적인 역할을 합니다. {sdk location}/build-tools/{sdk version} 네, 제공해주신 build-tools 디렉토리 구조를 매우 상세하게 설명해 드리겠습니다. 이 디렉토리는 Android 앱을 개발하고 빌드하는 데 있어 가장 핵심적인 역할을 하는 도구들을 모아놓은 곳입니다. 개요: build-tools 디렉토리의 역할 build-tools 는 Android SDK(소프트웨어 개발 키트)의 일부로, 개발자가 작성한 소스 코드(Java/Kotlin), 리소스 파일(XML 레이아웃, 이미지 등), 그리고 라이브러리들을 실제 Android 기기에서 실행될 수 있는 패키지 파일( .apk 또는 .aab )로 변환하는 데 필요한 모든 명령줄 도구들을 포함하고 있습니다. Gradle과 같은 현대적인 빌드 시스템이 이 도구들을 자동으로 호출하여 빌드 프로세스를 진행하지만, 각 도구의 역할을 이해하는 것은 고급 빌드 최적화나 문제 해결에 매우 중요합니다. 버전별 디렉토리 구조 ( 35.0.0 , 36.0.0 , 36.1.0-rc1 ) build-tools 는 하위 호환성을 유지하고 프로젝트별로 특정 버전의 도구를 사용할 수 있도록 버전별로 폴더가 나뉘어 있습니다. 35.0.0 , 36.0.0 : 안정화된 릴리스 버전입니다. 36.1.0-rc1 : rc1 은 \"Release Candidate 1\"을 의미하며, 정식 출시 전의 테스트 버전입니다. 프로젝트의 build.gradle 파일에 있는 buildToolsVersion \"35.0.0\" 과 같은 설정은 빌드 시 사용할 이 디렉토리 버전을 지정하는 역할을 합니다. 각 버전 폴더 안의 내용은 대부분 비슷하며, 버전이 올라갈수록 버그 수정, 성능 향상, 새로운 Android 플랫폼 기능 지원 등이 추가됩니다. 주요 도구 상세 설명 (버전 폴더 최상위) 각 버전 폴더 안에 있는 핵심 도구들의 역할은 다음과 같습니다. aapt / aapt2 (Android Asset Packaging Tool) 역할: Android 앱 빌드 과정에서 가장 중요한 도구 중 하나입니다. 리소스 파일( res 디렉토리의 XML, 이미지 등)을 분석하고 바이너리 형식으로 컴파일합니다. AndroidManifest.xml 파일을 처리합니다. 리소스 ID를 담고 있는 R.java (또는 R.kt ) 파일을 생성하여 코드에서 리소스를 참조할 수 있게 합니다. aapt2 는 aapt 의 개선된 버전으로, 증분 리소스 처리(incremental resource processing)를 지원하여 빌드 속도를 크게 향상시켰습니다. 현재는 aapt2 가 기본으로 사용됩니다. apksigner (APK Signer) 역할: 생성된 APK 파일에 디지털 서명을 합니다. Android 시스템은 앱을 설치하거나 업데이트할 때 이 서명을 확인하여 앱의 무결성(변조되지 않았음)과 개발자 신원을 보장합니다. 서명되지 않은 앱은 기기에 설치할 수 없습니다. zipalign 역할: APK 패키지 파일을 최적화하는 도구입니다. APK 내부의 압축되지 않은 데이터(예: 이미지 리소스)를 4바이트 경계에 맞게 정렬합니다. 이 작업을 통해 앱 실행 시 메모리 사용량이 줄어들고 리소스 접근 속도가 빨라집니다. 중요: zipalign 은 반드시 apksigner 로 서명하기 전에 실행해야 합니다. (Google Play에 업로드할 때는 서명 후에 실행해도 Google이 재정렬 및 재서명을 해줍니다.) d8 (DEXer) 역할: Java 또는 Kotlin 컴파일러가 생성한 .class 파일(자바 바이트코드)을 Android 런타임(ART)이 실행할 수 있는 .dex 파일(Dalvik Executable) 형식으로 변환합니다. 이 과정에서 코드 최적화, 축소(shrinking), 그리고 Java 8+의 새로운 언어 기능을 이전 Android 버전에서도 사용할 수 있도록 디슈가링(desugaring)하는 작업도 수행합니다. d8 은 이전의 dx 도구를 대체한 더 빠르고 효율적인 도구입니다. aidl (Android Interface Definition Language) 역할: 앱의 여러 프로세스 간 통신(IPC)을 위한 인터페이스를 생성하는 데 사용됩니다. 개발자가 .aidl 파일에 인터페이스를 정의하면, 이 도구가 해당 인터페이스를 구현하는 Java 코드를 자동으로 생성해 줍니다. 서비스(Service)와 다른 앱 컴포넌트 간의 통신에 주로 사용됩니다. dexdump 역할: .dex 파일의 내용을 사람이 읽을 수 있는 형태로 덤프(출력)하는 디버깅 도구입니다. 앱의 바이트코드를 직접 분석하거나 최적화 문제를 확인할 때 유용합니다. *-ld 파일들 (Linkers) aarch64-linux-android-ld , arm-linux-androideabi-ld , i686-linux-android-ld , x86_64-linux-android-ld 등 역할: NDK(Native Development Kit)를 사용하여 C/C++ 코드를 빌드할 때 사용되는 링커입니다. 컴파일된 네이티브 목적 파일( .o )들을 모아서 공유 라이브러리( .so )나 실행 파일을 만듭니다. 각 파일은 특정 CPU 아키텍처(arm64, armv7, x86 등)에 해당합니다. lld / lld-bin 역할: LLVM 프로젝트에서 개발한 새로운 고성능 링커입니다. 기존의 ld 링커보다 빠르며, NDK 빌드에서 사용됩니다. renderscript 디렉토리 상세 설명 RenderScript는 고성능 컴퓨팅, 특히 이미지 처리나 계산 집약적인 작업을 위해 만들어진 프레임워크입니다. 중요: RenderScript는 API 레벨 31부터 공식적으로 Deprecated(사용 중단)되었습니다. 하지만 하위 호환성을 위해 도구들은 여전히 포함되어 있습니다. llvm-rs-cc : RenderScript 소스 코드( .rs 파일)를 LLVM 비트코드로 컴파일하는 컴파일러입니다. clang-include : RenderScript 컴파일 시 필요한 Clang/LLVM 관련 C/C++ 헤더 파일들입니다. *intrin.h 파일들은 CPU 고유의 명령어(intrinsics)를 사용해 코드를 최적화하기 위한 헤더입니다. include : rs_*.rsh 파일들이 있으며, RenderScript 코드 내에서 사용할 수 있는 표준 API(수학, 시간, 변환 등)를 정의한 헤더 파일입니다. lib : RenderScript 지원 라이브러리들이 모여 있습니다. bc : 각 CPU 아키텍처별로 사전 컴파일된 RenderScript 핵심 라이브러리 비트코드( libclcore.bc )가 있습니다. packaged : RenderScript를 사용하는 앱에 포함되는 네이티브 라이브러리( .so )입니다. libRSSupport.so 는 RenderScript API를 이전 Android 버전에서도 사용할 수 있도록 하는 호환성 라이브러리입니다. androidx-rs.jar , renderscript-v8.jar : RenderScript를 Java/Kotlin 코드에서 호출하기 위한 자바 라이브러리입니다. lib 및 lib64 디렉토리 이 디렉토리들은 build-tools 에 포함된 명령줄 도구들이 내부적으로 사용하는 라이브러리를 담고 있습니다. lib : Java 라이브러리( .jar )가 위치합니다. d8.jar : d8 도구의 핵심 로직이 담긴 자바 라이브러리입니다. apksigner.jar : apksigner 도구의 핵심 로직입니다. lib64 : 네이티브 공유 라이브러리( .dylib for macOS, .so for Linux, .dll for Windows)가 위치합니다. libc++.dylib : C++ 표준 라이브러리. libLLVM_android.dylib : RenderScript 컴파일러와 같은 도구들이 사용하는 LLVM/Clang 라이브러리입니다. 기타 파일 NOTICE.txt : 이 도구들에 사용된 오픈소스 라이브러리의 라이선스 및 저작권 정보가 담겨 있습니다. package.xml , source.properties , runtime.properties : Android SDK Manager가 이 패키지를 인식하고 관리하기 위한 메타데이터 파일입니다. 패키지의 버전, 설명, 종속성 등의 정보가 포함됩니다. 결론 build-tools 디렉토리는 Android 앱의 \"생산 공장\"과 같습니다. 개발자가 제공한 설계도(소스 코드)와 재료(리소스)를 받아, aapt2 로 부품을 가공하고, d8 으로 엔진(로직)을 만들며, 이 모든 것을 zipalign 으로 최적화된 상자( .apk )에 담아 apksigner 로 품질 보증 도장(서명)을 찍는, 전체 빌드 과정의 핵심 엔진 역할을 수행하는 곳입니다.",
      "frontmatter": {
        "tags": [
          "android",
          "잡지식"
        ],
        "date": "2025-09-08T14:54:08+09:00",
        "lastmod": "2025-09-08T14:56:26+09:00",
        "share_link": "https://share.note.sx/69ay5h9s#00f5E3CyCrcG6NrV20fPP1D7UjkUvERu7w63RqY5TEQ",
        "share_updated": "2025-09-08T15:07:46+09:00"
      }
    },
    "ascii code": {
      "path": "/02.inbox/ascii-code/",
      "filename": "ascii code",
      "content": "Untitled Untitled 1 제어 문자 0 (Null) - 십진수 0에 해당하는 제어 문자입니다. 주로 문자열의 종료를 나타내는 데 사용됩니다. 1 (Start of Heading) - 통신 제어에서 사용되는 제어 문자로, 통신 헤더의 시작을 나타냅니다. 2 (Start of Text) - 통신 제어에서 사용되는 제어 문자로, 텍스트 데이터의 시작을 나타냅니다. 3 (End of Text) - 통신 제어에서 사용되는 제어 문자로, 텍스트 데이터의 종료를 나타냅니다. 4 (End of Transmission) - 통신 제어에서 사용되는 제어 문자로, 전송의 종료를 나타냅니다. 5 (Enquiry) - 통신 제어에서 사용되는 제어 문자로, 상대방에게 정보 요청을 나타냅니다. 6 (Acknowledge) - 통신 제어에서 사용되는 제어 문자로, 정보 수신을 확인하는 신호를 나타냅니다. 7 (Bell) - 터미널이나 출력 장치에서 경고음을 발생시키는 제어 문자입니다. 8 (Backspace) - 커서를 한 칸 뒤로 이동시키는 제어 문자입니다. 공백문자 9 (Horizontal Tab) - 가로 탭 문자로, 텍스트에서 일정한 간격으로 열을 정렬하거나 수평 간격을 만들기 위해 사용됩니다. 일반적으로 탭 문자는 다음 탭 정지 위치로 커서를 이동시킵니다. 10 (Line Feed) - 줄 바꿈 문자로, 텍스트에서 다음 줄로 이동하여 새로운 줄을 시작합니다. 주로 줄 바꿈을 나타내는 역할을 합니다. 11 (Vertical Tab) - 세로 탭 문자로, 주로 출력 장치에서 사용되며, 수직 간격을 만들기 위해 사용될 수 있습니다. 일반적으로 세로 탭은 다음 세로 탭 정지 위치로 커서를 이동시킵니다. 12 (Form Feed) - 용지 공급 문자로, 출력 장치에서 새로운 페이지를 시작하거나 용지를 공급하는 데 사용됩니다. 일반적으로 용지 공급 문자는 출력 장치의 내부 버퍼를 비우고 커서를 첫 번째 위치로 이동시킵니다. 13 (Carriage Return) - 복귀 문자로, 텍스트에서 커서를 현재 줄의 처음으로 이동시킵니다. 주로 줄의 끝으로 커서를 이동한 후 다음 줄로 이동하기 전에 사용됩니다. 제어문자 14 (Shift Out) - 출력 장치에 문자 세트 전환이 필요한 경우에 사용됩니다. 주로 7비트 ASCII 코드와 다른 문자 세트 간의 전환이나 확장 문자 세트의 사용을 나타내기 위해 사용됩니다. 15 (Shift In) - 출력 장치에서 이전 문자 세트로 전환하는 데 사용됩니다. Shift Out과 짝을 이루며, 문자 세트 전환이 필요한 경우에 사용됩니다. 16 (Data Link Escape) - 통신 제어에서 사용되며, 데이터 링크 계층에서 제어 문자가 전송되어야 함을 나타냅니다. 주로 프로토콜 간의 전환, 프레임 동기화 등에 사용됩니다. 17 (Device Control 1) - 주로 출력 장치 제어에 사용되는 제어 문자입니다. 특정 출력 장치의 기능을 활성화 또는 비활성화하기 위해 사용될 수 있습니다. 18 (Device Control 2) - 주로 출력 장치 제어에 사용되는 제어 문자입니다. 특정 출력 장치의 기능을 활성화 또는 비활성화하기 위해 사용될 수 있습니다. 19 (Device Control 3) - 주로 출력 장치 제어에 사용되는 제어 문자입니다. 특정 출력 장치의 기능을 활성화 또는 비활성화하기 위해 사용될 수 있습니다. 20 (Device Control 4) - 주로 출력 장치 제어에 사용되는 제어 문자입니다. 특정 출력 장치의 기능을 활성화 또는 비활성화하기 위해 사용될 수 있습니다. 21 (Negative Acknowledge) - 통신 제어에서 사용되며, 정보 수신이 실패하거나 확인이 불가능한 경우에 사용됩니다. 22 (Synchronous Idle) - 동기화된 통신에서 사용되며, 데이터 전송이 없는 상태를 나타냅니다. 23 (End of Transmission Block) - 통신 제어에서 사용되며, 전송 블록의 종료를 나타냅니다. 24 (Cancel) - 현재 진행 중인 작업을 취소하거나 중단하기 위해 사용됩니다. 25 (End of Medium) - 현재 데이터 저장 장치의 끝을 나타내는 제어 문자입니다. 26 (Substitute) - 통신 제어에서 사용되며, 데이터 전송 중에 오류가 발생한 경우 대체 문자로 사용될 수 있습니다. 27 (Escape) - 통신 및 컴퓨터 시스템에서 특정 동작이나 명령을 활성화하기 위해 사용됩니다. 주로 제어 시퀀스나 특수 명령을 나타내는 데 사용됩니다. 28 (File Separator) - 파일 분리자로, 데이터 레코드 내에서 파일 경계를 나타냅니다. 29 (Group Separator) - 그룹 분리자로, 데이터 레코드 내에서 그룹 경계를 나타냅니다. 30 (Record Separator) - 레코드 분리자로, 데이터 레코드를 분리하기 위해 사용됩니다. 31 (Unit Separator) - 유닛 분리자로, 데이터 레코드 내에서 유닛 경계를 나타냅니다. 127 (Delete) - delete 제어 문자로 해당 문자 이전의 문자를 삭제하는 역할을 합니다 vim에서 제어문자 입력 Unix and Linux 에서 사용: ctrl- v ctrl-m Windows에서 사용: ctrl- q ctrl-m Ctrl- V다음 입력 문자가 문자 그대로 삽입되어야 함을 vi(shell)에 알리고 ctrl- m는 캐리지 리턴을 위한 키 입력입니다.",
      "frontmatter": {
        "aliases": [
          "아식스"
        ],
        "tags": [
          "operating-system",
          "잡지식",
          "reference",
          "cs"
        ],
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2023-12-20T07:12:00+09:00"
      }
    },
    "asm parameter passing 인자 전달 방식": {
      "path": "/02.inbox/asm-parameter-passing-인자-전달-방식/",
      "filename": "asm parameter passing 인자 전달 방식",
      "content": "다음은 인텔 어셈블리 문법으로 각 파라미터 전달 방식을 구현한 예시 Register를 통한 파라미터 전달 section .data ; (데이터 섹션은 필요 시 정의) section .text global _start _start: ; 두 정수를 레지스터로 전달 (EAX=5, EBX=10) mov eax, 5 mov ebx, 10 call add_registers ; 함수 호출 ; 결과 확인 (EAX에 저장됨) ; ... (종료 코드 생략) add_registers: add eax, ebx ; EAX = EAX + EBX ret ; 결과를 EAX에 반환 특징: 레지스터( eax , ebx )에 직접 값 저장 → 빠른 접근 가능. 주의: 레지스터 수가 제한적이므로 복잡한 함수에는 부적합. Memory를 통한 파라미터 전달 (포인터 사용) section .data var1 dd 15 ; 32비트 정수 (15) var2 dd 25 ; 32비트 정수 (25) section .text global _start _start: ; 메모리 주소를 레지스터로 전달 mov esi, var1 ; ESI = var1의 주소 mov edi, var2 ; EDI = var2의 주소 call add_memory ; 함수 호출 ; ... (종료 코드 생략) add_memory: mov eax, [esi] ; EAX = [var1] (15) add eax, [edi] ; EAX += [var2] (25) ret ; 결과 반환 특징: 메모리 주소를 레지스터( esi , edi )로 전달 → 대용량 데이터 처리 가능. 주의: 메모리 접근 오버헤드 발생 (캐시 미스 시 성능 저하). 일반적으로 힙 영역에 적층 Stack을 통한 파라미터 전달 section .data ; (데이터 섹션은 필요 시 정의) section .text global _start _start: ; 스택에 파라미터 푸시 (역순으로 전달) push 30 ; 두 번째 인자 push 40 ; 첫 번째 인자 call add_stack ; 함수 호출 ; 스택 정리 (cdecl 규약: 호출자가 정리) add esp, 8 ; 2개의 DWORD(4바이트*2) 제거 ; ... (종료 코드 생략) add_stack: push ebp ; 베이스 포인터 보존 mov ebp, esp ; 스택 프레임 설정 ; [ebp+8] = 첫 번째 인자 (40) ; [ebp+12] = 두 번째 인자 (30) mov eax, [ebp+8] add eax, [ebp+12] pop ebp ; 베이스 포인터 복구 ret ; 결과 반환 특징: 스택을 통해 인자 전달 → 재귀 호출 등 복잡한 로직에 적합. 주의: 스택 오버플로우 위험 (너무 큰 데이터 전달 금지). 키 포인트 Register: 빠르지만 제한적 → 최적화된 코드에 사용. Memory: 대용량 데이터 처리 가능 → 구조체/배열 전달 시 유리. Stack: 함수 호출 관리 용이 → 대부분의 고수준 언어 기본 방식. 인텔 문법에서 mov eax, [ebx] 는 \"ebx가 가리키는 메모리 값 로드\"이며, push , pop 은 스택 조작 명령어입니다.",
      "frontmatter": {
        "tags": [
          "operating-system",
          "assembler"
        ],
        "date": "2025-03-11T11:35:00+09:00",
        "lastmod": "2025-03-11T11:35:00+09:00"
      }
    },
    "c, cpp 연산자 우선순위": {
      "path": "/02.inbox/c-cpp-연산자-우선순위/",
      "filename": "c, cpp 연산자 우선순위",
      "content": "c 연산자 우선순위-20231222103216 [](https://learn.microsoft.com/ko-kr/cpp/c-language/precedence-and-order-of-evaluation?view=msvc-170#precedence-and-associativity-of-c-operators) [](https://learn.microsoft.com/ko-kr/cpp/cpp/cpp-built-in-operators-precedence-and-associativity?view=msvc-170#c-operator-precedence-and-associativity-table)",
      "frontmatter": {
        "tags": [
          "c",
          "reference"
        ],
        "date": "2023-12-22T10:31:00+09:00",
        "lastmod": "2023-12-22T10:31:00+09:00"
      }
    },
    "c,c++ vs code": {
      "path": "/02.inbox/cc++-vs-code/",
      "filename": "c,c++ vs code",
      "content": "컴파일 시에 다음과 같은 3가지 환경 설정이 필요하다(project 내부에서만 작동) ==ccppproperties.json== (compiler path and IntelliSense settings) ==tasks.json== ==(build instructions) ==launch.json== ==(debugger settings)== mac 기준 task.json { \"tasks\": [ { //c++ 컴파일러 \"type\": \"shell\", \"label\": \"clang++ 빌드 및 터미널 실행\", \"command\": \"/usr/bin/clang++\", \"args\": [ \"-std=c++17\", \"-fcolor-diagnostics\", \"-fansi-escape-codes\", \"-g\", \"${file}\", \"-o\", \"${fileDirname}/${fileBasenameNoExtension}\", // 파일 실행부 \"&&\", \"${fileDirname}/${fileBasenameNoExtension}\", ], \"options\": { \"cwd\": \"${fileDirname}\" }, \"problemMatcher\": [ \"$gcc\" ], \"group\": { \"kind\": \"build\", \"isDefault\": true }, \"detail\": \"직접 실행 task\" }, { //c 컴파일러 \"type\": \"shell\", \"label\": \"clang 빌드 및 터미널 실행\", \"command\": \"/usr/bin/clang\", \"args\": [ \"-std=c11\", \"-fcolor-diagnostics\", \"-fansi-escape-codes\", \"-g\", \"${file}\", \"-o\", \"${fileDirname}/${fileBasenameNoExtension}\", // 파일 실행부 \"&&\", \"${fileDirname}/${fileBasenameNoExtension}\", ], \"options\": { \"cwd\": \"${fileDirname}\" }, \"problemMatcher\": [ \"$gcc\" ], \"group\": { \"kind\": \"build\", \"isDefault\": true }, \"detail\": \"직접 실행 task\" }, { //디버깅시에 사용되는 cpp 빌드 설정 \"type\": \"cppbuild\", \"label\": \"C/C++: clang++ 활성 파일 빌드\", \"command\": \"/usr/bin/clang++\", \"args\": [ \"-std=c++17\", \"-fcolor-diagnostics\", \"-fansi-escape-codes\", \"-g\", \"${file}\", \"-o\", \"${fileDirname}/${fileBasenameNoExtension}\" ], \"options\": { \"cwd\": \"${fileDirname}\" }, \"problemMatcher\": [ \"$gcc\" ], \"group\": { \"kind\": \"build\", \"isDefault\": true }, \"detail\": \"디버거에서 생성된 작업입니다.\" }, { //디버깅시에 사용되는 c 빌드 설정 \"type\": \"cppbuild\", \"label\": \"C/C++: clang 활성 파일 빌드\", \"command\": \"/usr/bin/clang\", \"args\": [ \"-std=c11\", \"-fcolor-diagnostics\", \"-fansi-escape-codes\", \"-g\", \"${file}\", \"-o\", \"${fileDirname}/${fileBasenameNoExtension}\" ], \"options\": { \"cwd\": \"${fileDirname}\" }, \"problemMatcher\": [ \"$gcc\" ], \"group\": { \"kind\": \"build\", \"isDefault\": true }, \"detail\": \"디버거에서 생성된 작업입니다.\" }, { //실행용 \"label\": \"exec\", \"type\": \"shell\", \"command\": \"${fileDirname}/${fileBasenameNoExtension}\", \"group\": { \"kind\": \"build\", \"isDefault\": true } } ], \"version\": \"2.0.0\" } launch.json { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"c++ 디버그\", \"type\": \"lldb\", \"request\": \"launch\", \"program\": \"${fileDirname}/${fileBasenameNoExtension}\", \"args\": [], \"preLaunchTask\": \"C/C++: clang++ 활성 파일 빌드\", \"stdio\": [null, null, null], \"terminal\": \"integrated\" }, { \"name\": \"c 디버그\", \"type\": \"lldb\", \"request\": \"launch\", \"program\": \"${fileDirname}/${fileBasenameNoExtension}\", \"args\": [], \"preLaunchTask\": \"C/C++: clang 활성 파일 빌드\", \"stdio\": [null, null, null], \"terminal\": \"integrated\" }, { \"name\": \"g++ - Build and debug active file\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"${fileDirname}/${fileBasenameNoExtension}\", \"args\": [], \"stopAtEntry\": true, \"cwd\": \"${workspaceFolder}\", \"environment\": [], \"externalConsole\": true, \"MIMode\": \"lldb\", \"preLaunchTask\": \"C/C++: g++ build active file\" } ] }",
      "frontmatter": {
        "aliases": [
          "tasks.json",
          "launch.json"
        ],
        "tags": [
          "setting",
          "vscode"
        ],
        "date": "2023-12-28T19:11:00+09:00",
        "lastmod": "2025-10-11T07:35:42+09:00"
      }
    },
    "cpp 메모리 할당 추척": {
      "path": "/02.inbox/cpp-메모리-할당-추척/",
      "filename": "cpp 메모리 할당 추척",
      "content": "operator new를 오버로딩하여 메모리 할당 시 로그를 출력 void* operator new(std::size_t count) { std::cout << count << \" bytes 할당 \" << std::endl; return malloc(count); } stdout 으로 할당된 양을 뽑는다",
      "frontmatter": {
        "aliases": [
          "new 재정의"
        ],
        "tags": [
          "cpp"
        ],
        "date": "2025-01-21T10:52:00+09:00",
        "lastmod": "2025-06-27T19:48:59+09:00"
      }
    },
    "cpp 문자열 split": {
      "path": "/02.inbox/cpp-문자열-split/",
      "filename": "cpp 문자열 split",
      "content": "vector<string> split(const string& input, const string& delimiter) { vector<string> ret; size_t pos = 0; string token; // input 문자열이 비어있거나 delimiter가 비어있으면 빈 벡터 반환 if (input.empty() || delimiter.empty()) { return ret; } string str = input; // 원본 문자열을 수정하지 않기 위해 복사 // 구분자가 문자열에 없을 때까지 반복 while ((pos = str.find(delimiter)) != string::npos) { token = str.substr(0, pos); // 구분자 이전의 문자열 추출 ret.push_back(token); // 벡터에 추가 str.erase(0, pos + delimiter.length()); // 구분자 이후의 문자열로 업데이트 } // 남은 문자열 추가 ret.push_back(str); // 마지막 토큰 추가 return ret; }",
      "frontmatter": {
        "aliases": [
          "string split",
          "delimiter"
        ],
        "tags": [
          "algorithm"
        ],
        "date": "2024-12-17T06:46:00+09:00",
        "lastmod": "2024-12-17T06:46:00+09:00"
      }
    },
    "db-book 답지 파일 다운받는 javascript": {
      "path": "/02.inbox/db-book-답지-파일-다운받는-javascript/",
      "filename": "db-book 답지 파일 다운받는 javascript",
      "content": "https://db-book.com/bib-dir/index.html 여기 위치에서 콘솔창은 연후 모든 pdf 파일을 다운받는다 // 모든 PDF 링크와 이름을 선택 const rows = document.evaluate( '/html/body/center/table/tbody/tr', document, null, XPathResult.ORDERED_NODE_SNAPSHOT_TYPE, null ); // 링크 배열 생성 const pdfData = []; for (let i = 0; i < rows.snapshotLength; i++) { const row = rows.snapshotItem(i); const nameElement = row.querySelector('td:nth-child(1)'); // 파일 이름 const linkElement = row.querySelector('td:nth-child(2) a'); // PDF 링크 // 요소가 존재하는지 확인 if (nameElement && linkElement) { const name = nameElement.textContent.trim(); // 파일 이름 const url = linkElement.href; // PDF 링크 pdfData.push({ url: url, name: name }); } else { console.warn(`Row ${i + 1} does not contain the expected elements.`); } } // PDF 다운로드 함수 const downloadFiles = async (pdfData) => { for (const { url, name } of pdfData) { const a = document.createElement('a'); a.href = url; a.download = name + '.pdf'; // 파일 이름 지정 document.body.appendChild(a); a.click(); document.body.removeChild(a); // 오류발생 가능성 때문에 다운로드 사이에 잠시 대기 오류발생 가능성 때문에 await new Promise(resolve => setTimeout(resolve, 100)); // 0.1초 대기 } }; // PDF 다운로드 수행 downloadFiles(pdfData);",
      "frontmatter": {
        "tags": [
          "javascript",
          "잡지식"
        ],
        "date": "2024-11-05T17:05:00+09:00",
        "lastmod": "2024-11-05T17:05:00+09:00"
      }
    },
    "di (의존성 주입) 의 타 프레임워크 접근 유튜브 댓글": {
      "path": "/02.inbox/di-의존성-주입-의-타-프레임워크-접근-유튜브-댓글/",
      "filename": "di (의존성 주입) 의 타 프레임워크 접근 유튜브 댓글",
      "content": "영상을보다가 좀 답답해서 글을 써봅니다. 호돌맨과 항로님이 그냥 코딩에대한 감각이 없는것같다는 생각이 듭니다. 두분이 좋아하는 방식은 데이터를 다루는 영역에서 유효한 방식인데 그게 두분 전문분야인 웹 백엔드죠. 그러다보니 조금 착각이 일어나시는 것 같습니다. 우선 백엔드의 환경에 대해서 알아야하는데 웹 백엔드는 환경을 통제하기 쉽고(내 서버에 배포) 일종의 요청의 처음과 끝이 존재하는 심플한 방식이고 데이터를 가공해서 스토리지에 저장하는것이 주 목적입니다. 이런 환경에서는 말씀하시는 클래스와 DI같은 레이어링을 위한 추상화 방식이 유효해집니다. 스토리지는 인터페이스는 같지만 실제 구현체가 여러종류일 수 있고 (다형성) 그런 구현체를 주입하기위한 DI는 유효할 수 있습니다, 두분이 좋아하시는 자바 스프링의 어노테이션은 나름 괜찮은 표현력을 제공하죠. 저도 어떤 환경에서 프로그래밍을 하던 데이터 레이어가 복잡해지면 그와 같은 방식을 선호합니다. 나쁘지않은 방식이죠 근데 웹 프론트 환경에서의 클래스와 DI는 복잡도를 더 높입니다. 두분이 선호하는 클래스와 DI방식의 프레임워크로는 앵귤러가 있는데요, 앵귤러는 어노테이션 DI 다 스프링처럼 제공합니다. 근데 앵귤러가 왜 시장에서 외면받았느냐를 분석해드리겠습니다. 실제로 웹 프론트엔드 환경에서 코딩을 해보시면 아시겠지만 js환경에서 앵귤러와같은 표현력을 제공하려면 추상화비용에서 오는 손실이 백엔드보다 더 큽니다. 랜더링이라는 개념이 있고 데이터와 상호작용을 해야해하고 이러한 구현들은 클라이언트 전체적인 아키텍쳐와 상호작용합니다. (백엔드와는 이런 부분들이 다르죠). 예를들어 클라이언트에선 뭔가 백그라운드에서 돌아가고 있을 수 도 있고, 거기에 필요한 데이터 영역이 UI와 상호작용할수도 있습니다. 이 뿐만 아니라 컴포넌트끼리의 상호작용까지 있습니다. 이런 환경에서 과도하게 추상화된 영역은 버그를 일으키고 의존성을 파악하기도 힘들어집니다. 디버깅도 어려워지죠. 작성하기 귀찮은 보일러플레이트는 덤이구요. 그래서 앵귤러는 웹 프론트엔드 환경에서 외면받았습니다. react에서도 mobx와 같은 도구들이 있는데요 컨셉은 좋습니다, 클래스로 탄탄하게 데이터를 다루는 표현력을 제공하겠다, 너무 좋죠. 근데 이게 생각보다 간단하지않습니다. 이런 도구들이 react와 붙으려면 역시 복잡한 의존관계를 관리해야하고 규모가 커질수록 이는 파악하기 어려워집니다. 앵귤러와 비슷한 딜레마가 있습니다, 그래서 메이저가 되진 못했죠. 결론적으로 자연스럽게 코딩하다보면, 호돌맨님과 향로님이 제시하시는 방식은 외면받을 수 밖에 없다는걸 깨달을 때가 되신것같은데 아직 깨닫지못했다는게 정말 신기할 따름입니다. 아키텍쳐만 정해주고 직접 코딩하시진 않아서 그런걸까요? 정말로 리액트에서 클래스랑 DI를 사용하는 방식이 생산성이높다는 생각이든다면 뭔가 어거지 보일러 플레이트를 만들면서 잘못 코딩하고 계신거같고 제 생각엔 그냥 효율성 안나오지만 보일러플레이트 정형화되게 착착 쌓고 이러다보니 아 코딩하기 편하군 이라는 착각에 빠져계신 것 같습니다. 실제 웹 프론트엔드에서 생산성높게 일하는 회사가 어떤 방식을 취하고 계신지 한번 살펴보기를 권합니다. 간략히 28 답글 답글 17개 @worldhello-o4w 2주 전(수정됨)) 길게 적다보니 알아듣기 힘들것같은 분들을 위해 데이터와 스토리지를 다루는 백엔드 영역에서 클래스와 DI는 유효한 방법이고 그러한 레이어에서는 필자도 그것을 즐겨 씀 그러나 처음과 끝이 있는 심플한 백엔드와는 다르게 클라이언트는 끝이없는 사용자와의 무한한 상호작용이고, 구현은 클라이언트 전체 아키텍쳐와 상호작용함 이런 환경에서 지불해야하는 DI와 클래스 방식의 추상화비용은 직접 코딩하는사람들에게는 불편함을 불러일으킴, 불편함 대비 이득이 큰 것도 아님 고로 DI와 클래스 접근방식은 웹 프론트엔드 환경에서는 맞지않음. 물론 웹 프론트엔드에서도 데이터를 다루는 영역이 복잡해진다면 해당 레이어는 그러한 접근방법을 택함, 그러나 UI와는 격리함 간략히 7 답글 @user-vc5vo4ol2g 2주 전 호돌맨은 내가 누군지 모르겠고, 향로님이 코딩에 대한 감각이 없어요?ㅋㅋㅋㅋ 얘 5년차 안됐네 답글 @worldhello-o4w 2주 전(수정됨)) @user-vc5vo4ol2g \"클래스나 DI 같은걸로 복잡도를 낮출 수 있는데 왜 웹 프론트엔드에서는 안하는지 모르겠다, 오히려 복잡도를 늘리는 방향으로 가고 있다\" 라고 발언하셔서 코딩감각이 없구나(향로님이 말씀하신 방식은 오히려 복잡도를 늘리는 방식) 라고 판단했고 이유는 위에 적었습니다. 호돌맨님은 옆에 같이 방송하는 분이십니다. 최소 동영상은 보고 답글 달아주시면 좋을 것 같습니다. 마지막으로 인신공격을 하려면 적절한 근거를 부탁드립니다. 감사합니다. 2 답글 @worldhello-o4w 2주 전(수정됨)) @min-j4d-h3u 의견 감사합니다. 우선 OOP에 대해서 말씀드리면 댓글분의 말씀이 틀리진않습니다. OOP라는게 거기에서만 유효한 방법은 아니지요 다만 스프링과 백엔드에서 제시하는 OOP는 대부분 행동과 개체를 묘사하기 위한 OOP라기보다는 데이터와 스토리지를 다루는데 특화된 부분으로 진화되어 있습니다. 그래서 그런 부분을 프론트엔드에 강제로 적용하려는 방식은 오히려 개발자들의 불편을 불러일으킨다고 말씀드릴 수 있겠습니다. Unreal엔진같은 게임엔진에서 제시하는 OOP스타일도 한번 보신다면 흐름 자체가 상당히 많은 부분이 다르다는것을 알 수 있습니다. 물론 기초가 되는 개념 자체는 비슷할 수 있겠지만 게임쪽에서는 어떤 한 방법론만을 따르지않기때문에 여러가지 영감을 얻어보실 수 있을 겁니다. 향로님이 영상에서 말씀하신 것 처럼 \"클래스나 DI 같은걸로 복잡도를 낮출 수 있는데 왜 웹 프론트엔드에서는 안하는지 모르겠다, 오히려 복잡도를 늘리는 방향으로 가고 있다\" 라는 웹 프론트 생태계를 싸잡아 비난하는 태도도 좋은 접근은 아닌것같습니다, 심지어 구독자수도 꽤 되는 채널에서 말이죠. 이러면 이 채널을 보는 많은 신입 개발자들에게 혼란을 줍니다. 심지어 틀린말씀이라 조금 강하게 지적해보았습니다. 추가로 네임드 개발자분과 다른관점 이라고 하셨는데 사실 향로님과 호돌맨님은 그냥 평범한 백엔드 개발자시지 정말 최전선의 개발 생태계에 큰 영향을 끼치시는 분들은 아닙니다, 본인들도 그건 잘 알고 있을겁니다. 오히려 글로벌 빅 테크회사에서 웹 프론트엔드 생태계 프레임워크를 개발하는분들이 전 세계적인 네임드 개발자시기때문에 이 경우에는 향로님과 호돌맨님이 그분들과 다른의견을 내고 계산상황, 즉 도전자라고 보는것이 옳을 것 같습니다. 생산성 높게 일하는 회사들로는 한국에서는 당근, 토스, 그리고 해외에서는 대표적으로 react를 만든 메타가 있을것같습니다. 그 외 빅테크들도 react를 적절하게 활용하고 있으니 어떤 방식으로 다루는지 보면 좋겠지요. 이 회사들에서 외부에 공개해놓은 자료들을 참고해보신다면 어떤 얘기들을 하고있는지 확인해볼 수 있습니다. 마침 토스에서 공개해놓은 자료도 있으니 링크를 첨부해봅니다. https://frontend-fundamentals.com/code/examples/form-fields.html 대안의 경우는 특별히 대안이 있다기보단 일반적인 웹 프론트생태계의 흐름에 호돌맨님과 향로님이 반박을 하고 계신상황이기에, 일반적인 웹 프론트 생태계의 아키텍쳐나 코드구조 멘탈모델을 확인해보시면 좋을것같습니다. 간략히 3 답글 @elecricecooker 2주 전 첫문장 들이박는거 보고 바로 쭉내렸습니다 1 답글 @user-xk4jb8si8w 2주 전 의존성 주입에 대해 좀 더 공부해보시는 게 좋을 거 같아요.. react에서 의존성 주입 라이브러리를 사용해서 프로젝트가 더 복잡해졌다고 하는데 애초에 의존성 주입 자체가 서로 간의 영향을 덜 받기위해 사용하는 기법이에요.. react에서 사용이 불편하단건 해당 라이브러리의 문제지 의존성 주입의 문제가 아닙니다.. 의존성 주입을 하기 위해 만드는 코드를 보일러 플레이트 코드라고 생각하시는 거 자체가 의존성 주입에 대한 이해도가 부족하다고 생각해요.. 혼자서 하는 단일 프로젝트는 속도를 위해 의존성 주입없이 하는게 편하긴 하지만 이 개념이 나온 이유에 대해 공부해보시는 걸 추천해요.. 답글 @worldhello-o4w 2주 전(수정됨)) @user-xk4jb8si8w 제 얘기는 영상을 찍으신 두 분이 제시하시는 DI방식에 대한 얘기였구요. 의존성 주입 이라는 개념자체를 배제하는것은 아닙니다. react에서도 props로 여러 요소들(함수 컴포넌트 등등)을 받는것도 의존성 주입이죠. 근데 DI framework같은 것들을 사용해서 뭔가 환경과 맞지않는 규격을 도입하려고하는게 좋지않은거구요, 그러므로 인해 멋들어지게 그런 개념을 도입해봤자 그 복잡성이라는것이 사라지지는 않는다는 뜻입니다. 그리고 코딩하기 불편하면 그냥 그 상황에서 안 좋은겁니다. react의 문제라기보단 react는 그런 방식의 코딩을 의도하지 않는거죠, 때문에 거기서 억지로 사용해야할 방법이 아니구요. (그럼 앵귤러는 잘 살아남았어야 합니다 DI를 편하게 지원하니깐) 개념은 훌륭한데 사용하기 불편해서 사용하지않는경우는 굉장히 많이 있습니다. 예를들어 effect-ts라는 라이브러리를 보시면 함수형 프로그래밍의 개념을 도입하여 부수 효과들을 어떻게 다룰지에 대한 굉장히 훌륭한 개념을 제시하고있습니다, rust에서 코딩하는것처럼 코딩할 수 있죠. 컨셉은 한번 보시길 추천드립니다. 근데 js에서 그런 개념을 네이티브하게 지원하지않고 해당 라이브러리에 대한 멘탈모델에 대한 학습비용, 그러한 개념을 js 환경에서 지원 안함으로써 커버해야하는 부분들, 이런 이유때문에 역시 메이저가 되진 못합니다. 대부분 이런것들은 어디선가 잘 작동하던 멘탈모델을 그대로 옮겨오고싶은것에서 기인하지만, 결국 사용이 불편한것들은 사용되지않습니다. 훌륭한 개념이라고 모든것을 도입해야하는것은 아닙니다. 전반적인 생태계에 대한 이해와 생태계가 주는 한계점을 파악하고, 적절한 코드와 인터페이스를 제시해야하죠. 경험과 공부를 더 해보는것을 추천드립니다, 특히 코딩은 좀 더 해보셔야할것같네요. 자세히 보기 1 답글 @user-xk4jb8si8w 2주 전 @worldhello-o4w 저는 훌륭한 개념이라고 어디든 도입해야 한다고 말한적 없습니다. 의존성 주입자체의 개념이 특정 객체에 대한 연결성과 해당 class에 대한 변화가 있으면 그거에 영향을 받는 부분들을 고려해서 사용되는 겁니다. 이런거 고려해서 react라고 무조건 의존성 주입 사용하라거나 하지말라는게 아니라 프로젝트 규모나 인원에 따라서 사용할지 안할지 고려하는 거구요. 본인 아는 만큼 보이는거니까 이해하겠지만 남들이 다른 이야기할때는 그 부분에 대해서도 생각해보시길 바랍니다. 답글 @user-xk4jb8si8w 2주 전 @worldhello-o4w 저는 공부하라고 말씀드린 것도 정말 의존성 주입에 대한 이해가 부족한 거 같아 말씀드리는 건데.. 이렇게 비꼬시는거 보면 더이상 이야기할 가치를 못 느낍니다. 1 답글 @worldhello-o4w 2주 전(수정됨)) @user-xk4jb8si8w 죄송한데 어떤말을 하고싶으신지 잘 모르겠습니다. 그래서 호돌맨님과 향로님의 접근방식이 맞다, 당신이 틀렸다 인지. 아니면 그저 의존성주입이라는 지식을 자랑하고 싶으셨던것인지 (솔직히 제가 당신보다는 많이 알것같긴합니다) 전자라면 이미 시장에서 여러가지 이유로 외면받는 구조라는것을 말씀드렸구요 후자면 대화를 더 해볼필요는 없을 것 같습니다. 대화할땐 의도를 명확하게 말씀해주시는걸 추천드립니다. 그리고 프로젝트 규모가 커진다고 스프링에서 사용하는 DI방식이 웹 프론트엔드에서 유효한건 아닙니다. 참고로 말씀드리면 저는 수백명의 엔지니어들이 작업하고있는 코드베이스에서 작업하고있습니다. 감사합니다. 자세히 보기 답글 @아무것도몰라요-d3l 2주 전(수정됨)) 자바스크립트를 많이 안해봐서 생기는 오해라고 봅니다. 자바랑 자바스크립트는 겉으로 보면 for, if, class 같은 문법이 공통으로 있지만 본질은 다른 언어이죠. 자바스크립트는 웹을 위해 태어났고 HTML문서를 다루기 위해 만들어진 언어입니다. 그래서 플랫폼도 다르고 사용 용도도 다르고 언어의 철학도 많이 다릅니다. 자바스크립트는 본질적으로 타입이 없는 동적 언어이며 객체지향보다는 함수형 프로그래밍에 더 가깝습니다. ES6부터 클래스 문법을 지원하지만 신택스 슈거일 뿐이고 근본은 프로토타입 기반 함수 중심 패러다임입니다. 문제를 DI로 해결할수도 있지만 자바스크립트는 일급 함수를 기본으로 제공하기 때문에 대부분의 경우 함수형 패러다임을 활용하면 간단하고 직관적으로 문제 해결이 가능하죠. 안드로이드나 IOS처럼 코드가 컴파일된 후에 실행되는 구조가 아니기 때문에 언제든 쉽게 외부 툴로 인해 변경될 수 있는 환경이고, 이럴때는 오히려 동적 언어가 더 큰 장점이 되죠 솔직히 말하면 안드로이드에서 XML로 UI를 구성하고 자바로 컨테이너 만들어서 쿵짝쿵짝 하는 코드보다가 일반적인 웹프론트엔드 HTML,CSS,JS로 조합딘 코드를 보면 코드수도 적고 더 직관적이고 더 유지보수가능하고 더 효율적인것을 많이 간과하고 있는것 같아요 문제를 DI를 통해서도 풀 수 있지만 DI를 쓰지 않아서 복잡도가 높아졌다고 주장하는건 공감이 안갑니다. 같은 추상화 레벨이고 Android코드 보고 웹 JS코드보면 DI로 푼거나 함수형 프로그래밍으로 풀어낸거나 비슷비슷합니다. 오히려 DI쪽이 보일러플레이트도 많고 디버깅하기에 더 복잡합니다. 자바스크립트가 기본으로 제공하는 일급 함수, 함수형 패러다임만으로도 문제를 더 간결하게 해결할 수 있다는걸 경험이 부족해 받아들이지못하는것으로 보입니다. 자세히 보기 5 답글 @user-yl7oh4jb7k 2주 전 저는 7년차 프론트 개발자 입니다, 허나 그간 BE 경험도 적지 않게 있습니다. 프론트와 백엔드의 궁국적인 차이는 결국 주체가 되는 부분이 무엇이냐에 갈리는것 이라고 생각합니다. 백엔드의 경우 리얼월드에 있는 기능과 관계 그리고 연관짓는거에 더 큰 관점을 둡니다. 프론트의 경우는 ui의 구조와 사용자 편의성 혹은 디자인에 큰 관심사를 지니며, 이런 관점에서 구조 및 추상화를 진행하게됩니다, 또한 사용자에게 보여지는 UI 는 떄때로 회사의 전략적인 이유로 변경을 하기도 합니다. 즉 각각의 개발자가 봐야하는 진리의 원천이 매번 변경된다는것을 의미하기도 하는데요. 이런 배경을 바탕으로 보았을때 OOP에서 사용하는 방식으로 못하지는 않습니다, 다만 진리의 원천이 매변 변경되는 상황에서 DI와 같은 방식이 어렵게 느껴집니다. 또한 많이 사용하게 되는 typescript 의 경우 데코레이터 및 OOP를 위한 타입을 보조해주는 기능이 적습니다. 개발에는 당연히 불가능하다는 거히 없다고 생각합니다만, 업계에서 자주 이야기되는 패러타임을 굳이 거스를 이유또한 없다고 생각합니다. 간략히 4 답글 @tjrals6665 2주 전(수정됨)) 음..지나가는 입장에서 일단 과한 표현들을 다 빼고 클래스와 (스프링 같은)DI를 사용하면 복잡도를 낮출수 있는가? 라고 하면 그렇지 않다고 생각합니다. 1. 클래스 실제로 클래스 기반 React가 hooks나오기 전에 주류였고요. hooks가 나오고 난 후 간결해지는 코드에 환영하며 다들 넘어갔지요. 저는 클래스로 대표되는 \"상속\"모델이 복잡도를 낮출 수 있는가에 대해 회의적인 입장입니다. 특히 UI를 다루는데 있어서요. 간단하게 안드로이드에서 머터리얼 버튼을 생각해볼까요? 대략 다음 구조를 가지고 있습니다. 아름답죠? `` // Framework Button public class Button extends TextView { ... } // AppCompat implementation public class AppCompatButton extends Button implements TintableBackgroundView { ... } // Material Design variant public class MaterialButton extends AppCompatButton implements Checkable, Shapeable { ... } `` 하지만 실제 사용은 어떨까요? 스타일의 우선순위 문제가 생길 수 있죠. 라이프사이클쪽도 문제가 생길 수 있고요. https://github.com/material-components/material-components-android/issues/1013 게다가 Class 방식의 상속은 필요치 않은 메서드나 동작들도 상속하게 되는데요. 요구사항이 복잡해질수도록 Sideeffect가 끼어들 가능성이 매우 높습니다. 때문에 가능하면 합성을 하라고 하지요. (하지만 합성을 선호한다면, function 대신 class를 써야할 타당한 이유가 없습니다) 그 후에 나온 SwiftUI, JetpackCompose는 왜 Class모델을 안쓰려고 할까요? Class를 충분히 쓸수 있는 기반을 가진 플랫폼들인데도요? Native 기준으로 Class는 성능 문제도 일으킵니다. 포인터 동등성으로 인한 리렌더링 어려움은 물론이고, 컴파일러 최적화에서도 이득을 보기가 힘듭니다. 때문에 UIKit와 달리 SwiftUI에서는 Struct기반으로 넘어갔어요. https://wwdcnotes.com/documentation/wwdcnotes/wwdc15-414-building-better-apps-with-value-types-in-swift/ C++에서는 성능문제를 극복하기 위해 CRTP, EBO등 괴상한 최적화 방법을 사용해야 함은 물론 생성/소멸자의 예기치 못한 동작등 때문에 Rust도 Class 모델을 포기한거고요. https://blog.rust-lang.org/2015/05/11/traits.html 어찌되었건 다시 돌아오자면 UI에서 Class로 얻는 이득이 크지 않습니다. React 기준 hooks보다 성능이 살짝 좋다고는 하는데, 복잡도를 줄여주는가에는 의문이 듭니다. 2. DI DI는 이미 React Context로 되고 있습니다. https://ko.react.dev/learn/passing-data-deeply-with-context 때문에 Spring or Dagger 스타일의 DI가 필요하다는 뜻이겠지요? 프론트에서 Dagger와 같은 컴파일타임 DI는 Typescript 특성상 쉽지 않을 것 같아 제외하구요. Spring 스타일이라 하더라도 크게 복잡성을 개선시킬 수 있을지 모르겠네요. 그래도 Async Context는 기대하고 있어요. https://github.com/tc39/proposal-async-context 저는 Kotlin context parameters나 Racket Parameterize와 같이 언어 레벨에서 대부분 DI 문제는 해결해주는게 맞다는 입장이라서요. https://github.com/Kotlin/KEEP/blob/master/proposals/context-parameters.md https://docs.racket-lang.org/guide/parameterize.html 정말로 복잡도를 낮추고 간단하게 만들고 싶다면, 다음과 같은 라이브러리나 기술을 탐색/투자해보시기 바랍니다. - 서버상태: tanstack-query, Relay(GraphQL), Isograph - 라이프타임 매니저: Bunshi, Bunja - 폼이나 오버레이: react-hook-form, useFunnel/useOverlay 그렇게 되면 상당한 보일러 플레이트 코드가 줄 가능성이 높다고 봅니다.",
      "frontmatter": {
        "tags": [
          "잡지식"
        ]
      }
    },
    "disk": {
      "path": "/02.inbox/disk/",
      "filename": "disk",
      "content": "mount 인수 없음 findmnt 추천2 fdisk -l (파티션 설정) lsblk * 추천1 df -a dmesg blkid [!NOTE] 순서 디스크 -> 파티션 -> 파일시스템 지정 -> 마운트 디스크 -> 파티션 : 파티션 정보를 저장하는 두 가지 다른 테이블 형식 mbr, gpt 방식을 먼저 정하자 fdisk parted gdisk cfdisk 파티션을 어떻게 사용할지 종류를 지정하자 linux, lvm, swap, EFI 등등 파티션 -> 파일지스템 지정: 파티션을 사용하기 전 어떠한 파일 시스템을 사용할지 정하자 mkfs.\\[파일 시스템] \\[옵션] \\[디스크 파티션] mkfs -t(파일 시스템) \\[옵션] \\[디스크 파이션] [!NOTE] 참조 hard-disk, nvme 장치를 리눅스에서는 ==sd% # (% 장치 순서 abc, \\# 은 파티션)== cdrom dvd 장치를 리눅스에서는 sr# (#은 숫자)이라고 부른다 여기는 파티션이 존재하지 않는다 2023-06-25_123904 디스크 파티셔닝 명령어 종류 fdisk mkfs mount fdisk 파일 시스템이란 데이터를 더 빠르게 읽고 저장할 수 있는 단위 블록(클러스터)을 소프트웨어적으로 계산해준다. 분산 저장된 연관된 데이터들을 빠르게 찾게 해준다. 디스크 조각(섹터)모음과 같이 디스크 공간을 효율적으로 사용하게 해준다. 하나의 섹터 크기는 512kb 최근에는 4MB 로 바꾸는 추세 섹터를 묶은 단위를 클러스터(윈도우) 또는 블록(유닉스) 클러스터가 생기면서 낭비되는 현상을 slack space 이라고 한다 fdisk: fdisk /dev/sdX : 디스크 파티션 테이블을 관리하는 fdisk 도구를 실행합니다. n : 새로운 파티션 생성 d : 파티션 삭제 t : 파티션 유형 설정 w : 변경사항 저장 및 종료 parted: parted /dev/sdX : 디스크 파티션 테이블을 관리하는 parted 도구를 실행합니다. mklabel : 새로운 파티션 테이블 생성 mkpart : 새로운 파티션 생성 rm : 파티션 삭제 print : 파티션 정보 출력 quit : parted 종료 gdisk (GPT 디스크 전용): gdisk /dev/sdX : GPT 디스크 파티션 테이블을 관리하는 gdisk 도구를 실행합니다. n : 새로운 파티션 생성 d : 파티션 삭제 t : 파티션 유형 설정 p : 파티션 정보 출력 w : 변경사항 저장 및 종료 cfdisk (터미널 기반의 파티션 관리 도구): cfdisk /dev/sdX : cfdisk를 실행하여 디스크 파티션 관리를 시작합니다. 화살표 키와 Enter 키를 사용하여 메뉴를 탐색하고 파티션 작업을 수행합니다. parted, fdisk, gdisk, cfdisk 외에도 다른 도구 및 프로그램도 있을 수 있습니다. 예를 들어, macOS에서는 diskutil 을 사용하여 파티션을 관리할 수 있습니다.",
      "frontmatter": {
        "aliases": [
          "디스크"
        ],
        "tags": [
          "linux",
          "command"
        ],
        "description": "디스크 관리",
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2023-12-20T07:12:00+09:00"
      }
    },
    "form 태그의 http 패킷(massage) 전송": {
      "path": "/02.inbox/form-태그의-http-패킷massage-전송/",
      "filename": "form 태그의 http 패킷(massage) 전송",
      "content": "클라이언트측 post 메서드 전송 예시 <form action=\"http://www.example.com/test\" method=\"POST\"> <label for=\"userId\">User ID:</label><br> <input type=\"text\" id=\"userId\" name=\"userId\"><br> <label for=\"password\">Password:</label><br> <input type=\"password\" id=\"password\" name=\"password\"><br> <input type=\"submit\" value=\"Submit\"> </form> 서버측으로 전송되는 패킷 예시 POST /test HTTP/1.1 Host: www.example.com User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 Accept-Language: en-US,en;q=0.5 Accept-Encoding: gzip, deflate, br Content-Type: application/x-www-form-urlencoded Content-Length: 43 Origin: http://www.example.com Connection: keep-alive Referer: http://www.example.com/test Cookie: PHPSESSID=5a1lvj45uk83a3k9opjkpo3jm2 \\n\\r userId=user&password=password 클라이언트측 get 메서드 html <form action=\"http://www.example.com/test\" method=\"GET\"> <label for=\"userId\">User ID:</label><br> <input type=\"text\" id=\"userId\" name=\"userId\"><br> <label for=\"password\">Password:</label><br> <input type=\"password\" id=\"password\" name=\"password\"><br> <input type=\"submit\" value=\"Submit\"> </form> 서버측으로 전송되는 패킷 예시 GET /test?userId=user&password=password HTTP/1.1 Host: www.example.com",
      "frontmatter": {
        "tags": [
          "network",
          "html"
        ],
        "date": "2024-02-13T15:15:00+09:00",
        "lastmod": "2024-02-13T15:15:00+09:00"
      }
    },
    "gcc defalut 버전 확인": {
      "path": "/02.inbox/gcc-defalut-버전-확인/",
      "filename": "gcc defalut 버전 확인",
      "content": "gcc -dM -E -x c - < /dev/null | grep __STDC_VERSION__ g++ -dM -E -x c++ - < /dev/null | grep __cplusplus C++98: 199711 C++11: 201103 C++14: 201402 C++17: 201703 C++20: 202002",
      "frontmatter": {
        "tags": [
          "잡지식",
          "c",
          "cpp"
        ],
        "date": "2024-12-23T13:30:00+09:00",
        "lastmod": "2024-12-23T13:30:00+09:00"
      }
    },
    "gcc include path 확인하기": {
      "path": "/02.inbox/gcc-include-path-확인하기/",
      "filename": "gcc include path 확인하기",
      "content": "echo | gcc -xc -E -v - echo | gcc -xc++ -E -v -",
      "frontmatter": {
        "tags": [
          "잡지식"
        ],
        "date": "2024-02-15T04:55:00+09:00",
        "lastmod": "2024-02-15T04:55:00+09:00"
      }
    },
    "git merge": {
      "path": "/02.inbox/git-merge/",
      "filename": "git merge",
      "content": "gitGraph commit id: \"H\" commit id: \"I\" branch topic commit id: \"B\" commit id: \"C\" checkout main commit id: \"J\" commit id: \"K\" 여기서 branch 를 merge 할 때 merge commit gitGraph commit id: \"D\" commit id: \"E\" branch topic commit id: \"B\" commit id: \"C\" checkout main commit id: \"F\" commit id: \"G\" merge topic 이러한 방식이 기본적인 e commit 를 base 로 한 merge 방식이다 git 은 main 의 commit 가 있을 경우 위의 방식을 선택한다 하지만 만약 gitGraph commit id: \"H\" commit id: \"I\" branch topic commit id: \"B\" commit id: \"C\" checkout main 이렇게 있을 때(merge 시에 잡히는 base 가 최신 커밋일때)는 fast foword merge 가 기본적인 방법이다 즉 여기서 git switch main 에서 git merge topic 실행시 gitGraph commit id: \"H\" commit id: \"I\" commit id: \"B\" commit id: \"C\" 이렇게 commit 된다 gitGraph TB: commit id: \"ZERO\" branch develop commit id:\"A\" checkout main commit id:\"ONE\" checkout develop commit id:\"B\" branch featureA commit id:\"FIX\" commit id: \"FIX-2\" checkout main commit id:\"TWO\" cherry-pick id:\"A\" commit id:\"THREE\" cherry-pick id:\"FIX\" checkout develop commit id:\"C\" merge featureA gitGraph commit id: \"H\" commit id: \"I\" branch topic commit id: \"B\" commit id: \"C\" checkout main commit id: \"J\" commit id: \"K\" merge topic id: \"M1\" type: NO_FF tag: \"main에서 merge\" gitGraph commit id: \"H\" commit id: \"I\" branch topic commit id: \"B\" commit id: \"C\" checkout main commit id: \"J\" commit id: \"K\" merge topic id: \"M1\" type: NO_FF tag: \"main에서 merge\" %%{init: { 'gitGraph': {'showBranches': true, 'showCommitLabel':true}} }%% gitGraph commit id: \"H\" commit id: \"I\" branch topic commit id: \"B\" commit id: \"C\" checkout main commit id: \"J\" commit id: \"K\" checkout topic merge main id: \"M2\" type: NO_FF tag: \"topic에서 merge\"",
      "frontmatter": {
        "tags": [
          "git"
        ],
        "date": "2024-11-09T13:23:00+09:00",
        "lastmod": "2024-11-09T13:23:00+09:00"
      }
    },
    "git reflog": {
      "path": "/02.inbox/git-reflog/",
      "filename": "git reflog",
      "content": "",
      "frontmatter": {
        "tags": [
          "git"
        ],
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2023-12-20T07:12:00+09:00"
      }
    },
    "gitstatus 프롬프트": {
      "path": "/02.inbox/gitstatus-프롬프트/",
      "filename": "gitstatus 프롬프트",
      "content": "segment meaning master HEAD가 위치한 브렌치 이름 #v1 HEAD가 위치한 태그 이름 (브랜치 일 때는 사용 x) @5fc6fca4 HEAD가 위치한 커밋 hash (브랜치 or hash 일때 사용 x) ⇣1 로컬 브랜치가 원격 브랜치보다 1개의 커밋만큼 뒤쳐져 있음 ⇡2 로컬 브랜치가 원격 브랜치보다 2개의 커밋만큼 앞서 있음 ⇠3 로컬 브랜치가 푸시 원격 브랜치보다 3개의 커밋만큼 뒤쳐져 있음 ?? ⇢4 로컬 브랜치가 푸시 원격 브랜치보다 4개의 커밋만큼 앞서 있음 ?? *5 5개의 스태시가 있음 merge 병합이 진행 중임 (다른 작업일 수도 있음) ~6 6개의 병합 충돌이 있음 +7 7개의 stage 에 변경사항 발생 !8 8개의 stage 에 없는 변경사항 ?9 9개의 추적되지 않은 파일",
      "frontmatter": {
        "tags": [
          "reference",
          "잡지식"
        ],
        "date": "2024-02-24T08:39:00+09:00",
        "lastmod": "2024-02-24T08:39:00+09:00"
      }
    },
    "h2 database 사용법": {
      "path": "/02.inbox/h2-database-사용법/",
      "filename": "h2 database 사용법",
      "content": "다운로드 링크 h2 homepage link tcp 포트 웹 접근 포트 file 접근 TCP 접근 TCP:9092 TCP:8082 jdbc:h2:~/test jdbc:h2:tcp://<서버IP>:9092/~/test main h2/bin/h2.sh == java -cp h2-2.3.232.jar org.h2.tools.Server -tcp -web -tcp : TCP 서버 실행 -tcpAllowOthers : TCP 서버에 외부 IP에서 접속 가능하게 함 -web : 웹 콘솔 실행 -webAllowOthers : 웹 콘솔에 외부 IP에서 접속 가능하게 함 webGUI 기본 link path localhost:8082 cli java -cp h2-2.3.232.jar org.h2.tools.Shell 추가 도구 도구명 설명 사용 예시 RunScript SQL 스크립트 파일을 실행하여 데이터베이스 초기화 또는 변경 적용 `` bash java -cp h2-*.jar org.h2.tools.RunScript -url jdbc:h2:~/test -user sa -script init.sql ` Recover | 크래시나 오류 후 데이터베이스 복구 | ` bash java -cp h2-*.jar org.h2.tools.Recover -dir ~/h2db/ ` ChangeFileEncryption | 데이터베이스 파일 암호화 방식 변경 | ` bash java -cp h2-*.jar org.h2.tools.ChangeFileEncryption -dir ~/h2db/ -oldPassword oldpass -newPassword newpass ` RunScript .sql - 주로 초기 데이터 삽입, 테이블 생성 등에 사용됩니다. Recover - 데이터베이스가 비정상 종료되었을 경우 로그를 분석해 데이터를 복구합니다. .h2.db ChangeFileEncryption` 인코딩된 H2 DB 파일의 암호를 변경하거나, 암호화 설정을 업데이트할 때 사용. 보안 강화 및 관리에 유용. H2의 Local, Server 개념 Embedded 모드 H2 DB를 시스템의 메인 메모리에서 (JVM 위에서) 구동시키는 방식으로 application이 종료된다면 저장, 수정된 Data가 손실(휘발) 된다. 즉 기본적으로는 영속적이지 않은 방식이다. → 데이터에 대한 영속성을 제공하는 방법은 존재한다. 메인 메모리에 DB를 띄워놓고 해당 DB를 사용하는 Application의 스레드로 데이터에 바로 접근함으로써 데이터 읽기, 쓰기에 대한 성능을 향상할 수 있으므로 유용하게 사용할 수 있으며, 데이터 캐싱 DB에 대해서도 H2를 고려할 수 있다고 한다. 하지만 JVM에서 데이터 연산에 사용되는 쓰레드를 인터럽트 하지 않을 수 있기에, IO 수행 시에 I/O Handler가 닫힘으로써 데이터베이스의 손상을 일으킬 수 있다. Server 모드 해당 이미지는 하나의 시스템에서 서버 모드를 사용하는 경우이다. 별도의 프로세스(JVM)를 통해 DB를 동작시켜 데이터베이스를 영속적으로 사용하는 방법이다. local 환경에서는 localhost의 9092포트를 통해 DB 콘솔에 접근할 수 있으며, 별도의 서버 위에서 동작시킬 경우에 여러 Application을 해당 데이터베이스에 동시적으로 연결할 수 있다. 서버 모드도 내부적으로는 Embedded 모드와 동일한 실행방식을 가지지만, 모든 데이터의 처리 흐름이 TCP/IP를 통하여 전송되기 때문에 Embedded 모드보다 상대적으로 느릴 수밖에 없다.",
      "frontmatter": {
        "tags": [
          "database",
          "잡지식"
        ],
        "date": "2025-05-26T14:22:00+09:00",
        "lastmod": "2025-05-26T14:22:00+09:00"
      }
    },
    "http는 stateless 한데 하위 스택의 tcp 는 stateful 이다 (HTTP1.1 기준)": {
      "path": "/02.inbox/http는-stateless-한데-하위-스택의-tcp-는-stateful-이다-http1.1-기준/",
      "filename": "http는 stateless 한데 하위 스택의 tcp 는 stateful 이다 (HTTP1.1 기준)",
      "content": "질문 Persistent HTTP 에서 기본적으로 http 는 stateless 해 하지만 어떻게 Persistent하게 get 요청을 보낼 수 있는 거야 즉 서버 측에서 이미 3way handshake 했다는 것을 어떻게 인지하는 거야?? 답변 요약 Stateless 상태를 유지하지 않는 시스템의 특성의 의미 (HTTP) Stateful 은 상태를 유지하며 동작하는 시스템의 특성을 나타냅니다. (TCP) Persistent 는 연결을 재사용하여 네트워크 효율성을 높이는 방식을 의미합니다. 따라서 HTTP는 stateless 지만, (일반적으로 사용하는)전송 계층에서 TCP의 stateful 특성을 활용(HTTP 헤더의 Connection: keep-alive 속성) 하여 persistent connection을 구현할 수 있다 핵심 개념: Stateless vs Stateful HTTP는 Stateless(무상태)이다: HTTP 프로토콜 자체는 이전 요청에 대한 정보를 유지하지 않습니다. 매 요청은 독립적으로 처리됩니다. 즉, 서버는 클라이언트가 이전에 어떤 요청을 보냈는지 기억하지 않습니다. TCP는 Stateful(상태 유지)이다: TCP는 연결 지향(connection-oriented) 프로토콜로, 연결 상태를 유지합니다. 3-way handshake를 통해 연결이 설정된 후, 해당 연결은 특정 시간 동안 유지되며, 이 연결을 통해 여러 데이터 패킷을 주고받을 수 있습니다. Persistent HTTP에서 GET 요청이 가능한 이유 TCP 연결 유지 Persistent HTTP에서는 하나의 TCP 연결을 통해 여러 개의 HTTP 요청과 응답을 주고받을 수 있습니다. TCP 연결은 stateful이므로, 서버는 이미 설정된 TCP 연결을 통해 클라이언트로부터 들어오는 데이터를 인식하고 처리할 수 있습니다. 즉, 서버는 \"이미 3-way handshake를 완료한 연결\"이라는 사실을 TCP 계층에서 관리하며, 이를 통해 추가적인 요청(GET 등)을 처리할 준비가 됩니다. HTTP 요청의 독립성 HTTP는 stateless이므로, 각 HTTP 요청은 독립적입니다. 클라이언트는 동일한 TCP 연결을 통해 새로운 HTTP 요청을 보내더라도, 이전 요청과 관련된 정보를 포함하지 않습니다. 서버는 매번 들어오는 HTTP 요청을 처음부터 해석하고 처리하며, 이전 요청에 의존하지 않습니다. 예시: Persistent HTTP 동작 과정 TCP 연결 설정 (3-way handshake): 클라이언트와 서버 간 TCP 연결이 설정됩니다. 이 연결은 일정 시간 동안 유지됩니다. 첫 번째 HTTP GET 요청: 클라이언트는 TCP 연결을 통해 첫 번째 HTTP GET 요청을 보냅니다. 서버는 이 요청을 처리하고 응답을 반환합니다. 두 번째 HTTP GET 요청: 클라이언트는 동일한 TCP 연결을 통해 두 번째 HTTP GET 요청을 보냅니다. 서버는 이 요청을 처리하고 응답을 반환합니다. TCP 연결 종료: 더 이상 요청이 없거나 타임아웃이 발생하면 TCP 연결이 종료됩니다. 서버가 어떻게 TCP 연결을 인지하는가? TCP는 연결 상태를 관리하는 프로토콜이므로, 서버는 다음과 같은 정보를 통해 연결을 인지합니다: 소켓(Socket): TCP 연결은 클라이언트와 서버 간의 소켓으로 표현됩니다. 소켓은 IP 주소와 포트 번호 쌍으로 식별됩니다. 서버는 각 클라이언트의 소켓 정보를 기반으로 연결 상태를 관리합니다. 연결 식별자(Connection Identifier): 서버는 각 TCP 연결을 고유한 식별자(예: 소켓 파일 디스크립터)로 관리합니다. 이 식별자를 통해 특정 클라이언트와의 연결을 추적합니다. 데이터 스트림(Stream): TCP는 데이터를 순차적으로 전송하는 스트림 기반 프로토콜입니다. 서버는 동일한 연결을 통해 들어오는 데이터를 순차적으로 처리합니다. HTTP의 Stateless 특성과 Persistent HTTP의 관계 HTTP Stateless: 각 HTTP 요청은 독립적이며, 서버는 이전 요청에 대한 정보를 유지하지 않습니다. 예를 들어, 첫 번째 GET 요청과 두 번째 GET 요청 사이에는 아무런 연관성이 없습니다. TCP Persistent: TCP 연결은 상태를 유지하므로, 동일한 연결을 통해 여러 개의 HTTP 요청을 보낼 수 있습니다. 즉, HTTP 요청은 무상태이지만, 이를 전달하는 TCP 연결은 상태를 유지합니다. 요약 HTTP는 Stateless: 각 HTTP 요청은 독립적이며, 서버는 이전 요청에 대한 정보를 유지하지 않습니다. TCP는 Stateful: TCP 연결은 상태를 유지하므로, 동일한 연결을 통해 여러 개의 HTTP 요청을 보낼 수 있습니다. Persistent HTTP 동작 원리: TCP 연결이 설정된 후, 이 연결을 통해 여러 개의 HTTP 요청(GET 등)을 순차적으로 보낼 수 있습니다. 서버는 TCP 연결 상태를 통해 클라이언트를 인지하며, 각 HTTP 요청은 독립적으로 처리됩니다. 따라서, HTTP의 Stateless 특성과 TCP의 Stateful 특성이 결합되어 Persistent HTTP가 가능하게 됩니다.",
      "frontmatter": {
        "tags": [
          "network"
        ],
        "date": "2025-03-25T15:09:00+09:00",
        "lastmod": "2025-03-25T15:09:00+09:00"
      }
    },
    "ios 터미널 접근": {
      "path": "/02.inbox/ios-터미널-접근/",
      "filename": "ios 터미널 접근",
      "content": "외부 접근 ios 는 모바일 운영체제로서 샌드박스 형태의 아키텍쳐를 취하고 있다 즉 앱 하나하나다 일종의 User 로서 로그인하게 되고 유저의 home 영역을 제외한 곳은 읽기 조차 금지 되어있다 하지만 android 는 초창기 부터 다른 A 앱에서 B 앱의 접근을 api 로서 허용하고 있으며 이를 활용해 삼성의 '내파일' '사진' '한글뷰어' 과 같은 앱들은 android 가 제공하는 공용 공간에 접근 api 를 사용하여 동일한 공간에 사용자의 파일을 몰아넣고 데스크탑과 비슷한 환경을 구축해놓았다 하지만 ios 의 경우에는 이런것이 매우 부족하다 하지만 ios 13 부터였나(?) 이러한 api 가 개발자들에게 지원되기 시작했고 이를 이를 통해 할 수 있는 포텐셜이 늘어나게 되었다 터미널의 구현 ios 에서 터미널을 구현한 인기있는 앱 2가지 종류가 보이는 것 같았다 ISH : 시스템 콜 수준의 변환을 통한 터미널 구현 : ios 와 리눅스의 시스템 콜의 호환성만 일치시키면 어떠한 아키텍쳐든 구현이 가능하고 이것을 실현시킨 앱이다 장점 : 사용자 수준에서 느껴지는 운영체제의 수준이 완벽하게 실행되며 데스크탑 운영체제에서 할 수 있는 모든 행동이 구현 가능하다 단점 : 느리다 a-shell : 실제로는 safari 의 브라우저 엔진인 webkit 을 사용하고 있으며 git, gawk 등등의 앱들은 웹어셈블리로 컴파일되어진 앱들이며 a-shell 에서 돌아간다 또한 웹 어셈블리로 다른 앱들을 컴파일하여 돌릴 수 있다 장점 : 위에보다는 훨씬 빠르다 (웹어셈블리는 네이티브 속도의 60% ~ 95% 속도를 보여준다고 알려저 있다) 단점 어셈블리어로 컴파일 되어 있는 앱들만 사용가능하다 ISH mount -t ios . {마운트할 path} 를 통해 파일앱의 폴더를 마운트 할 수 있다 a-shell 파일의 구조가 일반적인 unix 구조와 다르다 (샌드박스 정책 때문에) 패지키 관리 pkg {install | remove | list search } pakage-name 제공하는 패키지 종류 서버 m pickFolder : 먼저 apple 이 제공하는 api 를 통해 외부에 접근할 곳을 지정한다 지정한 곳은 bookmark 라는 a-shell 이 관리하는 공간에 저장된다 pickFolder 파일 선택 창이 나오고 연결한 포인트를 설정하면 bookmard 에 폴더이름으로 북마크 이름이 된다 showmarks # 모든 북마크 보기 jump {북마크 이름}` 또는 `cd ~{북마크 이름} # 북마크 이동 deletemarks {북마크 이름} renamemarks {예전 이름} {바꿀 이름} bookmark {정할 이름} # 현재 디렉토리에 대한 북마크를 추가 다른앱 연결 open {file-name} # 파일 공유 view {file-name} # 파일 뷰어 play {file-name} # 미디어 파일 재생 internalbrowser {https://google.com} # 내부 브라우저 사용",
      "frontmatter": {
        "tags": [
          "ios"
        ],
        "date": "2024-05-15T14:30:00+09:00",
        "lastmod": "2024-05-15T14:30:00+09:00"
      }
    },
    "ipv4 vs ipv6": {
      "path": "/02.inbox/ipv4-vs-ipv6/",
      "filename": "ipv4 vs ipv6",
      "content": "📋 IPv4 헤더 필드 필드 이름 크기 간단 설명 Version 4비트 IP 버전 (예: IPv4, IPv6) 지정 Header Length (IHL) 4비트 헤더 길이 (옵션 유무에 따라 달라짐) Type of Service (TOS) 8비트 QoS 설정 (DSCP + ECN 포함) Total Length 16비트 전체 패킷 크기 (헤더 + 데이터) Identification 16비트 프래그먼트 식별을 위한 고유 번호 Flags 3비트 프래그먼트 가능 여부 표시 (예: DF, MF) Fragment Offset 13비트 조각난 패킷의 위치 정보 Time To Live (TTL) 8비트 패킷 수명 (라우터 통과 시 감소, 0이 되면 폐기) Protocol 8비트 상위 계층 프로토콜 (TCP=6, UDP=17 등) Header Checksum 16비트 헤더 오류 검출용 체크섬 Source IP Address 32비트 보낸 사람 IP 주소 Destination IP Address 32비트 받는 사람 IP 주소 Options (선택적) 가변 추가 기능 제공 (보통 사용 안 함) Data (Payload) 가변 실제 전송할 데이터 (TCP/UDP 세그먼트 등) 📋 IPv6 헤더 필드 필드 이름 크기 간단 설명 Version 4비트 IP 버전 번호 (IPv6이므로 값은 항상 6 ) Traffic Class 8비트 트래픽 우선순위 지정 (IPv4의 TOS/DSCP와 유사, QoS 지원) Flow Label 20비트 특정 패킷 흐름(예: 실시간 음성/영상) 식별용 라벨 (QoS 및 Flow 기반 처리 지원) Payload Length 16비트 헤더 이후 데이터(payload)의 길이 (바이트 단위) Next Header 8비트 다음 헤더의 타입을 지정 (TCP=6, UDP=17 등, IPv4의 Protocol 필드와 유사) Hop Limit 8비트 패킷이 지나갈 수 있는 최대 라우터 수 (IPv4의 TTL 필드와 동일한 역할) Source Address 128비트 보낸 호스트의 IPv6 주소 Destination Address 128비트 받는 호스트의 IPv6 주소 Data (Payload) 가변 실제 전송되는 데이터 (TCP/UDP 세그먼트 또는 다른 프로토콜 데이터) 📊 IPv4 vs IPv6 헤더 비교표 (기능별 분류) ✅ 버전 및 기본 정보 기능 그룹 IPv4 필드 크기 설명 IPv6 필드 크기 설명 IP 버전 Version 4비트 IP 버전 식별 (IPv4=4, IPv6=6) Version 4비트 IP 버전 식별 (IPv6=6) 헤더 길이 Header Length (IHL) 4비트 헤더 총 길이 (옵션 포함 여부에 따라 다름) - - IPv6는 고정 헤더 길이 (40바이트) 전체 패킷 크기 Total Length 16비트 헤더 + 데이터 전체 크기 Payload Length 16비트 헤더 이후 데이터(payload) 길이만 지정 🎯 주소 지정 및 라우팅 기능 그룹 IPv4 필드 크기 설명 IPv6 필드 크기 설명 출발지 주소 Source IP Address 32비트 송신자 IPv4 주소 Source Address 128비트 송신자 IPv6 주소 목적지 주소 Destination IP Address 32비트 수신자 IPv4 주소 Destination Address 128비트 수신자 IPv6 주소 다음 헤더 - - - Next Header 8비트 다음 헤더 유형 지정 (TCP/UDP/확장헤더 등) ⚙️ QoS 및 트래픽 관리 기능 그룹 IPv4 필드 크기 설명 IPv6 필드 크기 설명 트래픽 우선순위 Type of Service (TOS) 8비트 DSCP(6비트) + ECN(2비트), QoS 설정 Traffic Class 8비트 DSCP(6비트) + ECN(2비트), QoS 설정 흐름 식별 - - - Flow Label 20비트 특정 흐름(flow) 식별용 라벨 (실시간 서비스 지원) 🔄 패킷 처리 및 포워딩 기능 그룹 IPv4 필드 크기 설명 IPv6 필드 크기 설명 생존 시간 Time To Live (TTL) 8비트 패킷의 최대 홉 수 (라우터 통과 시 감소) Hop Limit 8비트 동일한 기능 (IPv4 TTL과 동일) 체크섬 검사 Header Checksum 16비트 헤더 오류 검출용 (매 라우터마다 재계산) - - IPv6에서는 헤더 체크섬 제거 (TCP/UDP에서 처리) 프래그먼트 처리 IdentificationFlagsFragment Offset 16+3+13비트 패킷 분할 및 조립 정보 - - IPv6에서는 프래그먼트 처리 소스/목적지에서만 가능 🔧 확장성 및 추가 기능 기능 그룹 IPv4 필드 크기 설명 IPv6 필드 크기 설명 확장 옵션 Options 가변 선택적 기능 제공 (보통 사용 안 함) Extension Headers 가변 Next Header 로 연결되는 확장 헤더 구조 데이터 영역 Data (Payload) 가변 실제 전송 데이터 (TCP/UDP 등) Data (Payload) 가변 실제 전송 데이터 (TCP/UDP 등) ✅ 요약 비교 요약표 항목 IPv4 IPv6 특징 주소 길이 32비트 128비트 IPv6는 주소 고갈 문제 해결 헤더 길이 가변 (20~60바이트) 고정 (40바이트) IPv6는 단순화된 헤더 구조 QoS 지원 TOS (DSCP + ECN) Traffic Class (DSCP + ECN)Flow Label IPv6가 더 세밀한 QoS 지원 프래그먼트 중간 노드에서 가능 소스/목적지만 가능 IPv6는 라우터 부담 감소 체크섬 헤더 체크섬 있음 없음 IPv6는 TCP/UDP에서 처리 확장 기능 Options 필드 Extension Headers IPv6가 더 유연한 확장 구조 다음 헤더 Protocol 필드 Next Header 유사한 역할 수행 데이터평면, 제어 평면 데이터 평면 (Data Plane) → 실제 데이터 패킷을 전달하는 역할 (패킷 포워딩) 제어 평면 (Control Plane) → 어떤 경로로 패킷을 보낼지 결정하는 역할 (라우팅) 항목 데이터 평면 (Data Plane) 제어 평면 (Control Plane) 목적 패킷 전달 (포워딩) 경로 결정 (라우팅) 수행 작업 패킷 수신 → 목적지 주소 확인 → 다음 홉 선택 → 전달 라우팅 프로토콜 실행, 경로 계산, 라우팅 테이블 업데이트 처리 속도 매우 빠름 (마이크로초 이하) 상대적으로 느림 사용 메커니즘 포워딩 테이블 (FIB), 캐시 등 라우팅 프로토콜 (RIP, OSPF, BGP 등) 주요 구성 요소 라우터의 포워딩 엔진 라우터의 CPU, 라우팅 프로토콜 모듈 보안 영향 직접적인 패킷 유출/손실 영향 라우팅 정보 변조, 네트워크 다운 위험 IP 헤더 필드별로 데이터 평면 / 제어 평면 사용 여부 정리 필드 이름 크기 간단 설명 사용되는 평면 Version 4비트 IP 버전 (예: IPv4, IPv6) 지정 데이터 평면(패킷 포워딩 시 해석 필요) Header Length (IHL) 4비트 헤더 길이 (옵션 유무에 따라 달라짐) 데이터 평면(헤더 파싱 및 데이터 위치 식별) Type of Service (TOS) 8비트 QoS 설정 (DSCP + ECN 포함) 데이터 평면(QoS 기반 포워딩 정책 적용) Total Length 16비트 전체 패킷 크기 (헤더 + 데이터) 데이터 평면(패킷 처리 및 메모리 관리를 위해 사용) Identification 16비트 프래그먼트 식별을 위한 고유 번호 데이터 평면(프래그먼트 재조립 시 사용) Flags 3비트 프래그먼트 가능 여부 표시 (예: DF, MF) 데이터 평면(패킷 분할 및 재조합 결정) Fragment Offset 13비트 조각난 패킷의 위치 정보 데이터 평면(패킷 재조립 시 사용) Time To Live (TTL) 8비트 패킷 수명 (라우터 통과 시 감소, 0이 되면 폐기) 데이터 평면(패킷 생존 시간 관리 및 폐기 결정) Protocol 8비트 상위 계층 프로토콜 (TCP=6, UDP=17 등) 데이터 평면(상위 계층으로 전달 시 필요) Header Checksum 16비트 헤더 오류 검출용 체크섬 데이터 평면(패킷 손상 검사 후 전달 결정) Source IP Address 32비트 보낸 사람 IP 주소 데이터 평면, 제어 평면→ 포워딩에는 직접 사용되지 않지만, 로깅/필터링, 라우팅 프로토콜에서 활용 Destination IP Address 32비트 받는 사람 IP 주소 제어 평면, 데이터 평면→ 가장 핵심적인 필드→ 제어 평면: 경로 계산 (라우팅 테이블 매칭)→ 데이터 평면: 다음 홉 선택 및 포워딩 Options (선택적) 가변 추가 기능 제공 (보통 사용 안 함) 데이터 평면(특수한 경우에만 처리됨) Data (Payload) 가변 실제 전송할 데이터 (TCP/UDP 세그먼트 등) 데이터 평면(포워딩 대상인 실제 데이터)",
      "frontmatter": {
        "tags": [
          "network"
        ],
        "date": "2025-05-07T21:26:00+09:00",
        "lastmod": "2025-05-07T21:26:00+09:00"
      }
    },
    "java 리플렉션(reflection)": {
      "path": "/02.inbox/java-리플렉션reflection/",
      "filename": "java 리플렉션(reflection)",
      "content": "클래스 인터페이스의 메타 정보를 java.lang.Class 클래스 객체에서 관리한다 이를 통해 런타임 시점에 클래스의 정보를 확인할 수 있다 Class class = 클래스 이름.class; // 클래스 이름을 통해 얻는다 Class class = Class.forName(클래스 이름); // 클래스 이름을 통해 얻는다 Class class = 객체 참조 변수.getClass(); // 객체의 이름을 통해 얻는다 이렇게 얻어진 객체로 부터 여러가지 정보를 확인할 수 있다 메서드 info Package getPackage() 패키지 정보 읽기 String getSimpleName() 패키지를 제외한 타입 이름 String getName() 패키지를 포함한 전체 타입 이름 Constuctor[] getDeclaredConstructors() 생성자 정보 읽기 Method[] getDeclaredMethod() 메서드 정보 읽기 Field[] getDeclaredField() 필드 정보 읽기 등등 많은 것들 을 얻을 수 있다 제네릭은 런타임에는 타입을 알지 못한다",
      "frontmatter": {
        "tags": [
          "java"
        ],
        "date": "2024-01-27T11:29:00+09:00",
        "lastmod": "2024-01-27T11:29:00+09:00"
      }
    },
    "java 메모리(memory)": {
      "path": "/02.inbox/java-메모리memory/",
      "filename": "java 메모리(memory)",
      "content": "Pasted image 20240127113149-1",
      "frontmatter": {
        "tags": [
          "java",
          "cs"
        ],
        "date": "2024-01-29T05:38:00+09:00",
        "lastmod": "2024-01-29T05:38:00+09:00"
      }
    },
    "java 어노테이션(anotation)": {
      "path": "/02.inbox/java-어노테이션anotation/",
      "filename": "java 어노테이션(anotation)",
      "content": "어노테이션 적용대상 @Target 클래스 필드 메서드 또다른 어노테이션 어노테이션 유지정책 SOURCE=컴파일까지 컴파일 시점에 적용 CLASS=메모리 로딩때 까지 이후 제거 RUNTIME=계속 유지",
      "frontmatter": {
        "tags": [
          "java",
          "language"
        ],
        "date": "2024-01-05T21:01:00+09:00",
        "lastmod": "2024-01-05T21:01:00+09:00"
      }
    },
    "java 제네릭(generic)": {
      "path": "/02.inbox/java-제네릭generic/",
      "filename": "java 제네릭(generic)",
      "content": "java 는 제네릭 배열을 생성하지 못한다 java generic 원리 중요!!! Integer <: Number Double <: Number ArrayList<E> <: List<E> Collection<E> <: Iterable<E> Super type : Number 는 Integer 에 대해 Super type 이다 Sub type : Integer 는 Number 에 대해 Sub type 이다 variant 공변 : Number 와 Integer 는 공변한다 Invariant 불공변 : Integer 와 String 는 공변하지 않는다 Java에서는 해당 경우에 Substitution Principle을 적용하지 않습니다 public class Main{ public void static main(int argc, String[] argv){ List<Integer> ints = new ArrayList<>(); ints.add(1); ints.add(2); List<Number> nums = ints //Compile 에러 발생 nums.add(3.14); // 이와 같은 연산을 막기 위해 위에서 미리 방지한다 } } 즉 List\\는 List\\의 Subtype이 아니다 하지만 List\\는 Collection\\를 상속받으므로 List\\는 Collection\\의 Subtype이다. java는 지정된 generic type parametor에 대해서는 invariant 하다 단 상위경계타입 파라미터를 통해 가능하다 =upper-bound Type Parameter class Data{ private List<Number> list; public void addAll(List<Number> cols){ this.list.addAll(cols); } } public class Main { public static void main(String[] args) { Data data = new Data(); List<Integer> ints = Arrays.asList(1, 2, 3); List<Double> dbls = Arrays.asList(1.0, 2.8); List<Number> nums = Arrays.asList(5,4.5); data.addAll(ints); //Compile 에러 발생 data.addAll(dbls); //Compile 에러 발생 data.addAll(nums); } } 아래와 같이 코드를 변경 class Data{ private List<Number> list; public void addAll(List<Number> cols){ this.list.addAll(cols); } } public class Main { public static void main(String[] args) { Data data = new Data(); List<Integer> ints = Arrays.asList(1, 2, 3); List<Double> dbls = Arrays.asList(1.0, 2.8); List<Number> nums = Arrays.asList(5,4.5); data.addAll(ints); //Compile 에러 발생 data.addAll(dbls); //Compile 에러 발생 data.addAll(nums); } }",
      "frontmatter": {
        "aliases": [
          "non-feifiable",
          "reifiable",
          "supertype",
          "subtype",
          "variant",
          "invariant"
        ],
        "tags": [
          "java"
        ],
        "date": "2024-01-29T05:00:00+09:00",
        "lastmod": "2024-01-29T05:00:00+09:00"
      }
    },
    "java 컬렉션(Collections Framework)": {
      "path": "/02.inbox/java-컬렉션collections-framework/",
      "filename": "java 컬렉션(Collections Framework)",
      "content": "Pasted image 20240122181734 Pasted image 20240206060503 리스트 : 배열의 확장 배열리스트 : 크기를 내부에서 자동으로 정해주는 배열 (vector) 링크드리스트 : 인접한 위치에 저장되지 않고 포인터를 사용하여 연결되는 선형 데이터 구조 큐 : 선입선출 set : 집합 중복 불가 map = dictionary : key value 한쌍 구조",
      "frontmatter": {
        "tags": [
          "java"
        ],
        "date": "2024-01-22T18:17:00+09:00",
        "lastmod": "2024-01-22T18:17:00+09:00"
      }
    },
    "javascript 자료형": {
      "path": "/02.inbox/javascript-자료형/",
      "filename": "javascript 자료형",
      "content": "var let const 재선언 o x x 업데이트 o o x 호이스팅 o o o 자동 초기화 o x x 호이스팅: let, const 는 오류 발생(호이스팅은 되어도 자동 초기화 안해서 오류 발생) console.log(a) 변환 -> var a console.log(a) // undefined 기준 원시(primitive) number boolean null undifined 참조(reference) BigInt object array function 자바스크립트에서는 원시 타입(primitive type) 참조 타입(reference type)이라는 두 가지 자료형을 제공한다. 숫자, 불린값, null과 undefined는 원시 타입이다. 객체, 배열, 함수는 참조 타입이다.",
      "frontmatter": {
        "tags": [
          "javascript",
          "language"
        ],
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2023-12-20T07:12:00+09:00"
      }
    },
    "javascript 함수": {
      "path": "/02.inbox/javascript-함수/",
      "filename": "javascript 함수",
      "content": "var let const 재선언 o x x 업데이트 o o x 호이스팅 o o o 자동 초기화 o x x 호이스팅: let, const 는 오류 발생(호이스팅은 되어도 자동 초기화 안해서 오류 발생) console.log(a) 변환 -> var a console.log(a) // undefined 기준 원시(primitive) number boolean null undifined 참조(reference) BigInt object array function 자바스크립트에서는 원시 타입(primitive type) 참조 타입(reference type)이라는 두 가지 자료형을 제공한다. 숫자, 불린값, null과 undefined는 원시 타입이다. 객체, 배열, 함수는 참조 타입이다.",
      "frontmatter": {
        "tags": [
          "javascript",
          "language"
        ],
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2023-12-20T07:12:00+09:00"
      }
    },
    "linux locale setting": {
      "path": "/02.inbox/linux-locale-setting/",
      "filename": "linux locale setting",
      "content": "systemd 기반 리눅스 배포판 localectl 명령은 systemd 기반 리눅스 배포판 에서 사용할 수 있는 시스템 관리 도구로, 로케일(locale), 키보드 레이아웃, 가상 콘솔(Console) 설정 등을 관리하는 데 사용됩니다. 1\\. 로케일 및 키보드 설정 확인 # localectl System Locale: LANG=en_US.UTF-8 VC Keymap: n/a X11 Layout: us X11 Model: pc105 형식 의미 System Locale 현재 설정되어 있는 로케일(Locale)을 표시합니다. VC Keymap 가상콘솔에서 사용하는 키맵을 표시합니다. X11 Layout Xwindows에서 사용되는 키보드 레이아웃을 표시합니다. X11 Model 키보드 모델을 표시 합니다. 2\\. 설정 가능한 로케일(Locale) 확인 # localectl list-locales C.UTF-8 en_US.UTF-8 localectl 명령어로 list-locales 옵션을 사용하면 설정이 가능한 로케일(Locale) 목록을 출력합니다. 3\\. 로케일(Locale) 설정 사용법 : localectl set-locale \"[Locale]\" # localectl set-locale \"en_US.UTF-8\" localectl 명령어에 set-locale 옵션을 사용하여 로케일(Locale)을 설정할 수 있습니다. 4\\. 시스템에 설정되어 있는 로케일 정보 확인 # locale LANG=en_US.UTF-8 LANGUAGE= LC_CTYPE=\"en_US.UTF-8\" LC_NUMERIC=\"en_US.UTF-8\" LC_TIME=\"en_US.UTF-8\" LC_COLLATE=\"en_US.UTF-8\" LC_MONETARY=\"en_US.UTF-8\" LC_MESSAGES=\"en_US.UTF-8\" LC_PAPER=\"en_US.UTF-8\" LC_NAME=\"en_US.UTF-8\" LC_ADDRESS=\"en_US.UTF-8\" LC_TELEPHONE=\"en_US.UTF-8\" LC_MEASUREMENT=\"en_US.UTF-8\" LC_IDENTIFICATION=\"en_US.UTF-8\" LC_ALL= locale 명령어를 사용하면 LANG를 포함하여 설정되어 있는 전체 로케일(Locale) 정보를 확인할 수 있습니다. 5\\. 사용가능한 로케일 확인 # locale -a locale 명령어에서 -a 옵션을 사용하면 사용 가능한 로케일(Locale)를 확인하실 수 있으며, 원하시는 로케일(Locale) 이 없으신 경우 별도로 설치해야 됩니다.",
      "frontmatter": {
        "tags": [
          "setting",
          "linux",
          "잡지식"
        ],
        "date": "2025-05-20T18:55:00+09:00",
        "lastmod": "2025-05-20T18:55:00+09:00"
      }
    },
    "mac java 관리": {
      "path": "/02.inbox/mac-java-관리/",
      "filename": "mac java 관리",
      "content": "apple 에서 관리되는 방식이므로 brew 패키지 관리자를 통해 하는 방법이 아니다 설치된 java 보기 /usr/libexec/java_home -verbose 모든 JDK는 기본 위치인 /Library/Java/JavaVirtualMachines 에 놔두어집니다. 시스템은 기본적으로 가장 높은 버전을 선택합니다. 기본 선택에서 제외하려면 해당 JDK의 Contents/Info.plist 파일 이름을 Info.plist.disabled 로 변경합니다. 이렇게 하면 $JAVA_HOME 이 해당 JDK를 가리키거나 스크립트나 설정에서 명시적으로 참조할 때 해당 JDK를 여전히 사용할 수 있습니다. 단지 시스템의 java 명령어에서는 무시됩니다. 시스템 런처는 Info.plist 파일이 있는 JDK 중 가장 높은 버전을 사용합니다. 삭제할 때 여기도 확인 sudo rm -fr /Library/Internet\\ Plug-Ins/JavaAppletPlugin.plugin sudo rm -fr /Library/PreferencePanes/JavaControlPanel.prefPane",
      "frontmatter": {
        "tags": [
          "mac",
          "java"
        ],
        "date": "2024-05-20T11:00:00+09:00",
        "lastmod": "2025-06-20T02:16:33+09:00"
      }
    },
    "meterialize view 의 집계연산에서 삽입 삭제에 따른 변화": {
      "path": "/02.inbox/meterialize-view-의-집계연산에서-삽입-삭제에-따른-변화/",
      "filename": "meterialize view 의 집계연산에서 삽입 삭제에 따른 변화",
      "content": "아래는 집계 연산에서 count , sum , avg , min , max 의 삽입 및 삭제에 따른 변화를 중심으로 설명하겠습니다. Count 삽입 동작: 새로운 튜플이 삽입되면, 해당 튜플의 그룹이 이미 존재하는지 확인합니다. 존재하는 경우: 해당 그룹의 카운트를 1 증가시킵니다. 존재하지 않는 경우: 새로운 그룹을 추가하고 카운트를 1로 설정합니다. 삭제 동작: 삭제할 튜플이 있을 때, 해당 튜플의 그룹을 찾아 카운트를 감소시킵니다. 카운트 감소: 만약 카운트가 0이 된다면, 해당 그룹을 v 에서 삭제합니다. 예시 초기 상태: v = { (1, 2), (2, 3) } (그룹 1의 카운트 2, 그룹 2의 카운트 3) 삽입: 튜플 (1) 이 추가되면, v 는 { (1, 3), (2, 3) } 로 변경됩니다. 삭제: 튜플 (2) 가 삭제되면, v 는 { (1, 3), (2, 2) } 로 변경됩니다. Sum 삽입 동작: 새로운 튜플이 삽입되면, 해당 그룹의 값을 합산합니다. 존재하는 경우: 그룹의 합계에 새로운 튜플의 값을 추가합니다. 존재하지 않는 경우: 새로운 그룹을 추가하고 합계를 해당 튜플의 값으로 설정합니다. 삭제 동작: 삭제할 튜플이 있을 때, 해당 튜플의 그룹에서 값을 빼줍니다. 합계 감소: 만약 특정 그룹의 합계가 0이 되면, 해당 그룹을 v 에서 삭제합니다. 하지만 합계가 0일 때 단순히 삭제할 수 없는 이유는 다른 튜플이 여전히 존재할 수 있기 때문입니다. 예시 초기 상태: v = { (1, 50), (2, 60) } (그룹 1의 합계 50, 그룹 2의 합계 60) 삽입: 튜플 (1, 20) 이 추가되면, v 는 { (1, 70), (2, 60) } 로 변경됩니다. 삭제: 튜플 (1, 50) 가 삭제되면, v 는 { (1, 20), (2, 60) } 로 변경됩니다. Average (avg) 처리 방법 합계와 카운트 별도 유지: 평균을 계산하기 위해서는 합계와 카운트를 별도로 유지합니다. 삭제 시: 특정 그룹의 튜플이 삭제되면, 해당 그룹의 합계에서 삭제된 값만큼 빼고, 카운트도 감소시킵니다. 예시 초기 상태: 그룹 1의 합계 50, 카운트 2 (평균 25) 삭제: 튜플 (1, 20) 가 삭제되면, 그룹 1의 합계는 30, 카운트는 1이 되어 평균은 30이 됩니다. Min and Max 삽입 동작: 새로운 튜플이 삽입될 때, 해당 그룹의 최소값과 최대값을 업데이트합니다. 최소값/최대값 변경: 새로운 값이 기존의 최소값보다 작거나 최대값보다 클 경우, 해당 값을 업데이트합니다. 삭제 동작: 만약 삭제할 튜플이 최소 최대이면 해당 그룹의 다른 튜플들을 확인하여 새로운 최소값 또는 최대값을 찾아야 합니다. 비용 발생: 이 과정은 다른 튜플을 스캔해야 하므로 비용이 더 발생합니다. 예시 초기 상태: v = { (1, min=10, max=20), (2, min=30, max=40) } 삽입: 튜플 (1, 5) 가 추가되면, 그룹 1의 최소값이 5로 업데이트됩니다. 삭제: 튜플 (1, 20) 가 삭제되면, 그룹 1의 최대값을 찾기 위해 다른 튜플들을 확인해야 하므로 비용이 발생합니다.",
      "frontmatter": {
        "tags": [
          "dbms"
        ],
        "date": "2024-12-04T15:56:00+09:00",
        "lastmod": "2024-12-04T15:56:00+09:00"
      }
    },
    "mysql 명령 모음": {
      "path": "/02.inbox/mysql-명령-모음/",
      "filename": "mysql 명령 모음",
      "content": "mysql 로그인 mysql -u {name} -p # name 유저로 -p 패스워드를 사용해 로그인 하겠다 mysql 유저 생성(외부에서) mysqladmin -u {name} mysql 유저 정보 확인 USE mysql; SELECT user, host FROM user; /* mysql 유저 정보 확인 */ mysql 유저 권한 추가 GRANT ALL privileges ON DB명.* TO username@hostname IDENTIFIED BY '비밀번호'; mysql 유저 권환 확인 SHOW GRANTS FOR username@hostname; SHOW VARIABLES LIKE \"general_log%\"; 서버 관리를 하다보면 mysql 사용자 계정을 추가해 줄때가 있다.",
      "frontmatter": {
        "tags": [
          "database",
          "command"
        ],
        "date": "2024-03-05T16:15:00+09:00",
        "lastmod": "2024-03-05T16:15:00+09:00"
      }
    },
    "pragma": {
      "path": "/02.inbox/pragma/",
      "filename": "pragma",
      "content": "사용할 일은 잦은데~ 너무 무관심한 척 한 것 같다~ 매번 매번 사용해도 헷갈리는 pragma의 용법에 대해 모아 총정리 하였다. \\#pragma는 define 이나 include와 같이 \\#으로 시작하는 전처리구문(precompiler)의 하나이다. 컴파일러에 종속적인 구문이라 컴파일러가 변경되었을 경우 제대로된 동작을 보장하지 못하므로 프로젝트 진행중에 서로 다른 컴파일러를 사용한다면 사용하지 않음이 바람직 하겠다. 대신 대체하는 문법을 사용해야 되겠다. \\#pragma once 이것은 \"컴파일러에게 한번만 컴파일해!\" 라고 명령한다. 헤더의 중복을 막아준다. 무슨말인가 하면 a.h를 구현한 a.cpp, a.h는 독립적이다.(include가 없다.) b.h를 구현한 b.cpp, c.h, a.h순서로 include c.h를 구현한 c.cpp, a.h를 include 컴파일하면 b.h에서 c.h를 포함시키라고 되어있네? 하고 c.h에 들어가고 어? a.h를 포함하라고 그러네? 이러고 a.h를 포함한 c.h가 b.h로 돌아온다 그리고 a.h를 포함하라는 명령을 받고 a.h를 추가하다보면 같은 변수와 함수선언이 되어있다. 에러에러~ 같은 선언이 두 번 반복되니 당연히 충돌이 난다. 컴파일러가 똑똑하여 단순히 경고 처리만 해주고 알아서 하나로 종합해줄 수도 있지만 대부분의 기본적인 컴파일러는 이건 아니잖아~ 한다. 이럴 때 써주는 것이다. pragma once 이는 c기본문법을 사용하여 구현할 수 있다. \\#ifdef \\_MYCOMPILECK \\#define \\_MYCOMPILECK // 헤더 파일의 내용 선언 \\#endif \\#pragma comment() 기본적인 pragma comment()의 형식은 다음과 같다. \\#pragma comment( comment-type, \\[\"comment string\"\\] ) \\[\\] 안의 구문은 comment-type에 따라 필요할 경우 사용하는 것이다. comment type에는 compiler, exestr, lib, linker, user 등이 올 수 있다. \\#pragma comment( linker, \"/subsystem:windows\" ) \\#pragma comment( linker, \"/subsystem:console\" ) linker 를 사용하면 프로젝트를 console application인지 win32 application인지 명시해줄 수 있다. 또한 섹션의 설정을 할 수 있다. \\#pragme comment( linker, \"SECTION:.SHAREDATA,RWS\" ) \\#pragma data\\_seg(\"SHAREDATA\") 와 함께 사용하여 공유 메모리를 생성한다. 위의 명령어 대신 def 파일 안에 아래와 같이 해주어도 된다. SECTIONS SHAREDATA READ WRITE SHARED 이 중 가장 대표적인 사용법은 명시적인 라이브러리의 링크이다. \\#pragma comment(lib, \"xxxx.lib\") 와 같이 사용하여 해당 라이브러리를 링크시켜 준다. 여러사람이 같이 수행하는 프로젝트의 경우 이와 같은 방법을 사용하여 lib를 링크하는 것이 라이브러리가 링크되어있다는 사실을 알기에도 좋고 굳이 주석다라 설명할 필요도 없어 좋지 않나 싶다. (있다는 사실은 알지만 아직 프로젝트 수행중 실제로 사용해 본적은 없음) \\#pragma data\\_seg() pragma data\\_seg()의 형식은 다음과 같다. \\#pragma data\\_seg( \\[\"section-name\"\\[, \"section-class\"\\] \\] ) \\#pragma data\\_seg( \"SHAREDATA\" ) int x; char y; \\#pragma data\\_seg() DLL 파일을 만들어보면서 제일 많이 사용해 보았고 가장 헷갈려 했던 부분이기도 하다. DLL의 데이터 공유를 하기 위해 사용한다. 공유할 섹션을 만드는 것이다. 위의 명령어는 필수적으로 위에서 사용된 두 가지중 한가지 방법과 함께 사용 되어야 한다. \\#pragme comment( linker, \"SECTION:.SHAREDATA,RWS\" ) SECTIONS SHAREDATA READ WRITE SHARED 둘 다 해당 SECTION(SHAREDATA)의 허용 범위(?속성?)를 설정하는 것이다. READ, WRITE, SHARED 세 가지를 쓴다는 의미~ 해당 사항에 대해 msdn에서 자세한 정보를 발견하지 못해 적지 못하였다(검색능력의 부족!!) 이제 변수 x와 y는 해당 dll을 사용하는 외부 파일과 같이 공유할 수 있는 변수가 되었다.(외부에서 접근 가능하게 되었다.) 이렇게 공유하는 변수는 물론 new로 메모리를 할당한 변수도 공유 가능하다. 특히 new 나 memalloc(이건 아직 미확인이지만 같은 메모리 할당이므로 가능할 것으로 본다)으로 메모리할당한 변수들은 dll외부에서도 해제(delete) 가능하다. \\#pragma warning 특정 경고를 끄고 싶을 때 사용한다. 비쥬얼 스튜디오의 버전이 다르기 때문에 뜨는 경고는 더더욱이 귀찮은 존재이다.(하지만 수정해서 손해볼 것은 없다. 그것이 곧 버그로 이어질 수 있기 때문이다. 특히 형변환의 경우 강제 캐스팅하여 확실히 명시해주는 것이 좋다. 일부러 그 값을 떼어낸다는 프로그래머의 의지를 컴파일러에게 보여주자. 부지런할수록 후에 손이 가는 일이 적어진다. 노력하자~) 형식은 이와 같다. \\#pragma warning( warning-specifier : warning-number-list \\[; warning-specifier : warning-number-list...\\] ) \\#pragma warning( push\\[ ,n \\] ) \\#pragma warning( pop ) 실제 사용은 아래와 같이 한다. \\#pragma warning( disable:4996 ) \\#pragma message() 컴파일 중에 메세지를 뿌려준다. 말이 필요없다-.-/ \\#pragma message(\"merong\") Takes from: http://mayu.tistory.com/8 선행처리기중의 하나인 pragma에 관한 사용법을 정리하여 올립니다. 문법은 다음과 같습니다. \\#pragma directive-name \\#pragma는 이것을 지원하는 다른 compiler에서 방해가 없이 C++ Builder에서 원하는 지시어를 정의할 수 있도록 해줍니다. 만일 지시명을 인식하지 못한다면 에러 또는 경고 메세지를 수반하지 않고서 \\#pragma의 지시를 무시하게 됩니다. Borland C++ Builder에서 지원하는 \\#pragma지시어는 모두 18가지가 있습니다. 이제부터 그것들을 하나 하나 살펴보기로 하겠습니다. 1\\. \\#pragma anon\\_struct . 사용법 \\#pragma anon\\_struct on \\#pragma anon\\_struct off . Class에 익명의 구조체를 포함하여 compile하는것을 허락할 것인지를 지시합니다. 익명이란 tag를 갖지 않는다는것을 의미합니다. ex) \\#pragma anon\\_struct on struct S { int i; struct { // 익명구조체를 포함한다. int j ; float x ; }; class { // 익명 클래스를 포함한다. public: long double ld; }; S() { i = 1; j = 2; x = 3.3; ld = 12345.5;} }; \\#pragma anon\\_struct off void main() { S mystruct; mystruct.x = 1.2; // 포함된 data에 값을 할당한다. } //-------------------------------------------------------------------------- 2\\. \\#pragma argsused . argsused 프라그마는 함수 정의 사이에서만 허용되고 바로 다음 함수에만 영향을 미치며 경고 메세지를 disable시킵니다. 이 pragma를 사용하지 않은 경우 사용되지 않은 argument가 있으면 \"Parameter name is never used in function func-name\" 라는 경고 메세지를 표시하게 됩니다. ex) \\#pragma argsused void \\\\fastcall TImageForm::FileEditKeyPress(TObject\\* Sender, Char &Key) { if (Key == 0x13) { FileListBox1->ApplyFilePath(FileEdit->Text); Key = 0x0; } } 위의 예에서는 함수내에서 Sender라는 인수가 사용되지 않았지만 경고 메세지가 표시되지 않습니다. //-------------------------------------------------------------------------- 3\\. \\#pragma codeseg . 사용법 \\#pragma codeseg . codeseg 프라그마는 함수들을 위치시킬 group, class 또는 segment의 이름을 줄수 있도록 지시합니다. 만일 option없이 사용하였다면 함수의 배치를 위해서 default code segment가 사용되어질것입니다. 결국 이 pragma를 사용하지 않는 경우와 동일한 결과를 가져옵니다. //-------------------------------------------------------------------------- 4\\. \\#pragma comment . 사용법 \\#pragma comment (comment type, \"string\") . comment 프라그마는 출력되어지는 file에 주석을 기록시킬것을 지시합니다. comment type에 올수 있는 값들은 다음중의 하나가 될것입니다. \\* exestr linker가 \".OBJ\" file에 string을 기록합니다. 이렇게 기록된 string은 실행파일내부에 기록되어지며, 이것은 결코 메모리로 load되지 않습니다. 하지만 적당한 파일 검색 유틸리티를 사용하여 실행파일에서 string을 찾아볼 수 있습니다. \\* lib \".OBJ\" file에 주석의 내용을 기록합니다. library에 새로운 module을 추가하는 경우 에만 comment 프라그마를 사용하여 linker에게 결과 file에 명시할 수 있도록 지시할 수 있습니다. 다시 말하면 기존에 작성되어진 module에는 comment 프라그마를 사용하여 string을 추가 시킬수 없습니다. 새롭게 library를 작성한다면 예외일 수 있겠지요. linker는 최종의 library에서 string에 명시된 library module 이름을 포함 합니다. 여러개의 module들도 이름지어질 수 있으며 이름을 만들기 위하여 linke되어집니다. 예) comment ( lib, \\* ) : comment로 사용할 수 있는 명령은 여러 개 있는데, 그중 가장 대표적인 것이 lib 으로, 해당 라이브러리를 링크시켜준다 . 즉, 지정된 라이브러리 화일을 포함하여 컴파일한다. 프로젝트 설정에 라이브러리를 포함하는 것과 같다. 예2) comment( lib, \"ws2\\_32\" ) 이것은, 컴파일시 ws2\\_32.lib 파일을 링크하라는 명령입니다. 보통 Visual studio같은 IDE 개발환경에서는, 프로젝트 셋팅에서 해주지만, 혹 그런부분을 빼먹거나 환경이 바뀔때를 대비해서 이렇게 해두면 편하죠. 예3) \\#pragma comment( \"comment-type\" \\[, commentstring\\] ) comment type에는 compiler, exestr, lib, linker, user 등이 올 수 있습니다. 그 중 질문에서의 lib는 library file을 지정하는 것이라고 생각하시면 됩니다. comment string이라는 부분에는 file의 이름이나 path를 넣으시면 됩니다. \".lib\" file 아시죠? 그러니까 굳이 project settings에서 link tab에 있는 input에 .lib file을 쓰지 않고 \\#pragma comment ( lib, \"xxx.lib\" ) 라고 써도 된다는 거죠.. \\* user compiler는 \".OBJ\" file에 string을 기록합니다. 하지만 linker에 의해 string은 무시되어집니다. object 파일에만 그 내용이 남게 됩니다. //-------------------------------------------------------------------------- 5\\. \\#pragma exit . 사용법 \\#pragma startup function-name \\#pragma exit function-name . 이들 두 프라그마는 프로그램이 프로그램 시동시(main이 호출되기 전) 호출 되어야 할 함수와 프로그램 탈출(프로그램이 \\_exit를 통해 종료하기 바로 전) 을 명시할 수 있도록 합니다. 명시된 function-name은 반드시 인수를 취하지 않고 void를 return하는 미리 선언된 함수여야합니다. 다시 말하면 다음과 같이 선언될 수 있습니다. void func (void); priority는 반드시 64-255의 범위 내에 있는 정수여야하며 최상의 우선권은 0입니다. 0-63사이의 priority는 C library에서 사용하므로 사용자가 이를 사용해서는 안됩니다. 최상위 우선권을 가진 함수는 시동시에 맨 먼저 호출 되고 탈출시에 맨 마지막으로 호출됩니다. 우선권을 명시해 주지 않을 경우 기본적으로 100의 우선권을 갖게 됩니다. pragma startup 또는 exit에 사용된 함수명은 반드시 프라그마 라인에 도달하기 전에 정의(또는 선언)되어야함에 주의하십시요. ex) \\#include void startFunc(void) { printf(\"Startup Function.\\\\n\"); } \\#pragma startup startFunc 64 //우선권 64로 시동시에 맨 먼저 호출됩니다. void exit Func(void) { pirntf(\"Wrapping up execution.\\\\n\"); } \\#pragma exit exitFunc //기본적으로 우선권이 100으로 지정됩니다. void main(void) { printf(\"This is main.\\\\n\"); } //-------------------------------------------------------------------------- 6\\. \\#pragma hdrfile . 사용법 \\#pragma hdrfile \"filename.CSM\" . 이 지시어는 프리컴파일된 헤더를 저장할 파일의 이름을 설정합니다. IDE 프로젝트를 위한 디폴트 파일명은 .CSM이고 command line용 으로는 BC32DEF.CSM이라는 이름을 갖습니다. 프리컴파일된 헤더를 사용하지 않으면 이 지시어는 효력이 없으며 명령라인 컴파일러 옵션 -H=filename 또는 프리 컴파일된 헤더를 사용하면 프리 컴파일된 헤더를 저장하기 위해 사용되는 파일명을 변경할 수 있습니다. 명령라인 옵션은 다음과 같습니다. \\* 프리컴파일드 헤더를 사용하는 경우 H=filename \\* 프리컴파일드 헤더를 사용은 하지만 새로운 프리컴파일드 헤더파일을 변환하지 않는 경우 Hu \\* 프리컴파일드 헤더를 사용하지 않거나 새로운 프리컴파일드 헤더파일을 변환하지 않는 경우. (기본값) H- //-------------------------------------------------------------------------- 7\\. \\#pragma hdrstop . 사용법 \\#pragma hdrstop . 이 지시어는 프리컴파일에 적합한 헤더 파일의 목록을 종료시키는데, 이것을 사용하면 프리컴파일된 헤더가 사용하는 디스크 공간의 양을 줄일 수 있습니다. 프리컴파일드 헤더파일은 \\#pragma hdrstop이 선언되기 전에 \\#include를 사용하여 포함된 헤더파일들을 동일하게 프로젝트 내의 source들간에 공유시킬 수 있습니다. 그러므로 \\#pragma hdrstop전에 일반적인 헤더파일들을 포함하면 최상의 콤파일러의 성능을 얻을 수 있습니다. 확실하게 \\#pragma hdrstop 전에 \\#include를 사용한다면 모든 source file들에게 동일하게 적용되거나 아주 조그마한 변화만이 있을 것입니다. IDE 환경에서는 강화된 프리컴파일드 헤더의 성능을 가지는 코드로 변환합니다. 예를 들자면 다음의 New Application의 소스 파일인 \"Unit1.cpp\"는 다음과 같이 될것입니다. \\#include // 일반적인 헤더파일 \\#pragma hdrstop // 헤더파일의 리스트는 여기서 끝난다. \\#include \"Unit1.h\" // 헤더파일의 명시 //.... 이 pragma 지시어는 오직 source file에서만 사용하며, 헤더파일에서 사용했다 면 아무런 효과도 없을 것입니다. //-------------------------------------------------------------------------- 8\\. \\#pragma inline . 사용법 \\#pragma inline . 이 지시어는 명령 라인 콤파일러 옵션 -B 또는 IDE의 인라인 옵션과 동일 합니다. 이것은 컴파일러에게 프로그램 내에 인라인 어셈블리 언어 코드가 있음을 알려줍니다. 컴파일러는 \\#pragma inline을 만날때 -B옵션을 사용하여 스스로 재시동하므로 이 지시어는 파일의 상단에 배치되는 것이 최선입니다. 실제로 -B옵션과 \\#pragma inline을 모두 off시켜둘 수 있습니다. 그러면 컴파일러는 asm문을 만나자마자 스스로 재시동합니다. 이 옵션과 지시어의 목적은 컴파일 시간을 다소 절약하는 것입니다. //-------------------------------------------------------------------------- 9\\. \\#pragma intrinsic . 사용법 \\#pragma intrinsic \\[-\\]function-name . \\#pragma intrinsic를 사용하면 함수의 inline화를 위해 command-line 스위치나 IDE의 옵션이 무시되어집니다. intrinsic함수를 인라인화할 때는 그 함수를 사용하기 전에 반드시 그것을 위한 원형을 포함시켜야만 합니다. 이것은 인라인화 할 때 컴파일러가 인라인화한 함수를 내부적으로 인식하는 함수로 개명하는 매크로를 실제로 생성하기 때문입니다. 가령 strcpy 함수를 인라인 화 하기 위하여 다음과 같은 문장을 사용하였다면 \\#pragma intrinsic strcpy 컴파일러는 다음과 같은 매크로를 생성합니다. \\#define strcpy \\\\strcpy\\\\ 컴파일러는 두 선행 밑줄과 두 후미 밑줄을 사용하여 함수 호출을 인식하고 그 함수의 원형을 내부적으로 저장해 둔 원형과 부합시키려 합니다. 그러므로 원형을 공급하지 않거나 공급한 원형이 콤파일러 내부의 원형과 부합되지 않을 경우, 콤파일러는 그 함수를 인라인화 하려는 시도를 불식시키고 에러를 발생시킵니다. 이 프라그마 사용의 궁극적인 목적은 함수 호출에 대한 오버헤드를 줄위기 위한것입니다. 함수호출은 빨라지겠지만 그만큼 크기는 증가하게 될것입니다. ex) \\#pragma intrinsic strcpy \\#pragma intrinsic -strcpy //-------------------------------------------------------------------------- \\#pragma link . 사용법 \\#pragma link \"\\[path\\]modulename\\[.ext\\]\" . 이 지시어는 실행화일에 파일을 링크시킬것을 링커에세 지시합니다. 기본적으로 링커는 -L옵션으로 지정된 패스와 로칼 디렉토리에서 modulename을 찾습니다. path 아규먼트를 이용하여 디렉토리를 지정할 수도 있습니다. 또한 링커는 확장자를 \".obj\"를 기본으로 간주합니다. //-------------------------------------------------------------------------- \\#pragma message 컴파일 도중에 지정된 내용을 VC의 아웃풋 윈도우에 출력시켜 준다. 컴파일시 특정 문장을 표시 . 사용법 \\#pragma message (\"text\" \\[\"text\"\\[\"text\" ...\\]\\]) \\#pragma message text . \\#pragma message는 프로그램 코드 내에서 사용자 정의 메세지를 명시합니다. 첫번째 형식은 하나 이상의 스트링 상수들로 구성된 문장을 필요로 하고 메세지는 괄호안에 싸여있어야만 합니다.(이 형식은 MSC와 호환됩니다.) 두번째 형식은 경고 메세지의 문장을 위해 \\#pragma에 연속되는 문장을 사용합니다. \\#pragma의 두가지 형태와 함께 다른 메크로의 참조는 메세지가 디스플레이 되기전에 확장되어집니다. 사용자 정의 메세지가 디스플레이 되는것은 기본치이며 명령 라인 옵션의 Show Warnings를 사용하여 on/off 시킬 수 있습니다. 이 옵션은 콤파일러의 -wmsg에 해당합니다. ex) // msacm.h \\#if defined(UNICODE) && !defined(\\_UNICODE) \\#ifndef RC\\_INVOKED \\#pragma message(\"MSACM.H: defining \\_UNICODE because application defined UNICODE\") \\#endif \\#define \\_UNICODE \\#endif // ustring.h \\#pragma message osl/ustring.h has been replaced by winsys/string.h \\#include //-------------------------------------------------------------------------- \\#pragma obsolete . 사용법 \\#pragma obsolete identifier . \\#pragma obsolete 프로그램 코드에서 pragma의 선언 이후에 마주치게 되는 identifier의 첫번째 사용에 대해서 경고를 발생합니다. 경고는 identifier를 쓸모없는 상태로 만듭니다. ex) // io.h \\#if !defined(RC\\_INVOKED) /\\ Obsolete functions \\/ \\#pragma obsolete \\_chmod \\#pragma obsolete \\_close \\#pragma obsolete \\_creat \\#pragma obsolete \\_open \\#pragma obsolete \\_read \\#pragma obsolete \\_write /\\ restore default packing \\/ \\#pragma pack(pop) \\#if defined(\\\\STDC\\\\) \\#pragma warn .nak \\#endif \\#endif /\\ !RC\\_INVOKED \\/ //-------------------------------------------------------------------------- \\#pragma option . 사용법 \\#pragma option options \\#pragma option push options \\#pragma option pop . \\#pragma option은 프로그램 원시 코드 내에 명령라인 옵션을 포함시키고자 할 때 사용하며 push 또는 pop 옵션과 함께 사용되어질 수 있습니다. options는 임의의 명령라인 옵션(단, 아래에 수록된 것은 제외합니다.)이며 하나의 지시어 내에서 여러개의 option들을 나타낼 수 있습니다. 예를 들자면 다음과 같습니다. \\#pragma option -C \\#pragma option -C -A toggle option(-a, -K같은)은 comman line에서 on/off될수 있습니다. 이들 toggle option들은 option 다음에 마침표를 두면 그 명령라인, 구성 파일, 옵션 메뉴 설정값에 대해 옵션을 리털할 수 있으며 이를 이용하면 정확한 설정값을 기억하지 않고도(혹은 알 필요가 없거나) 옵션을 임시로 변경했다가 다시 그것을 디폴트로 복귀시킬 수 있습니다. pragma optino에 포함하여 나타날 수 없는 옵션들은 다음과 같습니다. B -c -dname Dname=string -efilename -E Fx -h -lfilename lexset -M -o P -Q -S T -Uname -V X -Y 다음의 경우에 \\#pragmas, \\#indluces, \\#define과 약간의 \\#ifs를 사용할 수 있습니다. \\* \\#if, \\#ifdef, \\#ifndef 또는 \\#elif지시어 내에서 두 밑줄로 시작하는 매크로명 (그리고 그에 따른 내장 매크로도 가능합니다.)의 사용 전. \\* 첫번째 실재 token이 발생하기 전(첫번째 C 또는 C++ 선언문) 특정 명령 라인 옵션은 이들 사건 앞의 \\#pragma option 내에서만 나타날 수 있는데 그러한 option들은 다음과 같습니다. Efilename -f -i\\# m\\* -npath -ofilename u -W -z 다른 option들은 어디서나 변경될 수 있는데 다음 option들은 함수 또는 대상 선언문 사이에서 변경될 경우 컴파일러에만 영향을 미칩니다. 1 -h -r 2 -k -rd a -N -v ff -O -y G -p -Z 다음의 option들은 언제든지 변경될 수 있으며 즉시 영향을 미칠 수 있습니다. A -gn -zE b -jn -zF C -K -zH d -wxxx 이들 option들은 그 명령 라인 상태로 재설정하기 위해 점(.)앞에 추가로 나타날 수 있습니다. push 또는 pop을 사용한 \\#pragma option 18:41, 21 February 2007 (PST)18:41, 21 February 2007 (PST)18:41, 21 February 2007 (PST)18:41, 21 February 2007 (PST)18:41, 21 February 2007 (PST)18:41, 21 February 2007 (PST)18:41, 21 February 2007 (PST)~~ 또한 콤파일러 지시어들을 쉽게 변경할 수 있도록 push 그리고 pop 아규먼트들 과 함께 \\#pragma option 지시어를 사용할 수도 있습니다. 잠재적으로 많은 컴파일러 옵션과 경고들을 변경하는 파일들을 포함하기 위해 \\#pragma option push를 사용할 수 있고, \\#pragma option pop은 단일 문장으로서 이전의 상태를 되돌려준다. 예를 들자면 다음과 같다. \\#pragma option push \\#include \\#pragma option pop \\#include \"mystuff.h\" \\#pragma option push 지시어는 첫번째로 모든 콤파일러 옵션들과 경고 설정들을 스택에 push한 후에 다른 옵션들이 존재한다면 이를 처리한다. 다음의 예는 \\#pragma option push가 옵션들을 사용하거나 혹은 그렇지 않을수 있음을 보여줍니다. \\#pragma option push -C -A \\#pragma option push \\#pragma option pop directive은 스택으로부터 옵션들과 경고들의 마지막 설정 을 pop함으로서 컴파일러 옵션과 경고들을 변경합니다. 만일 스택이 비어있고 option pop과 일치하는 option push가 없으며 아무것도 발생하지 않은경우 경고가 주어집니다. 다음은 빈 스택에대해서 경고를 발생시킵니다. \\#pragma option push \\#pragma option pop \\#pragma option pop /\\* 경고가 발생합니다. 권장하지는 않지만 지시어를 사용하여 이 경고를 off시킬 수 있습니다. \\#pragma warn -nop. 만일 pop의 다음에 어떤 옵셥들을 명시할려고 한다면 에러가 발생하게되며 pragma option pop 다음에는 어떤것도 허락하지 않습니다. 예를 들면, 다음은 에러를 발생합니다. \\#pragma option pop -C /\\* ERROR 만일 push된 옵션들의 스택이 파일의 시작과 마지막이 동일하지 않다면 다음과 같은 경고메세지가 발생합니다. Previous options and warnings not restored. 이 경고메세지를 off시키기 위하여 지시어 \\#pragma nopushoptwarn를 사용할 수 있습니다. //-------------------------------------------------------------------------- 14\\. \\#pragma pack 변수 정렬을 인위적으로 변경시킨다.(보통은 4바이트로 지정되어 있다.) . 사용법 \\#pragma pack(n) 위에서, n의 값으로, 1,2,4,8등이 올수 있으며, 특히 네트웍통신쪽을 개발할때 구조체의 멤버들 align할때 사용하는 것으로서, 빈번하게 사용됩니다. 구조체 정렬부분은 중요하지만, 여기서는 그쪽까지 언급하기에는 양이 많아서 여기까지만 설명함. \\#pragma pack(push, n) \\#pragma pack(pop) . \\#pragma pack 지시어는 콤파일러 옵션 -a와 함께 \\#pragma option을 사용하는 것과 동일합니다. n은 콤파일러가 저장된 메모리에 데이터를 정렬하는 방법을 결정하는 byte의 정렬이다. 보다 자세한 사항은 -a 콤파일러 옵션에 관한 내용을 참고하십시요. \\#pragma pack은 또한 \\#pragma option지시어에 push나 pop을 사용하는것과 동일한 기능을 제공 하도록 push나 pop 아규먼트와 함께 사용할 수 있습니다. 아래의 내용은 \\#pragma pack과 \\#pragma option을 비교한 내용입니다. ━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━ \\#pragma pack ┃ \\#pragma option ━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━━ \\#pragma pack(n) ┃ \\#pragma option -an \\#pragma pack(push, n) ┃ \\#pragma option push -an \\#pragma pack(pop) ┃ \\#pragma option pop ━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━━━━ 예) \\#pragma pack(push, before\\_pack, 1) 구조체 정렬을 1 바이트로 만든다. 1 바이트로 맞추기 전의 정렬 기준을 before\\_pack에 저장한다 \\#pragma pack(pop, before\\_pack) 위와 같이 한쌍으로 사용된다. before\\_pack에 저장된 정렬 기준으로 복원한다. 다음과 같이 사용할 수도 있다. \\#pragma pack(push, 1) \\#pragma pack(pop) 다음과 같이 복원없이 지정할 수도 있다. \\#pragma pack(1) 예) \\#pragma pack(pop) 이라...아마 이거랑 쌍이 되는게 있을텐데요 \\#pragma pack(push,1) 이라던지요. 이건 구조체를 1바이트로 바이트 정렬하는 겁니다. 디폴트로는 8바이트로 되어 있을겁니다. 뭐 구조체 크기가 실제로 계산한 것과 맞지 않는 경우가 있는데..이 바이트 정렬 떄문입니다. 따라서..반드시 맞아야 할 경우는... 위의 코드를 구조체 아래위로 써야 합니다. //-------------------------------------------------------------------------- \\#pragma package . 사용법 \\#pragma package(smart\\_init) \\#pragma package(smart\\_init, weak) . smart\\_init 아규먼트 \\#pragma package(smart\\_init)는 패키지된 유닛들이 의존할 순서를 결정하기위해 초기화 되어집니다.(패키지 소스파일 내에 기본적으로 포함됩니다.) 일반적으로, 패키지들을 생성하는 .CPP 파일들에 \\#pragma package를 사용할 수 있습니다. 이 프라크마는 유닛을 콤파일하는기위한 초기의 순서에 영향을 미칩니다. 초기화는 다음의 순서에 의하여 발생합니다. 만일 unitA가 unitB에 의존한다면 unitB는 반드시 unitA전에 초기화 되어져야만하는 사용(\"uses\")에 의존합니다. 링크의 순서(The link order) unit에서의 우선권의 순서.(Priority order within the unit.) 보통의 .OBJ 파일들은(unit들로 생성하지 않은), 첫째로 우선권의 순서에 따라 초기화가 일어나고서 링크가 됩니다. .OBJ 파일들의 링크 순서의 변경은 글로발 오브젝트가 호출되어져 생성되는 순서에 의해 변경됩니다. 다음의 예는 보통의 .OBJ 파일들과 unit들의 초기화에 어떤 차이점이 있는가를 보여줍니다. 세개의 unit 파일들 A,B,C가 \\#pragma package(smart\\_init)를 사용하여 \"smart initialized\"되고 우선권은 10, 20, 30의 값을 갖는다고 예를 듭니다. 함수는 우선권의 값과 parent .OBJ에 의하여 이름지어져 a10, a20, a30, b10등과 같은 이름을 갖습니다. 세가지는 모두 unit들이며 A는 B와 C를 사용하며 A,B,C의 순서로 링크되고 초기화의 순서는 다음과 같습니다. B10 B20 B30 C10 C20 C30 A10 A20 A30 위와 같이 되었다면 .OBJ 파일들은 (unit들이 아니다)다음의 순서가 되어질 것입니다. A10 B10 C10 A20 B20 C20 A30 B30 C30 \\#pragma package(smart\\_init)를 사용한 .CPP 파일들은 또한 \\#pragma package (smart\\_init)를 정의한 .CPP 파일로부터 다른 .OBJ 파일들을 참조하는 \\#pragma link를 필요로 하며 unit에 의해 결정되어져야만 합니다. \\#pragma link는, 결정 되지 않은 .OBJ는 라이브러리 등에 의하여 여전히 결정되어질 수 있도록 참조 할 수 있습니다. . weak packages \\#pragma package(smart\\_init, weak)지시어는 .OBJ 파일이 패키지의 .BPI와 .BPL 파일들에 정장되는 방법에 영향을 미칩니다. 만일 \\#pragma package(smart\\_ init, weak)가 unit파일 내에 나타난다면 콤파일러는 가능하다면 BPL들로부터 unit을 생략하고, 다른 에플리케이션이나 패키지에 의해 필요로 할 때면 비 패키지화된(non-packaged) 로칼 복사본의 unit을 생성합니다. 유닛이 이 지시어와 함께 콤파일 되었다는 것은 약하게 패키지화 되었음을 이야기 합니다. (\"weakly packaged\") \\#pragma package(smart\\_init, weak)는 동일한 외부 라이브러리(external librar y)들에 의존할수 있는 여러 패키지들 사이에서의 충돌을 제거하는데 사용되어 집니다. \\#pragma package(smart\\_init, weak) 지시어를 가지는 unit 파일들은 글로발 변수들을 갖지 않아야 합니다. //-------------------------------------------------------------------------- \\#pragma resource . 사용법 \\#pragma resource \"\\*.dfm\" . 이 프라그마는 form unit에 의해 선정되어지는 파일로서 일치되는 .DFM 파일과 헤더파일을 필요로 합니다. 이러한 모든 파일들은 IDE에 의해 관리되어집니다. 만일 폼을 위한 다른 변수들을 필요로한다면 pragma resource가 사용되어지고난 후에 즉시 선언되어져야만 합니다. 선언은 반드시 form이 되어져야만 합니다. TFormName \\*Formname; //-------------------------------------------------------------------------- \\#pragma startup . 사용법 \\#pragma startup function-name \\#pragma exit function-name . \\#pragma exit의 내용을 참조하십시요. //-------------------------------------------------------------------------- \\#pragma warn . 사용법 \\#pragma warn \\[+:-:.\\]www . warn지시어를 이용하면 특정 명령라인 옵션 -wxxx를 우선할 수 있습니다. \\#pragma warn -aus 스위치를 사용하면 함수 단위로 취급됩니다. 개별적인 변수들을 위해서 함수 내부에서 경고를 off시킬수는 없습니다. 함수 전체를 off시키거나 혹은 그렇지 않거나 둘중 하나입니다. ex) \\#pragma warn +xxx \\#pragma warn -yyy \\#pragma warn .zzz 위의 예에서는 xxx경고문은 on되고 yyy경고문은 off되며 zzz경고문은 파일의 컴파일이 시작할 때 갖고 있던 값으로 재저장됩니다. //----- End of Document ---------------------------------------------------- 19\\. \\#pragma once 한번 컴파일 되면 더 이상 컴파일 하지 않는다는 뜻입니다. 여러개의 cpp파일이 있을때, 하나의 cpp화일이 수정되면, 그 화일만 컴파일하고 나머지는 하지말아란 뜻이죠. ( 여러 번 인클루드 되는 것을 컴파일러 차원에서 막아줌 ) \\#ifndef ~ \\#endif 와 같은 역할을 한다. 매크로들의 중복 정의를 막고, 전역변수에 static 키워드를 써줄 필요가 없어짐. VC++ 에서는 되고, 다른 컴파일러는 안될 수도 있음. 분할 컴파일시 이것을 사용하면 빌드시 컴파일러에 의해 해당 파일을 단 한번만 열게 된다... 그러므로 컴파일 타임을 줄일 수 있고 모듈에서 가장 먼저 나온 \\#include문에서 해당 파일을 열게 되므로 재정의에 의한 오류를 방지할 수 있다... 예제) // exam.h file \\#ifndef \\\\EXAM\\\\ \\#define \\\\EXAM\\\\ ... \\#endif \\-> // exam.h file \\#pragma once ... //-------------------------------------------------------------------------- 20\\. \\#pragma data\\_seg dll에서 데이터 공유하고자 할 때 쓰임 예) 실행파일에는 코드영역과 데이터영역이 구분되어 있다는 것은 아시지요. ┌───────┐ │ │ │ │ │ 데이터영역 │ │ │ │ │ ├───────┤ │ │ │ │ │ 코드 영역 │ │ │ │ │ └───────┘ 이런 식의 그림은 아마 어디선가 많이 보셨을겁니다.(그림이 깨지네요.편집할 땐 제대로 보였는데...) 이런 영역을 section이라고 하지요. 위의 설명에 나오는 section이라는 용어가 이것입니다. 실제로 데이터 영역과 코드 영역외에도 exe나 dll에는 여러 section을 포함할 수 있습니다. 이건 dumpbin이라는 툴로 살펴볼 수 있습니다. 제 컴(Windows XP)에서 dumpbin c:\\\\windows\\\\notepadd.exe 를 해 보았더니 2000 .data 2000 .rsrc 7000 .text 이렇게 나오는 군요. 여기서 .data에는 초기화된 데이터가 .rsrc에는 리소스들이, .text에 코드가 들어갑니다. 이러한 .data, .rsrc, .text 등은 일반적으로 정해져 있는 것들입니다. 위 MSDN의 설명에 있는 section-name이라는 것이 바로 .data, .rsrc, .text 등을 뜻하는 겁니다. 즉, \\#pragma code\\_seg( .data ) 처럼 사용한다는 거지요. 그리고 Specifies a code section where functions are to be allocated.라는 설명은 Specifies a section where functions or data or etc. are to be allocated. 이렇게 이해하면 더 나을 듯 하네요. 그런데 이런 정해진 이름말고도 사용자가 새로운 영역을 정할 수 있습니다. \\#pragma data\\_seg(\"Shared\") DWORD g\\_dwThreadIdPMRestore = 0; HWND g\\_hwnd = NULL; \\#pragma data\\_seg() 이런 식으로 하면 g\\dwThreadIdPMRestored와 g\\hwnd가 디폴트 데이터 섹션인 .data에 배치되지 않고, Shared라는 이름으로 만들어진 섹션에 배치되는 것입니다. //-------------------------------------------------------------------------- \\#pragma warning 컴파일시에 어떤 조건(\\#if, \\#ifndef)에의해 개발자에게 어떤것을 알려주고 싶을 경우 사용. 예) \\#pragma warning(disable:xxxx) 지정된 xxxx번대의 경고 메세지를 디스플레이하는 것을 막는다. (xxxx는 번호) warning( disable : 4705 ) : 특정 warnning 을 체크하지 않음, 이럴 경우 4705번 warnning은 나타나지 않는다 \\#pragma warning(default:xxxx) 지정된 xxxx번의 경고 메세지의 설정을 원래의 프로젝트 설정으로 복원한다 //-------------------------------------------------------------------------- 22\\. \\#pragma code\\_seg MSDN에 있는 내용 \\#pragma code\\_seg( \\[\"section-name\"\\[,\"section-class\"\\] \\] ) Specifies a code section where functions are to be allocated. The code\\seg pragma specifies the default section for functions. You can, optionally, specify the class as well as the section name. Using \\#pragma code\\seg without a section-name string resets allocation to whatever it was when compilation began. //-------------------------------------------------------------------------- 23\\. \\#pragma deprecated C\\#의 Obsolete attribute와 비슷한 의미입니다. 즉, 경고가 발생한 클래스 혹은 메서드 등이 이후에는 지원되지 않음을 나타내는 의미입니다. 그러므로 당장은 문제가 없습니다. 컴파일러 specific 한..그런 옵션 지시자라고 보시면 됩니다. //-------------------------------------------------------------------------- \\#pragma는 표준 C/C++ 문법인데 각 compiler 마다 다른 명령을 제공한다... 고로 Linux나 Unix의 cc에서는 작동하지 않을 수도 있다는 것! (Visual C++에선 언제나 call...) pragma 는 \\#로 시작하는 전처리구문 지시자 중 컴파일러에 종속적인 명령으로, 컴파일러에 특정한 옵션 명령을 내리기 위해 사용한다. 이것은 컴파일러에 종속적이기 때문에 컴파일러를 변경했을 경우 실행을 보장하지 못한다. pragma 의 의미를 action 으로 많이들 알고 계시는데, 인터넷에서 제가 알아본바로는, 사전적인 의미는 \"만능\" 입니다. 어원설명 원문은, 아래를 참고하세요. A pragma (from the Greek word meaning action) is used to direct the actions of the compiler in particular ways, but has no effect on the semantics of a program (in general). Pragmas are used to control listing, to define an object configuration (for example, the size of memory), to control features of the code generated (for example, the degree of optimization or the level of diagnostics), and so on. Such directives are not likely to be related to the rest of the language in an obvious way. Hence the form taken should not intrude upon the language, but it should be uniform. Thus, the general form of pragmas is defined by the language. They start with the reserved word pragma followed by a pragma identifier, optionally followed by a list of arguments enclosed by parentheses, and terminated by a semicolon. The overall syntax of the pragma identifier and arguments is similar to that of a procedure call. Pragmas are allowed at places where a declaration or a statement is allowed; also at places where other constructs that play the role of declarations (for example clauses) are allowed. pragma 는 컴파일에게 그 뒤에오는 내용에 따라 어떤일을 하라는 전처리명령입니다. C++는 컴파일하고 나면 함수의 이름이 바뀌게 되는데 이것을 name mangling이라고 합니다. 그래서 만일 dll로 작성한 함수를 불러 사용하게 되면 같은 이름을 찾을 수 없다는 오류가 나게 됩니다. 만일 dll에서 printString라는 함수를 만들었다고 가정을 합니다. 그러면 컴파일하고 난 후의 함수의 이름은 printString@@YAXXZ와 같은 형태로 만들어 집니다. 그런데 이 함수를 불러 사용하는 곳은 printString이라는 것만 알지 위의 것과 같은 알지 못합니다. 그래서 \\#pragma라는 키워드를 사용해서 name mangling을 방지하게 되는 것입니다. 이 mangling된 이름을 찾기 위해서는 VC++ Tool에 보면 depends를 실행하고 만들dll을 drag&drop하면 이것을 찾을 수 있습니다. pragma comment(linker, \"/export:printString=?printString@@YAXXZ\")와 같이사용하면됩니다. 즉 \\#pragma는 원래의 함수 이름을 c++의 암호명에 대한 별칭으로 추가하도록 지시하며 링커에게 /export옵션을 전달해 주는것입니다. 그리고 dll에서는 원래의 함수를 export(수출??말이 좀 이상하지만 대부분 이렇게 많이 쓰니까..^^)해야 하고 이 함수를 호출하는 쪽에서는 import(수입) 옵션을 써 주어야만 함수를 제대로 호출할 수 있습니다. pragma 앞에 \\#이 있는 걸 보면 아시겠지만 pragma는 precompiler입니다. compile할 때 platform이 틀려지거나 cpu가 틀려지거나 할 때 compile option을 주게 됩니다. vc++을 써보셨으면 아실텐데, project settings( ALT+F7 )에서 c/c++ tab에 보면 project options이 있습니다. link tab에도 project options가 있죠. pragma가 바로 그런 역할을 하는 precompiler입니다. vc++이야 ide니까 project settings라는 편한 환경을 지원하지만 만약 code호환성을 생각한다면 pragma를 쓰는 게 좋죠. \\#pragma warn- // warning 디스어블 pragma warn+ // warning 인에이블 pragma opt- // 최적화 안 함 pragma opt+ // 최적화 함 pragma savereg- // 레지스터 저장 안 함 pragma savereg+ // 레지스터 저장 함 pragma library mylib.lib // 링크 라이브러리 지정 ※ MSDN / \"Programming Applications for Microsoft Windows\"(Jeffrey Richer 저) 참조",
      "frontmatter": {
        "tags": [
          "c",
          "cpp",
          "language"
        ],
        "date": "2024-01-04T12:18:00+09:00",
        "lastmod": "2024-01-04T12:18:00+09:00"
      }
    },
    "python 기본 가상환경 venv": {
      "path": "/02.inbox/python-기본-가상환경-venv/",
      "filename": "python 기본 가상환경 venv",
      "content": "python -m venv 가상환경이름 이제 가상 환경을 활성화합니다. 운영체제에 따라 활성화 방법이 다릅니다. (1) MacOS 경우 source 가상환경이름/bin/activate (2) WindowOS 경우 source 가상환경이름/Scripts/activate 정상적으로 가상환경이 실행되었다면, 터미널에서 현재 디렉토리 맨 앞에 상환경 이름이 괄호 안에 출력됩니다. 가상 환경을 비활성화하는 방법입니다. deactivate 생성했던 가상 환경을 삭제하는 방법입니다. sudo rm -rf 가상환경이름",
      "frontmatter": {
        "tags": [
          "python"
        ],
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2023-12-20T07:12:00+09:00"
      }
    },
    "python 람다(lambda)": {
      "path": "/02.inbox/python-람다lambda/",
      "filename": "python 람다(lambda)",
      "content": "익명함수로도 불리운다 인수는 무제한 가능 함수 이름(생략 가능) = lambda",
      "frontmatter": {
        "tags": [
          "python",
          "language"
        ],
        "date": "2023-12-28T07:20:00+09:00",
        "lastmod": "2023-12-28T07:20:00+09:00"
      }
    },
    "python 매개변수 전달": {
      "path": "/02.inbox/python-매개변수-전달/",
      "filename": "python 매개변수 전달",
      "content": "파이썬의 매개변수 전달 방식 : 혼합 방식 파이썬은 모든 것이 객체이며 다음 2 종류가 있음 불변 객체 (immutable object) : int, float, complex, tuples, string, bytes 등 가변 객체 (mutable object) : list, dict, set, bytearray 등 불변 객체가 매개변수로 전달될 때는 call-by-value 로 전달 가변 객체가 매개변수로 전달될 때는 call-by-reference 로 전달 [!NOTE] 참조 Call-by-value : 실 매개변수의 값이 전달되며, 함수 내에서 형식 매개변수의 값을 변경해도 실 매개변수의 값은 변하지 않음 Call-by-reference : 실 매개변수의 주소가 전달",
      "frontmatter": {
        "aliases": [
          "parametor"
        ],
        "tags": [
          "python",
          "language"
        ],
        "date": "2024-01-08T07:12:00+09:00",
        "lastmod": "2025-10-21T20:46:46+09:00"
      }
    },
    "python 연산자 우선순위": {
      "path": "/02.inbox/python-연산자-우선순위/",
      "filename": "python 연산자 우선순위",
      "content": "python 연산자 우선순위-2023122311595",
      "frontmatter": {
        "tags": [
          "python",
          "language",
          "reference"
        ],
        "date": "2024-05-12T22:27:40+09:00",
        "lastmod": "2025-10-21T20:46:10+09:00",
        "creation": "2023-12-23"
      }
    },
    "python 이름공간(namespace)": {
      "path": "/02.inbox/python-이름공간namespace/",
      "filename": "python 이름공간(namespace)",
      "content": "이름공간과 스코프 두가지 개념을 인지 %20image%2020240428050441.png) python 에는 포인터 개념이 내부적으로 작동 text=\"PythonGeeks\" print(\"id of text is:\",id(text)) print(\"id of PythonGeeks is:\",id(\"PythonGeeks\")) id of text is: 140569298174960 id of PythonGeeks is: 140569298174960 c/cpp 과 완벽히 동일 [!NOTE] namespace 네임스페이스(Namespaces)는 이름을 객체에 매핑(mapping)하는 것으로, 파이썬에서는 변수가 실제 객체를 참조하는 이름의 집합을 의미한다 [!NOTE] scope 변수(variable)나 함수(function)와 같은 식별자(identifier)들이 유효(valid)하고, 접근(access) 가능한 범위(range)를 말한다 namespace 의 종류 built in namespace : 인터프리터가 시작될 때 생성 global namespace : 모든 전역 객체 local namespace : 함수 내부 def func(): var1 = 3 print(\"지역공간 namespace : \" , locals()) var = 3 print(\"built in namespace : \" , dir()) print(\"전역공간 namespace : \" , globals()) func() built in namespace : built in namespace : ['ArithmeticError', 'AssertionError', 'AttributeError', 'BaseException', 'BaseExceptionGroup', 'BlockingIOError', 'BrokenPipeError', 'BufferError', 'BytesWarning', 'ChildProcessError', 'ConnectionAbortedError', 'ConnectionError', 'ConnectionRefusedError', 'ConnectionResetError', 'DeprecationWarning', 'EOFError', 'Ellipsis', 'EncodingWarning', 'EnvironmentError', 'Exception', 'ExceptionGroup', 'False', 'FileExistsError', 'FileNotFoundError', 'FloatingPointError', 'FutureWarning', 'GeneratorExit', 'IOError', 'ImportError', 'ImportWarning', 'IndentationError', 'IndexError', 'InterruptedError', 'IsADirectoryError', 'KeyError', 'KeyboardInterrupt', 'LookupError', 'MemoryError', 'ModuleNotFoundError', 'NameError', 'None', 'NotADirectoryError', 'NotImplemented', 'NotImplementedError', 'OSError', 'OverflowError', 'PendingDeprecationWarning', 'PermissionError', 'ProcessLookupError', 'RecursionError', 'ReferenceError', 'ResourceWarning', 'RuntimeError', 'RuntimeWarning', 'StopAsyncIteration', 'StopIteration', 'SyntaxError', 'SyntaxWarning', 'SystemError', 'SystemExit', 'TabError', 'TimeoutError', 'True', 'TypeError', 'UnboundLocalError', 'UnicodeDecodeError', 'UnicodeEncodeError', 'UnicodeError', 'UnicodeTranslateError', 'UnicodeWarning', 'UserWarning', 'ValueError', 'Warning', 'ZeroDivisionError', '__build_class__', '__debug__', '__doc__', '__import__', '__loader__', '__name__', '__package__', '__spec__', 'abs', 'aiter', 'all', 'anext', 'any', 'ascii', 'bin', 'bool', 'breakpoint', 'bytearray', 'bytes', 'callable', 'chr', 'classmethod', 'compile', 'complex', 'copyright', 'credits', 'delattr', 'dict', 'dir', 'divmod', 'enumerate', 'eval', 'exec', 'exit', 'filter', 'float', 'format', 'frozenset', 'getattr', 'globals', 'hasattr', 'hash', 'help', 'hex', 'id', 'input', 'int', 'isinstance', 'issubclass', 'iter', 'len', 'license', 'list', 'locals', 'map', 'max', 'memoryview', 'min', 'next', 'object', 'oct', 'open', 'ord', 'pow', 'print', 'property', 'quit', 'range', 'repr', 'reversed', 'round', 'set', 'setattr', 'slice', 'sorted', 'staticmethod', 'str', 'sum', 'super', 'tuple', 'type', 'vars', 'zip'] 전역공간 namespace : {'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7f2a74ccb350>, '__spec__': None, '__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__file__': '/home/shinnk/source_main/personal/python/class/namepace.py', '__cached__': None, 'func': <function func at 0x7f2a74c704a0>, 'var': 3} 지역공간 namespace : {'var1': 3} Pasted image 20240428050445 함수 내부에서 Pasted image 20240428050454 a. Built-in scope: 모든 식별자들에 영향을 끼친다 b. Global scope: 모든 식별자에 영향을 끼친다 c. Local scope: 함수 내부의 식별자들에 영향을 끼친다 d. Enclosed or nonlocal scope: 위치된 곳과 내부의 함수에 영향을 끼친다 global 로 변수를 선언하면 변수를 조회할때 global namespace 부터 조회하게 된다 nonlocal 키워드를 사용하면 변수를 조회할때 한단계 위의 namespace 부터 조회하게 된다 a = 3 print(locals()) print(globals()) locals() # 위치한곳의 namespace 조회 globals() # global name space 조회 print(__builtins__.__dict__) # biltins name 조회 dir() # namespace 조회 아님 하지만 builtins 의 경우 이미 저장되어 있으므로 이렇게 조회함 객체.__dict__() # 객체의 namespace 조회 [!NOTE] Title 변수가 호출되면 어떠한 객체와 연결되어 있는지 확인하기 위해 가장 가까운 scope 부터 확인 만약 함수 내부의 변수라면 local -> enclosed -> global -> built-in namespace 순으로 확인하며 먼저 발견되면 그것을 사용한다 global 은 현제 파일의 전역 이름과 모듈의 전역 이름이 있는데 현제 파일의 전역파일이 먼저 조회된다",
      "frontmatter": {
        "aliases": [
          "namepace"
        ],
        "tags": [
          "python",
          "language"
        ],
        "date": "2023-12-24T09:14:00+09:00",
        "lastmod": "2023-12-24T09:14:00+09:00"
      }
    },
    "python 인수 규칙": {
      "path": "/02.inbox/python-인수-규칙/",
      "filename": "python 인수 규칙",
      "content": "def print( *values: object, sep: str | None = \" \", end: str | None = \"\\n\", file: SupportsWrite[str] | None = None, flush: Literal[False] = False ) -> None 파이썬의 print 내장함수 구현체이다 가변인자#위치%20가변%20인자%20*args) 를 사용함 정확하게는 위치 가변인자를 사용하였다 : object 매개변수의 올 수 있는 타입을 지정하여 오류를 막는다 : str | None str 또는 None 가 올 수 있다는 이야기이다 = \" \" None 일때의 기본값을 지정해준다",
      "frontmatter": {
        "tags": [
          "python",
          "language"
        ],
        "date": "2023-12-22T07:18:00+09:00",
        "lastmod": "2025-10-21T20:45:21+09:00"
      }
    },
    "register x64": {
      "path": "/02.inbox/register-x64/",
      "filename": "register x64",
      "content": "%20image%2020240417101228.png) Pasted image 20240417225940 vscode c 디버깅 시에 나오는 레지스터 종류 other register : x0 - x28 fp w0 - w28 v0 - v31 fpsr fpsr far esr exception cpu lr sp pc cpsr IEEE single s0 - s31 IEEE double d0 - d31",
      "frontmatter": {
        "tags": [
          "assembler"
        ],
        "date": "2025-05-25T23:35:00+09:00",
        "lastmod": "2025-05-25T23:35:00+09:00"
      }
    },
    "spring HandlerAdapter 구현체": {
      "path": "/02.inbox/spring-handleradapter-구현체/",
      "filename": "spring HandlerAdapter 구현체",
      "content": "스프링 MVC의 HandlerAdapter 는 다양한 유형의 핸들러(컨트롤러)를 실행하는 인터페이스입니다. 각 HandlerAdapter 구현체는 특정 유형의 핸들러를 지원합니다. 아래에서 언급된 6가지 구현체를 체계적으로 설명합니다: AbstractHandlerMethodAdapter 역할: 메서드 기반 핸들러 어댑터의 추상 클래스. 특징: HandlerMethod 를 처리하는 어댑터의 기본 기능을 제공합니다. RequestMappingHandlerAdapter 의 상위 클래스로, 메서드 단위 처리 로직을 공통화합니다. 사용 예시: 구체적인 구현체( RequestMappingHandlerAdapter )에서 확장되어 사용됩니다. HandlerFunctionAdapter 역할: 함수형 프로그래밍 스타일 핸들러( HandlerFunction )를 지원합니다. 특징: RouterFunction 과 함께 사용되며, 람다 표현식으로 핸들러를 정의합니다. Spring 5+에서 도입된 함수형 엔드포인트를 처리합니다. 예시: @Bean public RouterFunction<ServerResponse> route() { return RouterFunctions.route() .GET(\"/api/users\", request -> ServerResponse.ok().body(...)) .build(); } HttpRequestHandlerAdapter 역할: HttpRequestHandler 인터페이스 구현체를 처리합니다. 특징: 서블릿 API( HttpServletRequest , HttpServletResponse )를 직접 사용하는 레거시 코드와 호환됩니다. @Controller 애노테이션 없이도 핸들러를 등록할 수 있습니다. 예시: public class LegacyHandler implements HttpRequestHandler { @Override public void handleRequest(HttpServletRequest request, HttpServletResponse response) { // 직접 응답을 생성합니다. } } RequestMappingHandlerAdapter 역할: 애노테이션 기반 컨트롤러( @RequestMapping , @RestController )를 처리합니다. 특징: @GetMapping , @PostMapping , @PathVariable , @RequestBody 등을 지원합니다. 현대적인 스프링 애플리케이션에서 가장 많이 사용되는 어댑터입니다. 예시: @RestController public class UserController { @GetMapping(\"/users\") public List<User> getUsers() { return userService.findAll(); } } SimpleControllerHandlerAdapter 역할: Controller 인터페이스 구현체를 처리합니다. 특징: 과거에 사용되던 방식으로, Controller 인터페이스의 handleRequest() 메서드를 호출합니다. @Controller 애노테이션 없이 빈으로 등록해야 합니다. 예시: public class OldController implements Controller { @Override public ModelAndView handleRequest(HttpServletRequest request, HttpServletResponse response) { return new ModelAndView(\"viewName\"); } } SimpleServletHandlerAdapter 역할: 일반 서블릿( javax.servlet.Servlet )을 핸들러로 사용합니다. 특징: 기존 서블릿을 스프링 MVC에서 재사용할 수 있도록 합니다. 서블릿의 service() 메서드를 직접 호출합니다. 예시: @WebServlet(\"/legacy\") public class LegacyServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse res) { res.getWriter().write(\"Legacy Servlet Response\"); } } 📌 요약 어댑터명 처리 대상 주요 사용 사례 HandlerFunctionAdapter HandlerFunction 함수형 라우팅(람다 기반) HttpRequestHandlerAdapter HttpRequestHandler 서블릿 API 기반 레거시 코드 RequestMappingHandlerAdapter @RequestMapping 기반 컨트롤러 현대적인 REST API 개발 SimpleControllerHandlerAdapter Controller 인터페이스 과거 버전 호환용 컨트롤러 SimpleServletHandlerAdapter 일반 서블릿 기존 서블릿 통합 💡 핵심 포인트 현대적인 개발에서는 RequestMappingHandlerAdapter 가 주력으로 사용됩니다. 함수형 프로그래밍은 HandlerFunctionAdapter 로 처리합니다. 레거시 코드 통합 시 HttpRequestHandlerAdapter 또는 SimpleServletHandlerAdapter 를 사용합니다. SimpleControllerHandlerAdapter 는 거의 사용되지 않으며, @Controller 애노테이션으로 대체되었습니다.",
      "frontmatter": {
        "tags": [
          "spring",
          "reference"
        ],
        "date": "2025-03-10T00:14:00+09:00",
        "lastmod": "2025-03-10T00:14:00+09:00"
      }
    },
    "spring RequestMapping 구현체": {
      "path": "/02.inbox/spring-requestmapping-구현체/",
      "filename": "spring RequestMapping 구현체",
      "content": "스프링 MVC의 HandlerMapping 구현체는 다양한 방식으로 요청을 핸들러(컨트롤러)에 매핑합니다. 각 클래스의 역할과 특징을 체계적으로 정리했습니다: AbstractHandlerMapping 역할: 모든 HandlerMapping 의 기반 추상 클래스. 특징: 인터셉터( HandlerInterceptor ) 관리 및 실행 로직을 제공합니다. getHandler() 메서드를 구현해 실제 핸들러를 찾는 로직을 정의합니다. 사용 예시: 다른 구체적인 HandlerMapping 클래스들이 이 클래스를 상속받아 확장합니다. AbstractUrlHandlerMapping 역할: URL 기반 매핑을 위한 추상 클래스. 특징: URL 패턴과 핸들러를 연결하는 공통 로직을 제공합니다. urlMap 또는 handlerMap 을 사용해 URL-핸들러 매핑 정보를 저장합니다. 하위 클래스: BeanNameUrlHandlerMapping SimpleUrlHandlerMapping AbstractDetectingUrlHandlerMapping BeanNameUrlHandlerMapping 역할: 스프링 빈 이름을 URL로 매핑합니다. 특징: 빈 이름이 / 로 시작하는 경우, 해당 URL로 매핑됩니다. XML 또는 자바 설정으로 빈을 등록할 때 이름을 URL로 지정합니다. 예시: @Component(\"/hello\") // URL: /hello public class HelloController implements Controller { ... } SimpleUrlHandlerMapping 역할: 정적 URL 매핑을 명시적으로 설정합니다. 특징: URL과 핸들러를 직접 연결하는 urlMap 을 제공합니다. XML/자바 설정으로 유연하게 매핑할 수 있습니다. 예시: @Bean public SimpleUrlHandlerMapping handlerMapping() { SimpleUrlHandlerMapping mapping = new SimpleUrlHandlerMapping(); Map<String, Object> urlMap = new HashMap<>(); urlMap.put(\"/api/products\", productController()); return mapping; } AbstractDetectingUrlHandlerMapping 역할: 자동 URL 감지를 위한 추상 클래스. 특징: 빈의 이름이나 메타데이터를 분석해 URL을 자동으로 생성합니다. 주로 레거시 시스템이나 특정 프레임워크에서 사용됩니다. 하위 클래스: ControllerClassNameHandlerMapping (클래스 이름을 URL로 매핑, 예: HelloController → /hello* ) RequestMappingHandlerMapping 역할: 애노테이션 기반 컨트롤러 매핑 (가장 일반적). 특징: @RequestMapping , @GetMapping , @PostMapping 등을 분석해 URL을 매핑합니다. @Controller 또는 @RestController 로 선언된 클래스의 메서드를 처리합니다. 예시: @RestController public class UserController { @GetMapping(\"/users\") public List<User> getUsers() { ... } } RequestMappingInfoHandlerMapping 역할: @RequestMapping 메타데이터 기반 매핑. 특징: RequestMappingHandlerMapping 의 상위 클래스로, RequestMappingInfo 객체를 사용해 세부적인 매핑 조건(HTTP 메서드, 헤더 등)을 처리합니다. 내부적으로 RequestMappingHandlerMapping 에서 확장되어 사용됩니다. RouterFunctionMapping 역할: 함수형 프로그래밍 스타일로 라우팅을 정의합니다. 특징: RouterFunction 과 HandlerFunction 을 사용해 람다 기반 라우팅을 구현합니다. Spring WebFlux 또는 반응형 프로그래밍에서 주로 사용됩니다. 예시: @Bean public RouterFunction<ServerResponse> route() { return RouterFunctions.route() .GET(\"/api/users\", request -> ServerResponse.ok().body(...)) .build(); } WebSocketHandlerMapping 역할: WebSocket 요청 처리를 위한 매핑. 특징: WebSocket 엔드포인트( /websocket )와 WebSocketHandler 를 연결합니다. @EnableWebSocket 과 함께 사용됩니다. 예시: @Configuration @EnableWebSocket public class WebSocketConfig implements WebSocketConfigurer { @Override public void registerWebSocketHandlers(WebSocketHandlerRegistry registry) { registry.addHandler(myHandler(), \"/websocket\"); } } AbstractHandlerMethodMapping 역할: 메서드 단위 매핑을 위한 추상 클래스. 특징: @RequestMapping 과 같은 애노테이션을 메서드 단위로 분석합니다. RequestMappingHandlerMapping 의 상위 클래스로, 메서드-URL 매핑 정보를 관리합니다. 📌 요약 클래스명 주요 역할 AbstractHandlerMapping 모든 HandlerMapping 의 기본 기능(인터셉터 처리 등) 제공 AbstractUrlHandlerMapping URL 기반 매핑의 추상화 BeanNameUrlHandlerMapping 빈 이름을 URL로 매핑 SimpleUrlHandlerMapping 정적 URL과 핸들러를 직접 연결 RequestMappingHandlerMapping 애노테이션 기반 컨트롤러 매핑 RouterFunctionMapping 함수형 라우팅( RouterFunction ) 지원 WebSocketHandlerMapping WebSocket 엔드포인트 매핑 💡 핵심 포인트 현대적인 스프링 앱에서는 RequestMappingHandlerMapping 과 RouterFunctionMapping 이 주로 사용됩니다. 레거시 시스템에서는 BeanNameUrlHandlerMapping 또는 SimpleUrlHandlerMapping 을 볼 수 있습니다. WebSocket이나 특수 프로토콜은 WebSocketHandlerMapping 으로 처리합니다.",
      "frontmatter": {
        "tags": [
          "spring",
          "reference"
        ],
        "date": "2025-03-10T00:09:00+09:00",
        "lastmod": "2025-08-19T22:16:22+09:00"
      }
    },
    "spring View 구현체": {
      "path": "/02.inbox/spring-view-구현체/",
      "filename": "spring View 구현체",
      "content": "스프링 MVC의 View 는 모델 데이터를 클라이언트에게 렌더링하는 최종 형식을 결정하는 인터페이스입니다. 아래에서 언급된 21개의 View 구현체를 목적별로 체계적으로 정리했습니다: 기본 추상 클래스 1 AbstractView 역할: 모든 뷰 구현체의 기본 추상 클래스. 특징: render() 메서드를 구현해 모델 데이터를 응답으로 변환합니다. 커스텀 뷰를 만들 때 상속받아 사용합니다. 사용 예시: public class CustomCsvView extends AbstractView { @Override protected void renderMergedOutputModel(Map<String, Object> model, HttpServletRequest request, HttpServletResponse response) { // CSV 생성 로직 } } 피드(Feed) 뷰 1 AbstractAtomFeedView 역할: Atom 형식의 피드를 생성합니다. 특징: Atom 1.0 스펙을 준수하는 XML 피드를 생성합니다. Rome 라이브러리를 내부적으로 사용합니다. 예시: 블로그 글 목록을 Atom 피드로 제공. 2 AbstractRssFeedView 역할: RSS 형식의 피드를 생성합니다. 특징: RSS 2.0 스펙을 준수하는 XML 피드를 생성합니다. Rome 라이브러리를 사용합니다. 예시: 뉴스 사이트의 RSS 피드. 3 AbstractFeedView 역할: Atom/RSS 피드의 공통 로직을 제공합니다. 특징: AbstractAtomFeedView 와 AbstractRssFeedView 의 부모 클래스입니다. JSON/XML 뷰 1 MappingJackson2JsonView 역할: JSON 형식의 응답을 생성합니다. 특징: Jackson 2 라이브러리를 사용해 모델 데이터를 JSON으로 변환합니다. @ResponseBody 대신 뷰를 통해 JSON을 반환할 때 사용됩니다. 예시: @Bean public View jsonView() { return new MappingJackson2JsonView(); } 2 MappingJackson2XmlView 역할: XML 형식의 응답을 생성합니다. 특징: Jackson 2의 XML 확장을 사용해 모델 데이터를 XML로 변환합니다. 예시: 레거시 시스템과의 XML 통신. 3 MarshallingView 역할: XML/JSON 변환을 위한 마샬링 뷰. 특징: JAXB, Castor 등 다양한 마샬러를 지원합니다. Marshaller 인터페이스를 구현한 라이브러리를 사용합니다. PDF 뷰 1 AbstractPdfView 역할: PDF 문서를 생성합니다. 특징: iText 라이브러리를 사용해 PDF를 생성합니다. 모델 데이터를 테이블, 텍스트 등으로 렌더링합니다. 예시: public class InvoicePdfView extends AbstractPdfView { @Override protected void buildPdfDocument(Map<String, Object> model, Document document, PdfWriter writer) { // PDF 문서 생성 로직 } } 2 AbstractPdfStamperView 역할: 기존 PDF 템플릿에 데이터 채우기. 특징: iText 의 PdfStamper 를 사용해 정적 PDF 양식을 동적으로 채웁니다. 계약서, 청구서 등 고정된 양식에 데이터를 입력할 때 사용됩니다. 엑셀 뷰 1 AbstractXlsView 역할: 레거시 Excel(.xls) 파일 생성. 특징: Apache POI 라이브러리를 사용해 Excel 97-2003 형식(.xls)을 생성합니다. 예시: 재무 데이터 보고서. 2 AbstractXlsxView 역할: Excel 2007+ 형식(.xlsx) 파일 생성. 특징: Apache POI 의 XSSF API를 사용해 최신 Excel 형식을 지원합니다. 3 AbstractXlsxStreamingView 역할: 대용량 Excel 파일 스트리밍. 특징: 메모리 사용을 최소화하기 위해 데이터를 스트리밍 방식으로 작성합니다. 수십만 행의 데이터를 처리할 때 유용합니다. 템플릿 뷰 1 FreeMarkerView 역할: FreeMarker 템플릿을 렌더링합니다. 특징: FreeMarkerViewResolver 와 함께 사용됩니다. HTML, 텍스트 등 다양한 형식을 지원합니다. 예시: 동적 HTML 페이지 생성. 2 GroovyMarkupView 역할: Groovy 템플릿을 렌더링합니다. 특징: Groovy Markup Template을 사용해 뷰를 생성합니다. 간결한 문법으로 XML/HTML을 생성합니다. 3 ScriptTemplateView 역할: 스크립트 기반 템플릿(예: React, Nashorn)을 지원합니다. 특징: JavaScript 엔진을 사용해 뷰를 렌더링합니다. 서버 측에서 React 컴포넌트를 렌더링할 때 사용됩니다. JSP/리소스 뷰 1 InternalResourceView 역할: JSP 파일을 렌더링합니다. 특징: InternalResourceViewResolver 와 함께 사용됩니다. JstlView 의 부모 클래스입니다. 2 JstlView 역할: JSTL 태그를 지원하는 JSP 뷰. 특징: JSTL의 <fmt:message> , <c:forEach> 등을 사용할 수 있습니다. 특수 목적 뷰 1 RedirectView 역할: HTTP 리다이렉트를 수행합니다. 특징: redirect:/newPath 또는 외부 URL( https://example.com )로 이동합니다. Post/Redirect/Get 패턴 구현에 사용됩니다. 예시: return new ModelAndView(new RedirectView(\"/home\")); 2 XsltView 역할: XML 데이터를 XSLT로 변환합니다. 특징: XML 데이터와 XSLT 스타일시트를 결합해 HTML 등을 생성합니다. 📌 요약 뷰 클래스 주요 형식 사용 사례 MappingJackson2JsonView JSON REST API 응답 AbstractPdfView PDF 계약서, 보고서 생성 AbstractXlsxView Excel(.xlsx) 데이터 분석 리포트 FreeMarkerView HTML 동적 웹 페이지 RedirectView HTTP 리다이렉트 Post-Redirect-Get 패턴 💡 핵심 포인트 JSON/XML은 MappingJackson2JsonView 로 처리합니다. PDF/Excel은 AbstractPdfView , AbstractXlsxView 를 확장해 구현합니다. 리다이렉트는 RedirectView 를 사용합니다. 템플릿 엔진은 각각 전용 뷰 클래스(예: FreeMarkerView )를 사용합니다.",
      "frontmatter": {
        "tags": [
          "spring",
          "reference"
        ],
        "date": "2025-03-10T00:27:00+09:00",
        "lastmod": "2025-03-10T00:27:00+09:00"
      }
    },
    "spring ViewResolver 구현체": {
      "path": "/02.inbox/spring-viewresolver-구현체/",
      "filename": "spring ViewResolver 구현체",
      "content": "스프링 MVC의 ViewResolver 는 뷰 이름을 실제 View 객체로 변환하는 역할을 합니다. 다양한 구현체가 있으며, 각각의 특징과 사용 사례를 체계적으로 정리했습니다: AbstractCachingViewResolver 역할: 뷰 캐싱 기능을 제공하는 추상 클래스. 특징: 뷰 객체를 캐시하여 반복적인 뷰 생성을 방지합니다. 하위 클래스(예: UrlBasedViewResolver )가 캐싱 로직을 재사용할 수 있도록 합니다. 사용 예시: InternalResourceViewResolver 가 이 클래스를 상속받아 JSP 뷰 캐싱을 처리합니다. AbstractTemplateViewResolver 역할: 템플릿 기반 뷰(예: JSP, Thymeleaf)를 처리하는 추상 클래스. 특징: 템플릿 엔진 설정(예: prefix , suffix )을 공통으로 관리합니다. UrlBasedViewResolver 와 함께 사용됩니다. 사용 예시: FreeMarkerViewResolver 가 이 클래스를 상속받아 FreeMarker 템플릿을 처리합니다. BeanNameViewResolver 역할: 스프링 빈 이름으로 뷰를 조회합니다. 특징: 뷰 이름이 스프링 빈 이름과 일치하는 View 객체를 찾아 반환합니다. 커스텀 뷰(예: PDF 생성 뷰)를 빈으로 등록해 사용할 때 유용합니다. 예시: @Bean public View pdfView() { return new AbstractPdfView() { // PDF 뷰 구현 @Override protected void buildPdfDocument(Map<String, Object> model, Document document, PdfWriter writer) { // PDF 생성 로직 } }; } ContentNegotiatingViewResolver 역할: 요청의 Accept 헤더 또는 쿼리 파라미터에 따라 뷰를 선택합니다. 특징: 클라이언트가 원하는 형식(JSON, XML, HTML 등)에 맞는 뷰를 반환합니다. 내부적으로 다른 ViewResolver 를 조합해 동작합니다. 예시: @Configuration public class WebConfig implements WebMvcConfigurer { @Override public void configureViewResolvers(ViewResolverRegistry registry) { registry.enableContentNegotiation(new JsonView(), new XmlView()); } } FreeMarkerViewResolver 역할: FreeMarker 템플릿을 처리합니다. 특징: 뷰 이름을 FreeMarker 템플릿 파일 경로(예: views/user.ftl )로 변환합니다. FreeMarkerConfigurer 와 함께 설정됩니다. 예시: @Bean public FreeMarkerViewResolver freeMarkerViewResolver() { FreeMarkerViewResolver resolver = new FreeMarkerViewResolver(); resolver.setPrefix(\"/WEB-INF/views/\"); resolver.setSuffix(\".ftl\"); return resolver; } GroovyMarkupViewResolver 역할: Groovy 템플릿을 처리합니다. 특징: Groovy Markup Template(예: user.tpl )을 렌더링합니다. GroovyMarkupConfigurer 로 템플릿 설정을 관리합니다. 예시: @Bean public GroovyMarkupViewResolver groovyViewResolver() { GroovyMarkupViewResolver resolver = new GroovyMarkupViewResolver(); resolver.setPrefix(\"/views/\"); resolver.setSuffix(\".tpl\"); return resolver; } InternalResourceViewResolver 역할: JSP 뷰를 처리하는 가장 일반적인 리졸버. 특징: 뷰 이름을 JSP 파일 경로(예: /WEB-INF/views/home.jsp )로 변환합니다. prefix 와 suffix 로 경로를 설정합니다. 예시: @Bean public InternalResourceViewResolver viewResolver() { InternalResourceViewResolver resolver = new InternalResourceViewResolver(); resolver.setPrefix(\"/WEB-INF/views/\"); resolver.setSuffix(\".jsp\"); return resolver; } ResourceBundleViewResolver 역할: 프로퍼티 파일로 뷰를 정의합니다. 특징: views.properties 파일에 뷰 이름과 클래스 정보를 저장합니다. 다국어 뷰 또는 외부 설정이 필요한 경우에 사용됩니다. 예시: # views.properties home.class=org.springframework.web.servlet.view.JstlView home.url=/WEB-INF/views/home.jsp ScriptTemplateViewResolver 역할: 스크립트 기반 템플릿(예: Nashorn, React)을 처리합니다. 특징: JavaScript 또는 다른 스크립트 엔진으로 뷰를 렌더링합니다. ScriptTemplateConfigurer 로 스크립트 엔진을 설정합니다. 예시: @Bean public ScriptTemplateViewResolver scriptViewResolver() { ScriptTemplateViewResolver resolver = new ScriptTemplateViewResolver(); resolver.setPrefix(\"templates/\"); resolver.setSuffix(\".jsx\"); return resolver; } UrlBasedViewResolver 역할: URL 기반 뷰를 직접 매핑합니다. 특징: 뷰 이름을 URL 경로로 직접 변환합니다. InternalResourceViewResolver 의 부모 클래스입니다. 예시: @Bean public UrlBasedViewResolver urlBasedViewResolver() { UrlBasedViewResolver resolver = new UrlBasedViewResolver(); resolver.setViewClass(JstlView.class); resolver.setPrefix(\"/WEB-INF/views/\"); resolver.setSuffix(\".jsp\"); return resolver; } ViewResolverComposite 역할: 다중 ViewResolver 를 조합합니다. 특징: 여러 리졸버를 순차적으로 실행해 적절한 뷰를 찾습니다. 우선순위를 설정할 수 있습니다. 예시: @Bean public ViewResolverComposite compositeResolver() { ViewResolverComposite composite = new ViewResolverComposite(); composite.addResolver(new InternalResourceViewResolver()); composite.addResolver(new FreeMarkerViewResolver()); return composite; } XmlViewResolver 역할: XML 파일로 뷰를 정의합니다. 특징: views.xml 과 같은 XML 설정 파일에서 뷰 빈을 로드합니다. ResourceBundleViewResolver 와 유사하지만 XML 형식을 사용합니다. 예시: <!-- views.xml --> <beans> <bean id=\"home\" class=\"org.springframework.web.servlet.view.JstlView\"> <property name=\"url\" value=\"/WEB-INF/views/home.jsp\"/> </bean> </beans> XsltViewResolver 역할: XSLT(XML 변환) 뷰를 처리합니다. 특징: XML 데이터를 XSLT 스타일시트로 변환합니다. XsltView 클래스를 사용해 렌더링합니다. 예시: @Bean public XsltViewResolver xsltViewResolver() { XsltViewResolver resolver = new XsltViewResolver(); resolver.setPrefix(\"/WEB-INF/xsl/\"); resolver.setSuffix(\".xsl\"); return resolver; } 📌 요약 리졸버명 주요 기능 InternalResourceViewResolver JSP 뷰 처리 (가장 일반적) FreeMarkerViewResolver FreeMarker 템플릿 처리 ContentNegotiatingViewResolver 요청 형식(JSON/XML)에 따라 뷰 선택 BeanNameViewResolver 스프링 빈 이름으로 뷰 조회 ViewResolverComposite 다중 리졸버 조합 XsltViewResolver XML을 XSLT로 변환 💡 핵심 포인트 JSP는 InternalResourceViewResolver 로 처리합니다. 템플릿 엔진(FreeMarker, Groovy)은 각각 전용 리졸버를 사용합니다. 다중 포맷 지원(JSON, XML)은 ContentNegotiatingViewResolver 로 구현합니다. 커스텀 뷰는 BeanNameViewResolver 또는 ResourceBundleViewResolver 로 관리합니다.",
      "frontmatter": {
        "tags": [
          "spring",
          "reference"
        ],
        "date": "2025-03-10T00:23:00+09:00",
        "lastmod": "2025-03-10T00:23:00+09:00"
      }
    },
    "spring controller 추상화 단계별 어노테이션 파라미터": {
      "path": "/02.inbox/spring-controller-추상화-단계별-어노테이션-파라미터/",
      "filename": "spring controller 추상화 단계별 어노테이션 파라미터",
      "content": "추상화 단계별 설명 Level 1: Servlet API 직접 사용 (가장 낮은 추상화 단계) Spring이 있기 전, Java 웹 개발의 근간인 Servlet API를 직접 사용하는 방식입니다. Spring Controller에서도 이 객체들을 직접 파라미터로 받아 모든 것을 수동으로 제어할 수 있습니다. Level 2: 기본 매핑과 요청 데이터 추출 Servlet API를 직접 다루는 불편함을 줄이고, 특정 URL 요청을 특정 메서드에 연결(매핑)하고 요청 데이터를 쉽게 추출하는 단계입니다. Level 3: 데이터 바인딩 및 응답 데이터 처리 요청 파라미터들을 객체(DTO)에 자동으로 담아주거나, 응답할 데이터를 모델에 담아 View로 전달하는 등 데이터 처리를 자동화하는 단계입니다. Level 4: REST API를 위한 추상화 전통적인 HTML View 반환이 아닌, JSON/XML 같은 데이터 자체를 응답하는 RESTful API 개발에 특화된 고수준 추상화 단계입니다. Level 5: 편의성을 위한 조합 및 축약 (가장 높은 추상화 단계) 여러 어노테이션의 기능을 하나로 합치거나, 코드를 더 간결하게 만들어주는 '문법적 설탕(Syntactic Sugar)' 단계입니다. 상세 설명 (낮은 추상화 -> 높은 추상화 순) Level 1: Servlet API 직접 사용 (Lowest Abstraction) 이 단계에서는 Spring의 도움을 최소한으로 받고, Java Servlet의 핵심 객체를 직접 다룹니다. 파라미터 문법 예시 설명 HttpServletRequest public void method(HttpServletRequest request) HTTP 요청 정보를 모두 담고 있는 객체입니다. 헤더, 파라미터, 쿠키, 세션, Body 등 모든 요청 데이터에 직접 접근할 수 있습니다. ( request.getParameter(\"name\") 처럼 사용) HttpServletResponse public void method(HttpServletResponse response) HTTP 응답을 제어하는 객체입니다. 응답 상태 코드(Status Code), 헤더, 쿠키를 설정하거나 응답 Body에 직접 데이터를 쓸 수 있습니다. ( response.getWriter().write(\"hello\") 처럼 사용) HttpSession public void method(HttpSession session) 세션 객체를 직접 다룰 때 사용합니다. 세션에 데이터를 저장( session.setAttribute(...) )하거나 조회( session.getAttribute(...) )할 수 있습니다. Level 2: 기본 매핑과 요청 데이터 추출 URL과 메서드를 연결하고, URL의 특정 부분을 파라미터로 쉽게 가져옵니다. 어노테이션/파라미터 문법 예시 설명 @Controller @Controller public class MyController { ... } 클래스 레벨에 붙이며, 해당 클래스가 Spring MVC의 컨트롤러임을 나타냅니다. Spring이 이 클래스를 스캔하여 웹 요청을 처리하는 핸들러로 사용합니다. @RequestMapping @RequestMapping(\"/hello\") or @RequestMapping(value=\"/hello\", method=RequestMethod.GET) 특정 URL 경로와 HTTP 메서드를 처리할 메서드에 연결(매핑)합니다. 클래스와 메서드 레벨 모두에 사용할 수 있습니다. HTTP 메서드를 명시하지 않으면 모든 메서드(GET, POST 등)를 허용합니다. @RequestParam public void method(@RequestParam(\"name\") String name) URL의 쿼리 파라미터( ?name=John )나 form-data 값을 메서드 파라미터에 바인딩합니다. required (기본값 true), defaultValue 속성을 통해 필수 여부나 기본값을 지정할 수 있습니다. @PathVariable @RequestMapping(\"/users/{userId}\") public void method(@PathVariable(\"userId\") Long id) URL 경로의 일부를 변수로 사용할 때 씁니다. ( /users/123 에서 123 을 id 변수에 바인딩). RESTful API에서 리소스를 식별할 때 주로 사용됩니다. Level 3: 데이터 바인딩 및 응답 데이터 처리 요청 데이터를 객체에 자동으로 담거나, View에 전달할 데이터를 편리하게 관리합니다. 어노테이션/파라미터 문법 예시 설명 @ModelAttribute public String method(@ModelAttribute UserDto userDto) 여러 요청 파라미터를 객체(DTO, VO)의 필드에 자동으로 바인딩합니다. 예를 들어 ?name=John&age=30 요청이 오면, name 과 age 필드를 가진 UserDto 객체를 생성하고 값을 채워줍니다. 또한, 이 어노테이션이 붙은 객체는 자동으로 Model에 추가되어 View에서 사용할 수 있습니다. Model / Map public String method(Model model) View에 전달할 데이터를 담는 컨테이너 역할을 합니다. 메서드 파라미터로 선언하면 Spring이 자동으로 객체를 주입해줍니다. model.addAttribute(\"key\", value) 형태로 데이터를 추가하면 View(JSP, Thymeleaf 등)에서 해당 데이터를 사용할 수 있습니다. Map 도 동일하게 동작합니다. ModelAndView public ModelAndView method() { return new ModelAndView(\"viewName\"); } Model과 View를 하나로 합친 객체입니다. 처리 결과를 보여줄 View의 이름과 View에 전달할 데이터를 함께 담아 반환할 수 있습니다. 최근에는 Model 파라미터와 String (View 이름) 반환을 더 선호하는 추세입니다. Level 4: REST API를 위한 추상화 JSON/XML과 같은 메시지 기반 통신을 위한 핵심적인 추상화입니다. 어노테이션/파라미터 문법 예시 설명 @RequestBody public void method(@RequestBody UserDto userDto) 요청의 Body에 담겨 오는 데이터(주로 JSON, XML)를 Java 객체로 변환(역직렬화, Deserialization)해줍니다. 내부적으로 HttpMessageConverter (주로 Jackson)가 동작하여 이 변환 과정을 자동으로 처리합니다. 클라이언트가 JSON 데이터를 보내면, 해당 JSON 구조와 일치하는 DTO 객체에 값을 채워줍니다. @ResponseBody @ResponseBody public UserDto method() { ... } 메서드가 반환하는 Java 객체를 HTTP 응답 Body에 직접 써넣도록 지시합니다. View를 찾는 것이 아니라, 반환된 객체를 JSON이나 XML 등의 데이터 형식으로 변환(직렬화, Serialization)하여 클라이언트에 전송합니다. REST API의 응답을 만들 때 필수적입니다. ResponseEntity<T> public ResponseEntity<UserDto> method() { ... } @ResponseBody 의 확장판으로, 응답 데이터(Body)뿐만 아니라 HTTP 상태 코드(Status Code)와 헤더(Header)까지 세밀하게 제어하여 반환하고 싶을 때 사용합니다. ResponseEntity.ok(body) , ResponseEntity.created(uri).build() 와 같이 유연한 응답 구성이 가능합니다. Level 5: 편의성을 위한 조합 및 축약 (Highest Abstraction) 자주 사용되는 패턴을 하나의 어노테이션으로 묶어 코드의 가독성과 생산성을 높입니다. 어노테이션/파라미터 문법 예시 설명 @RestController @RestController public class MyApiController { ... } @Controller + @ResponseBody 의 조합입니다. 이 어노테이션을 클래스에 붙이면, 해당 컨트롤러의 모든 메서드에는 기본적으로 @ResponseBody 가 적용됩니다. 따라서 모든 메서드가 View를 반환하는 대신 데이터(JSON 등)를 반환하게 되므로, REST API를 만들 때 매우 편리합니다. @GetMapping @GetMapping(\"/users/{id}\") @RequestMapping(method = RequestMethod.GET) 의 축약형입니다. HTTP GET 요청을 처리하는 핸들러를 간결하게 매핑할 수 있습니다. @PostMapping @PostMapping(\"/users\") @RequestMapping(method = RequestMethod.POST) 의 축약형입니다. HTTP POST 요청을 처리합니다. @PutMapping @PutMapping(\"/users/{id}\") @RequestMapping(method = RequestMethod.PUT) 의 축약형입니다. HTTP PUT 요청을 처리합니다. @DeleteMapping @DeleteMapping(\"/users/{id}\") @RequestMapping(method = RequestMethod.DELETE) 의 축약형입니다. HTTP DELETE 요청을 처리합니다. @PatchMapping @PatchMapping(\"/users/{id}\") @RequestMapping(method = RequestMethod.PATCH) 의 축약형입니다. HTTP PATCH 요청을 처리합니다. 요약: 추상화의 흐름 Servlet API ( HttpServletRequest ) \"이 URL을 이 메서드에 연결해줘\" ( @RequestMapping ) \"URL 파라미터는 이 변수에 넣어줘\" ( @RequestParam ) \"여러 파라미터를 이 객체에 알아서 채워줘\" ( @ModelAttribute ) \"요청 Body의 JSON을 이 객체로 바꿔줘\" ( @RequestBody ) \"메서드 반환값을 바로 JSON으로 응답해줘\" ( @ResponseBody ) \"이 컨트롤러는 전부 REST API용이니 모든 메서드에 @ResponseBody 를 붙여줘\" ( @RestController ) \" @RequestMapping(method=GET) 대신 @GetMapping 으로 간단히 쓰자\" ( @GetMapping 등) 이처럼 Spring은 개발자가 저수준의 반복적인 작업을 하지 않고, 비즈니스 로직에 집중할 수 있도록 점점 더 편리하고 높은 수준의 추상화를 제공하는 방향으로 발전해왔습니다. 추상화 단계별 설명 Spring 프레임워크는 개발자가 웹 개발의 복잡한 저수준 세부 사항에서 벗어나 비즈니스 로직에 집중할 수 있도록 다양한 수준의 추상화를 제공합니다. 여기서는 가장 낮은 추상화 단계부터 높은 추상화 단계까지, Spring이 어떻게 개발 경험을 간소화하는지 단계별로 살펴보겠습니다. Level 1: Servlet API 직접 사용 (가장 낮은 추상화 단계) 이 단계는 Spring의 도움을 최소한으로 받고, Java Servlet의 핵심 객체를 직접 다루는 방식입니다. HTTP 요청/응답의 모든 요소를 바닥부터 제어해야 할 때, 또는 기존 Servlet 기반 코드를 Spring으로 마이그레이션할 때 유용합니다. HttpServletRequest (예시: public void method(HttpServletRequest request) ) HTTP 요청 정보를 모두 담고 있는 객체입니다. 헤더, 파라미터, 쿠키, 세션, Body, 요청 URI, 원격지 IP 등 모든 요청 데이터에 직접 접근할 수 있습니다. 기본 사용: String name = request.getParameter(\"name\"); String userAgent = request.getHeader(\"User-Agent\"); 추가 예시: 동일 이름의 여러 파라미터 받기: String[] interests = request.getParameterValues(\"interest\"); (예: ?interest=coding&interest=music 요청 시) 모든 헤더 이름 조회: Enumeration<String> headerNames = request.getHeaderNames(); 요청 Body 직접 읽기 (JSON): String jsonBody = request.getReader().lines().collect(Collectors.joining(System.lineSeparator())); HttpServletResponse (예시: public void method(HttpServletResponse response) ) HTTP 응답을 제어하는 객체입니다. 응답 상태 코드(Status Code), 헤더, 쿠키를 설정하거나 응답 Body에 직접 데이터를 쓸 수 있습니다. 기본 사용: response.setStatus(HttpServletResponse.SC_CREATED); (201 상태 코드 설정) response.getWriter().write(\"hello world\"); 추가 예시: 커스텀 헤더 추가: response.setHeader(\"X-Custom-Auth\", \"some-token\"); 파일 다운로드를 위한 헤더 설정: response.setContentType(\"application/octet-stream\"); response.setHeader(\"Content-Disposition\", \"attachment; filename=\\\"data.csv\\\"\"); 에러 응답 보내기: response.sendError(400, \"Invalid parameter\"); HttpSession (예시: public void method(HttpSession session) ) 세션 객체를 직접 다룰 때 사용합니다. 세션에 데이터를 저장( setAttribute ), 조회( getAttribute ), 무효화( invalidate )할 수 있습니다. 기본 사용: session.setAttribute(\"cart\", new Cart()); Cart cart = (Cart) session.getAttribute(\"cart\"); session.invalidate(); 추가 예시: 세션 타임아웃 설정: session.setMaxInactiveInterval(1800); (30분) 세션 생성 시간 확인: long creationTime = session.getCreationTime(); java.security.Principal (예시: public String getUsername(Principal principal) ) 현재 인증된 사용자 정보를 담고 있는 객체입니다. Spring Security와 함께 사용할 경우, principal.getName() 을 통해 로그인한 사용자의 ID를 얻을 수 있습니다. 참고: 최신 Spring Security에서는 @AuthenticationPrincipal 어노테이션을 사용하여 더 타입-세이프하게 사용자 객체를 주입받는 것을 권장합니다. java.util.Locale (예시: public void method(Locale locale) ) 요청의 지역 정보를 담고 있는 객체입니다. 클라이언트의 Accept-Language 헤더를 기반으로 결정되며, 다국어 처리 시 현재 언어에 맞는 메시지를 보여주는 데 사용됩니다. 예시: locale.getLanguage() 는 \"ko\", locale.getCountry() 는 \"KR\", locale.toString() 는 \"ko_KR\" 등을 반환할 수 있습니다. InputStream / Reader (예시: public void method(InputStream input, Reader reader) ) 요청 Body의 내용을 직접 스트림으로 읽을 때 사용합니다. 대용량 파일을 처리하거나, Spring의 메시지 컨버터를 거치지 않은 원본 데이터를 읽고 싶을 때 유용합니다. 예시 (파일 업로드 처리): Files.copy(input, new File(\"uploaded-file.dat\").toPath()); OutputStream / Writer (예시: public void method(OutputStream output, Writer writer) ) 응답 Body에 직접 스트림으로 데이터를 쓸 때 사용합니다. 대용량 파일을 다운로드시키거나, 특정 형식의 응답을 수동으로 구성할 때 사용됩니다. 예시 (대용량 CSV 생성): try (PrintWriter printWriter = new PrintWriter(writer)) { printWriter.println(\"id,name\"); printWriter.println(\"1,John\"); } Level 2: 기본 매핑과 요청 데이터 추출 이 단계에서는 URL과 메서드를 연결하고, URL이나 헤더 등에서 원하는 값을 편리하게 추출할 수 있습니다. @Controller (예시: @Controller public class MyController { ... } ) 클래스 레벨에 붙이며, 해당 클래스가 Spring MVC의 컨트롤러임을 나타냅니다. Spring이 이 클래스를 스캔하여 웹 요청을 처리하는 핸들러로 사용합니다. @RequestMapping (예시: @RequestMapping(value=\"/users\", method=RequestMethod.POST, consumes=\"application/json\", produces=\"application/json\") ) 특정 URL 경로와 HTTP 메서드를 처리할 메서드에 연결(매핑)합니다. 속성 상세: params : 특정 파라미터가 있을 때만 매핑 (예: params=\"mode=edit\" ). headers : 특정 헤더가 있을 때만 매핑 (예: headers=\"X-API-VERSION=2\" ). consumes : 요청의 Content-Type 을 제한. (예: application/json ) produces : 응답의 Content-Type 을 지정. (예: application/xml ) 추가 예시 (클래스/메서드 조합): @RequestMapping(\"/users\") public class UserController { @RequestMapping(\"/{id}\") public void getUser() { /* -> /users/{id} */ } } @RequestParam (예시: public void search(@RequestParam(value=\"q\", required=false, defaultValue=\"spring\") String query) ) URL의 쿼리 파라미터( ?q=... )나 x-www-form-urlencoded 형식의 form-data 값을 메서드 파라미터에 바인딩합니다. 추가 예시: 모든 파라미터 받기: public void allParams(@RequestParam Map<String, String> paramMap) 여러 값 받기: public void listParams(@RequestParam List<String> interest) (예: ?interest=a&interest=b ) Java 8 Optional 사용: public void optionalParam(@RequestParam Optional<String> query) @PathVariable (예시: @RequestMapping(\"/users/{userId}/orders/{orderId}\") public void method(@PathVariable Long userId, @PathVariable Long orderId) ) URL 경로의 일부를 변수로 사용할 때 씁니다. (예: /users/123/orders/456 에서 123 과 456 을 각각 userId , orderId 변수에 바인딩). 추가 예시: 변수명 다를 때: @PathVariable(\"userId\") Long id 정규식으로 제한: @GetMapping(\"/members/{memberId:[0-9]+}\") public void getMember(@PathVariable Long memberId) Map 으로 받기: @GetMapping(\"/{var1}/{var2}\") public void pathVars(@PathVariable Map<String, String> pathVarMap) @RequestHeader (예시: public void method(@RequestHeader(\"User-Agent\") String userAgent, @RequestHeader(name=\"Accept-Language\", required=false) String lang) ) 요청 헤더의 특정 값을 파라미터로 받아옵니다. 추가 예시: 모든 헤더 받기 (Map): @RequestHeader Map<String, String> headerMap 모든 헤더 받기 (객체): @RequestHeader MultiValueMap<String, String> multiValueMap , @RequestHeader HttpHeaders headers @CookieValue (예시: public void method(@CookieValue(value=\"JSESSIONID\", required=false) String sessionId) ) 요청에 포함된 쿠키의 값을 파라미터로 받아옵니다. 추가 예시: 쿠키가 없을 때 예외: required=true (기본값)이고 쿠키가 없으면 MissingRequestCookieException 발생. 기본값 설정: @CookieValue(defaultValue = \"guest\") String visitorId Level 3: 데이터 바인딩 및 응답/세션 처리 이 단계에서는 요청 데이터를 객체에 자동으로 담거나, View에 전달할 데이터 및 세션을 편리하게 관리할 수 있습니다. @ModelAttribute (예시 1: public String saveUser(@ModelAttribute UserDto userDto) , 예시 2: @ModelAttribute(\"categories\") public List<Category> getCategories() { ... } ) (파라미터에서 사용 시) 여러 요청 파라미터를 객체(DTO, VO)의 필드에 자동으로 바인딩합니다. (예: ?name=John&age=30 -> UserDto 객체). 생략 가능합니다. (메서드에 사용 시) 특정 메서드 위에 붙이면, 해당 컨트롤러의 모든 요청 처리 전에 이 메서드가 먼저 실행되고 반환값이 Model에 자동으로 추가됩니다. 추가 예시 (수정 폼): @ModelAttribute(\"user\") public User findUser(@PathVariable Long id) { return userRepository.findById(id); } public String updateUser(@ModelAttribute(\"user\") User user) { /* ... */ } (위 예시는 findUser 로 DB에서 user를 조회해 모델에 넣고, 요청 파라미터로 그 user 객체를 덮어씁니다.) Model / ModelMap / Map (예시: public String method(Model model) ) View에 전달할 데이터를 담는 컨테이너 역할을 합니다. model.addAttribute(\"key\", value) 형태로 데이터를 추가하면 View(JSP, Thymeleaf 등)에서 해당 키로 데이터를 사용할 수 있습니다. Model , ModelMap , Map 은 사실상 동일하게 동작합니다. ModelAndView (예시: public ModelAndView method() { ModelAndView mav = new ModelAndView(\"user/profile\"); mav.addObject(\"user\", user); return mav; } ) Model과 View를 하나로 합친 객체입니다. 처리 결과를 보여줄 View의 이름과 View에 전달할 데이터를 함께 담아 반환할 수 있습니다. 추가 예시 (상태 코드 설정): mav.setStatus(HttpStatus.CREATED); @SessionAttributes (예시: @Controller @SessionAttributes(\"user\") ) 컨트롤러 클래스 레벨에 사용하여, Model에 추가된 특정 이름의 속성을 HTTP 세션에도 저장하도록 지정합니다. 여러 페이지에 걸쳐 특정 객체(ex: 장바구니, 폼 데이터)를 유지해야 할 때 편리합니다. SessionStatus 와 함께 사용: public String complete(SessionStatus status) { status.setComplete(); return \"redirect:/\"; } (세션 데이터 정리) @SessionAttribute (예시: public void method(@SessionAttribute(name=\"user\", required=false) User loggedInUser) ) HTTP 세션에 저장된 특정 속성을 직접 파라미터로 받아올 때 사용합니다. @SessionAttributes 와 달리, 세션에 있는 값을 직접 조회하는 용도입니다. 다른 곳(e.g., 필터)에서 세션에 넣은 값을 꺼낼 때 유용합니다. RedirectAttributes (예시: public String save(RedirectAttributes redirectAttributes) { redirectAttributes.addFlashAttribute(\"message\", \"저장되었습니다!\"); return \"redirect:/users\"; } ) 리다이렉트 시 데이터를 전달하는 데 특화된 객체입니다. addFlashAttribute : 1회성 데이터로, 리다이렉트된 페이지에서만 사용되고 세션에서 즉시 사라집니다. (Post-Redirect-Get 패턴에 유용). URL에 노출되지 않습니다. addAttribute : URL 쿼리 파라미터로 추가됩니다. (예: return \"redirect:/users/{id}\" 와 함께 사용 시 {id} 에 바인딩, 나머지는 쿼리 파라미터) Level 4: REST API를 위한 추상화 이 단계는 전통적인 HTML View 반환이 아닌, JSON/XML과 같은 데이터 자체를 응답하는 RESTful API 개발에 특화된 고수준 추상화입니다. @RequestBody (예시: public UserDto createUser(@RequestBody CreateUserRequest request) ) 요청의 Body에 담겨 오는 데이터(주로 JSON, XML)를 Java 객체로 변환(역직렬화)합니다. 내부적으로 HttpMessageConverter (주로 Jackson 라이브러리)가 동작합니다. 추가 예시: 유효성 검사: public void create(@Valid @RequestBody UserDto userDto, BindingResult bindingResult) Raw 데이터 받기: public void rawJson(@RequestBody String jsonString) Map으로 받기: public void mapBody(@RequestBody Map<String, Object> dataMap) @ResponseBody (예시: @ResponseBody @GetMapping(\"/api/users/1\") public UserDto getUser() { ... } ) 메서드가 반환하는 Java 객체를 HTTP 응답 Body에 직접 써넣도록 지시합니다. View를 찾는 것이 아니라, 반환된 객체를 JSON이나 XML 등의 데이터 형식으로 변환(직렬화)하여 클라이언트에 전송합니다. HttpEntity<T> (예시: public String process(HttpEntity<String> httpEntity) { String body = httpEntity.getBody(); HttpHeaders headers = httpEntity.getHeaders(); ... } ) 요청의 헤더와 바디를 함께 감싼 객체입니다. @RequestBody 와 @RequestHeader 를 합친 것과 유사하며, 요청의 모든 요소를 한 번에 받아 분석할 때 유용합니다. 응답 시에도 사용 가능합니다. ResponseEntity<T> (예시: public ResponseEntity<UserDto> getUser(@PathVariable Long id) { UserDto user = userService.findById(id); HttpHeaders headers = new HttpHeaders(); headers.add(\"X-Custom-Header\", \"value\"); return new ResponseEntity<>(user, headers, HttpStatus.OK); } ) @ResponseBody 의 확장판으로, 응답 데이터(Body)뿐만 아니라 HTTP 상태 코드(Status Code)와 헤더(Header)까지 세밀하게 제어하여 반환하고 싶을 때 사용합니다. 추가 예시 (빌더 패턴): 성공: return ResponseEntity.ok(user); 생성됨: URI location = ...; return ResponseEntity.created(location).build(); 콘텐츠 없음: return ResponseEntity.noContent().build(); 요청 오류: return ResponseEntity.badRequest().body(\"Error message\"); @ResponseStatus (예시: @ResponseStatus(HttpStatus.CREATED) @PostMapping(\"/users\") public void createUser(...) ) 성공적인 응답의 HTTP 상태 코드를 지정하는 어노테이션입니다. void 를 반환하거나 데이터만 반환하면서도 상태 코드를 200 OK가 아닌 201 Created, 204 No Content 등으로 설정하고 싶을 때 간편하게 사용합니다. 추가 예시 (예외 클래스에 적용): @ResponseStatus(value = HttpStatus.NOT_FOUND, reason = \"User not found\") public class UserNotFoundException extends RuntimeException {} Level 5: 편의성을 위한 조합 및 축약 (가장 높은 추상화 단계) 이 단계에서는 자주 사용되는 패턴을 하나의 어노테이션으로 묶어 코드의 가독성과 생산성을 높입니다. '문법적 설탕(Syntactic Sugar)'의 대표적인 예시들입니다. @RestController (예시: @RestController @RequestMapping(\"/api/users\") public class UserApiController { ... } ) @Controller + @ResponseBody 의 조합입니다. 이 어노테이션을 클래스에 붙이면, 해당 컨트롤러의 모든 메서드에는 기본적으로 @ResponseBody 가 적용됩니다. 따라서 모든 메서드가 View를 반환하는 대신 데이터(JSON 등)를 반환하게 되므로, REST API를 만들 때 매우 편리합니다. @GetMapping (예시: @GetMapping(\"/{id}\") ) @RequestMapping(method = RequestMethod.GET) 의 축약형입니다. @PostMapping (예시: @PostMapping ) @RequestMapping(method = RequestMethod.POST) 의 축약형입니다. @PutMapping (예시: @PutMapping(\"/{id}\") ) @RequestMapping(method = RequestMethod.PUT) 의 축약형입니다. @DeleteMapping (예시: @DeleteMapping(\"/{id}\") ) @RequestMapping(method = RequestMethod.DELETE) 의 축약형입니다. @PatchMapping (예시: @PatchMapping(\"/{id}\") ) @RequestMapping(method = RequestMethod.PATCH) 의 축약형입니다. @ControllerAdvice / @RestControllerAdvice (예시: @RestControllerAdvice public class GlobalExceptionHandler { @ExceptionHandler(IllegalArgumentException.class) @ResponseStatus(HttpStatus.BAD_REQUEST) public ErrorResponse handleIllegalArgument(Exception e) { ... } } ) 전역 설정을 위한 고수준 추상화입니다. @ExceptionHandler : 여러 컨트롤러에서 발생하는 특정 예외를 한 곳에서 공통으로 처리합니다. @ModelAttribute : 모든 컨트롤러에 공통으로 필요한 모델 데이터를 추가합니다. @InitBinder : 모든 컨트롤러에 적용될 데이터 바인딩 설정을 합니다. @RestControllerAdvice 는 @ControllerAdvice 와 @ResponseBody 를 합친 것으로, 예외 처리 결과 자체를 JSON 같은 데이터로 응답할 때 사용합니다. 요약: 추상화의 흐름 Spring은 개발자가 저수준의 반복적인 작업을 하지 않고, 비즈니스 로직에 집중할 수 있도록 점점 더 편리하고 높은 수준의 추상화를 제공하는 방향으로 발전해왔습니다. Servlet API (HttpServletRequest) \"HTTP 요청의 모든 것을 내가 직접 다룰게.\" → @RequestMapping \"이 URL을 이 메서드에 연결하는 건 Spring에게 맡길게.\" → @RequestParam , @RequestHeader , @CookieValue \"요청에서 값 꺼내는 귀찮은 일도 Spring이 알아서 해줘.\" → @ModelAttribute \"파라미터가 많아도 괜찮아. 객체로 한 번에 받을 수 있어.\" → @RequestBody \"요청 Body의 JSON을 이 객체로 바꾸는 건 이제 신경 안 쓸래.\" → @ResponseBody \"메서드 반환값을 바로 JSON으로 응답해줘. Java 객체를 다시 JSON으로 바꾸는 것도 Spring이 알아서 해줘.\" → @RestController \"매번 @ResponseBody 붙이기 귀찮으니 이 컨트롤러는 전부 REST API용으로 @RestController 만 선언할게.\" → @GetMapping 등 HTTP 메서드별 축약 어노테이션 \"코드를 더 짧고 명확하게 만들자.\" → @RestControllerAdvice \"모든 컨트롤러에서 발생하는 예외는 여기서 한 번에 처리하자. 중복되는 예외 처리 코드는 한 곳에 모아서 관리할래.\" 각 단계의 특징을 이해하면 상황에 맞는 최적의 도구를 선택하여 효율적으로 개발할 수 있습니다.",
      "frontmatter": {
        "tags": [
          "spring"
        ],
        "date": "2025-07-09T07:40:09+09:00",
        "lastmod": "2025-07-12T13:15:36+09:00"
      }
    },
    "sql 에서 백틱, 작은따옴표, 큰따옴표 의미": {
      "path": "/02.inbox/sql-에서-백틱-작은따옴표-큰따옴표-의미/",
      "filename": "sql 에서 백틱, 작은따옴표, 큰따옴표 의미",
      "content": "각 데이터베이스 관리 시스템(DBMS)에서 백틱(\\ ## Oracle ): Oracle에서는 백틱을 사용하지 않습니다. 특별한 의미를 가지지 않으며, 식별자나 문자열을 나타내는 데 사용되지 않습니다. 작은따옴표(\\'): 문자열 리터럴을 정의하는 데 사용됩니다. 예를 들어, SELECT '문자열' FROM DUAL; 과 같이 사용할 수 있습니다. 큰따옴표(\"): 식별자(테이블 이름, 컬럼 이름 등)를 정의하는 데 사용됩니다. 대소문자를 구분하며, 기본적으로 Oracle은 대소문자를 구분하지 않지만, 큰따옴표로 묶인 식별자는 대소문자를 구분합니다. 예를 들어, SELECT \"Column\" FROM \"MyTable\"; 과 같이 사용할 수 있습니다. MySQL 백틱(\\ - **작은따옴표(')**: 문자열 리터럴을 정의하는 데 사용됩니다. - **큰따옴표(\")**: 기본 설정(SQL 모드)에 따라 다르지만, ANSI_QUOTES SQL 모드가 활성화되어 있을 때 식별자를 묶는 데 사용될 수 있습니다. 그렇지 않은 경우에는 작은따옴표와 같이 문자열 리터럴을 정의하는 데 사용됩니다. ## PostgreSQL ): PostgreSQL에서는 백틱을 사용하지 않습니다. 특별한 의미를 가지지 않으며, 식별자나 문자열을 나타내는 데 사용되지 않습니다. 작은따옴표('): 문자열 리터럴을 정의하는 데 사용됩니다. 큰따옴표(\"): 식별자를 묶는 데 사용됩니다. PostgreSQL에서는 큰따옴표로 묶인 식별자가 대소문자를 구분합니다. 예를 들어, SELECT \"Column\" FROM \"MyTable\"; 에서 \"MyTable\"과 \"Column\"은 대소문자를 구분하는 식별자입니다.",
      "frontmatter": {
        "tags": [
          "database",
          "잡지식"
        ],
        "date": "2024-05-21T21:17:00+09:00",
        "lastmod": "2024-05-21T21:17:00+09:00"
      }
    },
    "sse (server sent event)": {
      "path": "/02.inbox/sse-server-sent-event/",
      "filename": "sse (server sent event)",
      "content": "Back package com.example.sse.controller; import org.springframework.http.MediaType; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.servlet.mvc.method.annotation.SseEmitter; import java.io.IOException; import java.util.Random; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; @RestController public class SseController { private final Random random = new Random(); @GetMapping(value = \"/sse\", produces = MediaType.TEXT_EVENT_STREAM_VALUE) public SseEmitter handleSse() { SseEmitter emitter = new SseEmitter(60_000L); // 타임아웃 설정 // ExecutorService executor = Executors.newSingleThreadExecutor(); executor.execute(() -> { try { for (int i = 0; i < 10; i++) { // 10번의 무작위 이벤트 int delay = random.nextInt(3000) + 1000; // 1~4초 사이 랜덤 지연 Thread.sleep(delay); String eventData = \"Event at \" + System.currentTimeMillis(); emitter.send(eventData); } emitter.complete(); } catch (IOException | InterruptedException e) { emitter.completeWithError(e); } finally { executor.shutdown(); } }); return emitter; } } produces = MediaType.TEXT_EVENT_STREAM_VALUE : 클라이언트에게 text/event-stream 형식의 데이터를 보낸다는 의미 → SSE 방식. 반환형은 SseEmitter 객체이며, 서버에서 클라이언트로 일방향 통신을 가능하게 합니다. SseEmitter emitter = new SseEmitter(60_000L); // 타임아웃 60초 클라이언트가 60초 동안 응답을 받지 않으면 연결이 종료됩니다. 별도의 스레드에서 비동기적으로 작업 수행. 총 10번의 이벤트를 랜덤한 시간 간격(1~4초)으로 전송. emitter.send(data) 로 클라이언트에게 데이터를 실시간 전송. Front <!DOCTYPE html> <html lang=\"ko\"> <head> <meta charset=\"UTF-8\"> <title>SSE 실시간 이벤트</title> </head> <body> <h2>실시간 이벤트 수신</h2> <ul id=\"eventList\"></ul> <script> const eventSource = new EventSource(\"/sse\"); eventSource.onmessage = function(event) { const li = document.createElement(\"li\"); li.textContent = event.data; document.getElementById(\"eventList\").appendChild(li); }; eventSource.onerror = function(err) { console.error(\"SSE 오류:\", err); }; </script> </body> </html>",
      "frontmatter": {
        "tags": [
          "web"
        ],
        "date": "2025-05-26T14:09:00+09:00",
        "lastmod": "2025-05-26T14:09:00+09:00"
      }
    },
    "terminal auto logging": {
      "path": "/02.inbox/terminal-auto-logging/",
      "filename": "terminal auto logging",
      "content": "Terminal Auto Logger 프로젝트 개요 프로젝트 이름: terminal auto logger 로그 디렉토리: ~/.local/log/terminal_auto_log 핵심 동작: terminal_auto_logger.sh : 쉘 시작 시 실행되어 로깅 환경을 설정하고, 실제 로깅을 담당하는 Executor 스크립트를 호출합니다. _tal_executor.sh : script 명령을 직접 실행하고, 세션이 종료된 후 로그 파일의 이름을 최종적으로 결정합니다. (내부 헬퍼 스크립트) Executor 스크립트 생성: _tal_executor.sh 이 스크립트는 실제 로깅을 실행하고 세션 종료 후 파일명 변경을 담당하는 핵심 로직입니다. 사용자가 직접 실행하는 것이 아닌, 메인 스크립트에 의해 호출됩니다. 파일 위치: ~/.local/bin/_tal_executor.sh #!/bin/bash # 이 스크립트는 terminal_auto_logger.sh에 의해 호출되는 내부 헬퍼입니다. # 사용자가 직접 실행하지 마세요. # 인자 파싱 TEMP_LOG_FILE=\"$1\" LOG_DIR=\"$2\" PID=\"$3\" TTY_NAME=\"$4\" START_TIMESTAMP=\"$5\" # 'script' 명령으로 실제 로깅 세션 시작 # -q: 시작/종료 메시지 숨김 # -f: 실시간으로 버퍼를 비워 파일에 기록 # -a: 파일에 이어쓰기 (메타데이터를 먼저 기록했으므로 필수) script -qfa \"$TEMP_LOG_FILE\" EXIT_CODE=$? # script 명령의 종료 코드 캡처 # 세션 종료 후 로그 파일 이름 변경 # 임시 로그 파일이 존재하고 내용이 있을 때만 이름 변경을 시도 if [ -s \"$TEMP_LOG_FILE\" ]; then END_TIMESTAMP=$(date +\"%Y%m%d_%H%M%S\") # 종료 코드에 따라 최종 파일명 결정 if [ \"$EXIT_CODE\" -eq 0 ]; then # 정상 종료 (exit, logout 등) END_PART=\"$END_TIMESTAMP\" else # 비정상 종료 (kill, 시스템 다운 등) END_PART=\"aborted\" fi FINAL_LOG_FILE=\"${LOG_DIR}/terminal_(${START_TIMESTAMP} ~ ${END_PART})_${PID}_${TTY_NAME}.log\" mv \"$TEMP_LOG_FILE\" \"$FINAL_LOG_FILE\" else # 세션 동안 아무런 출력이 없었으면 임시 파일 삭제 rm -f \"$TEMP_LOG_FILE\" fi 메인 스크립트 개선: terminal_auto_logger.sh 이 스크립트는 쉘 설정 파일( .bashrc , .zshrc )에서 호출되며, 모든 안정성 검사와 환경 설정을 담당합니다. 파일 위치: ~/.local/bin/terminal_auto_logger.sh #!/bin/bash # --- 안정성 검사 --- # 1. 'script' 명령어가 없으면 즉시 종료 if ! command -v script &> /dev/null; then echo \"Terminal Auto Logger: 'script' command not found. Logging disabled.\" >&2 return 1 fi # 2. 이미 로깅 중이거나(재귀 방지), tmux/screen 세션 내에서는 실행하지 않음 if [ -n \"$TERMINAL_AUTO_LOGGER_ACTIVE\" ] || [ -n \"$TMUX\" ] || [[ \"$TERM\" == screen* ]]; then return 0 fi # --- 로깅 준비 --- # 로그 디렉토리 생성 LOG_DIR=\"$HOME/.local/log/terminal_auto_log\" mkdir -p \"$LOG_DIR\" # 로그 파일명에 사용할 변수 설정 TTY_NAME=$(tty | sed 's|/|_|g' | sed 's|[^a-zA-Z0-9_-]||g') START_TIMESTAMP=$(date +\"%Y%m%d_%H%M%S\") PID=$$ # 종료 후 이름이 변경될 임시 로그 파일 TEMP_LOG_FILE=\"${LOG_DIR}/session_${START_TIMESTAMP}.tmp\" # --- 로깅 시작 --- # 로깅 세션 메타데이터를 임시 파일에 기록 { echo \" SCRIPT_SESSION_START\" echo \"=================================================\" echo \" Project: Terminal Auto Logger\" echo \" User: $(whoami)\" echo \" Host: $(hostname)\" echo \" TTY: $(tty)\" echo \" Shell: $SHELL\" echo \" Started: $(date)\" echo \" Ended: \" echo \"=================================================\" } > \"$TEMP_LOG_FILE\" # 이어쓰기가 아닌 새로 쓰기 # 재귀 실행 방지를 위한 환경 변수 설정 export TERMINAL_AUTO_LOGGER_ACTIVE=1 # 현재 쉘을 Executor 스크립트로 대체하여 실행 # 필요한 모든 정보를 인자로 전달 exec ~/.local/bin/_tal_executor.sh \\ \"$TEMP_LOG_FILE\" \\ \"$LOG_DIR\" \\ \"$PID\" \\ \"$TTY_NAME\" \\ \"$START_TIMESTAMP\" 설치 및 설정 1단계: 디렉토리 생성 사용자 로컬 바이너리 디렉토리와 로그 디렉토리를 생성합니다. ( ~/.local/bin 은 많은 시스템에서 자동으로 PATH에 추가됩니다.) mkdir -p ~/.local/bin mkdir -p ~/.local/log/terminal_auto_log 2단계: 스크립트 저장 및 권한 설정 위에서 작성한 두 스크립트 코드를 각각 ~/.local/bin/terminal_auto_logger.sh 와 ~/.local/bin/_tal_executor.sh 에 저장한 후, 실행 권한을 부여합니다. chmod +x ~/.local/bin/terminal_auto_logger.sh chmod +x ~/.local/bin/_tal_executor.sh 3단계: 쉘 설정 파일에 적용 ( ~/.bashrc 또는 ~/.zshrc ) 쉘 설정 파일 가장 아래쪽에 다음 코드를 추가합니다. Bash 사용자 ( ~/.bashrc ): # Terminal Auto Logger (안정성 강화 버전) # ~/.local/bin 이 PATH에 있는지 확인하고, 없다면 추가 if [[ \":$PATH:\" != *\":$HOME/.local/bin:\"* ]]; then export PATH=\"$HOME/.local/bin:$PATH\" fi if [[ -t 1 && -z \"$TERMINAL_AUTO_LOGGER_ACTIVE\" ]]; then source ~/.local/bin/terminal_auto_logger.sh fi Zsh 사용자 ( ~/.zshrc ): # Terminal Auto Logger (안정성 강화 버전) # ~/.local/bin 이 PATH에 있는지 확인하고, 없다면 추가 if [[ \":$PATH:\" != *\":$HOME/.local/bin:\"* ]]; then export PATH=\"$HOME/.local/bin:$PATH\" fi if [[ -o interactive && -z \"$TERMINAL_AUTO_LOGGER_ACTIVE\" ]]; then source ~/.local/bin/terminal_auto_logger.sh fi 4단계: 적용 source ~/.bashrc (또는 source ~/.zshrc )를 실행하거나 새 터미널을 열면 로깅이 자동으로 시작됩니다. 💡 주요 개선 사항 및 작동 원리 2-Script 구조의 안정성: 메인 스크립트( terminal_auto_logger.sh )는 환경 검사와 설정만 담당하여 역할이 명확합니다. exec 를 통해 쉘 프로세스가 Executor 스크립트( _tal_executor.sh )로 완전히 대체됩니다. Executor는 script 가 종료될 때까지 기다린 후, 그 종료 상태(성공/실패)를 확인하여 후처리(파일명 변경)를 수행합니다. 이 구조 덕분에 세션 종료 시점의 정보를 파일명에 안정적으로 반영할 수 있습니다. 정확한 파일명 생성: 세션이 시작되면 .tmp 확장자를 가진 임시 파일에 로그가 기록됩니다. 사용자가 exit 등으로 정상 종료하면 script 는 종료 코드 0 을 반환하고, 파일명은 ... (시작시간 ~ 종료시간).log 형식으로 변경됩니다. 세션이 kill 명령으로 강제 종료되거나 비정상적으로 끝나면, script 는 0 이 아닌 종료 코드를 반환하고, 파일명은 ... (시작시간 ~ aborted).log 형식으로 변경되어 문제 상황을 명확히 알려줍니다. 예측 가능한 동작: tmux / screen 환경이나 이미 로깅 중인 세션에서는 중복 실행되지 않아 충돌을 원천적으로 방지합니다. 환경 변수명을 TERMINAL_AUTO_LOGGER_ACTIVE 로 명확하게 지정하여 다른 프로그램과의 잠재적 충돌 가능성을 최소화했습니다. 로그 디렉토리와 스크립트 위치를 ~/.local 아래로 표준화하여 시스템을 깔끔하게 유지합니다. 이 개선된 솔루션은 요구하신 모든 사항을 만족시키면서, 예측 불가능한 상황에 더욱 잘 대처할 수 있도록 설계되었습니다. 안정적이고 신뢰도 높은 터미널 활동 기록 시스템으로 활용하실 수 있습니다.",
      "frontmatter": {
        "tags": [
          "project"
        ],
        "date": "2025-09-09T17:16:26+09:00",
        "lastmod": "2025-09-09T17:17:45+09:00"
      }
    },
    "time 명령어": {
      "path": "/02.inbox/time-명령어/",
      "filename": "time 명령어",
      "content": "실제 경과 시간을 나타내는 \"Real\", 프로세스에서 사용된 CPU 시간만을 나타내는 \"User\" 및 \"Sys\"는 커널 내에서 실행되는 코드에서 사용된 CPU 시간을 나타냅니다. Real(실제 시간): 호출의 시작에서 끝까지의 경과 시간으로, 다른 프로세스에 의해 사용된 시간 조각과 프로세스가 차단된 시간(예를 들어 I/O 완료를 기다리는 경우)을 포함한 모든 경과 시간입니다. User(사용자 모드 시간): 프로세스 내에서 사용자 모드 코드(커널 외부)에서 소비한 CPU 시간입니다. 이는 프로세스를 실행하는 데 사용된 실제 CPU 시간만을 나타냅니다. 다른 프로세스 및 프로세스가 차단된 시간은 이 숫자에 포함되지 않습니다. Sys(커널 모드 시간): 프로세스 내에서 커널 내에서 실행된 CPU 시간으로, 커널 내에서 시스템 콜 내에서 실행된 CPU 시간을 의미합니다. 이는 여전히 사용자 공간에서 실행되는 라이브러리 코드와는 대조적입니다. \"User\"와 마찬가지로 이는 프로세스에 의해 사용된 CPU 시간만을 나타냅니다. User+Sys(사용자 및 커널 모드 시간 합): 이것은 프로세스가 실제로 사용한 CPU 시간을 나타냅니다. 여러 스레드가 있는 경우 (이 프로세스가 여러 프로세서를 가진 컴퓨터에서 실행 중인 경우) 이 값은 일반적으로 \"Real\"에서 보고된 경과 시간을 초과할 수 있습니다. time 명령어로 보고된 이 통계는 다양한 시스템 호출로부터 수집됩니다. 'User' 및 'Sys'는 wait(2) 또는 times(2) (POSIX)에서 나올 수 있습니다. 'Real'은 gettimeofday(2) 호출에서 얻은 시작 및 종료 시간에서 계산됩니다. 커널 모드와 사용자 모드에 대한 간단한 소개에서 '커널' 또는 '슈퍼바이저' 모드는 CPU가 운영 중인 특권 모드를 나타냅니다. 일부 특권된 동작은 CPU가 이 모드에서 작동할 때만 수행할 수 있으며 이러한 동작은 응용 프로그램 코드에서 사용할 수 없습니다. \"System\" 호출은 사용자 모드에서 실행되는 코드를 가지며 이는 C 프로그램에서 직접 호출됩니다. 그러나 뒷면에서는 I/O와 같은 특정 서비스를 수행하기 위해 하나 이상의 시스템 호출을 커널에 발행할 수 있습니다. 마지막으로 'sys'에 대한 추가 정보로 \"sys\" 시간은 사용자 모드에서 수행할 수 없는 작업들을 나타냅니다. 메모리 할당이나 하드웨어(하드디스크, 네트워크 등)에 접근하는 것과 같은 작업은 커널의 지도에서 수행되며 이 작업은 'sys' 시간으로 계산됩니다. 그러나 단순한 \"모든 malloc 호출이 'sys' 시간에 포함될 것\"이라고 할 수는 없습니다. malloc 호출은 자체적인 처리를 수행하며 그 중간에 어딘가에서 커널을 호출할 것입니다. 그 후 커널 호출에서 반환한 후에도 'user' 시간이 일부 소요됩니다. 언제 이 전환이 발생하고 얼마나 많은 것이 커널 모드에서 소요되는지는 구현에 따라 다를 수 있습니다.",
      "frontmatter": {
        "tags": [
          "command",
          "linux",
          "잡지식"
        ],
        "date": "2023-12-21T04:32:00+09:00",
        "lastmod": "2023-12-21T04:32:00+09:00"
      }
    },
    "wsl 에서 폴더가 초록색 배경으로 보이는 이유": {
      "path": "/02.inbox/wsl-에서-폴더가-초록색-배경으로-보이는-이유/",
      "filename": "wsl 에서 폴더가 초록색 배경으로 보이는 이유",
      "content": "전제조건 초록색 이유 선택 전제 조건 윈도우 파일과 유닉스 파일의 파일 시스템 차이 전제조건 wsl 우분투 배포판을 기준 debian 계열은 거이 비슷 wsl 측에서 바라본 윈도우 파일 시스템(ntfs) 를 wsl 환경에서 윈도우측 파일시스템을 /mnt 폴더 아래에 보조메모리 디바이스 장치 이름 으로 마운트 되어있다 ModificationRequired shinnk@DESKTOP-KRSG68U:/mnt$ mount C:\\ on /mnt/c type 9p (rw,noatime,dirsync,aname=drvfs;path=C:\\;uid=1000;gid=1000;symlinkroot=/mnt/,mmap,access=client,msize=65536,trans=fd,rfd=5,wfd=5) ls, dir, grep, 등등의 명령은 화면에 적절한 색깔을 입혀서 표시할 필요가 있는데 이때 LS_COLORS 라는 환경변수를 참조해서 적절히 색깔을 입혀서 출력한다 그렇다면 LS_COLORS 환경변수를 확인해보자 set | grep \"LS_COLORS\" 명령을 통해 확인해보면 여러가지 설정이 지정되어있는 것을 확인 할 수 있다 예를 들어 di=01;34: 를 보면 di(directory) 라는 변수에 01;34 라는 변수가 할당되어 있고 ( : 는 구분자이다) 01;34는 SRG(select graphic rendition) 의 문법을 따른 방식이다) ANSI escape code 를 참조 01 굵은 글꼴 bold 효과 ; => 속성 구분자 34 => 3비트 컬러 표시에서 전경색(글자색) 34번 색상 파란색 [](https://en.wikipedia.org/wiki/ANSIescapecode#Colors)을 나타낸다 shinnk@DESKTOP-KRSG68U:~$ set | grep \"LS_COLORS\" LS_COLORS='rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31' 잠깐 추가적으로 LSCOLORS 환경변수는 정적으로 로드된 것일까?? 최소한 우분투에서는 dircolors 라는 실행파일을 통해 로그인 셸이 실행될 때각 셸(sh, zsh, bash 등등)에 맞는 설정을(LSCOLORS 환경변수에 적절한 값을 로드) 하도록 만든다 아래는 .bashrc 파일의 일부이다 셸에 로그인 될때 .bashrc 가 실행되고 이 파일에서 dircolors 를 실행시킨다 또한 LS_COLORS 를 사용하는 ls,grep 와 같은 프로그램에 옵션을 기본적으로 추가시켜 alias 시켜둔다 # enable color support of ls and also add handy aliases if [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\" alias ls='ls --color=auto' #alias dir='dir --color=auto' #alias vdir='vdir --color=auto' alias grep='grep --color=auto' alias fgrep='fgrep --color=auto' alias egrep='egrep --color=auto' fi echo $TERM # `TERM` 변수의 출력이 `xterm-256color`, `screen-256color`, `tmux-256color`와 같은 값이면 256색을 지원하는 터미널입니다. 실제로는 이렇게 출력 되더락도 24비트 컬러(16,777,216가지 색상)를 지원한다 for i in {0..255}; do printf \"\\x1b[38;5;${i}mcolour${i}\\n\" done # 모든 컬러 확인 모든 디렉토리는 굵은 파란색으로 보여지게 하고 있다 그렇다면 모든 사용자에게 w 쓰기 권한이 주어진다면 왜 초록색 배경으로 보이게 될까 초록색으로 보이는 이유 env | grep \"LS_COLORS\" |grep ow 라고 입력하면 ow 변수(모든 사용자가 쓰기가능한 디렉토리)에 입력된 값이 보인다 아까 보았던 LS_COLORS 에 설정된 값중 ow 변수를 확인해 보았다 ( ow=34;42 나의 경우 이미 설정된 값은 이렇다) 이것은 34(전경색 파란색) 42(배경색 초록색) 이다 윈도우 파일시스템인 ntfs 의 경우 유닉스에서 관리하는 일반적인 ext4 zfs 등등의 파일시스템과는 달리 파일의 권한을 저장하는 방식이 다르다 (ACL 이라고 하는데 아직 모르겠다) 그런데 wsl 측에서 윈도우 파일 시스템을 마운트 할 때 모든 파일(폴더 포함)에 쓰기 권한을 (rwx 중 x 를 설정 해두었다) wsl 의 사용자와 윈도우 사용자가 다르기 때문에 파일 이동 및 수정을 자유롭게 하기 위해서 이렇게 한것 같다 그래서 ow 변수에 설정된 값이 작동하는 것이다 말이 매우 길었지만 결론은 매우 간단하다(?) 윈도우측 사용자와 wsl linux 측 사용자의 차이로 인한 파일 이동 및 수정에 불편함을 만들지 않기 위해 모든 사용자가 쓰기 가능한 디렉토리라고 정하였고 이때 linux 에서는 윈도우 측 파일에 외부 사용자의 쓰기(x) 권한이 등록 되어 있으므로 사용자가 이것을 건들 가능성이 있으니 보안의 목적으로 특별한 색깔로 보였던 것이다 추가 전제 조건 LS_CLOLORS 의 ow 변수 env | grep \"LS_COLORS\" |grep ow 라고 입력하면 ow 변수에 입력된 값이 보인다 ( ow=34;42 나의 경우 이미 설정된 값은 이렇다) 이것은 34(전경색 파란색) 42(배경색 초록색) 이다 그러면 ow 쓰기가능한 디렉토리 조금더 명확히 이해하기 이해해보기 위해 LS_COLORS 을 정확하게 이해해보자 아까 보았던 ~/.bashrc 파일의 일부이다 if [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\" alias ls='ls --color=auto' #alias dir='dir --color=auto' #alias vdir='vdir --color=auto' alias grep='grep --color=auto' alias fgrep='fgrep --color=auto' alias egrep='egrep --color=auto' fi dircolors 명령이 실행가능할 때 AND .dircolos 폴더가 있으면 그것을 사용하고 없으면 dircolors 기본 설정을 사용한다는 의미이다 즉 dircolors가 LSCOLORS 를 설정한다 이후 The LS_COLORS environment variable can change the settings. Use the dircolors command to set it. ls manual 페이지에 처럼 ls 명령이 LSCOLORS 환경변수를 통해 적절히 출력되게 된다 dircolors 수동 설정 시 적용가능한 변수목록 Pasted image 20240611092335론: bashrc 파일에서 dircolors 존재하는지 확인 -> dircolors 이 LSCOLORS 변수 설정 -> ls 명령 시행시에 LSCOLORS 변수를 사용해서 출력 실제 LS_COLORS 를 통해 파일이 보이는 방식을 보여주는 스크립트 echo \"$LS_COLORS\" | sed 's/:/\\n/g' | while IFS== read -r key value; do if [[ -n \"$value\" ]]; then printf \"\\e[${value}m%s\\e[m (%s)\\n\" \"$key\" \"$value\" fi done 또는 declare -A descriptions=( [bd]=\"block device\" [ca]=\"file with capability\" [cd]=\"character device\" [di]=\"directory\" [do]=\"door\" [ex]=\"executable file\" [fi]=\"regular file\" [ln]=\"symbolic link\" [mh]=\"multi-hardlink\" [mi]=\"missing file\" [no]=\"normal non-filename text\" [or]=\"orphan symlink\" [ow]=\"other-writable directory\" [pi]=\"named pipe, AKA FIFO\" [rs]=\"reset to no color\" [sg]=\"set-group-ID\" [so]=\"socket\" [st]=\"sticky directory\" [su]=\"set-user-ID\" [tw]=\"sticky and other-writable directory\" ) # Split LS_COLORS by colon IFS=':' color_prev=\"\" for ls_color in $LS_COLORS; do # Extract color code and file type color=\"${ls_color#*=}\" type=\"${ls_color%=*}\" # Get description if available desc=\"${descriptions[$type]}\" # Print newline when color changes (except first) if [ -n \"$color_prev\" ] && [ \"$color\" != \"$color_prev\" ]; then echo fi # Print colored type + description printf '\\e[%sm%s%s\\e[m ' \"$color\" \"$type\" \"${desc:+ ($desc)}\" # Remember last color color_prev=\"$color\" done echo 윈도우 파일시스템(NTFS) 와 유닉스 파일시스템 NTFS 파일 시스템은 기본적으로 Windows 운영 체제에서 사용되는 파일 시스템입니다. 유닉스 기반 시스템에서 NTFS 파일 시스템을 마운트하고 접근할 때 권한 설정은 다음과 같은 방식으로 처리됩니다: NTFS-3G 드라이버 사용: 대부분의 리눅스 배포판은 NTFS 파일 시스템을 읽고 쓸 수 있는 NTFS-3G 드라이버를 사용합니다. 이 드라이버는 NTFS 파일 시스템의 파일과 디렉터리에 접근할 수 있게 해줍니다. 마운트 옵션: NTFS-3G를 사용하여 NTFS 파일 시스템을 마운트할 때, 기본적으로 모든 파일과 디렉터리가 특정 사용자의 소유로 설정됩니다. 일반적으로 uid 와 gid 마운트 옵션을 사용하여 파일과 디렉터리의 소유자와 그룹을 지정할 수 있습니다. sudo mount -t ntfs-3g /dev/sdX1 /mnt/ntfs -o uid=1000,gid=1000 위 명령어에서 /dev/sdX1 은 NTFS 파티션, /mnt/ntfs 는 마운트 지점, uid=1000 과 gid=1000 은 특정 사용자의 UID와 GID를 의미합니다. 권한 설정: NTFS 파일 시스템은 유닉스 파일 시스템과는 다르게 파일 권한을 저장하지 않습니다. 그래서 마운트할 때 지정한 UID와 GID가 모든 파일과 디렉터리의 소유자와 그룹으로 설정됩니다. 또한, umask , fmask , dmask 옵션을 사용하여 파일과 디렉터리의 권한을 설정할 수 있습니다. sudo mount -t ntfs-3g /dev/sdX1 /mnt/ntfs -o uid=1000,gid=1000,umask=022 여기서 umask=022 는 파일과 디렉터리의 기본 권한을 설정합니다. 권한 변경: NTFS 파일 시스템은 유닉스 파일 시스템과 달리 파일 시스템 수준에서 파일 권한을 저장하지 않기 때문에, NTFS 파일 시스템에 있는 파일의 권한을 변경하려고 하면 실제로는 변경되지 않습니다. 마운트 옵션을 통해서만 권한을 제어할 수 있습니다. tput colors stty -a 위의 두개는 뭐야?? #ModificationRequired",
      "frontmatter": {
        "tags": [
          "잡지식",
          "shell"
        ],
        "date": "2024-06-10T21:31:00+09:00",
        "lastmod": "2025-08-11T00:49:10+09:00"
      }
    },
    "x86-64 레지스터 최소 설명": {
      "path": "/02.inbox/x86-64-레지스터-최소-설명/",
      "filename": "x86-64 레지스터 최소 설명",
      "content": "x86-64 아키텍처에서 각 레지스터는 특정 목적이나 일반적인 연산에 사용됩니다. 이들은 64비트 레지스터이며, 일부는 하위 32비트, 16비트, 8비트 단위로도 사용할 수 있습니다. 여기서는 주요 레지스터와 그 역할을 설명하겠습니다. 범용 레지스터 (General-Purpose Registers) 레지스터 역할/사용 목적 RAX - 주로 산술 연산(곱셈, 나눗셈)의 결과 저장- 함수 호출 시 반환값 저장 RBX - 일반 데이터 저장 (callee-saved: 함수 호출 후 유지됨) RCX - 반복 연산의 카운터로 사용 ( loop 명령어에서 사용)- 함수 호출 시 세 번째 인수 전달 (Windows x64 ABI 기준) RDX - 입출력 연산 및 곱셈/나눗셈에서 사용- 함수 호출 시 두 번째 인수 전달 RSI - 문자열 조작에서 소스 주소 (Source Index)- 함수 호출 시 첫 번째 인수 전달 (Linux x86-64 ABI 기준) RDI - 문자열 조작에서 대상 주소 (Destination Index)- 함수 호출 시 첫 번째 인수 전달 (Windows x64 ABI 기준) RBP - 베이스 포인터 (Base Pointer)- 함수 호출 시 스택 프레임을 추적 RSP - 스택 포인터 (Stack Pointer)- 현재 스택의 최상단을 가리킴 R8~R15 - 추가 범용 레지스터- 함수 호출 시 추가 인수 전달 (Linux 및 Windows x64 ABI) 특수 목적 레지스터 레지스터 역할/사용 목적 RIP - 명령 포인터 (Instruction Pointer)- 현재 실행 중인 명령어의 주소 RFLAGS - 플래그 레지스터- 연산 결과의 상태(예: 캐리, 오버플로, 제로 등) 저장 RSP - 스택 포인터- 함수 호출 및 지역 변수 할당 관리 RBP - 베이스 포인터- 스택 프레임의 기준점 역할 세그먼트 레지스터 레지스터 역할/사용 목적 CS 코드 세그먼트 (Code Segment) DS 데이터 세그먼트 (Data Segment) SS 스택 세그먼트 (Stack Segment) ES, FS, GS 추가 세그먼트- 주로 OS 및 특정 상황에서 사용 레지스터 크기 x86-64에서 각 레지스터는 이름에 따라 하위 크기를 참조할 수 있습니다. 레지스터 이름 크기 (비트) 설명 RAX 64비트 전체 레지스터 EAX 32비트 하위 32비트 AX 16비트 하위 16비트 AH/AL 8비트 상위 8비트(AH) / 하위 8비트(AL) 예를 들어, RAX 의 하위 16비트에 접근하려면 AX 를 사용하며, 하위 8비트는 AL 을, 상위 8비트는 AH 를 통해 접근할 수 있습니다. 함수 호출 규약 (Calling Convention) Windows x64 ABI 첫 번째~네 번째 인수: RCX, RDX, R8, R9 레지스터에 전달 반환값: RAX 레지스터 나머지 인수: 스택에 저장 Linux x64 ABI (System V) 첫 번째~여섯 번째 인수: RDI, RSI, RDX, RCX, R8, R9 레지스터에 전달 반환값: RAX 레지스터 나머지 인수: 스택에 저장 실제 사용 예시 산술 연산 mov rax, 5 ; RAX에 5 저장 add rax, 3 ; RAX에 3 더하기 함수 호출 mov rdi, 10 ; 첫 번째 인수에 10 전달 (Linux x64 기준) call my_function ; 함수 호출 스택 사용 push rax ; RAX 값을 스택에 저장 pop rbx ; 스택에서 값을 가져와 RBX에 저장 레지스터와 스택의 관계 또는 함수 호출 규약에 대해 더 알고 싶다면 말씀해주세요!",
      "frontmatter": {
        "aliases": "[]",
        "tags": [
          "register"
        ],
        "date": "2024-12-20T09:02:00+09:00",
        "lastmod": "2024-12-20T09:02:00+09:00"
      }
    },
    "xmap 출력": {
      "path": "/02.inbox/xmap-출력/",
      "filename": "xmap 출력",
      "content": "%20image%2020241207204591.png) 2910149: ./a.out 프로세스 ID 2910149의 실행 파일 ./a.out 에 대한 메모리 맵입니다. Address Kbytes RSS Dirty Mode Mapping 각 열의 의미: Address: 메모리 주소 Kbytes: 해당 영역의 크기(KB) RSS: 실제 메모리에 올라간 크기(KB) Dirty: 수정된 페이지 수(KB) Mode: 메모리 접근 권한 Mapping: 매핑된 파일이나 영역의 설명 0000561ff3b48000 4 4 4 r---- a.out 실행 파일의 읽기 전용 영역입니다. 0000561ff3b49000 4 4 4 r-x-- a.out 실행 권한이 있는 코드 영역입니다. 0000561ff3b4a000 4 4 4 r---- a.out 추가적인 읽기 전용 데이터 영역입니다. 0000561ff3b4b000 4 4 4 r---- a.out 또 다른 읽기 전용 영역입니다. 0000561ff3b4c000 4 4 4 rw--- a.out 쓰기 및 읽기 가능한 데이터 영역입니다(예: 전역 변수). 0000561ff4380000 132 4 4 rw--- [ anon ] 익명 메모리 매핑 영역으로 힙(heap) 영역일 수 있습니다. 00007f79538e0000 4 0 0 ----- [ anon ] 보호된 메모리 영역입니다. 00007f79538e1000 8192 8 8 rw--- [ anon ] 스레드 스택 영역으로 8MB의 크기를 갖습니다. 그 이후 줄들은 각 스레드별 스택 영역을 나타냅니다(스레드마다 약 8MB씩 할당): 00007f79547fa000 부터 00007f7956fff000 까지 반복적으로 나타나는 8MB 크기의 rw--- [ anon ] 영역들은 각 스레드의 스택입니다. 마지막 부분: 00007f795d0e6000 부터 00007f795d356000 까지는 라이브러리( libc.so.6 , ld-linux-x86-64.so.2 )의 메모리 매핑 영역입니다. 00007ffc9bac7000 132 16 16 rw--- [ stack ] 는 메인 스레드의 스택입니다. 00007ffc9bbd2000 부터는 환경 변수나 프로그램 인자들이 저장된 영역일 수 있습니다. void* child_routine(void* param) { int id = (int)param; printf(\"My thread ID %i\\n\", id); sleep(100); pthread_exit(0); } int main() { pthread_t thread[NUMTHREAD]; int param[NUMTHREAD]; void* return_value[NUMTHREAD]; for (int i = 0; i < NUMTHREAD; i++) { param[i] = i; pthread_create(&thread[i], 0, child_routine, (void*)param[i]); } for (int i = 0; i < NUMTHREAD; i++) { pthread_join(&thread[i], 0); } } shinnk@DESKTOP-KRSG68U:~$ ./a.out & [1] 2910149 shinnk@DESKTOP-KRSG68U:~$ My thread ID 0 My thread ID 1 My thread ID 2 My thread ID 3 My thread ID 4 My thread ID 5 My thread ID 6 My thread ID 7 My thread ID 8 My thread ID 9 pmap -x 2910149 2910149: ./a.out Address Kbytes RSS Dirty Mode Mapping 0000561ff3b48000 4 4 4 r---- a.out 0000561ff3b49000 4 4 4 r-x-- a.out 0000561ff3b4a000 4 4 4 r---- a.out 0000561ff3b4b000 4 4 4 r---- a.out 0000561ff3b4c000 4 4 4 rw--- a.out 0000561ff4380000 132 4 4 rw--- [ anon ] 00007f79538e0000 4 0 0 ----- [ anon ] 00007f79538e1000 8192 8 8 rw--- [ anon ] 00007f79547f9000 4 0 0 ----- [ anon ] 00007f79547fa000 8192 8 8 rw--- [ anon ] 00007f7954ffa000 4 0 0 ----- [ anon ] 00007f7954ffb000 8192 8 8 rw--- [ anon ] 00007f79557fb000 4 0 0 ----- [ anon ] 00007f79557fc000 8192 8 8 rw--- [ anon ] 00007f7955ffc000 4 0 0 ----- [ anon ] 00007f7955ffd000 8192 8 8 rw--- [ anon ] 00007f79567fd000 4 0 0 ----- [ anon ] 00007f79567fe000 8192 8 8 rw--- [ anon ] 00007f7956ffe000 4 0 0 ----- [ anon ] 00007f7956fff000 8192 8 8 rw--- [ anon ] 00007f79577ff000 4 0 0 ----- [ anon ] 00007f7957800000 8192 2048 2048 rw--- [ anon ] 00007f7958000000 132 4 4 rw--- [ anon ] 00007f7958021000 65404 0 0 ----- [ anon ] 00007f795c0e1000 4 0 0 ----- [ anon ] 00007f795c0e2000 8192 8 8 rw--- [ anon ] 00007f795c8e2000 4 0 0 ----- [ anon ] 00007f795c8e3000 8204 16 16 rw--- [ anon ] 00007f795d0e6000 160 160 0 r---- libc.so.6 00007f795d10e000 1620 1024 0 r-x-- libc.so.6 00007f795d2a3000 352 128 0 r---- libc.so.6 00007f795d2fb000 4 0 0 ----- libc.so.6 00007f795d2fc000 16 16 16 r---- libc.so.6 00007f795d300000 8 8 8 rw--- libc.so.6 00007f795d302000 52 20 20 rw--- [ anon ] 00007f795d31a000 8 4 4 rw--- [ anon ] 00007f795d31c000 8 8 0 r---- ld-linux-x86-64.so.2 00007f795d31e000 168 168 0 r-x-- ld-linux-x86-64.so.2 00007f795d348000 44 44 0 r---- ld-linux-x86-64.so.2 00007f795d354000 8 8 8 r---- ld-linux-x86-64.so.2 00007f795d356000 8 8 8 rw--- ld-linux-x86-64.so.2 00007ffc9bac7000 132 16 16 rw--- [ stack ] 00007ffc9bbd2000 16 0 0 r---- [ anon ] 00007ffc9bbd6000 8 4 0 r-x-- [ anon ] ---------------- ------- ------- ------- total kB 150272 3772 2236",
      "frontmatter": {
        "tags": [
          "command",
          "linux",
          "system-programing",
          "잡지식"
        ],
        "date": "2024-12-22T12:04:00+09:00",
        "lastmod": "2024-12-22T12:04:00+09:00"
      }
    },
    "yt-dlp 명령어 모음": {
      "path": "/02.inbox/yt-dlp-명령어-모음/",
      "filename": "yt-dlp 명령어 모음",
      "content": "yt-dlp -F # 다운받을 수 있는 모든 포멧종류 yt-dlp -f [{포멧 id},{확장자}] # 포멧 지정 다운 yt-dlp -x --audio-format [오디오 포맷] [URL] F 표시 목록 ID: 동영상 화질/오디오 옵션의 고유 식별자입니다. EXT: 파일 확장자입니다. 일반적으로 mp4, webm, m4a 등이 사용됩니다. RESOLUTION: 동영상의 해상도입니다. 예를 들어 1920x1080은 1080p, 1280x720은 720p를 의미합니다. FPS: 동영상의 프레임 레이트입니다. 초당 프레임 수를 나타냅니다. CH: 오디오 채널 수입니다. 일반적으로 2채널 스테레오 오디오가 사용됩니다. FILESIZE: 파일 크기입니다. TBR: 평균 비트레이트(Transmission Bitrate)입니다. 동영상 및 오디오의 평균 데이터 전송률을 나타냅니다. PROTO: 동영상 스트리밍 프로토콜입니다. 일반적으로 https, m3u8 등이 사용됩니다. VCODEC: 동영상 코덱입니다. 예를 들어 avc1.4D401F는 H.264 코덱을 의미합니다. VBR: 동영상 비트레이트입니다. ACODEC: 오디오 코덱입니다. 예를 들어 mp4a.40.2는 AAC 코덱을 의미합니다. ABR: 오디오 비트레이트입니다. ASR: 오디오 샘플링 레이트입니다. MORE INFO: 추가 정보입니다. 화질, 오디오 옵션 등에 대한 설명이 포함됩니다. 이 정보를 통해 동영상의 화질, 오디오 품질, 파일 크기 등을 확인할 수 있습니다. 사용자의 네트워크 환경이나 기기 성능에 따라 적절한 옵션을 선택할 수 있습니다.",
      "frontmatter": {
        "tags": [
          "command"
        ],
        "date": "2024-06-02T01:03:00+09:00",
        "lastmod": "2024-06-02T01:03:00+09:00"
      }
    },
    "가변인자(variadic)": {
      "path": "/02.inbox/가변인자variadic/",
      "filename": "가변인자(variadic)",
      "content": "가변 인자 매크로의 모든 것 - 개론 가변 인자 매크로의 모든 것 - 과거 가변 인자 매크로의 모든 것 - 현재 #1 가변 인자 매크로의 모든 것 - 현재 #2 가변 인자 매크로의 모든 것 - 현재 #3 가변 인자 매크로의 모든 것 - 미래 가변 인자 매크로의 모든 것 - 구현 가변 인자 매크로의 모든 것 - 보충 여러개의 데이터를 묶어 하나의 변수에 대입 (패킹) vs 컬랙션 속의 요소들을 여러 개의 변수에 나누어 대입(언패킹) 컬랙션 (list tuple dictionary set)... 기본 구분 numbers = (1, 2, 3, 4, 5) # 패킹 a, b, c, d, e = numbers # 언패킹 필요없는 요소 생략 a, _, _, d, e = numbers # 필요 없는 요소를 _ 변수에 대입 a, b, *rest = numbers # 1, 2를 제외한 나머지를 rest에 대입 print(a, b, rest) 1 2 [3, 4, 5] a, *rest, e = numbers # 1, 5를 제외한 나머지를 rest에 대입 print(rest) [2, 3, 4] 무조건 리스트로 반환 x = [10, 20, 30] print_numbers(*x) 10 20 30 num_dict = {'a':1,'b':2,'c':3,'d':4,'e':5} num_set = {1,2,3,4,5} a3,b3,*rest3 = num_dict a4,b4,*rest4 = num_set print(a3, b3, rest3) # a b ['c', 'd', 'e'] print(a4, b4, rest4) # 1 2 [3, 4, 5] 함수 인자 함수의 인자를 보내는 방식을 2가지 위치로 매핑 하는 방법과 키워드로 매핑하는 방법 위치로 매칭하는 방법 func('py', 'thon') #호출시 키워드로 매칭하는 방법 func(b='thon', a='py') #호출시 print(a,b,sep=\" 그리고 \") 위치 가변 인자 \\*args 언패킹 방식을 함수의 인자에 사용 위치를 사용하므로 위치 가변 인자라고 한다 함수인자의 언패킹 방식과 일반 언패킹 방식은 리스트와 튜플로 다르다 num_tuple = (1,2,3,4,5) num_list = [1,2,3,4,5] a1,b1,*rest1 = num_list a2,b2,*rest2 = num_tuple print(a1, b1, rest1) # 1 2 [3, 4, 5] print(a2, b2, rest2) # 1 2 [3, 4, 5] # 그냥 언패킹은 무조건 리스트 def unpacking1(a3,b3,*rest3): # 1 2 (3, 4, 5) 함수내부에서의 언패킹은 무조건 튜플 print(a3,b3,rest3) unpacking1(1,2,3,4,5) 키워드 가변 인자 \\\\kwargs 함수는 임의의 개수의 키워드 인자도 받을 수 있다. 예: def f(x, y, **kwargs): ... 함수 호출. f(2, 3, flag=True, mode='fast', header='debug') 추가적인 키워드를 딕셔너리로 전달한다. def f(x, y, **kwargs): # x -> 2 # y -> 3 # kwargs -> { 'flag': True, 'mode': 'fast', 'header': 'debug' } 튜플과 딕셔너리를 전달하기 튜플을 가변 인자로 확장할 수 있다. numbers = (2,3,4) f(1, *numbers) # f(1,2,3,4)와 같음 마찬가지로 딕셔너리를 키워드 인자로 확장할 수 있다. options = { 'color' : 'red', 'delimiter' : ',', 'width' : 400 } f(data, **options)",
      "frontmatter": {
        "tags": [
          "python",
          "language"
        ],
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2025-10-21T20:40:27+09:00"
      }
    },
    "계층 기반 폴더 구조 vs 서비스 단위 기반 폴더 구조": {
      "path": "/02.inbox/계층-기반-폴더-구조-vs-서비스-단위-기반-폴더-구조/",
      "filename": "계층 기반 폴더 구조 vs 서비스 단위 기반 폴더 구조",
      "content": "좋은 질문입니다. DTO, Repository, Domain, Service 와 같은 계층 기반 폴더 구조(Layered Architecture)와 서비스 단위 기반 폴더 구조(Vertical Slice / Feature-based Architecture) 중 어느 것이 더 나은지는 프로젝트의 규모, 팀 구조, 유지보수성, 확장성 등에 따라 달라집니다. 아래에서 두 방식의 특징과 장단점을 비교하고, 어떤 상황에서 어떤 방식이 적합한지 정리해 드릴게요. ✅ 1. 계층 기반 구조 (Layered / Horizontal Architecture) src/ ├── dto/ │ ├── UserRequestDto.java │ └── UserResponseDto.java ├── domain/ │ ├── User.java │ └── UserStatus.java ├── repository/ │ └── UserRepository.java ├── service/ │ └── UserService.java ├── controller/ │ └── UserController.java └── config/ └── DatabaseConfig.java ✅ 장점 일관성 있는 구조: 모든 개발자가 동일한 위치에서 동일한 타입의 클래스를 찾을 수 있음. 초기 개발이 쉬움: 특히 작은 프로젝트나 학습용 프로젝트에 적합. Spring Boot 등의 프레임워크와 자연스럽게 맞음: MVC 패턴과 잘 어울림. ❌ 단점 변경이 어렵다: 하나의 기능을 수정할 때 여러 패키지를 왔다갔다 해야 함 (예: 유저 생성 기능 수정 시 controller → service → repository → dto 모두 확인). 결합도 증가: 모든 기능이 동일한 계층에 몰려 있어, 관련 없는 기능들끼리 묶임. 대규모 프로젝트에서 관리 어려움: 기능이 많아질수록 파일 수가 많아지고, 관련 코드를 찾기 어려워짐. ✅ 2. 서비스(기능) 단위 구조 (Vertical Slice / Feature-based Architecture) src/ ├── user/ │ ├── User.java │ ├── UserRepository.java │ ├── UserService.java │ ├── UserController.java │ ├── UserRequestDto.java │ └── UserResponseDto.java ├── order/ │ ├── Order.java │ ├── OrderService.java │ └── OrderController.java └── common/ ├── config/ └── exception/ ✅ 장점 기능 단위로 코드가 묶여 있어 유지보수 용이: 유저 관련 기능은 user/ 폴더 안에 모두 있음. 변경이 쉬움: 기능 하나를 수정할 때 관련된 모든 코드를 한 폴더에서 찾을 수 있음. 팀 기반 개발에 유리: 팀이 기능 단위로 분업하기 쉬움 (예: A팀은 user, B팀은 order 담당). 확장성 좋음: 새 기능 추가 시 폴더 하나 생성하면 됨. ❌ 단점 중복 가능성: 공통 로직(예: DTO, 유틸)이 각 폴더에 중복될 수 있음 → common 또는 shared 패키지로 해결 필요. 초기 설계가 중요: 공통 모듈과 기능 모듈의 경계를 잘 설정해야 함. 익숙하지 않은 구조: 전통적인 계층 구조에 익숙한 개발자에게는 어색할 수 있음. ✅ 결론: 어떤 걸 선택해야 할까? 프로젝트 유형 추천 구조 작은 프로젝트, 학습용, MVP ✅ 계층 기반 구조 (간단하고 직관적) 중대형 프로젝트, 장기 운영, 팀 개발 ✅ 서비스/기능 단위 구조 (유지보수성 ↑) 도메인 중심 설계(DDD) 적용 ✅ 기능 단위 + bounded context 기반 구성 마이크로서비스 아키텍처 ✅ 각 서비스 내부는 기능 단위 구조 권장 🔁 보완 전략 (하이브리드 접근) 실제로는 하이브리드로 구성하는 경우도 많습니다: src/ ├── user/ │ ├── controller/ │ ├── service/ │ ├── repository/ │ └── dto/ ├── order/ │ ├── controller/ │ └── service/ ├── common/ │ ├── dto/ │ ├── exception/ │ └── util/ └── config/ → 기능 단위로 묶되, 내부는 계층 구조를 유지. 이 방식은 유지보수성과 일관성을 동시에 잡을 수 있어 실무에서 많이 사용됩니다. ✅ 추천 🔔 \"기능 단위 기반 구조 + 내부 계층 분리\" 를 추천합니다. 예: com.example.app.user ├── User.java ├── UserController.java ├── UserService.java ├── UserRepository.java ├── UserCreateRequest.java └── UserResponse.java 이렇게 하면: 기능 단위로 코드를 찾기 쉽고, 각 기능 내부는 역할에 따라 분리되어 있어 체계적입니다. 📚 참고 아키텍처 패턴 Vertical Slice Architecture Clean Architecture Domain-Driven Design (DDD)",
      "frontmatter": {
        "tags": [
          "design"
        ],
        "date": "2025-07-31T09:15:00+09:00",
        "lastmod": "2025-10-21T20:40:31+09:00"
      }
    },
    "날짜 형식 포멧": {
      "path": "/02.inbox/날짜-형식-포멧/",
      "filename": "날짜 형식 포멧",
      "content": "https://momentjs.com/docs/#/displaying/format/ Token Output Month M 1 2 ... 11 12 Mo 1st 2nd ... 11th 12th MM 01 02 ... 11 12 MMM Jan Feb ... Nov Dec MMMM January February ... November December Quarter Q 1 2 3 4 Qo 1st 2nd 3rd 4th Day of Month D 1 2 ... 30 31 Do 1st 2nd ... 30th 31st DD 01 02 ... 30 31 Day of Year DDD 1 2 ... 364 365 DDDo 1st 2nd ... 364th 365th DDDD 001 002 ... 364 365 Day of Week d 0 1 ... 5 6 do 0th 1st ... 5th 6th dd Su Mo ... Fr Sa ddd Sun Mon ... Fri Sat dddd Sunday Monday ... Friday Saturday Day of Week (Locale) e 0 1 ... 5 6 Day of Week (ISO) E 1 2 ... 6 7 Week of Year w 1 2 ... 52 53 wo 1st 2nd ... 52nd 53rd ww 01 02 ... 52 53 Week of Year (ISO) W 1 2 ... 52 53 Wo 1st 2nd ... 52nd 53rd WW 01 02 ... 52 53 Year YY 70 71 ... 29 30 YYYY 1970 1971 ... 2029 2030 YYYYYY -001970 -001971 ... +001907 +001971 Note: [](https://tc39.es/ecma262/#sec-expanded-years) (Covering the full time value range of approximately 273,790 years forward or backward from 01 January, 1970) Y 1970 1971 ... 9999 +10000 +10001 Note: This complies with the ISO 8601 standard for dates past the year 9999 Era Year y 1 2 ... 2020 ... Era N, NN, NNN BC AD Note: Abbr era name NNNN Before Christ, Anno Domini Note: Full era name NNNNN BC AD Note: Narrow era name Week Year gg 70 71 ... 29 30 gggg 1970 1971 ... 2029 2030 Week Year (ISO) GG 70 71 ... 29 30 GGGG 1970 1971 ... 2029 2030 AM/PM A AM PM a am pm Hour H 0 1 ... 22 23 HH 00 01 ... 22 23 h 1 2 ... 11 12 hh 01 02 ... 11 12 k 1 2 ... 23 24 kk 01 02 ... 23 24 Minute m 0 1 ... 58 59 mm 00 01 ... 58 59 Second s 0 1 ... 58 59 ss 00 01 ... 58 59 Fractional Second S 0 1 ... 8 9 SS 00 01 ... 98 99 SSS 000 001 ... 998 999 SSSS ... SSSSSSSSS 000[0..] 001[0..] ... 998[0..] 999[0..] Time Zone z or zz EST CST ... MST PST Note: as of 1.6.0, the z/zz format tokens have been deprecated from plain moment objects. Read more about it here. However, they do work if you are using a specific time zone with the moment-timezone addon. Z -07:00 -06:00 ... +06:00 +07:00 ZZ -0700 -0600 ... +0600 +0700 Unix Timestamp X 1360013296 Unix Millisecond Timestamp x 1360013296123",
      "frontmatter": {
        "aliases": [
          "날짜 형식의 표현 방식"
        ],
        "tags": [
          "reference",
          "잡지식"
        ],
        "date": "2023-12-21T03:07:00+09:00",
        "lastmod": "2023-12-21T03:07:00+09:00"
      }
    },
    "네트워크 관련 명령어 모음": {
      "path": "/02.inbox/네트워크-관련-명령어-모음/",
      "filename": "네트워크 관련 명령어 모음",
      "content": "unix 공통 ifconfig netstat ping traceroute : 지나간 네트워크 대역 확인 nc(netcat) : nmap : route : 라우팅 테이블? arp : 동일 네트워크 맥주소 확인 프로토콜 telnet linux ss -ltan : -l listen, -a all, -t tcp, -n numeric ip ip route ip addr window ipconfig netstat ping arp : 동일 네트워크 맥주소 확인 프로토콜 tracert ================= mac 맥 환경에서, 잘모르는 프레임웍 이용한 서버 동작시, 특정프로그램 이용시, 내가 사용하는 것 외 데이터가 세어나가는지, 내가 모르는 포트를 열어놓은 프로세스가 있는지 (찾아서 kill하려고), 혹은 방화벽 제외 / NAT환경에서의 포트포워딩 정책에 SNAT, DNAT등에 포트 값을 알아내야 할 경우 lsof와 netstast과 다음과 같은 옵션으로 찾으면 된다. (기타 : linux의 경우 netstat -anp grep 포트번호 or LISTEN or EST등의 소켓 상태로 필터링해서 사용해왔음) lsof (list open files) 이용시 $ sudo lsof -iTCP -sTCP:LISTEN -n -P",
      "frontmatter": {
        "tags": [
          "network"
        ],
        "date": "2024-02-11T20:16:00+09:00",
        "lastmod": "2024-02-11T20:16:00+09:00"
      }
    },
    "네트워크 인터페이스": {
      "path": "/02.inbox/네트워크-인터페이스/",
      "filename": "네트워크 인터페이스",
      "content": "Linux enp0s1: | | | v | | en| | --> ethernet v | p0| --> bus number (0) v s1 --> slot number (1) * Two character prefixes based on the type of interface: * en — Ethernet * sl — serial line IP (slip) * wl — wlan * ww — wwan * * Type of names: * b<number> — BCMA bus core number * c<bus_id> — CCW bus group name, without leading zeros [s390] * o<index>[d<dev_port>] — on-board device index number * s<slot>[f<function>][d<dev_port>] — hotplug slot index number * x<MAC> — MAC address * [P<domain>]p<bus>s<slot>[f<function>][d<dev_port>] * — PCI geographical location * [P<domain>]p<bus>s<slot>[f<function>][u<port>][..][c<config>][i<interface>] * — USB port number chain 1. 온보드 장치에 대해 펌웨어/BIOS 제공 인덱스 번호를 통합한 이름(예: eno1) 2. 펌웨어/BIOS 제공 PCI Express 핫플러그 슬롯 인덱스 번호를 포함하는 이름(예: ens1) 3. 하드웨어 커넥터의 물리적/지리적 위치를 통합한 이름(예: enp2s0) 4. 인터페이스의 MAC 주소를 통합한 이름(예: enx78e7d1ea46da) 5. 예측할 수 없는 고전적인 커널 기반 ethX 이름 지정(예: eth0) mac OS ap1 : 액세스 포인트. MacBook을 연결을 공유하는 무선 호스트로 사용하는 경우에 사용됩니다. awdl0 : Apple Wireless Direct 링크. AirDrop, Airplay 등을 위한 WIFI p2p 연결. Bluetooth에도 사용됩니다. llw0 : 대기 시간이 짧은 WLAN 인터페이스. Skywalk 시스템에서 사용됩니다. utun0 : 터널링 인터페이스. 터널 트래픽에 대한 VPN 연결이나 Back To My Mack와 같은 소프트웨어에 사용됩니다. utun1 : utun01과 동일 lo0 : 루프백(localhost) gif0 : 소프트웨어 네트워크 인터페이스 stf0 : 6to4 터널 인터페이스 en0 : 물리적 무선 en1 : 썬더볼트 1 en2 : 썬더볼트 2 en3 : 썬더볼트 3 en4 : 썬더볼트 4 en5 : iBridge / Apple T2 컨트롤러 en6 : 블루투스 팬 en8 : 아이폰 USB en9 : VM 네트워크 인터페이스 en10 : 아이패드 bridge0 : Thunderbolt 브리지",
      "frontmatter": {
        "aliases": [
          "network interface"
        ],
        "tags": [
          "잡지식"
        ],
        "date": "2024-02-04T21:22:00+09:00",
        "lastmod": "2024-02-04T21:22:00+09:00"
      }
    },
    "대소문자 구분 불가 문제": {
      "path": "/02.inbox/대소문자-구분-불가-문제/",
      "filename": "대소문자 구분 불가 문제",
      "content": "window os(NTFS), mac os(APFS) 파일 시스템의 경우 대소문자 구분이 되지 않는다 위의 두 os는 사람들이 많이 쓰는 os 이므로 이 문제에 관해 git 은 대소문자 구분을 하지 않는 것을 기본으로 설정한다 이에 대해 두가지 방법이 있다 설정 자제를 변경(한프로젝트 에서만 동작)(global 은 따로 설정) git config core.ignorecase false # default true git rm -r --cached . # 캐쉬 삭제 git 의 기능을 사용한다( 전체 config 변경에 따른 사이드 이팩트 원천 봉쇄) git mv test Test",
      "frontmatter": {
        "tags": [
          "git"
        ],
        "date": "2023-12-20T08:28:00+09:00",
        "lastmod": "2025-07-02T04:19:23+09:00"
      }
    },
    "대칭키 비대칭기 암호화": {
      "path": "/02.inbox/대칭키-비대칭기-암호화/",
      "filename": "대칭키 비대칭기 암호화",
      "content": "단방향 암호화 : 일종의 해시 단일문자 대응 암호 블록 암호화 PGP TLS IPsec , SHA-256, SHA-1, MD5 등 db 에 비밀번호를 저장할 떄는 단방향 암호화 데이터 무결성 검증 (예: 파일 해시 비교, TCP UDP 의 checksum) 양방향 암호화 대칭키: 암호화와 복호화에 동일한 키를 사용하는 암호 방식이다. $K{a} = K{b}$ AES, DES 비대칭키: 개인키(비밀) 공개키(공개)로 키가 나뉘며 1개의 키로 암호화 1개의 키로 복호화 하는 방법 $K{a} \\neq K{b}$ server or client: $A, B$ 공개키: $K^{+}{A,B}$ , 개인키: $K^{-}{A,B}$ public key 암호화 => 종단간 암호화 $K^{+}{A}(K^{-}{A}(m)) = m$ private key 암호화 => 인증 $K^{-}{A}(K^{+}{A}(m)) = m$ 기밀성 메세지 무결성 종단점 인증 운영보안 운영보안 블럭 암호화 k == 64 이면 64 비트블록으로 쪼개어 각 블록을 독립적으로 암호화 한다 대칭키 대칭키의 경우 키를 미리 공유되어 있다는 가정이 있어야 한다 비대칭키 = 공개키 암호화 321x314 321x314 Public Key로 암호화 하는 경우 상대방의 Public key로 data를 암호화 하고 전송하면, data를 수신한 사람은 자신의 Private key로 data를 복호화 한다. A 키로 암호화 한다면, B키로 복호화가 가능하고, B키로 암호화를 한다면 A키로 복호화가 가능한 것이다. Public Key는 널리 배포될 수 있기 때문에 많은 사람들이 한 명의 Private Key 소유자에게 data를 보낼 수 있다. Private Key로 암호화 하는 경우 Private Key의 소유자가 Private Key로 data를 암호화하고 Public Key와 함께 전달한다. 이 과정에서 Public Key와 data를 획득한 사람은 Public key를 이용하여 복호화가 가능하다. 이런 위험에도 불구하고 이 방법을 사용하는 이유는 data 보호의 목적 보다는 Public Key data 제공자의 신원을 보장해주기 때문이다. 암호화된 data가 Public Key로 복호화 된다는 것은 Public Key와 쌍을 이루는 Private Key에 의해서 암호화 되었다는 것을 의미한다. 즉 data 제공자의 신원 확인이 보장된다는 것이다. 이 방법이 공인인증체계의 기본 바탕이 되는 전자 서명이라는 것이다. #ModificationRequired HTTPS와 비대칭 키 초기 설정부터 데이터 전송까지의 모든 과정 simulation CA(Certification Authority)로부터 얻는 것 CA는 신뢰할 수 있는 제3자 기관으로, 엔터티(사람, 서버, 라우터 등)의 신원을 검증하고 해당 엔터티의 공개 키에 대한 인증서(Certificate)를 발급하는 역할을 합니다. CA로부터 받는 주요 정보는 다음과 같습니다: 인증서 (Certificate): 인증서는 특정 엔터티의 공개 키와 해당 엔터티의 식별 정보(예: 웹 서버의 도메인 이름, 조직 이름, IP 주소 등)를 묶어서 CA가 디지털 서명한 것입니다. 이 인증서는 송신자(예: 웹 서버)가 자신의 공개 키를 수신자(예: 클라이언트)에게 안전하게 제공할 수 있도록 합니다. 클라이언트는 CA의 공개 키를 사용하여 서버 인증서의 유효성을 검증합니다. 만약 인증서가 CA에 의해 올바르게 서명되었고 유효 기간 내에 있다면, 클라이언트는 해당 인증서에 포함된 공개 키가 주장된 엔터티(서버)에게 실제로 속한다고 신뢰할 수 있습니다. 이는 Trudy와 같은 공격자가 Bob으로 가장하여 자신의 공개 키를 Alice에게 보내는 신원 위조(masquerading) 공격을 방지하는 데 필수적입니다. RSA 알고리즘 덧셈: $[(a \\mod n) + (a \\mod n)] = (a + b) \\mod n$ 뺄셈: $[(a \\mod n) - (a \\mod n)] = (a - b) \\mod n$ 곱셈: $[(a \\mod n) \\cdot (a \\mod n)] = (a \\cdot b) \\mod n$",
      "frontmatter": {
        "tags": [
          "잡지식",
          "encryption"
        ],
        "date": "2025-06-20T08:33:18+09:00",
        "lastmod": "2025-06-20T19:29:29+09:00"
      }
    },
    "대학교 운영체제": {
      "path": "/02.inbox/대학교-운영체제/",
      "filename": "대학교 운영체제",
      "content": "cpu 동작 과정 Pasted video 20240318100401 cpu 구조 Pasted image 20240318163101 주요 부품 레지스터 PC 프로그램 카운터 (progam counter) : 다음에 실행될 메모리 주소를 저장 MAR 메모리 주소 레지스터 (memory Address registor) : pc 에서 주소를 넘겨 받아 메인 메모리에 접근해 데이터를 MBR (memory buffer registor)에 저장하는 역할 MBR 메모리 버퍼 레지스터 (memory buffer registor) : MBR 을 통해 가져온 데이터를 임시 저장, 명령 부분은 IR(Instruction Register) 에 저장, 연산에 사용될 데이터는 AC(Accumulator) 누산기 레지스터에 저장, 메모리에 저장될 데이터를 임시 저장 IR 명령어 레지스터 (Instruction Register) : 현재 실행되고 있는 명령을 저장하는 레지스터 AC 누산기 (Accumulator) : ALU 연산을 위한 레지스터로서 연산해야 할 값 또는 연산 결과를 임시 저장 coutrol 제어장치 : 명령어 레지스터 IR 에 있는 명령을 받아 해석하고 장치들을 제어 ALU : 산술 논리 연산 장치 추가 부품 BR(Base Register): 명령의 시작 주소를 기억하는 레지스터 MSR(Major Status Register): CPU의 주 상태를 저장하는 레지스터 플래그 레지스터(Flag Register): 상태를 기억하는 레지스터(오버플로우, 언더플로, 캐리, 인터럽트 등의 PSW를 기억) PSW(Program Status Word)는 시스템 내부의 순간순간의 상태를 기록하고 있는 정보를 말합니다. 상태 플래그 커널모드 사용자 모드 비트가 들어감 (플래그 레지스터 인가 MSR 레시스터인가) 명령 동작과정 기계어 로 적힌 프로그램은 각각의 몇 비트 컴퓨터 인지에 따라 명령어의 크기 + 데이터의 크기 == n 비트 컴퓨터 이다 A = B + C ===== LOAD [10] //10 주소의 데이터를 레지스터로 로드 ADD [11] //11 주소의 데이터와 더함 STORE [12] //12 주소에 저장 100 주소가 현재 pc 에 저장되어 있다 pc 의 값을 MAR 에 저장 MAR 의 값을 참조하여 메모리의 데이터를 로드하여 MBR 에 저장 MBR 의 명령어와 데이터중 명령어만을 IR 에 저장 ==> 인출 PC 값을 증가시켜 다음 명령의 주소를 저장(n 비트 크기 만큼) 명령어 레지스터의 값을 제어장치로 해석한다 제어장치는 10 주소를 MAR에 저장, 메모리 주소에 접근하여 데이터를 MBR 에 저장한다 ==> 해석 PC값을 MAR로 복사, MAR의 메모리 주소를 참조해 명령과 데이터 주소를 MBR,IR에 저장 ==> 인출 PC 값 증가 IR 의 값을 제어장치가 해석하는데 add 이므로 누산기에 저장된 이전 값을 ALU 전송 11 주소의 값을 MAR 저장, 11 주소의 데이터를 MBR 에 저장 => 인출 제어장치는 MBR 의 값을 누산기에 저장, 누산기에 저장된 값을 ALU 로 전송 ALU 의 계산된 값을 누산기로 저장 ==>실행 PC 값을 MAR 로 복사, MAR 의 메모리 주소를 참조하여 명령 및 데이터 주소를 MBR, IR 에 저장 PC 값 증가 MBR 의 값을 제어장치가 해석하여 12 주소를 MAR 에 저장, 누산기의 값을 MBR 에 저장 제어장치가 MBR 을 MAR 에 저장된 주소값에 저장 ==> 저장 인터럽트 인터럽트는 컴퓨터 하드웨어나 소프트웨어가 CPU에게 어떤 사건이 발생했음을 알리는 방법 기준에 따라 분류 기준의 분류 발생 원인에 따른 분류 하드웨어 인터럽트: 외부 하드웨어 장치로부터 오는 신호에 의해 발생합니다. 예를 들어, 키보드 입력, 마우스 클릭, 네트워크 패킷 수신 등이 있습니다. 하드웨어 인터럽트는 특정 하드웨어 장치가 데이터 처리 준비가 완료되었거나, 데이터 전송이 필요한 상황 등을 CPU에 알립니다. 소프트웨어 인터럽트: 프로그램 내부에서 명령을 통해 의도적으로 발생시키는 인터럽트입니다. 이는 주로 운영체제의 서비스를 요청할 때 사용되며, 시스템 호출이 이에 해당합니다. 처리 방식에 따른 분류 마스커블 인터럽트 (Maskable Interrupt): 이 인터럽트는 일시적으로 차단(마스크)할 수 있습니다. 우선순위에 따라 처리할 수 있으며, 더 중요한 작업을 우선적으로 처리하기 위해 일시적으로 차단될 수 있습니다. 논마스커블 인터럽트 (Non-Maskable Interrupt, NMI): 이 인터럽트는 차단할 수 없습니다. 시스템에 중대한 오류나 긴급 상황이 발생했을 때 사용되며, 반드시 즉시 처리되어야 합니다. 용도에 따른 분류 벡터 인터럽트: 인터럽트 발생 시 처리할 인터럽트 서비스 루틴(ISR)의 주소가 인터럽트 벡터에 의해 직접 지정됩니다. 이 방식은 처리해야 할 ISR을 빠르게 찾을 수 있게 해줍니다. 논벡터 인터럽트: 모든 인터럽트가 공통의 인터럽트 서비스 루틴을 호출하고, ISR 내에서 실제 발생한 인터럽트의 종류를 판단하여 적절한 처리를 하는 방식입니다. 이는 벡터 인터럽트에 비해 처리 속도가 느릴 수 있습니다. 인터럽트 처리과정 하드웨어적 처리 상태 레지스터 값을 저장 프로세서 모드를 커널 모드로 변경 (모드 비트를 변경) PC 레지스터 값을 저장 발생된 인터럽트의 벡터 값(ISR 주소)을 PC 레지스터에 저장 (OS 내의 인터럽트 서비 스 루틴을 수행하게 됨) 소프트웨어적 처리 (OS가 수행함) CPU 레지스터들의 값을 저장 (메모리에 저장) 인터럽트 처리 코드를 수행 CPU 레지스터 값을 복원 (저장된 값들을 CPU 레지스터에 load) 상태 레지스터 값을 복원 (프로세서 모드가 이전 모드로 변경됨) PC 레지스터 값을 복원 (인터럽트가 발생하여 중단된 곳으로 돌아가게 됨) 메모리 구조 메모리 ROM (비휘발성) 부트로더 RAM (휘발성) IVT 인터럽트 백터 테이블 os(os 내부에 인터럽트 실제 데이터 존재) 응용프로그램 부팅 과정 IR 값에 기본적으로 0 저장되어 있음 주소 0 의 명령을 IR 로 가져오고 실행(부트로더가 실행됨)(커널모드) 부트로더가 OS 를 메모리에 적제 및 시작 메모리 주소를 PC 에 저장 제어장치는 PC 에 저장된 메모리 주소를 참고하여 IR 에 명령을 저장 계속 실행 .... 운영체제는 인터럽트에 의해 구동되는 프로그램 타이머 인터럽트 : 주기적 인터럽트 발생 응용프로그램 시스템 호출을 수행 입출력 장치 : 입출력을 마치면 인터럽트를 발생시킴 프로세스 상태 Pasted image 20240325151120 admitted :허가 / 파일의 소유권을 확인후 허가여부를 판단 new -> ready scheduler dispatch : 스케줄러가 실행한다 i/o or event wait : 입출력을 기다림 io or event completion : 입출력 exit : 시스템 콜을 통한 정상적인 프로세스 종료 procectin PCB process control block TCP Thread control block 프로세스 레지스터등 모든 자원이 분리 쓰레드 Code Data 공유, stack register(당연) 분리 Pasted image 20240401163758 CPU 사용률 (utilization) 단위시간당 CPU 사용시간의 비율 (0~100%) 처리량 (Throughput) 단위시간당 완료된 프로세스의 개수 반환시간 (Turnaround time) 특정 프로세스를 실행하는데 걸린 총 시간 대기시간 (Waiting time) 프로세스가 준비 큐에서 대기하면서 기다린 시간의 합 응답시간 (Response time) 작업을 요청한 후 첫번째 응답이 나올 때까지의 시간 (응답이 시작되는 데까지 걸린 시간이며 응답을 출력하는데 걸리는 시간은 아님) IPC (Inter-process communication) 공유 메모리(Shared memory), 메시지 전달(Message passing) data transfer byte stream pipe named pipe(fifo) socket(stream) message posix message queue sysV message queue socket(datagram) shared memory file memory mapping anonymous mapping file mapping shared memory sysV shared memory posix shared memory synchronization semaphore posix memaphore sysV semaphore file lock file lock record lock 프로세스 스케줄링 선점식과 비선점식 선점식 cpu 사용시간 통제 주체 => 프로그램 비선점식 cpu 사용시간 통제 주체 => os 스케줄러 인터럽트 기준 CPU 사용률 (utilization) : 단위시간당 CPU 사용시간의 비율 (0~100%) 처리량 (Throughput) : 단위시간당 완료된 프로세스의 개수 반환시간 (Turnaround time) : 특정 프로세스를 실행하는데 걸린 총 시간 대기시간 (Waiting time) : 프로세스가 준비 큐에서 대기하면서 기다린 시간의 합 응답시간 (Response time) : 작업을 요청한 후 첫번째 응답이 나올 때까지의 시간 (응답이 시작되는 데까지 걸린 시간이며 응답을 출력하는데 걸리는 시간은 아님) FCFS (First-Come First-Served) : 먼저온 순서대로 운좋게 짧은 응답시간의 프로세스가 먼저 들어갈 수록 대기시간이 짧다 비선점식 SJF (Shortest Job First) : 가장 빨리 끝나는 놈 먼저 비선점식 : 다음프로세스를 진행 할 때 큐에서 가장 적은 cpu burst 크기를 가진 프로세스를 선택하여 실행Pasted image 20240502145412 선점식 : 프로세스 진행중에 남은 cpu burst 크기보다 작은 cpu burst 가 나타나면 프로세스를 뺏아 작은 cpu burst 프로세스를 실행Pasted image 20240502145914 지수평균 : 이전 cpu burst 크기를 이용해 지수 평균을 방법을 사용해 추청함 Pasted image 20240502163408 Pasted image 20240502170522 비슷하게 우선순위 알고리즘이 있다 위의 경우는 cpu burst 시간이 우선순위로 설정되는 경우이다 즉 우선순위에 따라 cpu 를 점유할 수 있게 한다 선점식 비선점식 모두 가능 하지만 문제는 위와 동일하게 기아상태가 발생할수 있다는 것이다 RR (Round Robin) : 단위시간을 process 에게 cpu를 선점하도록 허용(시분할) 준비큐는 fcfs, 선점방식, 80%(cpu burst round robin 처리 후면 작업 (background job, 일괄처리) => fcfs 처리 전면작업이 무조건 먼저 수행된다Pasted image 20240502165941 우선순위, sjf(short job first) 와 마찬가지로 기아상태의 우려가 있다 MFQ (Multi-level Feedback Queue) MQ 업글 버전 다단계 큐 스케줄링(MQ)과 다르게 다단계 피드백 큐 스케줄링(MFQ)은 프로세스가 큐 사이를 이동함 CPU를 많이 사용하는 프로세스는 낮은 우선순위 큐로 이동시킴 입출력 중심의 프로세스와 대화식 프로세스들을 높은 우선순위의 큐에 이동시킴  낮은 우선순위의 큐에서 오래 대기하는 프로세스들을 높은 우선순위의 큐로 이동 (기아 상태를 예방, 에이징) Pasted image 20240502170321 새로운 프로세스는 Q0 에 들어가서 FCFS로 처리됨 CPU를 할당받으면 8 밀리 초 동안 실행되고 이 시간동안 완료되지 않는다면 Q1으로 이동됨. 이 시간동안 CPU 버스트를 끝내고 I/O 버스트로 가면 계속 Q0 에 위치함 Q1 에서 다시 FCFS로 CPU를 할당받고, 16 밀리 초동안 실행되고 이 시간동안 완 료되지 않는다면 Q2로 이동됨 이 예에서, CPU 버스트가 8 밀리초 이하인 프로세스가 제일 높은 우선순위를 가 지게 되고, 8 밀리 이상 16 밀리초 이하인 프로세스가 다음의 우선순위를 가짐. 이보다 긴 프로세스는 자동적으로 Q2 로 이동되어 낮은 우선순위를 가짐 그러므로 처음에 이야기한 cpu burst 가 큰 프로세스는 낮은 우선순위 큐로 자동적으로 천천히 이동되고 io 가 많은 프로세스는 최대한 Q0 와 가까이 위치함 HRN (Highest Response-rate Next) : 가변적 우선순위의 비선점식 + shj SJF의 약점인 긴 프로세스와 짧은 프로세스의 불평등을 보완 $우선순위 = \\frac {대기시간 + CPU burst } {CPU burst}$ cpu burst 를 기준으로 했던 sjf 에서 대기시간의 가중치를 만들어줌 간트 차트를 통해 프로세스 스케줄링을 시각적으로 표현할 수 있다 교착상태 교착상태 조건 모두 만족해야 교착 가능성 상호배제(Mutual exclusion) 한번에 오직 한 프로세스만이 자원을 사용할 수 있다. 점유와 대기(Hold and wait) 프로세스가 적어도 하나의 자원을 점유하면서 다른 프로세스가 점유하고 있는 자 원을 추가로 얻기위해 대기한다. 비선점(No preemption) 점유된 자원은 강제로 반환될 수 없고, 점유하고 있는 프로세스가 작업을 마치고 자원을 자발적으로 반환한다. 순환대기(Circular wait) 대기하고 있는 프로세스 집합 {P0 , P1 , …, Pn }에서 P0 은 P1이 점유한 자원을 대기하 고, P1은 P2가 점유한 자원을 대기하고, Pn–1 은 계속해 Pn을 대기하며, Pn은 P0이 점유한 자원을 대기한다. 문제 프로세서에서 산술, 논리 연산을 수행하는 장치는 무엇인가? => ALU 프로세서의 산술 연산의 결과, 프로세서 동작 모드 등을 저장하는 레지스터는 무엇인가? => 상태 레지스터 프로세서의 모드 중에서 제한 없이 모든 명령어를 사용할 수 있는 모드는 => 커널 모드 refresh가 필요한 RAM은 무엇인가 => dram ROM의 종류로서 데이터를 삭제하고 기록할 수 있는 ROM은 무엇인가? => eprom(자외선 삭제,전류 기록), eeprom(전류 삭제, 전류 기록) 저장장치 계층을 구분하는 성질 3가지는 무엇인가? => 속도, 가격, 휘발성 저장장치 계층의 저장장치 중에서 CPU와의 데이터 전송이 하드웨어적으로 이루어지는 것은 무엇인가? 하드웨어 인터럽트의 예를 2가지 들어보시오. => cpu 타이머 인터럽트, io 인터럽트 소프트웨어 인터럽트 예를 2가지 들어보시오. => systemcall, trap 예외 인터럽트 인터럽트가 발생하면 수행되는 운영체제의 코드를 무엇이라 하는가? => isr 인터럽트 벡터 테이블이란 무엇인가? => isr 의 처리 코드의 주소가 적혀있는 테이블 인터럽트 처리 과정 중, 하드웨어적인 처리 과정을 적으시오. => 상태 레지스터 저장 프로세서 모드 커널모드로 변경 현제 저장된 pc 레지스터 저장 발생된 isr 주소를 pc 레지스터로 저장 마이크로 커널 구조의 장단점을 설명하시오. 다음은 C 언어로 작성한 프로그램이다. #include <stdio.h> main() { printf(\"hello, world \\n\"); } (가) 이 프로그램의 수행을 라이브러리와 시스템 호출 관점에서 설명하시오. => printf 는 stdio.h 에 속한 라이브러리 함수로서 리눅스에선는 write 시스템 콜이 호출되면서 사용자가 원하는 동작을 수행하게 된다 (나) 리눅스와 윈도우즈가 제공하는 시스템 호출은 서로 다르다. 이 C 프로그램을 리눅스에서 컴파일하여 수행이 가능한가? 윈도우즈에서 컴파일하여 수행이 가능한가? => printf 라는 표준 c 라이브러리는 내부적으로 각 os 에 시스템콜에 대응되게 수행한다 (다) 위 (나)에서 작성한 답안에 대하여 그 이유를 설명하시오. => stdio.h 는 표준 c 라이브러리로서 각 컴파일러 제조사들은 각 운영체제에 맞게 라이브러리를 제공한다 프로세스의 상태 중에서, CPU를 할당받기를 기다리는 상태는 무엇인가? => ready 프로세스의 상태가 실행 상태에서 준비 상태로 전이하게 하는 사건은 무엇인가 => interrupt PCB에서 레지스터 저장 영역의 용도는 무엇인가? => 프로세스의 상태변화시에 cpu 에 이 레지스터 값을 세팅하게 된다 문맥교환(context switching)에 하드웨어 지원이 미치는 영향을 논하시오. => 문맥교환시 cpu 레지스터를 pcb(process control block) 에 저장하게 되는데 모든 레지스터를 한번에 저장할 수 있는 명령어를 하드웨어 제조사에서 제공하게 된다면 소프트웨어로 구현한 문맥교환 보다 빠르게 동작을 수행 할 수 있게 된다 I/O bound 프로세스와 CPU bound 프로세스가 적절히 혼합되어 수행되는 것이 좋은 이유를 설명하시오. => cpu bound 프로세스가 실행되고 있을 때 io bound 프로세스가 io 작업을 실행하고 있을 수 있다 즉 cpu, io 자원을 유휴 상태를 최대한 줄일 수 있다 장기 중기 단기 스케줄러를 설명하시오 => 장기 스케줄러 : 어떤 프로세스를 ready queue 로 보낼지 결정하는 스케줄러 즉 메인 메모리와 보조 메모리의 스케줄링 메인 메모리는 한정(상대적으로 보조메모리에 비해 적다)되어 있기 때문에, 실행할 수 있는 프로세스보다 많은 프로세스가 메모리에 올라오면 대용량 메모리(일반적으로 하드디스크)에 임시로 저장된다. 장기 스케줄러는 하드디스크의 프로세스 중 하나를 선택하여 메모리를 할당하고 Ready Queue로 보내는 역할을 한다. 중기 스케줄러(Swapper) : 우선순위가 낮은 프로세스 일정시간동안 활성화 되지 않은 프로세스들을 내린다 단기 스케줄러 : 현제 어떤 프로세스를 실행할 것이가에 대한 문제 최대한 컴퓨터 자원을 잘 활용해야 한다 프로세스간 통신을 위한 메시지 전달 방법에서 직접통신과 간접통신의 특징을 비교하여 설명 하시오. => 직접통신의 경우 보내는 측의 관점에서는 받는 측의 pid 를 직접 적고 받는 측에서는 보내는 측의 pid 를 직접 적어서 메세지를 받게 된다 하지만 이와 달리 간접통신에서는 보내는 측에서는 mailbox 혹은 포트라는 추상적인 객체를 통해 전달하고 받는 측은 여러개가 될 수 있다 int pthreadcreate(pthreadt \\thread, const pthread_attr_t \\attr, void \\(\\start_routine) (void \\), void \\arg); 함수의 매개변수들을 설명하시오. => 쓰레드 id, 쓰레드 정보, 쓰레드 시작 함수, 쓰레드 함수의 매개변수 쓰레드의 장점 4가지를 설명하시오. => 빠른 응답성, 자원 공유, 경제성, 멀티 프로세서 활용 쓰레드 구현 방식 2가지는 무엇인가? => 사용자 수준의 쓰레드 라이브러리, 커널 쓰레드 다중 쓰레딩 모델 중에서 many-to-one 모델의 특징을 설명하시오. => 많은 사용자 수준의 쓰레드를 생성해도 실제로는 하나의 커널 쓰레드와 대응된다 Pasted image 20240502170596Pasted image 20240502170556 Pasted image 20240502170579Pasted image 20240502170540 Pasted image 20240502180544Pasted image 20240502180537 교착상태가 발생하는 4가지 조건과 그 의미를 쓰시오. 프로세스와 자원 간의 관계를 나타내는 그래프는 무엇인가? 다음은 교착상태 발생 조건 중 어떤 조건을 제거하기 위한 것인가? 프로세스가 수행되기 전에 필요한 모든 자원을 할당시켜 준다. 자원이 점유되지 않은 상태에서만 자원을 요구하도록 한다. 가. 상호배제 나. 점유와 대기 다. 비선점 라. 순환대기 교착상태 예방방법의 문제점은 무엇인가? 교착상태의 해결 방법 중 은행가 알고리즘(banker's algorithm)과 관계되는 것은? 가. Avoidance 나. Prevention 다. Detection 라. Recovery 외부 단편화 문제 : 내부 단편화 문제 : 실행파일로 부터 생성된 프로세스의 크기(code segment ~ stack segment) 보다 더 큰 공간을 할당 받을 때 생기는 문제 즉 더 큰 공간을 할당하는 방법을 사용할 때 생긴다 페이징방법을 사용할 때 더 큰 공간을 할당하게 된다 페이징 페이지 테이블 프로세스마다 하나씩 페이지 테이블을 가지고 있다 TLB 페이지 테이블 의 캐쉬용 하드웨어 frame = page page 가 4KB 라고 가정하면 프레임이 4KB(킬로바이트)인 경우, 우리는 이를 페이지 크기와 동일하다고 가정할 수 있습니다. 4KB는 2의 12승 바이트(1KB = 2^10 바이트이므로, 4KB = 2^2 * 2^10 = 2^12 바이트)입니다. 이는 메모리 주소에서 하위 12비트가 페이지 내에서의 위치, 즉 페이지 변위(offset)를 나타낸다는 것을 의미합니다. 따라서 주소의 앞쪽 비트들은 페이지 번호를 나타내게 됩니다. 예를 들어, 32비트 시스템에서 메모리 주소는 총 32비트로 표현됩니다. 여기서 하위 12비트가 페이지 변위를 나타낸다면, 남은 상위 20비트는 페이지 번호를 나타내는 데 사용됩니다. 따라서, 프레임이 4KB인 경우, 주소의 앞쪽 20비트가 페이지 번호를 나타내고, 뒤쪽 12비트가 페이지 내 변위를 나타냅니다. TLB 는 문맥교환 대상인가?? -> 아니다 문맥교환 대상이 아닌경우 문맥교환시 TLB 를 다 지워야하지 않나?? -> 문맥교환시 계속 flush(모든 비트를 0으로 만들어야 한다) 메모리 관리 주소 바인딩 컴파일 시간 : 적재 위치를 변경하고 싶으면 다시 컴파일 해야함 적재시간 : 적재 할 때 결정 실행시간 : 적재후 실행시간에도 실시간으로 위치가 변경할 수 있음 지금은 적재시간과 실행시간 주소 바인딩을 섞어서 사용한다 주소의 종류 논리주소 : 프로세스가 실행하면서 cpu가 생성하는 주소 & 컴파일러가 생성한 주소 물리주소 : 기억장치가 나타내는 주소 주소변환 장치 : MMU 주소변환 하드웨어 장치 MMU relocation register : 주소 변환을 할 변위값 레지스터 relocation register 에 연속할당 기법의 경우 프로세스의 절대 주소가 담기지만 페이징에서는 PT(페이지 테이블) 에 담긴 주소를 담는다 4가지 할당 기법 먼저 알아햐 할 것 외부 단편화 : 가용 공간의 합은 충분한데 공간에 연속되지 않았을 때 생기는 문제 내부 단편화 : 할당된 기억공간 대비 프로세스가 사용하지 않는 문제 연속 할당 기법 : (MS-DOS) 빈공간을 찾아서 프로세스를 생성해야 한다 Pasted image 20240614105295 최초 적합 : 충분한 것 중에서 첫번째 가용 공간에 할당 최적 적합 : 중분 한 것 중에서 가장 작은 가용 공간에 할당 최악 적합 : 충분한 것 중에서 가장 작은 가용 공간에 할당 외부 단편화 문제만 발생 : 프로세스의 크기만큼 바이트 단위로 할당하기 때문에 없다 페이징 : 페이지 라는 단위로 쪼개는 방식 프레임(512B ~ 4KB) 페이지크기 = 프레임크기 페이지 테이블 = 페이지 매핑 테이블 = 페이지 맵 테이블 : 어떤 페이지를 점유하고 있는가를 각각의 프로세스 마다 기록됨 즉 PCB 에 담겨짐Pasted image 20240614110044 페이지 번호 + offset(페이지 내부에서 위치) K = 2^10 M = 2^20 G 2^30 TLB : 하드웨어 캐쉬 MMU 내부에 있음 병렬 검색 가능 TLB 검색후 있으면 hit 판정 없으면 페이지 테이블 검색 페이지 보호 : 페이지마다 보호용 비트를 두고 MMU 가 보호용 비트를 검사한다 읽기 전용 페이지에 대해 쓰기를 하면 MMU 는 인터럽트를 발생시킴 공유 페이지 : 하나의 동일한 실행파일을 여러사람이 사용하면 프로세스마다 동일한 코드와 데이터 부분의 메모리를 사용하게 된다 이때 코드의 부분은 읽기 전용 파일이기 때문에 프로세스 2개 이상에서 동일마게 유지된다 그러면 우리는 2개 이상 생성하지 않고 1개의 코드부분을 공유하면 된다 현대의 페이지 테이블 졸라 크다 거이 32 비트 컴퓨터는 페이지크기가 4k 이면 2^20 1M 이고 각 항목이 4B 라면 4MB 이다 64 비트 컴퓨터는 어마어마 해진다 모든 프로세스마다 이만큼 먹고 있는 것이다 즉 해결방법 계층적 테이블 : 사용하지 않는 페이지들이 존재하므로 단계를 나누어 outer table 만들어 사용한다 해쉬된 페이지 테이블 : 해쉬함수를 이용하여 접근에 필요한 가짓수를 빠르게 줄인다 역 페이지 테이블 : ??? 세그멘테이션 : 페이지 테이블이랑 비슷 단 나누는 단위를 코드 수준에서 나온다 그래서 외부 단편화 문재가 발생할 수 있게 된다 페이지화 된 세그멘테이션 : 현재 cpu 외부 단편화 문제 해결 인텔 386 요구 페이지 Pasted image 20240614123916 모든걸 미리 담지 않고 유효비트를 참조하여 1이면 그냥 원래대로 실행 0이면 인터럽트후 위의 과정으로 적재를 시킨 후에 계속 실행 페이지 교체 새 페이지를 적재할 때, 비어있는 프레임이 없다면 사용 중인 프레임을 하나 선택하여 디스크로 스왑 아웃시키고 새 페이지를 적재시키는 것 이때, 선택된 페이지 프레임을 희생(victim) 프레임이라 부름 Pasted image 20240614124176 페이지 교체 알고리즘 FIFO (First-in First-out) OPT (Optimal algorithm) LRU (Least Recently Used) Second chance LFU (Least Frequently Used) MFU (Most Frequently Used) 1단계 디렉토리 이름(Naming) 문제 : 같은 이름을 가진 파일을 여러 개 만들 수 없다 그룹핑(Grouping) 문제 : 파일들을 여러개씩 그룹화하여 관리할 수 없다 2단계 디렉토리 공유 문제 : 여러 사용자가 한 파일을 공유할 수 없음 이름 문제 : 한 사용자는 같은 파일 이름을 여러 개 사용할 수 없다 그룹핑 문제 : 한 사용자는 파일들을 여러개씩 그룹화하여 관리할 수 없다 트리구조 그룹핑 기능이 좋음 비순환 그래프 \b디랙토리 FCFS (First Come First Served) 요청이 들어온 순서대로 디스크 요청을 처리합니다. 디스크 헤드의 이동 거리가 길어질 수 있습니다. 요청 처리 시간이 길어질 수 있습니다, 특히 요청이 분산되어 있을 때. SSTF (Shortest Seek Time First) 현재 디스크 헤드 위치에서 가장 가까운 트랙의 요청을 먼저 처리합니다. 디스크 헤드 이동 거리를 최소화하려는 알고리즘입니다. 평균 응답 시간이 짧아질 수 있습니다. 특정 요청이 계속해서 뒤로 밀리는 \"기아\" 현상이 발생할 수 있습니다. SCAN 디스크 헤드가 한쪽 끝에서 시작하여 다른 쪽 끝으로 이동하면서 모든 요청을 처리합니다. 끝에 도달하면 반대 방향으로 이동하며 요청을 처리합니다. 엘리베이터 알고리즘이라고도 불립니다. 요청이 고르게 처리됩니다. 디스크 헤드의 이동이 일정합니다. 양방향으로 이동하므로, 끝에서 끝으로 이동하는 시간이 길어질 수 있습니다. C-SCAN (Circular SCAN) SCAN 알고리즘과 비슷하지만, 한쪽 끝에서 다른 쪽 끝으로 이동한 후, 다시 처음으로 돌아가서 요청을 처리합니다. 디스크 헤드는 항상 한 방향으로만 이동합니다. 요청 처리 순서가 균일합니다. 디스크 헤드가 한쪽 방향으로만 이동하므로 SCAN보다 예측이 쉽습니다. 단점: 끝에서 처음으로 돌아가는 동안의 이동 시간은 비효율적일 수 있습니다. 이러한 디스크 스케쥴링 알고리즘은 각각의 특성과 장단점을 가지고 있으며, 시스템의 요구사항과 환경에 따라 선택될 수 있습니다. c LOOK c-scan 과 비슷하지만 끝가지 이동하는 것이 아닌 최소 최대 요청 블록 까지만 이동합니다",
      "frontmatter": {
        "series": "운영체제_류연승",
        "series_weight": "1",
        "date": "2024-03-18T05:46:00+09:00",
        "lastmod": "2024-03-18T05:46:00+09:00"
      }
    },
    "드라이버는 커널인가": {
      "path": "/02.inbox/드라이버는-커널인가/",
      "filename": "드라이버는 커널인가",
      "content": "\"모든 드라이버는 커널의 일부로 간주되는가?\" ✅ 하드웨어 제어를 직접 하는 전통적인 장치 드라이버는 '커널의 일부'로 간주됩니다. ❌ 하지만 모든 드라이버가 커널 코드 트리에 포함되거나, 커널 공간에서만 동작하는 것은 아닙니다. 전통적인 장치 드라이버 → 커널의 일부 (예: 네트워크 카드, USB, 디스크, TTY 드라이버 등) 이들은 다음 조건을 충족하므로 커널의 일부로 간주됩니다: 커널 공간(kernel space)에서 실행됨 하드웨어 레지스터, 인터럽트, DMA 등에 접근 시스템 콜이나 VFS(가상 파일 시스템)와 통합됨 커널의 메모리 관리, 스케줄러와 상호작용 예시: // 커널 소스 트리에서 볼 수 있는 드라이버 코드 예 drivers/usb/core/hub.c drivers/tty/pty.c drivers/net/ethernet/intel/e1000/e1000_main.c → 이 코드들은 리눅스 커널 소스 트리( https://git.kernel.org )에 포함되어 있고, 컴파일 시 커널의 일부가 되거나, 로드 가능한 모듈로 분리됩니다. 로드 가능한 모듈도 \"커널의 일부\"인가? => 네, 삽입된 순간부터는 '커널의 일부'입니다. 예를 들어: $ sudo modprobe usbserial → 이 모듈은 .ko 파일로 디스크에 있지만, 로드되면 커널 메모리에 적재되어 커널과 같은 권한으로 실행됩니다. 사용자 공간에서 동작하는 드라이버는? 이 경우는 정의상 논란이 있지만, 일반적으로는 \"드라이버\"라고는 해도 커널의 일부는 아님. 예 1: FUSE(Filesystem in Userspace) 파일 시스템 드라이버처럼 동작하지만, 사용자 공간에서 실행 커널의 fuse.ko 모듈이 중간에서 시스템 콜과 연결 $ sshfs user@remote:/ ~/remote_folder → 이건 사용자 공간에서 동작하는 FUSE 기반 드라이버 ➡️ 하지만 진짜 하드웨어 제어는 커널이 함. FUSE는 \"인터페이스 제공 + 중계\" 역할. 예 2: UIO / VFIO 특정 PCI 장치를 사용자 공간에서 제어 (예: DPDK, SR-IOV) 커널이 메모리 매핑만 해주고, 나머지는 앱이 처리 ➡️ 이 경우도 커널의 도움 없이는 불가능. 완전히 독립된 건 아님. 그럼 \"드라이버\"라는 말의 의미는? 여기서 혼동이 생길 수 있어요. \"드라이버\"라는 단어는 맥락에 따라 다르게 쓰입니다: 맥락 의미 커널의 일부? 전통적 의미 (리눅스 커널 개발자 관점) 커널 공간에서 하드웨어 제어 ✅ 예 일반 사용자 관점 \"장치를 쓰게 해주는 소프트웨어\" ❌ 아닐 수 있음 임베디드/고성능 컴퓨팅 사용자 공간에서 동작하는 고속 드라이버 ⚠️ 부분적으로만 TTY/PTY 드라이버는? 커널 내부 코드 ( drivers/tty/* ) pty.c , n_tty.c (line discipline) 등은 모두 커널 소스에 있음 /dev/tty , /dev/pts/* 장치 파일은 커널이 관리 확실히 커널의 일부입니다. 결론: \"모든 드라이버는 커널의 일부인가?\" 질문 답변 모든 드라이버가 커널 소스 트리에 포함되는가? ❌ 아니요. FUSE, 외부 모듈 등은 별도 모든 드라이버가 커널 공간에서 실행되는가? ❌ 아니요. FUSE, UIO 등은 userspace 전통적인 장치 드라이버는 커널의 일부인가? ✅ 예 — 정의상 커널 공간에서 동작해야 함 커널의 일부로 간주되는 기준은? ✔️ 커널과 같은 공간에서 실행되고, 커널 API 사용, 시스템 리소스 제어",
      "frontmatter": {
        "tags": [
          "잡지식",
          "operating-system"
        ],
        "date": "2025-08-11T02:58:32+09:00",
        "lastmod": "2025-08-11T03:00:33+09:00"
      }
    },
    "디버거 cli 명령어 모음": {
      "path": "/02.inbox/디버거-cli-명령어-모음/",
      "filename": "디버거 cli 명령어 모음",
      "content": "GDB 🔹 기본 실행 및 종료 명령어 설명 gdb <executable> 실행 파일로 GDB 시작 run 또는 r 프로그램 실행 (인자: run arg1 arg2 ) quit 또는 q GDB 종료 🔹 중단점 (Breakpoint) 명령어 설명 break main.cpp:10 또는 b main.cpp:10 파일의 특정 라인에 중단점 설정 break main 또는 b main 함수 이름에 중단점 설정 info breakpoints 또는 i b 현재 중단점 목록 보기 delete 1 ID가 1인 중단점 삭제 disable 1 중단점 비활성화 enable 1 중단점 활성화 clear main.cpp:10 특정 위치의 중단점 제거 🔹 스텝 실행 (Stepping) 명령어 설명 step 또는 s 한 줄 실행 (함수 내부로 들어감) next 또는 n 한 줄 실행 (함수 내부로 안 들어감) finish 현재 함수 끝까지 실행 후 반환 continue 또는 c 다음 중단점까지 실행 🔹 변수 및 메모리 보기 명령어 설명 print <var> 또는 p <var> 변수 값 출력 (식 평가 가능) display <var> 매 스텝마다 자동으로 변수 출력 undisplay 1 display 목록에서 제거 x/16xb 0x12345678 메모리 덤프 ( x/[count][format][size] address ) set variable x = 10 변수 값 변경 🔹 스택 및 프레임 명령어 설명 backtrace 또는 bt 콜 스택 출력 frame 2 또는 f 2 특정 프레임으로 이동 up / down 스택 프레임 위/아래로 이동 🔹 프로세스 및 스레드 명령어 설명 info threads 현재 스레드 목록 thread 2 특정 스레드로 전환 kill 디버그 중인 프로세스 강제 종료 🔹 도움말 및 설정 명령어 설명 help 전체 도움말 help <command> 특정 명령어에 대한 도움말 set args arg1 arg2 실행 인자 설정 run < input.txt 표준 입력 리다이렉션 (쉘 수준에서 처리) set environment VAR=value 환경 변수 설정 🔹 LLDB ↔ GDB 명령어 비교 (참고) LLDB GDB b main b main br list info breakpoints v 또는 frame variable info locals expr x = 5 set variable x = 5 process launch run memory read x/... settings set target.run-args ... set args ... 💡 팁 GDB는 .gdbinit 파일을 통해 시작 시 자동으로 명령어를 실행할 수 있습니다. 최신 GDB(8.0+)는 Python 스크립팅을 지원하여 고급 디버깅이 가능합니다. ARM64 환경에서도 잘 작동하지만, 크로스 디버깅 시 gdb-multiarch 나 대상 아키텍처 전용 GDB를 사용해야 할 수 있습니다. LLDB 🔹 기본 실행 및 종료 명령어 설명 lldb <executable> 실행 파일로 LLDB 시작 run 또는 r 프로그램 실행 process launch 프로그램 실행 (옵션 사용 가능) quit 또는 q LLDB 종료 🔹 중단점 (Breakpoint) 명령어 설명 breakpoint set --file main.cpp --line 10 파일의 특정 라인에 중단점 설정 b main.cpp:10 위와 동일 (간단한 형태) breakpoint set --name main 함수 이름에 중단점 설정 b main 위와 동일 breakpoint list 또는 br list 현재 중단점 목록 보기 breakpoint delete 1 ID가 1인 중단점 삭제 breakpoint disable 1 중단점 비활성화 breakpoint enable 1 중단점 활성화 🔹 스텝 실행 (Stepping) 명령어 설명 step 또는 s 한 줄 실행 (함수 내부로 들어감) next 또는 n 한 줄 실행 (함수 내부로 안 들어감) finish 현재 함수 끝까지 실행 후 반환 continue 또는 c 다음 중단점까지 실행 🔹 변수 및 메모리 보기 명령어 설명 frame variable 또는 v 현재 프레임의 지역 변수 출력 print <var> 또는 p <var> 변수 값 출력 (식 평가 가능) expr <expression> 표현식 평가 및 실행 (변수 수정도 가능) memory read --size 1 --count 16 0x12345678 메모리 덤프 🔹 스택 및 프레임 명령어 설명 bt 또는 thread backtrace 콜 스택 출력 frame select 2 특정 프레임으로 이동 up / down 스택 프레임 위/아래로 이동 🔹 프로세스 및 스레드 명령어 설명 thread list 현재 스레드 목록 process status 현재 프로세스 상태 확인 process kill 디버그 중인 프로세스 강제 종료 🔹 도움말 및 설정 명령어 설명 help 전체 도움말 help <command> 특정 명령어에 대한 도움말 settings set target.run-args arg1 arg2 실행 인자 설정 settings set target.input-path input.txt 표준 입력 리다이렉션 🔹 GDB ↔ LLDB 명령어 비교 (참고) GDB LLDB run run break main b main step step next next print x p x info breakpoints br list bt bt",
      "frontmatter": {
        "tags": [
          "reference",
          "ai-content"
        ],
        "date": "2025-10-11T13:27:45+09:00",
        "lastmod": "2025-10-11T13:27:52+09:00"
      }
    },
    "리스트 컴프리헨션(List Comprehension)": {
      "path": "/02.inbox/리스트-컴프리헨션list-comprehension/",
      "filename": "리스트 컴프리헨션(List Comprehension)",
      "content": "%20image%2020231223111957.png) 컴퓨터 계산 기준 output 식 발견 뒤의 for , if 좌측에서 우측으로 순서대로 순회후 out put을 계산 for문 for 문이 무조건 한개는 있어야 한다 lst = [1,2,3,4,5,6,7,8,9,10] a = [x+1 for x in lst] print(a) [2, 3, 4, 5, 6, 7, 8, 9, 10, 11] [ (x, y) for x in ['쌈밥', '치킨', '피자'] for y in ['사과', '아이스크림', '커피']] [('쌈밥', '사과'), ('쌈밥', '아이스크림'), ('쌈밥', '커피'), ('치킨', '사과'), ('치킨', '아이스크림'), ('치킨', '커피'), ('피자', '사과'), ('피자', '아이스크림'), ('피자', '커피')] 좌측부터 계산 if 문 lst = [1,2,3,4,5,6,7,8,9,10] c = [x for x in lst if x > 4] print(c) [5, 6, 7, 8, 9, 10] 중첩 if 문 d = [x for x in lst if x > 4 if x%2 == 0] [6, 8, 10] if else lst = [1,2,3,4,5,6,7,8,9,10] e = [x if x > 4 else 'less than 4' for x in lst] print(e) ['less than 4', 'less than 4', 'less than 4', 'less than 4', 5, 6, 7, 8, 9, 10] [!NOTE] if else 문은 왜 앞에다가 사용하는가?? if else 는 여기서는 삼항 연산자로 사용된다 처음에 예시로 든 (x+1) 과 (x if x > 4 else 'less than 4') 이렇게 하나의 output 이다 문으로 보지 않도록 하자 nested matrix = [ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], ] [[row[i] for row in matrix] for i in range(4)] [1, 5, 9](1,%205,%209)",
      "frontmatter": {
        "tags": [
          "python",
          "language"
        ],
        "date": "2023-12-23T11:09:00+09:00",
        "lastmod": "2023-12-23T11:09:00+09:00"
      }
    },
    "메모리 최대 크기": {
      "path": "/02.inbox/메모리-최대-크기/",
      "filename": "메모리 최대 크기",
      "content": "문제 발생 이전 메모리의 최대 크기는 RAM 에 저장할 수 있는 bit 최대 개수를 의미한다 word 는 bus 의 크기를 이야기 한다 하지만 직접 CPU 를 만들다 보니(logisim) (그래서 컴퓨터는 어떻게 동작하나요 1) 지금까지 나는 byte 단위로 메모리에 접근한 것이 아닌 word 단위로 접근 했었고 이것이 8비트 cpu 에서는 word 와 byte 가 운 좋게 8bit 로 일치되어 메모리 최대 크기를 계산 할 때 주소폭 크기 (data bit width = word) 2^(address bit width) 즉 8 2^8 즉 2048b ~= 256B 라고 계산 했었다 답은 맞지만 16비트 32 비트로 cpu 를 개선해나가는 과정에서 문제가 발생했다 학교에서 학교에서는 32 비트의 cpu가 가질 수 있는 메모리의 최대 크기가 4GiB 라고 했고 이유는 2^32 Byte 라고 했다 왜 Byte 인지는 이번에 알게 되었다 문제 발생 나의 32 비트 cpu 의 경우 logisim 으로 구현 했는데 logisim 소프트웨어 자체의 한계로 인해 address bit width 의 크기는 24 비트로 제한 data bit width 는 32 비트로 제한되었다 나의 경우는 data bit width 의 크기를 word 로 맞추어 넣고 시작했기 때문에 정확하게 메모리의 크기는 32 2^(24) = 64MiB 로 나오게 된다 만약 logsim 이 32 비트 address bit width 크기가 32 제한 이었다면 32 2^(32) = 64GiB 가 나오게 되고 내가 이미 알던 32비트 컴퓨터의 메모리 최대 4GiB 의 크기와 다르게 된다 즉 이때 부터 궁금증이 시작되었고 여러가지를 찾아보니 메모리 주소당 데이터 용량이 1바이트인 이유 주소지정단위와 주소비트수의 관계/워드와 명령어형식/pc의 증가폭 2개를 보고 많이 배워 다시 정리 해본다 이해 과정 위에서 말하는 것중 가장 정리되어 있는 글을 퍼온 것이다 1 Word 는 CPU 가 한 클럭에 받아들일 수 있는 Data 의 너비 (Data bus width) 입니다. 보통 8 bit, 16 bit, 32 bit, 64 bit 컴퓨터라고 하면, Data Bus Width 를 의미하는 것입니다. 하지만, 정확하게는 외부의 데이터를 내부에 저장해두는 레지스터의 크기입니다. 외부에 핀이 4 pin 밖에 없지만, 내부 레지스터가 4 bit씩 2개 연결되어 2 클럭에 저장이 된다면, 실제 Word 크기는 8 bit 가 되는 것입니다. 2. Data 는 피연산 대상인 수(Number)만을 의미하지는 않고, OP Code(Operator Code) Operand, ... 조합의 Instruction 일 수도 있으며, 여러개의 Operand 가 다음 Word 로 들어오는 경우도 있습니다. 즉, Inst. Operand 1 + Operand 2 + Operand 3 + ... 식으로 연결되어서 각 클럭마다 입력받을 수 있고 SIMD(Single Instruction Multi Data, 다중 피연산자처리 단일 명령어) 종류의 Instruction 에서 나타나는 형태입니다. 이 때, 이러한 종류의 명령어 길이에 따라 Program Counter 의 값이 2 Word 이상 늘어나게 됩니다. 3. Word 는 1. 에서 말했듯이 8, 16, 32 bit 너비일 수 있습니다. 바이트로 환산하면 1,2,4 bytes가 됩니다. 메모리 주소는 1 byte 단위로 지정(Addressing)하기 때문에, 32 bit 컴퓨터에서는 한 클럭마다 4의 배수로 Program Counter 값이 바뀔 것입니다. 4. PC(Program Counter) 레지스터는 실행할 명령어가 담겨있는 메모리주소를 저장하는 공간이기 때문에 Address Bus Width 와 같거나 더 큽니다. 5. Address Bus Width 는 꼭 Data Bus Width 와 일치하지는 않습니다. 32 bit CPU 이지만, 24 bit 메모리 주소 버스를 갖는 i386 CPU 처럼, 대부분은 비대칭적인 경우가 많습니다. 현재 64 bit CPU 인 x86_64 는 물리적 메모리 주소 버스 폭이 48 bit 입니다. 그러나 PC 레지스터의 크기는 64 bit 입니다. A. 주소공간(Address Space) 이라는 개념에 대해 설명듣지 못하셨을 겁니다. 주 메모리의 한계용량을 의미하고, i686(IA32, x86) 구조에서는 32 bit, 즉 2^32 bytes 의 영역인 4 GB 가 주 메모리 한계공간입니다. i686은 Data Bus Width 가 32 bit, 즉 4 Bytes 이기 때문에, 32 bit 의 메모리 핀 중, 30 핀만 사용한다고 하더라도 충분히 4 GB 주소공간을 지정할 수 있었을 겁니다. 즉, 주소 지정 단위(Address Unit)가 어떻게 지정되느냐에 따라, Address Pin 의 갯수, 혹은 Address Bus Width 가 달라지는 셈입니다. 64 bit 컴퓨터인 x86_64 구조에서는 주소지정단위가 64bit, 즉 8 bytes 이므로, 48 bit 로 나타낼 수 있는 2^48 = 64 TBytes 의 공간을 8 Bytes 씩 지정한다고 했을 때, 45 bit 의 Address Bus Width 를 갖게 되는 셈입니다. 하지만, 최신 코어에서는 64 bit 컴퓨터라서 꼭 8 bytes 씩 단위를 쓰지는 않고, 16 bit (2 bytes), 32 bit (4 bytes) 단위로도 접근해서 쓰기 때문에, 동작하는 환경에 따라 주소공간이 각각 달라진다는 점을 알고 계시면 좋겠습니다. 가장 잘 정리되어 있는 글이지만 내가 만든 cpu 의 경우 byte 단위로 메모리를 읽는 것이 아닌 data bit width 로 32 비트로 접근 하였고 총 4Byte 로 접근 한 것이다 그래서 PC 명령어를 클락 주기 마다 4 증가 한 것이 아닌 1만 증가 했던 것이다 logisim 제한이 없는 가정에서 내가 만든 CPU 의 메모리 MAX 예상치와 학교에서 배운 메모리 MAX 예상치가 달랐던 것이다 결론 결론적으로 메모리 MAX 계산은 메모리 접근은 1Byte 단위로 한다는 가정하에 계산한 것이다 추가 최신 컴퓨터의 경우 (아니 최신도 아니지만... ) 메모리 클럭이 따로 존재한다 cpu 클럭에 메모리 클럭이 따라가지 못한다 그러면 캐쉬메모리는 동일한 클럭을 사용하는 가? 아니다 그러면 어떻게 그 사이를 중계하는 알고리즘을 어떻게 사용하는 걸까",
      "frontmatter": {
        "tags": [
          "cs",
          "잡지식",
          "cpu"
        ],
        "date": "2024-08-10T17:45:00+09:00",
        "lastmod": "2025-09-22T13:58:18+09:00"
      }
    },
    "문서 객체 모델 (DOM)": {
      "path": "/02.inbox/문서-객체-모델-dom/",
      "filename": "문서 객체 모델 (DOM)",
      "content": "_ The HTML DOM document object is the owner of all other objects in your web page. _ The document object represents your web page. If you want to access any element in an HTML page, you always start with accessing the document object. Below are some examples of how you can use the document object to access and manipulate HTML. _ Method Description document.getElementById(id) Find an element by element id document.getElementsByTagName(name) Find elements by tag name document.getElementsByClassName(name) Find elements by class name _ Property Description element.innerHTML = new html content Change the inner HTML of an element element.attribute = new value Change the attribute value of an HTML element element.style.property = new style Change the style of an HTML element Method Description element.setAttribute(attribute, value) Change the attribute value of an HTML element _ Method Description document.createElement(element) Create an HTML element document.removeChild(element) Remove an HTML element document.appendChild(element) Add an HTML element document.replaceChild(new, old) Replace an HTML element document.write(text) Write into the HTML output stream _ Method Description document.getElementById(id).onclick = function(){code} Adding event handler code to an onclick event _ The first HTML DOM Level 1 (1998), defined 11 HTML objects, object collections, and properties. These are still valid in HTML5. Later, in HTML DOM Level 3, more objects, collections, and properties were added. Property Description DOM document.anchors Returns all \\ elements that have a name attribute 1 document.applets Deprecated 1 document.baseURI Returns the absolute base URI of the document 3 document.body Returns the \\ element 1 document.cookie Returns the document's cookie 1 document.doctype Returns the document's doctype 3 document.documentElement Returns the \\ element 3 document.documentMode Returns the mode used by the browser 3 document.documentURI Returns the URI of the document 3 document.domain Returns the domain name of the document server 1 document.domConfig Obsolete. 3 document.embeds Returns all \\ elements 3 document.forms Returns all \\ elements 1 document.head Returns the \\ element 3 document.images Returns all \\ elements 1 document.implementation Returns the DOM implementation 3 document.inputEncoding Returns the document's encoding (character set) 3 document.lastModified Returns the date and time the document was updated 3 document.links \"Returns all \\ and \\ elements that have a href attribute\" 1 document.readyState Returns the (loading) status of the document 3 document.referrer Returns the URI of the referrer (the linking document) 1 document.scripts Returns all \\ elements 3 document.strictErrorChecking Returns if error checking is enforced 3 document.title Returns the \\ element 1 document.URL Returns the complete URL of the document 1",
      "frontmatter": {
        "tags": [
          "javascript",
          "language"
        ],
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2023-12-20T07:12:00+09:00"
      }
    },
    "백엔드 구조 변화 역사": {
      "path": "/02.inbox/백엔드-구조-변화-역사/",
      "filename": "백엔드 구조 변화 역사",
      "content": "모델 1에서 모델 2로의 진화: 웹 개발 아키텍처 심층 분석 현대 백엔드 개발의 핵심 철학 서론: 왜 아키텍처는 중요한가? 소프트웨어 개발은 단순히 '동작하는 코드'를 만드는 행위에서 그치지 않습니다. 시간이 지나면서 요구사항은 끊임없이 변화하고, 새로운 기술이 등장하며, 비즈니스는 확장됩니다. 이러한 변화의 파도 속에서 흔들리지 않는 견고하고 유연한 시스템을 구축하는 것, 이것이 바로 '아키텍처'의 역할입니다. 제공된 텍스트는 웹 애플리케이션 개발 아키텍처의 중요한 변곡점인 모델 1과 모델 2의 차이를 통해, 좋은 아키텍처가 무엇이며 어떻게 발전해 왔는지를 심도 있게 이야기하고 있습니다. 이는 단순히 기술의 변화가 아닌, '문제 해결 방식'에 대한 패러다임의 전환을 의미합니다. 제1장: 혼돈의 시대, 모델 1 아키텍처 모델 1 아키텍처는 초창기 웹 개발의 직관적인 접근 방식이었습니다. 웹 페이지(JSP, ASP, PHP 등) 하나가 하나의 요청을 처음부터 끝까지 모두 책임지는 구조입니다. 핵심 구조: All-in-One 페이지 사용자가 list.jsp 라는 페이지를 요청했다고 가정해 봅시다. 모델 1 구조에서 이 list.jsp 파일 안에는 다음과 같은 코드들이 뒤섞여 있습니다. ① 요청 분석 코드 (Controller의 역할): 사용자가 검색어를 입력했는지, 특정 페이지 번호를 요청했는지 등의 파라미터를 분석하는 자바 코드. ② 비즈니스 로직 및 데이터 처리 코드 (Service & Repository의 역할): 데이터베이스에 연결하여 게시글 목록을 조회하는 SQL 쿼리와 JDBC 코드. ③ 화면 출력 코드 (View의 역할): 조회된 데이터를 <table> , <li> 등의 HTML 태그를 사용하여 웹 페이지 형태로 그려내는 코드. <%-- list.jsp (모델 1 예시) --%> <%@ page import=\"java.sql.*, java.util.*\" %> <html> <head><title>게시판 목록</title></head> <body> <h1>게시판 목록</h1> <% // ① 요청 분석 + ② 데이터 처리 (Controller + Service + Repository) Connection conn = null; PreparedStatement pstmt = null; ResultSet rs = null; List<Map<String, Object>> boardList = new ArrayList<>(); try { // DB 연결 String dbUrl = \"jdbc:mysql://localhost:3306/mydb\"; conn = DriverManager.getConnection(dbUrl, \"user\", \"password\"); // SQL 실행 String sql = \"SELECT id, title, writer, created_at FROM board ORDER BY id DESC\"; pstmt = conn.prepareStatement(sql); rs = pstmt.executeQuery(); // 결과(ResultSet)를 Array(List<Map>)로 변환 while (rs.next()) { Map<String, Object> board = new HashMap<>(); board.put(\"id\", rs.getInt(\"id\")); board.put(\"title\", rs.getString(\"title\")); board.put(\"writer\", rs.getString(\"writer\")); boardList.add(board); } } catch (Exception e) { e.printStackTrace(); } finally { // 자원 해제 if (rs != null) rs.close(); if (pstmt != null) pstmt.close(); if (conn != null) conn.close(); } %> <!-- ③ 화면 출력 (View) --> <table border=\"1\"> <tr> <th>번호</th> <th>제목</th> <th>작성자</th> </tr> <% for (Map<String, Object> board : boardList) { %> <tr> <td><%= board.get(\"id\") %></td> <td><%= board.get(\"title\") %></td> <td><%= board.get(\"writer\") %></td> </tr> <% } %> </table> </body> </html> 모델 1의 치명적인 문제점 텍스트에서 지적한 \"소스에 대한 응집이 높아 특정 단계에 대한 기능을 수정해도... 결국 전체의 결과물을 확인하고 수정해야 하는 비효율적인 문제\"가 바로 여기서 발생합니다. 높은 결합도(High Coupling): 디자인을 수정하기 위해 HTML 태그 하나를 바꾸려 해도, 복잡한 자바 코드 한가운데를 헤쳐나가야 합니다. 반대로, DB 테이블의 컬럼 이름이 writer 에서 author 로 바뀌면, SQL 쿼리뿐만 아니라 HTML을 출력하는 부분의 board.get(\"writer\") 코드까지 모두 수정해야 합니다. 디자이너와 개발자의 작업 영역이 완전히 겹쳐 협업이 불가능에 가깝습니다. 낮은 재사용성: '게시글 목록을 조회하는 로직'은 매우 유용한 기능입니다. 하지만 모델 1에서는 이 로직이 list.jsp 라는 특정 '화면'에 종속되어 있습니다. 만약 모바일 앱을 위해 게시글 목록 데이터만 JSON 형태로 제공해야 한다면? list.jsp 의 코드를 복사-붙여넣기하여 api_list.jsp 같은 파일을 새로 만들어야 합니다. 이는 중복 코드의 양산이며, 유지보수의 재앙을 불러옵니다. 테스트의 어려움: '데이터베이스 조회 기능'만 따로 떼어내어 잘 동작하는지 테스트할 방법이 없습니다. 반드시 웹 서버를 실행하고, 브라우저로 list.jsp 를 요청해서 눈으로 확인해야 합니다. 이는 단위 테스트(Unit Test)의 부재로 이어져 코드의 안정성을 심각하게 저해합니다. \"어레이(Array) 문제\": 텍스트가 명확히 짚은 이 문제는 매우 중요합니다. 위 예제에서 데이터는 List<Map<String, Object>> 형태로 전달됩니다. 다음 단계의 코드는 map.get(\"title\") 과 같이 문자열 키에 의존해야 합니다. 만약 키 이름을 titel 로 오타를 내도 컴파일 시점에는 오류를 잡을 수 없고, 실행 후 페이지가 깨지거나 오류가 발생한 뒤에야 문제를 인지할 수 있습니다. 데이터의 구조가 명확하지 않아 개발자의 실수를 유발하기 매우 쉬운 구조입니다. 제2장: 역할의 분리, 모델 2 아키텍처의 등장 (MVC 패턴) 이러한 모델 1의 혼돈을 해결하기 위해 등장한 것이 바로 모델 2 아키텍처, 즉 우리에게 친숙한 MVC(Model-View-Controller) 패턴입니다. 모델 2의 핵심 철학은 '관심사의 분리(Separation of Concerns)'입니다. 각자 잘하는 일에만 집중하자는 것입니다. Controller: 사용자의 요청을 가장 먼저 받는 '교통 경찰'입니다. 요청(URL, 파라미터 등)을 분석하여 어떤 작업이 필요한지 판단하고, 그 작업을 실제 일꾼인 'Service'에게 위임합니다. 작업이 끝나면 결과를 받아 어떤 'View'에게 전달하여 화면을 그리게 할지 결정합니다. Model: 실질적인 데이터와 비즈니스 로직을 담당하는 영역입니다. \"게시글을 저장한다\", \"사용자 레벨을 업그레이드한다\"와 같은 핵심 로직이 여기에 포함됩니다. 현대 개발에서는 이를 다시 Service와 Repository로 세분화합니다. View: Controller로부터 전달받은 데이터를 화면에 '그리는' 일만 합니다. JSP, Thymeleaf 등이 여기에 해당하며, 내부에는 비즈니스 로직이 전혀 없고 오직 표현 로직만 존재합니다. 모델 2의 정제: Controller-Service-Repository 구조 텍스트에서 설명하듯, 현대적인 모델 2 구현은 Model 영역을 더욱 구체적으로 분화하여 책임과 역할을 명확히 합니다. Controller: 오직 웹 요청과 응답에만 집중합니다. HTTP 헤더를 분석하고, 요청 본문을 객체로 변환하며, 인증/인가를 확인하고, 적절한 Service 메소드를 호출한 뒤, 그 결과를 JSON, HTML 등 요청된 형식으로 변환하여 응답합니다. 예시: /posts (GET 요청) -> PostController.getPostList() 호출* Service: '비즈니스 로직'의 중심입니다. 트랜잭션 관리, 여러 데이터 소스 조합 등 애플리케이션의 핵심 정책과 규칙을 구현합니다. 이 계층은 웹(HTTP)이나 데이터베이스(SQL) 기술에 의존하지 않는 순수한 자바 코드로 작성되는 것을 지향합니다. 예시: PostService.getPostList() -> 게시글 목록 조회 로직 수행. 필요하다면 UserService 를 호출하여 작성자 정보를 함께 가져올 수도 있음.* 재사용성의 핵심: 텍스트의 \"웹에서 작동했던 게시판의 기능을 단말 어플로 확장해야 할 때 서비스의 고유 기능을 유지한 상태에서 컨트롤러의 기능만 수정\"할 수 있다는 설명이 바로 이 지점입니다. 모바일 앱을 위한 JSON 응답이 필요하면, 기존 PostService 는 그대로 두고 ApiPostController 를 새로 만들어 동일한 서비스를 호출하기만 하면 됩니다. 핵심 로직의 재사용성이 극대화됩니다. Repository(DAO): 데이터 영속성(Persistence)만을 전담합니다. 즉, 데이터베이스에 데이터를 저장(Save), 조회(Find), 수정(Update), 삭제(Delete)하는 역할만 수행합니다. 예시: PostRepository.findAll() -> SELECT * FROM post 쿼리 실행* 유지보수의 효율성: 텍스트의 \"테이블 구조 변경 시 리파지토리 수정만으로 적용이 가능\"하다는 설명이 여기에 해당합니다. 데이터베이스가 MySQL에서 PostgreSQL로 바뀌거나, ORM 기술(JPA 등)을 도입할 때, 오직 Repository 계층의 코드만 수정하면 Service나 Controller는 아무런 영향을 받지 않습니다. 소통의 규약: DTO (Data Transfer Object) 모델 1의 '어레이 문제'를 해결하는 모델 2의 해법이 바로 DTO입니다. 텍스트에서는 '데이터 전달 객체'라고 표현했습니다. DTO란? 계층 간 데이터 교환을 위해 사용하는, 데이터 필드(멤버 변수)와 그에 대한 Getter/Setter 메소드만으로 이루어진 순수한 데이터 운반용 객체입니다. 장점: 명확한 계약: DTO는 그 자체로 계층 간에 \"우리는 이런 구조의 데이터를 주고받을 것이다\"라는 명확한 약속(Contract)이 됩니다. 타입 안정성(Type Safety): map.get(\"title\") 대신 postDto.getTitle() 을 사용합니다. getTitle() 메소드는 항상 문자열(String)을 반환함이 보장되며, 만약 getTitel() 과 같이 오타를 내면 컴파일 시점에 즉시 오류를 발견할 수 있습니다. 개발 편의성: IDE의 자동완성 기능 등을 통해 어떤 데이터가 있는지 쉽게 파악할 수 있어 생산성이 향상됩니다. 제3장: 궁극의 유연성, 의존성 주입 (Dependency Injection) 모델 2 구조로 역할을 분리하고 DTO로 소통 규약을 정했지만, 마지막 문제가 남아있습니다. \"실제 구현에서 각각의 기능을 연동하기 위한 코드가 생성되어 기능의 독립성이 사라지는 문제\"입니다. PostController 가 PostService 를 사용하려면 어떻게 해야 할까요? 가장 단순한 방법은 다음과 같습니다. public class PostController { private PostService postService = new PostServiceImpl(); // ★ 문제 지점! // ... } PostController 가 PostServiceImpl 이라는 '구체적인 구현 클래스'를 직접 생성하고 있습니다. 이를 '강한 결합(Tight Coupling)'이라고 합니다. 이 코드의 문제는, 만약 PostService 의 구현체를 테스트용 TestPostServiceImpl 로 바꾸고 싶을 때 PostController 의 코드를 직접 수정해야 한다는 것입니다. 각 기능의 독립적인 개발과 테스트가 다시 어려워집니다. 의존성 주입(DI)과 제어의 역전(IoC) 이 문제를 해결하는 기술이 바로 의존성 주입(DI)입니다. DI의 근간에는 제어의 역전(Inversion of Control, IoC)이라는 원리가 있습니다. 기존 방식: PostController 가 자신이 사용할 PostService 객체를 직접 생성(제어)한다. IoC/DI 방식: PostController 는 PostService 객체를 생성하지 않는다. 단지 \"나는 PostService 타입의 객체가 필요해!\"라고 선언만 해둔다. 그러면 외부의 누군가(DI 컨테이너, 예: 스프링 프레임워크)가 PostController 에게 필요한 PostService 객체를 만들어서 주입(연결)해준다. 객체를 생성하고 연결하는 '제어'의 흐름이 개발자 코드에서 프레임워크로 역전된 것입니다. \"동물이 오리로 변하고 강아지로 변한다\"는 비유의 해석 텍스트의 이 비유는 DI의 핵심을 완벽하게 설명합니다. 동물 (Animal): 이것이 바로 인터페이스(Interface)입니다. 소리를 내다(makeSound()) 라는 '기능(메소드)'을 약속(정의)합니다. 오리 (Duck), 강아지 (Dog): 이것이 구현체(Implementation)입니다. 동물 인터페이스를 구현하여, 소리를 내다() 메소드를 꽤액꽤액 또는 멍멍 으로 구체화합니다. 사육사 (Zookeeper): 동물 을 필요로 하는 클라이언트 코드(예: Controller , Service )입니다. // 인터페이스 (약속) public interface Animal { String makeSound(); } // 구현체 1 public class Duck implements Animal { @Override public String makeSound() { return \"꽤액꽤액\"; } } // 구현체 2 public class Dog implements Animal { @Override public String makeSound() { return \"멍멍\"; } } // 클라이언트 (스프링에서의 예시) @RestController public class ZookeeperController { private final Animal animal; // '구현체'가 아닌 '인터페이스'에 의존! // 생성자를 통해 외부에서 Animal 객체를 주입받음 (DI) @Autowired public ZookeeperController(Animal animal) { this.animal = animal; } @GetMapping(\"/sound\") public String hearSound() { // 주입된 객체가 Dog라면 \"멍멍\", Duck이라면 \"꽤액꽤액\"이 반환됨 return animal.makeSound(); } } ZookeeperController 는 Dog 인지 Duck 인지 전혀 모릅니다. 단지 Animal 인터페이스에 정의된 makeSound() 를 호출할 뿐입니다. 어떤 동물이 주입될지는 스프링 프레임워크가 설정(Configuration)에 따라 런타임(실행 시점)에 결정하여 '동적으로' 연결해줍니다. 이것이 바로 텍스트에서 말한 \"인터페이스로 연동하여 기능이 실행되는 런타임에서 인터페이스의 구현체를 주입하는 방식\"이며, \"동적 생성\"의 진정한 의미입니다. 이러한 DI를 통해, 우리는 테스트 시에는 실제 DB에 접근하는 RealPostRepository 대신, 메모리에서 가짜 데이터를 반환하는 MockPostRepository 를 PostService 에 주입하여 DB 없이도 Service 로직을 완벽하게 테스트할 수 있게 됩니다. 각 계층이 완벽하게 분리되어 독립적인 개발과 테스트가 가능해지는 것입니다. 결론: 정적인 코드에서 동적인 아키텍처로 모델 1에서 모델 2로의 발전, 그리고 DI의 도입은 단순히 코드를 정리하는 수준을 넘어 개발의 패러다임을 바꾼 혁신입니다. 모델 1: 모든 것이 얽힌 정적인 구조. 변경에 취약하고 재사용이 불가능하며 테스트가 어렵다. 모델 2 (MVC): 역할과 책임을 분리하여 느슨한 결합을 추구. 유지보수성과 재사용성의 기틀을 마련. 모델 2 + DI: 인터페이스를 통한 의존성 주입으로 결합을 끊어내고 동적인 구조를 완성. 각 기능의 완벽한 독립성과 테스트 용이성을 확보하여 유연하고 확장 가능한 시스템을 구축. \"백엔드 개발자는 기본적인 문법의 영적이고 확장적인 기능을 파악하여 정적인 기능을 동쪽으로 유도할 줄 아는 능력이 필요합니다\"는 이 모든 과정을 함축합니다. 훌륭한 백엔드 개발자는 단순히 문법에 맞춰 코드를 작성하는 사람이 아닙니다. 변화를 예측하고, 각 기능이 독립적으로 존재하며 서로 유연하게 협력할 수 있는 '구조'를 설계할 줄 아는 사람입니다. 딱딱하게 굳어있는 정적인 코드를, 언제든 다른 부품으로 교체할 수 있는 유연하고 살아있는 동적인 시스템으로 만드는 능력, 이것이 바로 모델 2와 DI가 우리에게 가르쳐주는 핵심 교훈이자 현대 백엔드 개발자가 갖춰야 할 가장 중요한 역량입니다. 문제 순수 Java 코드(Plain Old Java Object, POJO)로 시작하여 프레임워크 없이 각 개념을 구현해보는 실습입니다. Stage 1: 모델 1 - 모든 것이 섞여있는 코드 🎯 목표: 왜 모델 1 아키텍처가 유지보수에 재앙인지 코드로 직접 경험합니다. 하나의 클래스 안에서 요청 분석, 데이터 처리, 화면 생성이 모두 일어나는 상황의 문제점을 느껴봅니다. 📝 문제: PostManager라는 클래스를 만드세요. 이 클래스는 main 메소드를 가지고 있으며, 실행하면 게시판 목록을 HTML 문자열 형태로 콘솔에 출력해야 합니다. 요구사항: PostManager 클래스 안에 게시물 데이터를 List<Map<String, Object>> 형태로 하드코딩하여 가지고 있으세요. (DB 연결 흉내) 이 데이터를 <table> 태그를 사용한 HTML 형식으로 변환하는 로직을 구현하세요. 모든 로직(데이터 정의, HTML 생성)은 main 메소드 또는 PostManager 클래스 내의 private 메소드에 전부 포함되어야 합니다. 🔑 핵심 질문: 만약 테이블의 <th> 순서를 바꾸거나 <td> 에 CSS 클래스를 추가하는 등 디자인을 변경하고 싶다면 코드를 얼마나, 어디를 수정해야 하나요? 게시물 목록 데이터를 HTML이 아닌 JSON 형식으로도 제공해야 한다는 새로운 요구사항이 생겼다면 어떻게 해야 할까요? 코드 재사용이 가능한가요? Stage 2: 모델 2 (MVC) - 역할의 분리 🎯 목표: 모델 1의 문제를 해결하기 위해 '관심사의 분리'를 적용합니다. Controller, Service, View의 역할을 하는 클래스들을 만들어 코드를 분리합니다. 📝 문제: Stage 1의 PostManager를 Controller, Service, View 역할로 나누어 리팩토링하세요. 요구사항: PostController : 요청을 받는 진입점. PostService 를 호출하여 데이터를 받고, 받은 데이터를 PostView 에 넘겨 최종 결과를 받아옵니다. PostController 내부에 private PostService postService = new PostServiceImpl(); 와 같이 서비스 객체를 직접 생성해야 합니다. PostService : 비즈니스 로직과 데이터 처리를 담당. 게시물 목록 데이터를 List<Map<String, Object>> 형태로 반환하는 메소드를 제공합니다. (Stage 1의 데이터 처리 로직을 그대로 가져옵니다) PostView : 데이터를 받아 최종 HTML을 생성하는 역할만 합니다. 로직 없이 데이터를 화면에 그리는 데 집중합니다. 🔑 핵심 질문: 이제 디자인 변경(View 수정)이나 데이터 소스 변경(Service 수정)이 이전보다 쉬워졌나요? PostController 가 PostServiceImpl 을 직접 생성( new )하고 있습니다. 만약 테스트를 위해 다른 가짜 Service 객체를 사용하고 싶다면, PostController 의 코드를 수정해야만 하는가요? 이것이 어떤 문제를 일으킬까요? (이것이 바로 강한 결합(Tight Coupling) 입니다.) Stage 3: DTO 도입 - 명확한 데이터 계약 🎯 목표: 모델 2 구조에서 계층 간 데이터를 Map 이 아닌 DTO(Data Transfer Object)로 주고받도록 개선합니다. '어레이 문제'를 해결하고 타입 안정성을 확보하는 과정을 이해합니다. 📝 문제: Stage 2의 코드에서 List>를 사용하는 모든 부분을 PostDto를 사용하도록 변경하세요. 요구사항: PostDto 클래스를 생성하세요. id , title , writer 필드와 각 필드에 대한 Getter 메소드를 가져야 합니다. PostService 는 이제 List<PostDto> 를 반환하도록 수정합니다. 내부적으로 Map 을 PostDto 객체로 변환하는 과정이 필요합니다. PostController 와 PostView 는 Map 대신 PostDto 를 사용하여 데이터를 처리하고 화면을 생성하도록 수정합니다. map.get(\"title\") 대신 dto.getTitle() 을 사용하게 됩니다. 🔑 핵심 질문: dto.getTitel() 처럼 오타를 냈을 때, 컴파일 시점에 오류를 발견할 수 있나요? Map 을 사용할 때와 비교하여 어떤 점이 좋아졌나요? DTO를 사용함으로써 Service 와 Controller 사이에 \"어떤 형태의 데이터를 주고받을지\"에 대한 약속이 명확해졌나요? Stage 4: 의존성 주입 (DI) - 궁극의 유연성 확보 🎯 목표: 인터페이스를 도입하고 외부에서 의존성을 주입하여 각 컴포넌트 간의 결합을 끊어냅니다. 이를 통해 기능의 교체가 유연해지고 테스트가 쉬워지는 것을 확인합니다. 📝 문제: Stage 3의 강한 결합 문제를 해결하기 위해 의존성 주입(DI) 원리를 적용하세요. 요구사항: PostService 인터페이스를 만드세요. ( List<PostDto> getPosts() 메소드 선언) 기존 PostService 클래스의 이름을 PostServiceImpl 로 바꾸고 PostService 인터페이스를 구현( implements )하도록 하세요. PostController 가 더 이상 PostServiceImpl 을 직접 생성하지 않도록 수정합니다. 대신 PostService 인터페이스 타입의 멤버 변수를 선언하고, 생성자를 통해 외부에서 PostService 구현체를 주입받도록 변경하세요. public class PostController { private final PostService postService; // 생성자를 통해 의존성 주입 public PostController(PostService postService) { this.postService = postService; } // ... } Main (또는 Application ) 클래스를 만들어 DI 컨테이너의 역할을 하도록 합니다. 이 클래스가 PostServiceImpl 객체를 생성하고, 이 객체를 PostController 의 생성자에 인자로 넘겨주어 전체 시스템을 조립하고 실행합니다. (심화) TestPostServiceImpl 이라는 PostService 의 또 다른 구현체를 만드세요. 이 클래스는 테스트용 고정 데이터(\"테스트 제목 1\", \"테스트 제목 2\" 등)를 반환하도록 합니다. Main 클래스에서 PostServiceImpl 대신 TestPostServiceImpl 을 주입했을 때, Controller 코드를 전혀 바꾸지 않고도 프로그램의 동작이 바뀌는 것을 확인하세요. 🔑 핵심 질문: '사육사( Controller )'는 '동물( Service 인터페이스)'에게 일을 시킬 뿐, 실제 일하는 동물이 '오리( ServiceImpl )'인지 '강아지( TestServiceImpl )'인지 더 이상 신경 쓰지 않게 되었습니다. 이로 인해 얻는 가장 큰 이점은 무엇일까요? 이제 PostService 로직만 따로 테스트하는 것이 얼마나 쉬워졌나요? 이것이 바로 단위 테스트(Unit Test)의 시작입니다.",
      "frontmatter": {
        "tags": [
          "spring",
          "ai-content"
        ],
        "date": "2025-07-04T06:32:40+09:00",
        "lastmod": "2025-10-22T13:24:21+09:00"
      }
    },
    "벨만 포드 알고리즘 상세과정": {
      "path": "/02.inbox/벨만-포드-알고리즘-상세과정/",
      "filename": "벨만 포드 알고리즘 상세과정",
      "content": "이 표는 시작 노드 u에서 다른 모든 노드(v, w, x, y, z)까지의 최단 거리를 벨만-포드 알고리즘을 사용하여 찾는 과정을 보여줍니다. 벨만-포드 알고리즘은 거쳐가는 간선(hop)의 개수를 1개부터 점차 늘려가며 최단 거리를 갱신하는 방식입니다. 기본 개념: 벨만-포드 알고리즘 벨만-포드 알고리즘의 핵심 아이디어는 다음과 같습니다. k개의 간선까지만 사용했을 때의 최단 거리를 계산합니다. 이전 단계(k-1개 간선 사용)의 최단 거리 정보를 이용해 현재 단계(k개 간선 사용)의 최단 거리를 갱신합니다. 공식: D[u] > D[v] + cost(v, u) 이면 D[u] = D[v] + cost(v, u) 로 갱신합니다. (v를 거쳐 u로 가는 거리가 더 짧다면 갱신) 주어진 표에서 \"up to k hops\"는 최대 k개의 간선을 사용했을 때 u로부터 각 노드까지의 최단 거리를 의미합니다. 단계별 논리 설명 각 단계에서 비용(Cost)과 다음 경유지(to neighbor of u)가 어떻게 결정되는지 살펴보겠습니다. 이 과정을 이해하려면 각 노드 간의 직접적인 연결 비용을 알아야 하지만, 표의 변화를 통해 역으로 추적해 보겠습니다. 🔎 1단계: 최대 1개의 간선 사용 (up to 1 hop) 이 단계에서는 시작 노드 u와 직접 연결된 이웃 노드까지만의 거리를 계산합니다. v까지의 비용: 2, 경유지: (v) 논리: u에서 v까지 직접 연결된 간선의 비용이 2입니다. w까지의 비용: 5, 경유지: (w) 논리: u에서 w까지 직접 연결된 간선의 비용이 5입니다. x까지의 비용: 1, 경유지: (x) 논리: u에서 x까지 직접 연결된 간선의 비용이 1입니다. y, z까지의 비용: ∞ 논리: u에서 y와 z로는 직접 연결된 간선이 없으므로, 초기 비용은 무한대(∞)입니다. 🔎 2단계: 최대 2개의 간선 사용 (up to 2 hops) 이제 최대 2개의 간선을 사용하여 갈 수 있는 경로를 계산합니다. 즉, 1단계에서 계산된 이웃 노드(v, w, x)를 거쳐 다른 노드로 가는 경로를 고려합니다. v까지의 비용: 2, 경유지: (v) 논리: u에서 v로 가는 직접 경로(비용 2)보다 더 짧은 경로가 발견되지 않았습니다. w까지의 비용: 4, 경유지: (x) 논리: 기존의 u→w 직접 경로(비용 5)보다 u→x→w 경로가 더 짧다는 것을 의미합니다. 계산: u→x 비용(1) + x→w 비용(3) = 4 입니다. (따라서 x와 w는 비용 3으로 연결되어 있음을 알 수 있습니다.) 비용이 5에서 4로 갱신되고, 경유지도 w에서 x로 변경되었습니다. x까지의 비용: 1, 경유지: (x) 논리: u에서 x로 가는 직접 경로(비용 1)가 여전히 최단 거리입니다. y까지의 비용: 2, 경유지: (x) 논리: u에서 y로 가는 새로운 경로가 발견되었습니다. u→x→y 경로입니다. 계산: u→x 비용(1) + x→y 비용(1) = 2 입니다. (x와 y는 비용 1로 연결되어 있음을 알 수 있습니다.) z까지의 비용: 10, 경유지: (w) 논리: u에서 z로 가는 새로운 경로가 발견되었습니다. u→w→z 경로입니다. 계산: u→w 비용(5) + w→z 비용(5) = 10 입니다. (w와 z는 비용 5로 연결되어 있음을 알 수 있습니다.) 🔎 3단계: 최대 3개의 간선 사용 (up to 3 hops) 최대 3개의 간선을 사용하여 가는 경로를 계산합니다. 2단계에서 갱신된 거리 정보를 활용합니다. w까지의 비용: 3, 경유지: (x) 논리: 기존 u→x→w 경로(비용 4)보다 더 짧은 경로가 발견되었습니다. u→x→y→w 경로일 가능성이 높습니다. 계산: u→x→y 비용(2) + y→w 비용(1) = 3 입니다. (y와 w는 비용 1로 연결되어 있음을 알 수 있습니다.) 비용이 4에서 3으로 갱신되었습니다. z까지의 비용: 4, 경유지: (x) 논리: 기존 u→w→z 경로(비용 10)보다 훨씬 짧은 경로가 발견되었습니다. u→x→y→z 경로일 가능성이 있습니다. 계산: u→x→y 비용(2) + y→z 비용(2) = 4 입니다. (y와 z는 비용 2로 연결되어 있음을 알 수 있습니다.) 비용이 10에서 4로 크게 줄고, 경유지도 w에서 x로 변경되었습니다. v, x, y는 변경 없음: 더 짧은 경로가 발견되지 않았습니다. 🔎 4단계 및 5단계: 최대 4, 5개 간선 사용 논리: 3단계에서 계산된 비용과 비교했을 때 어떤 노드로의 비용도 더 이상 줄어들지 않았습니다. 이는 3개의 간선을 사용하는 경로에서 이미 모든 노드까지의 최단 거리가 구해졌음을 의미합니다. 벨만-포드 알고리즘은 이렇게 더 이상 거리 갱신이 일어나지 않으면 최단 경로 탐색을 완료합니다. 보통 (전체 노드 개수 - 1) 만큼 반복하면 최단 거리를 보장하지만, 그전에 갱신이 멈추면 조기 종료할 수 있습니다. 최종 결과 요약 표의 마지막 줄(up to 4 또는 5 hops)이 시작 노드 u에서 각 노드까지의 최종 최단 거리입니다. u → v: 비용 2 (경로: u→v) u → w: 비용 3 (경로: u→x→y→w) u → x: 비용 1 (경로: u→x) u → y: 비용 2 (경로: u→x→y) u → z: 비용 4 (경로: u→x→y→z) 경유지(Next Hop) 표기의 장점 최단 거리 알고리즘에서 비용(Cost)만 계산하지 않고 다음 경유지(표에서는 'to neighbor of u')를 함께 기록하는 이유는 실제 경로를 복원하기 위해서입니다. 비용 vs. 경로: 단순히 'u에서 z까지 최단 거리는 4'라는 사실만 아는 것과, 'u → x → y → z'라는 경로를 아는 것은 큰 차이가 있습니다. 내비게이션 앱이 \"목적지까지 요금은 5,000원입니다\"라고만 알려주고 경로를 알려주지 않는다면 무용지물인 것과 같습니다. 경로 재구성 (Path Reconstruction): 최종적으로 계산된 경유지 정보를 따라가면 전체 경로를 알아낼 수 있습니다. 예시: z로 가는 최종 경유지는 (x)입니다. 이는 최단 경로가 u → x ... 로 시작함을 의미합니다. (실제 알고리즘에서는 모든 노드에 대해 직전 노드(predecessor)를 저장하므로) x 는 y 를 통해 왔고, y 는 u 에서 온 x 를 통해 왔다는 것을 역추적하여 u → x → y → z 전체 경로를 완성할 수 있습니다. 결론적으로 경유지 정보는 알고리즘이 찾은 최단 거리의 '증거'이자 '실행 계획'이 됩니다. 🔎 2단계 (최대 2개 간선 사용) 상세 비교 과정 2단계는 1단계의 결과에, 간선을 하나 더 추가했을 때 더 짧은 경로가 생기는지 모든 가능성을 확인하는 과정입니다. 시작 정보 (1단계 결과): D(v) = 2 (경유지 v) D(w) = 5 (경유지 w) D(x) = 1 (경유지 x) D(y) = ∞ D(z) = ∞ 이제 각 목적지(v, w, x, y, z)에 대해, u의 직접 이웃(v, w, x)을 거쳐 가는 경로와 기존 경로를 비교합니다. 목적지: v 기존 최단 거리: u → v (비용 2) 새로운 경로 탐색: u → w → v: 비용 D(w) + cost(w,v) = 5 + ? u → x → v: 비용 D(x) + cost(x,v) = 1 + ? 결론: u → v 직접 경로(비용 2)보다 더 짧은 2-hop 경로가 발견되지 않았습니다. 변경 없음. 목적지: w 기존 최단 거리: u → w (비용 5) 새로운 경로 탐색: u → v → w: 비용 D(v) + cost(v,w) = 2 + ? u → x → w: 비용 D(x) + cost(x,w) = 1 + 3 = 4 비교: 새로운 경로 u → x → w 의 비용(4)이 기존 경로 비용(5)보다 더 저렴합니다. 결론: w 까지의 최단 거리를 4로 갱신하고, 경유지를 (w) 에서 (x) 로 변경합니다. 목적지: x 기존 최단 거리: u → x (비용 1) 새로운 경로 탐색: u → v → x: 비용 D(v) + cost(v,x) = 2 + ? u → w → x: 비용 D(w) + cost(w,x) = 5 + ? 결론: u → x 직접 경로(비용 1)는 이미 매우 저렴하여, 다른 노드를 거쳐 가는 경로가 더 짧아질 가능성이 거의 없습니다. 변경 없음. 목적지: y 기존 최단 거리: ∞ (경로 없음) 새로운 경로 탐색: u → v → y: 비용 D(v) + cost(v,y) = 2 + ? u → w → y: 비용 D(w) + cost(w,y) = 5 + ? u → x → y: 비용 D(x) + cost(x,y) = 1 + 1 = 2 비교: 새로운 경로 u → x → y 의 비용(2)이 기존 비용( ∞ )보다 더 저렴합니다. 결론: y 까지의 최단 거리를 2로 갱신하고, 경유지를 (x) 로 설정합니다. 목적지: z 기존 최단 거리: ∞ (경로 없음) 새로운 경로 탐색: u → v → z: 비용 D(v) + cost(v,z) = 2 + ? u → w → z: 비용 D(w) + cost(w,z) = 5 + 5 = 10 u → x → z: 비용 D(x) + cost(x,z) = 1 + ? 비교: 새로운 경로 u → w → z 의 비용(10)이 기존 비용( ∞ )보다 더 저렴합니다. (u→x를 통한 경로는 아직 z로 이어지지 않음) 결론: z 까지의 최단 거리를 10으로 갱신하고, 경유지를 (w) 로 설정합니다. 이러한 모든 비교 과정을 거쳐 표의 \"up to 2 hops\" 행이 완성되는 것입니다. 이와 같은 '완화(relaxation)' 과정을 모든 간선에 대해 반복하며 최적의 해를 찾아 나가는 것이 벨만-포드 알고리즘의 핵심입니다.",
      "frontmatter": {
        "tags": [
          "network",
          "university",
          "algorithm"
        ],
        "date": "2025-06-12T15:19:04+09:00",
        "lastmod": "2025-06-12T15:40:59+09:00"
      }
    },
    "브라우저 기반 수강신청 알림 시스템 설계서": {
      "path": "/02.inbox/브라우저-기반-수강신청-알림-시스템-설계서/",
      "filename": "브라우저 기반 수강신청 알림 시스템 설계서",
      "content": "명지대학교 수강신청 빈자리 알림 크롬 확장프로그램 프로젝트 설계서 프로젝트 개요 명지대학교 수강신청 시스템의 '미리담기 내역' 페이지( https://class.mju.ac.kr/main/bag )에서 '인원초과' 상태인 강의에 빈자리가 발생했을 때, 사용자에게 데스크탑 알림을 보내주는 크롬 확장 프로그램을 개발한다. 이 프로그램은 사용자가 수동으로 새로고침하며 자리를 확인하는 불편함을 해소하는 것을 목표로 한다. 목표 특정 페이지에서만 동작: https://class.mju.ac.kr/main/bag URL에서만 확장 프로그램의 기능이 활성화된다. UI 요소 삽입: '인원초과'가 표시된 강의 항목에 '알림 대기' 버튼을 동적으로 추가한다. 백그라운드 모니터링: 사용자가 '알림 대기' 버튼을 클릭하면, 해당 강의를 백그라운드에서 주기적으로 확인한다. 실시간 알림: 모니터링 중인 강의에 빈자리( [인원초과] 문자열 제거)가 확인되면 즉시 사용자에게 크롬 데스크탑 알림을 보낸다. 메모리 기반 데이터 관리: 알림 대기 중인 강의 목록, CSRF 토큰 등 모든 상태 정보는 chrome.storage 를 사용하지 않고, 확장 프로그램의 실행 시간 동안 메모리(JavaScript 변수)에만 저장한다. 비-간섭적 작동: 백그라운드 확인 작업은 현재 사용자가 보고 있는 페이지를 새로고침하거나 UI를 변경하지 않는다. 아키텍처 및 구성 요소 확장 프로그램은 다음과 같은 세 가지 주요 파일로 구성된다. manifest.json (확장 프로그램 설정 파일) 확장 프로그램의 이름, 버전, 설명 등 기본 정보 정의 필요한 권한 요청 ( notifications , alarms , scripting ) Content Script가 실행될 URL 지정 ( https://class.mju.ac.kr/main/bag ) Background Script(Service Worker) 등록 content_script.js (콘텐츠 스크립트) 역할: 웹 페이지 DOM(Document Object Model)에 직접 접근하여 UI를 조작하고, 백그라운드 스크립트와 통신하는 중개자. 주요 기능: 페이지 로드 시, 현재 URL이 https://class.mju.ac.kr/main/bag 인지 확인. div.coursedata 요소를 모두 순회하며, 내부에 [ 인원초과 ] 텍스트가 포함된 span.fullclass 가 있는지 확인. 조건에 맞는 강의의 div.btnarea 내부에 '알림 대기' 버튼을 생성하고 삽입. '알림 대기' 버튼에 클릭 이벤트 리스너를 추가. 버튼 클릭 시, 해당 강의의 고유 식별자(예: data-coursecls 값)와 페이지의 CSRF 토큰( meta[name=\"_csrf\"] 태그의 content 값)을 추출. 추출된 (강의 ID, CSRF 토큰) 정보를 Background Script로 메시지 전송. (선택) 알림이 시작되면 버튼의 텍스트를 '알림 대기중...'으로 변경하고 비활성화하여 중복 요청 방지. background.js (백그라운드 스크립트 - Service Worker) 역할: 확장 프로그램의 핵심 두뇌. 보이지 않는 곳에서 주기적인 작업을 수행하고 상태를 관리. 주요 기능: 상태 관리 (메모리): monitoredCourses : 알림 대기 중인 강의 ID를 저장하는 Set 또는 Map 객체. (예: new Set(['0710', '0084']) ) csrfToken : Content Script로부터 최초로 전달받은 CSRF 토큰을 저장하는 변수. 메시지 수신: Content Script로부터 메시지( chrome.runtime.onMessage )를 수신 대기. 메시지 수신 시, 전달받은 강의 ID를 monitoredCourses 에 추가하고, csrfToken 을 업데이트. 알림 대기 목록에 강의가 추가되면 주기적인 확인 작업을 시작/유지. 주기적 작업 ( chrome.alarms API 사용): 일정 시간(예: 5초) 간격으로 알람을 설정. setInterval 보다 Service Worker 환경에 더 적합. 알람이 울리면 등록된 모든 monitoredCourses 에 대해 확인 작업 수행. 데이터 확인 (Fetch): 저장된 csrfToken 을 사용하여 https://class.mju.ac.kr/main/bag 에 POST 요청을 보냄 (요청 형식은 제공된 명세와 동일하게 구성). 응답으로 받은 HTML 텍스트를 파싱. 상태 비교 및 알림: 파싱된 HTML 내에서, monitoredCourses 에 포함된 각 강의 ID에 해당하는 div.coursedata 를 찾음. 해당 강의의 span.fullclass 요소에서 [ 인원초과 ] 문자열이 사라졌는지 확인. 빈자리가 확인되면 chrome.notifications.create API를 사용하여 사용자에게 데스크탑 알림을 표시. (예: \"'알고리즘' 강의에 빈자리가 생겼습니다!\"). 알림을 보낸 강의 ID는 monitoredCourses 목록에서 제거하여 더 이상 확인하지 않도록 함. 데이터 관리 전략 저장소: 모든 데이터는 background.js 내의 JavaScript 변수(전역 스코프)에 저장된다. 데이터 생명주기: 알림 대기 강의 목록 ( monitoredCourses ): 사용자가 '알림 대기' 버튼을 누를 때 추가되고, 빈자리 알림이 발생하거나 브라우저가 종료되면 초기화된다. CSRF 토큰 ( csrfToken ): 사용자가 첫 '알림 대기' 버튼을 누를 때 content_script.js 가 페이지에서 추출하여 background.js 로 전달하며, 이후 모든 백그라운드 fetch 요청에 사용된다. 브라우저 종료 시 사라진다. 장점: 구현이 간단하고 빠르다. 단점: 브라우저를 닫거나 크롬을 재시작하면 모든 알림 대기 상태가 초기화된다. 이는 요구사항에 부합하는 설계이다. 동작 시나리오 (User Flow) 사용자가 명지대학교 수강신청 사이트에 로그인 후, https://class.mju.ac.kr/main/bag 페이지로 이동한다. content_script.js 가 실행되어 [ 인원초과 ] 가 붙은 '알고리즘' 강의 옆에 '알림 대기' 버튼을 생성한다. 사용자가 '알고리즘' 강의의 '알림 대기' 버튼을 클릭한다. content_script.js 는 '알고리즘'의 강의 ID('0710')와 현재 페이지의 CSRF 토큰 값을 가져와 background.js 로 전송한다. 버튼은 '알림 대기중...'으로 변경된다. background.js 는 메시지를 수신하고, 내부 변수 monitoredCourses 에 '0710'을 추가하고, csrfToken 변수에 토큰 값을 저장한다. background.js 는 chrome.alarms 를 이용해 5초 간격의 확인 작업을 시작한다. 5초 후, background.js 는 저장된 csrfToken 을 이용해 /main/bag 페이지의 최신 정보를 fetch 로 가져온다. 가져온 HTML을 분석하여 '0710' 강의에 여전히 [ 인원초과 ] 가 있는지 확인한다. 아직 자리가 없다. (시간 경과 후) 다음 확인 주기에서 fetch 한 HTML에서 '0710' 강의에 [ 인원초과 ] 문구가 사라진 것을 감지한다. background.js 는 chrome.notifications.create 를 호출하여 \"'0710 알고리즘' 강의에 빈자리가 생겼습니다!\" 라는 알림을 띄운다. background.js 는 monitoredCourses 목록에서 '0710'을 제거하고, 해당 강의에 대한 모니터링을 중단한다. content_script.js 가 생성한 알림 버튼도 같이 제거한다 요청된 url 에서 강의 데이터 구조 <div id=\"bagresult\" class=\"courselist\" data-req=\"Y\" data-bag=\"N\" data-reqdel=\"N\" data-bagdel=\"Y\" role=\"list\"> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-0419\" data-addtime=\"808151054\" data-coursecls=\"0419\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0419\"> [ 인원초과 ] </span> <span>0419 빅데이터인포그래픽(KCU)</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 균컴114 / 학점 : 3 / 담당교수 : 이규연 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>온라인(원격수업)</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0419\">457</span> / 수강인원 : <span id=\"listencnt-0419\">108</span> / 제한인원 : <span id=\"takecnt-0419\">108</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0419\"> 신청완료 </span> <button id=\"req-0419\" class=\"classbtn reqbtn sorthide\" aria-label=\"0419 빅데이터인포그래픽(KCU) 수강신청하기\" data-coursecls=\"0419\" data-curinum=\"KMQ01114\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0419\" data-curinum=\"KMQ01114\" aria-label=\"0419 빅데이터인포그래픽(KCU) 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-5469\" data-addtime=\"808164217\" data-coursecls=\"5469\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-5469\"> [ 인원초과 ] </span> <span>5469 인체안의전쟁(KCU)</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 균여133 / 학점 : 3 / 담당교수 : 미배정 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>온라인(원격수업),KCU</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-5469\">433</span> / 수강인원 : <span id=\"listencnt-5469\">176</span> / 제한인원 : <span id=\"takecnt-5469\">176</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied5469\"> 신청완료 </span> <button id=\"req-5469\" class=\"classbtn reqbtn sorthide\" aria-label=\"5469 인체안의전쟁(KCU) 수강신청하기\" data-coursecls=\"5469\" data-curinum=\"KMO02133\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"5469\" data-curinum=\"KMO02133\" aria-label=\"5469 인체안의전쟁(KCU) 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-0233\" data-addtime=\"814100108\" data-coursecls=\"0233\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0233\"> [ 인원초과 ] </span> <span>0233 문화속디자인여행(KCU)</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 기문217 / 학점 : 3 / 담당교수 : 이규연 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>온라인(원격수업)</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0233\">598</span> / 수강인원 : <span id=\"listencnt-0233\">108</span> / 제한인원 : <span id=\"takecnt-0233\">108</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0233\"> 신청완료 </span> <button id=\"req-0233\" class=\"classbtn reqbtn sorthide\" aria-label=\"0233 문화속디자인여행(KCU) 수강신청하기\" data-coursecls=\"0233\" data-curinum=\"KMC02217\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0233\" data-curinum=\"KMC02217\" aria-label=\"0233 문화속디자인여행(KCU) 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-5431\" data-addtime=\"814100118\" data-coursecls=\"5431\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-5431\"> [ 인원초과 ] </span> <span>5431 영화제작에대해알고싶은모든것(KCU)</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 균문122 / 학점 : 3 / 담당교수 : 미배정 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>온라인(원격수업),KCU</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-5431\">516</span> / 수강인원 : <span id=\"listencnt-5431\">176</span> / 제한인원 : <span id=\"takecnt-5431\">176</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied5431\"> 신청완료 </span> <button id=\"req-5431\" class=\"classbtn reqbtn sorthide\" aria-label=\"5431 영화제작에대해알고싶은모든것(KCU) 수강신청하기\" data-coursecls=\"5431\" data-curinum=\"KML02122\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"5431\" data-curinum=\"KML02122\" aria-label=\"5431 영화제작에대해알고싶은모든것(KCU) 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-0084\" data-addtime=\"814104427\" data-coursecls=\"0084\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0084\"> [ 인원초과 ] </span> <span>0084 영어회화2</span> <span> (<span>L3</span>) </span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 교필109 / 학점 : 1 / 담당교수 : 데본 </span> </div> <div class=\"course-schedule\"> <span> <span>월12:00~12:50(Y9230), 수12:00~12:50(Y9230)</span> </span> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0084\">48</span> / 수강인원 : <span id=\"listencnt-0084\">11</span> / 제한인원 : <span id=\"takecnt-0084\">11</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0084\"> 신청완료 </span> <button id=\"req-0084\" class=\"classbtn reqbtn sorthide\" aria-label=\"0084 영어회화2 수강신청하기\" data-coursecls=\"0084\" data-curinum=\"KMA02109\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0084\" data-curinum=\"KMA02109\" aria-label=\"0084 영어회화2 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-0710\" data-addtime=\"814104302\" data-coursecls=\"0710\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0710\"> [ 인원초과 ] </span> <span>0710 알고리즘</span> </div> <div class=\"course-basicinfo\"> <span> 3학년 / 컴공316 / 학점 : 3 / 담당교수 : 조민경 </span> </div> <div class=\"course-schedule\"> <span> <span>월09:00~10:50(Y5437), 목09:00~09:50(Y5437)</span> </span> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0710\">53</span> / 수강인원 : <span id=\"listencnt-0710\">30</span> / 제한인원 : <span id=\"takecnt-0710\">30</span> </div> <div class=\"course-past\" data-grade=\"D+\"> ※ 과거 이수성적 : 2023-2학기 : 알고리즘 D+ </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0710\"> 신청완료 </span> <button id=\"req-0710\" class=\"classbtn reqbtn sorthide\" aria-label=\"0710 알고리즘 수강신청하기\" data-coursecls=\"0710\" data-curinum=\"JEJ02316\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0710\" data-curinum=\"JEJ02316\" aria-label=\"0710 알고리즘 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-0762\" data-addtime=\"814100722\" data-coursecls=\"0762\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span style=\"display:none\" class=\"fullclass\" id=\"fullsign-0762\"> [ 인원초과 ] </span> <span>0762 산업경영공학개론</span> </div> <div class=\"course-basicinfo\"> <span> 1학년 / 산업213 / 학점 : 3 / 담당교수 : 한민탁 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>아너칼리지및타전공수강,전공자수강X,전공이해기초교과,MJU자율수강제강좌,온라인강의</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0762\">6</span> / 수강인원 : <span id=\"listencnt-0762\">7</span> / 제한인원 : <span id=\"takecnt-0762\">50</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0762\"> 신청완료 </span> <button id=\"req-0762\" class=\"classbtn reqbtn sorthide\" aria-label=\"0762 산업경영공학개론 수강신청하기\" data-coursecls=\"0762\" data-curinum=\"JEO01213\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0762\" data-curinum=\"JEO01213\" aria-label=\"0762 산업경영공학개론 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-0238\" data-addtime=\"814102933\" data-coursecls=\"0238\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0238\"> [ 인원초과 ] </span> <span>0238 MJ사회봉사</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 기사167 / 학점 : 3 / 담당교수 : 유우연 </span> </div> <div class=\"course-schedule\"> <span> <span>월17:00~19:50(Y501)</span> </span> </div> <div class=\"course-remark\"> 비고 : <b>N/P과목</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0238\">56</span> / 수강인원 : <span id=\"listencnt-0238\">17</span> / 제한인원 : <span id=\"takecnt-0238\">17</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0238\"> 신청완료 </span> <button id=\"req-0238\" class=\"classbtn reqbtn sorthide\" aria-label=\"0238 MJ사회봉사 수강신청하기\" data-coursecls=\"0238\" data-curinum=\"KMD02167\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0238\" data-curinum=\"KMD02167\" aria-label=\"0238 MJ사회봉사 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> </div><div id=\"bagresult\" class=\"courselist\" data-req=\"Y\" data-bag=\"N\" data-reqdel=\"N\" data-bagdel=\"Y\" role=\"list\"> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-0419\" data-addtime=\"808151054\" data-coursecls=\"0419\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0419\"> [ 인원초과 ] </span> <span>0419 빅데이터인포그래픽(KCU)</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 균컴114 / 학점 : 3 / 담당교수 : 이규연 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>온라인(원격수업)</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0419\">457</span> / 수강인원 : <span id=\"listencnt-0419\">108</span> / 제한인원 : <span id=\"takecnt-0419\">108</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0419\"> 신청완료 </span> <button id=\"req-0419\" class=\"classbtn reqbtn sorthide\" aria-label=\"0419 빅데이터인포그래픽(KCU) 수강신청하기\" data-coursecls=\"0419\" data-curinum=\"KMQ01114\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0419\" data-curinum=\"KMQ01114\" aria-label=\"0419 빅데이터인포그래픽(KCU) 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-5469\" data-addtime=\"808164217\" data-coursecls=\"5469\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-5469\"> [ 인원초과 ] </span> <span>5469 인체안의전쟁(KCU)</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 균여133 / 학점 : 3 / 담당교수 : 미배정 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>온라인(원격수업),KCU</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-5469\">433</span> / 수강인원 : <span id=\"listencnt-5469\">176</span> / 제한인원 : <span id=\"takecnt-5469\">176</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied5469\"> 신청완료 </span> <button id=\"req-5469\" class=\"classbtn reqbtn sorthide\" aria-label=\"5469 인체안의전쟁(KCU) 수강신청하기\" data-coursecls=\"5469\" data-curinum=\"KMO02133\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"5469\" data-curinum=\"KMO02133\" aria-label=\"5469 인체안의전쟁(KCU) 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-0233\" data-addtime=\"814100108\" data-coursecls=\"0233\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0233\"> [ 인원초과 ] </span> <span>0233 문화속디자인여행(KCU)</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 기문217 / 학점 : 3 / 담당교수 : 이규연 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>온라인(원격수업)</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0233\">598</span> / 수강인원 : <span id=\"listencnt-0233\">108</span> / 제한인원 : <span id=\"takecnt-0233\">108</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0233\"> 신청완료 </span> <button id=\"req-0233\" class=\"classbtn reqbtn sorthide\" aria-label=\"0233 문화속디자인여행(KCU) 수강신청하기\" data-coursecls=\"0233\" data-curinum=\"KMC02217\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0233\" data-curinum=\"KMC02217\" aria-label=\"0233 문화속디자인여행(KCU) 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-5431\" data-addtime=\"814100118\" data-coursecls=\"5431\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-5431\"> [ 인원초과 ] </span> <span>5431 영화제작에대해알고싶은모든것(KCU)</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 균문122 / 학점 : 3 / 담당교수 : 미배정 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>온라인(원격수업),KCU</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-5431\">516</span> / 수강인원 : <span id=\"listencnt-5431\">176</span> / 제한인원 : <span id=\"takecnt-5431\">176</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied5431\"> 신청완료 </span> <button id=\"req-5431\" class=\"classbtn reqbtn sorthide\" aria-label=\"5431 영화제작에대해알고싶은모든것(KCU) 수강신청하기\" data-coursecls=\"5431\" data-curinum=\"KML02122\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"5431\" data-curinum=\"KML02122\" aria-label=\"5431 영화제작에대해알고싶은모든것(KCU) 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-0084\" data-addtime=\"814104427\" data-coursecls=\"0084\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0084\"> [ 인원초과 ] </span> <span>0084 영어회화2</span> <span> (<span>L3</span>) </span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 교필109 / 학점 : 1 / 담당교수 : 데본 </span> </div> <div class=\"course-schedule\"> <span> <span>월12:00~12:50(Y9230), 수12:00~12:50(Y9230)</span> </span> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0084\">48</span> / 수강인원 : <span id=\"listencnt-0084\">11</span> / 제한인원 : <span id=\"takecnt-0084\">11</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0084\"> 신청완료 </span> <button id=\"req-0084\" class=\"classbtn reqbtn sorthide\" aria-label=\"0084 영어회화2 수강신청하기\" data-coursecls=\"0084\" data-curinum=\"KMA02109\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0084\" data-curinum=\"KMA02109\" aria-label=\"0084 영어회화2 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-0710\" data-addtime=\"814104302\" data-coursecls=\"0710\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0710\"> [ 인원초과 ] </span> <span>0710 알고리즘</span> </div> <div class=\"course-basicinfo\"> <span> 3학년 / 컴공316 / 학점 : 3 / 담당교수 : 조민경 </span> </div> <div class=\"course-schedule\"> <span> <span>월09:00~10:50(Y5437), 목09:00~09:50(Y5437)</span> </span> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0710\">53</span> / 수강인원 : <span id=\"listencnt-0710\">30</span> / 제한인원 : <span id=\"takecnt-0710\">30</span> </div> <div class=\"course-past\" data-grade=\"D+\"> ※ 과거 이수성적 : 2023-2학기 : 알고리즘 D+ </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0710\"> 신청완료 </span> <button id=\"req-0710\" class=\"classbtn reqbtn sorthide\" aria-label=\"0710 알고리즘 수강신청하기\" data-coursecls=\"0710\" data-curinum=\"JEJ02316\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0710\" data-curinum=\"JEJ02316\" aria-label=\"0710 알고리즘 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-0762\" data-addtime=\"814100722\" data-coursecls=\"0762\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span style=\"display:none\" class=\"fullclass\" id=\"fullsign-0762\"> [ 인원초과 ] </span> <span>0762 산업경영공학개론</span> </div> <div class=\"course-basicinfo\"> <span> 1학년 / 산업213 / 학점 : 3 / 담당교수 : 한민탁 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>아너칼리지및타전공수강,전공자수강X,전공이해기초교과,MJU자율수강제강좌,온라인강의</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0762\">6</span> / 수강인원 : <span id=\"listencnt-0762\">7</span> / 제한인원 : <span id=\"takecnt-0762\">50</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0762\"> 신청완료 </span> <button id=\"req-0762\" class=\"classbtn reqbtn sorthide\" aria-label=\"0762 산업경영공학개론 수강신청하기\" data-coursecls=\"0762\" data-curinum=\"JEO01213\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0762\" data-curinum=\"JEO01213\" aria-label=\"0762 산업경영공학개론 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> <div class=\"coursedata filter\" role=\"listitem\" id=\"coursedataid-0238\" data-addtime=\"814102933\" data-coursecls=\"0238\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0238\"> [ 인원초과 ] </span> <span>0238 MJ사회봉사</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 기사167 / 학점 : 3 / 담당교수 : 유우연 </span> </div> <div class=\"course-schedule\"> <span> <span>월17:00~19:50(Y501)</span> </span> </div> <div class=\"course-remark\"> 비고 : <b>N/P과목</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0238\">56</span> / 수강인원 : <span id=\"listencnt-0238\">17</span> / 제한인원 : <span id=\"takecnt-0238\">17</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0238\"> 신청완료 </span> <button id=\"req-0238\" class=\"classbtn reqbtn sorthide\" aria-label=\"0238 MJ사회봉사 수강신청하기\" data-coursecls=\"0238\" data-curinum=\"KMD02167\" style=\"display: block;\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0238\" data-curinum=\"KMD02167\" aria-label=\"0238 MJ사회봉사 미리담기 삭제하기\" style=\"display: block;\"> 미리담기 삭제 </button> <span class=\"sortshow\" style=\"display: none;\">≡</span> </div> </div> </div> 기술적 고려사항 CSRF 토큰 및 세션: 사용자의 세션이 만료되거나 로그아웃되면 백그라운드 fetch 요청이 실패할 수 있다. 이 경우, 알람을 중지하고 사용자에게 재로그인 및 페이지 새로고침이 필요함을 알리는 것을 고려할 수 있다 (고급 기능). 요청 주기: 서버에 과도한 부하를 주지 않기 위해 fetch 요청 간격을 너무 짧지 않게 설정해야 한다 (예: 3~10초). 웹사이트 구조 변경: 명지대학교 수강신청 사이트의 HTML 구조(클래스 이름, ID 등)가 변경되면 확장 프로그램이 오작동할 수 있다. 이는 유지보수 시 대응해야 할 부분이다. 오류 처리: fetch 실패, 네트워크 오류 등에 대한 예외 처리를 구현하여 안정성을 높여야 한다.",
      "frontmatter": {
        "tags": [
          "project"
        ],
        "date": "2025-08-14T11:58:36+09:00",
        "lastmod": "2025-08-19T01:08:32+09:00"
      }
    },
    "브라우저 기반 수강신청 알림": {
      "path": "/02.inbox/브라우저-기반-수강신청-알림/",
      "filename": "브라우저 기반 수강신청 알림",
      "content": "이 사이트를 기준으로 알림 확장 프로그램을 만들꺼야 만약 현재 사이트 url 이 https://class.mju.ac.kr/main/bag 일때만 작동 사이트에서 [인원초과] 가 들어가 있는 강의에는 수강신청, 미리담기 삭제 옆에 알림 대기 버튼을 삽입 정해진 시간마다 백그라운드로 뒤에서 작동 fetch(\"https://class.mju.ac.kr/main/bag\", { \"headers\": { \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,/;q=0.8,application/signed-exchange;v=b3;q=0.7\", \"accept-language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\", \"cache-control\": \"no-cache\", \"content-type\": \"application/x-www-form-urlencoded\", \"pragma\": \"no-cache\", \"sec-ch-ua\": \"\"Not;A=Brand\";v=\"99\", \"Google Chrome\";v=\"139\", \"Chromium\";v=\"139\"\", \"sec-ch-ua-mobile\": \"?0\", \"sec-ch-ua-platform\": \"\"Windows\"\", \"sec-fetch-dest\": \"document\", \"sec-fetch-mode\": \"navigate\", \"sec-fetch-site\": \"same-origin\", \"sec-fetch-user\": \"?1\", \"upgrade-insecure-requests\": \"1\" }, \"referrer\": \"https://class.mju.ac.kr/main/class\", \"body\": \"_csrf=8e4b9cbe-fc28-4a14-82f1-3e34e72aba4b\", \"method\": \"POST\", \"mode\": \"cors\", \"credentials\": \"include\" }); 요청해서 백단에서 해당 강의가 [인원초과] 문자가 사라지면 사용자에게 알림을 알리는 방식 백그라운드에서 일정 시간마다 요청할때는 초기에 요청했던 초반 html 에서 csrf 토큰을 가져와햐야해 백그라운드에서 일정시간마다 요청한다고 화면을 갱신하지는 마 해당 백그라운드의 의미는 [인원초과] 가 아닌지만 확인하면되 인원초과가 사라지면 기존에 사이트에서 해당강의가 [인원초과] 라고 판단했던 것은 강제 refresh 아래는 초기 https://class.mju.ac.kr/main/bag 사이트의 요청 결과야 <!DOCTYPE html> <html lang=\"ko\"> <head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, user-scalable=no\"> <meta id=\"_csrf\" name=\"_csrf\" content=\"60ce7efa-b9bf-49ee-b384-149961502d34\"/> <meta id=\"_csrf_header\" name=\"_csrf_header\" content=\"X-CSRF-TOKEN\"/> <!-- mobile 주소표시줄 색깔 (다크테마는 적용되지 않음) --> <meta name=\"theme-color\" content=\"#003f9b\"> <meta name=\"apple-mobile-web-app-status-bar-style\" content=\"#003f9b\"> <meta name=\"robots\" content=\"noindex,nofollow\"/> <title>명지대학교 수강신청시스템</title> <link rel=\"stylesheet\" href=\"/css/style.css\"/> <script src=\"/js/jquery-3.6.3.min.js\"></script> <script> $(document).ready(function(){ $(\".reqbtn\").click(function(){ const curiNum = $(this).data(\"curinum\"); const courseCls = $(this).data(\"coursecls\"); saveLecture(true, $(this)); }); $(\".delbtn\").click(function(){ const curiNum = $(this).data(\"curinum\"); const courseCls = $(this).data(\"coursecls\"); removeBeforeConfirm(false, $(this)); }); isBagSorting(false); //미리담기 내역이 1개 이하로 있을 경우 버튼이 있는 div영역을 감춘다. if($(\"div.coursedata\").length <= 1){ $(\"div.sortbtn\").hide(); } }); </script> </head> <body> <!-- header --> <header> <script> /* 개발자도구 방지 로직 */ /* !function() { function detectDevTool(allow) { if(isNaN(+allow)) allow = 100; var start = +new Date(); debugger; var end = +new Date(); if(isNaN(start) || isNaN(end) || end - start > allow) { alert(MSG_ALERT_DEVTOOL); location.href = \"/logout\"; } } if(window.attachEvent) { if (document.readyState === \"complete\" || document.readyState === \"interactive\") { detectDevTool(); window.attachEvent('onresize', detectDevTool); window.attachEvent('onmousemove', detectDevTool); window.attachEvent('onfocus', detectDevTool); window.attachEvent('onblur', detectDevTool); } else { setTimeout(argument.callee, 0); } } else { window.addEventListener('load', detectDevTool); window.addEventListener('resize', detectDevTool); window.addEventListener('mousemove', detectDevTool); window.addEventListener('focus', detectDevTool); window.addEventListener('blur', detectDevTool); } }(); */ </script> <script src=\"/js/jquery-3.6.3.min.js\"></script> <script src=\"/js/netfunnel.js\"></script> <script src=\"/js/languagecookie.js\"></script> <script src=\"/js/customjs.js\"></script> <script src=\"/js/Sortable.min.js\"></script> <script src=\"/js/jquery-sortable.js\"></script> <script src=\"/js/mbuster_meta.js\"></script> <script src=\"/js/mbuster_api.js\"></script> <script src=\"/js/msg_ko.js\"></script> <script> const CSRFHEADER = document.querySelector('meta[name=\"_csrf_header\"]').content; const CSRFTOKEN = document.querySelector('meta[name=\"_csrf\"]').content; const isSeason = JSON.parse('false'.toLowerCase()); const userIpAddress = '222.233.240.27'; const MAXSESSIONTIME = 1800; var timeoutSec = MAXSESSIONTIME; $(document).ready(function(){ //mbuster /* MBUSTER_API({ clientIp: userIpAddress, user_login_id: \"60222100\" }); */ let currentPagecode = \"bag\"; $(`nav.nav-gnb li[data-pagecode='${currentPagecode}']`).removeClass(\"gnb-link\").removeAttr(\"aria-label\"); $(`nav.nav-gnb li.gnb-link`).click(function(){ let pagecode = $(this).data(\"pagecode\"); moveClassPage(pagecode); }); setLanguageCookie(); for(const element of document.getElementsByClassName(\"entersearch\")) { element.addEventListener(\"keypress\", function(event){ if(event.key === 'Enter'){ validateInput(false); searchCourse(); } }); } setInterval(updateTimer, 1000); }); $(document).mouseup(function(e){ //알람 구역(알림버튼 + 알림 내용 팝업) 외의 부분을 클릭했을 때 if($(e.target).parents(\"div#optbtn\").length == 0){ $(\"#dropdownCheck\").prop(\"checked\", false); } }); </script> <form method=\"post\" name=\"mainfrm\" action=\"/\"><input type=\"hidden\" name=\"_csrf\" value=\"60ce7efa-b9bf-49ee-b384-149961502d34\"/> </form> <form method=\"post\" name=\"reqfrm\" action=\"/\" target=\"_self\"><input type=\"hidden\" name=\"_csrf\" value=\"60ce7efa-b9bf-49ee-b384-149961502d34\"/> <input type=\"hidden\" name=\"curiNum\"/> <input type=\"hidden\" name=\"courseCls\"/> <input type=\"hidden\" name=\"pageFrom\"/> </form> <!-- <a id=\"page-top\"></a> --> <div id=\"sessiontimelayer\" role=\"none\"> <div> <span id=\"remaintimetext\">로그인 남은 시간</span> <span id=\"remaintime\"></span> </div> <div> <button onclick=\"javascript:resetTimer(false);\" tabindex=\"-1\">연장</button> </div> </div> <div id=\"optbtnlayer\"> <div id=\"logoarea\" role=\"none\"> <img src=\"/images/classlogo.png\" width=\"180\" alt=\"명지대학교 수강신청시스템\"/> </div> <div id=\"optbtn\" role=\"menu\"> <input id=\"dropdownCheck\" type=\"checkbox\"/> <label for=\"dropdownCheck\"> <span id=\"username\">신년기</span> <img id=\"maruicon\" alt=\"opt\" src=\"/images/icon/icon_maru.png\"/> <span class=\"caret\"></span> </label> <ul id=\"profile\"> <li class=\"stdinfo\"> <span>60222100</span> <span id=\"stdname\">신년기</span> </li> <li class=\"stdinfo\"> <span>현재학년</span> : <span>4</span> </li> <li class=\"stdinfo\"> <span>수강신청학년</span> : <span>4</span> </li> <li class=\"divider\"></li> <li role=\"menuitem\"> <a href=\"/main?lang=en\" class=\"profile_language\"> Change Language : English </a> </li> <li role=\"menuitem\"> <a href=\"/download/manual_ko.pdf\" class=\"profile_manual\" target=\"_blank\" download> 수강신청 사용설명서 </a> </li> <li role=\"menuitem\"> <a href=\"/logout\" class=\"profile_logout\"> 로그아웃 </a> </li> </ul> </div> </div> <div id=\"statuslayer\"> <div id=\"statusshow\" class=\"mobileblock\" onclick=\"javascript:showCdtStatus(true);\"> 수강신청 대상단계 및 신청학점 보이기 </div> <table id=\"cdtstatus\" class=\"defaultTable pcblock\" onclick=\"javascript:showCdtStatus(false);\" aria-describedby=\"수강신청에서 보여지는 단계는 신청이 가능한 교과목을 의미하며 MSI와는 다릅니다.\"> <thead> <tr> <th class=\"tooltip topoutline\" colspan=\"3\"> 수강신청 대상단계 <span class=\"qmark\">(?)</span> <span class=\"tooltiptext\" role=\"tooltip\"> 수강신청에서 보여지는 단계는 신청이 가능한 교과목을 의미하며 MSI와는 다릅니다. </span> </th> </tr> <tr> <th class=\"topdetail\"> 영어 </th> <th class=\"topdetail\"> 영어회화 </th> <th class=\"topdetail\"> 미적분학 </th> </tr> </thead> <tbody> <tr> <td>영어3 (R4)</td> <td>영어회화2 (L3)</td> <td>미적분학2 (A3)</td> </tr> </tbody> <thead> <tr> <th class=\"topdetail\"> 수강가능학점 </th> <th class=\"topdetail\"> 총 수강신청 강좌 수 </th> <th class=\"topdetail\"> 총 수강신청 학점 수 </th> </tr> </thead> <tbody> <tr> <td>18</td> <td id=\"stat-sugcnt\">6</td> <td id=\"stat-sugcdt\">15</td> </tr> </tbody> </table> </div> <nav id=\"class-gnb\" class=\"nav-gnb\"> <ul> <li data-pagecode=\"bag\" class=\"gnb-link\" aria-label=\"미리담기내역\"> 미리담기내역 </li> <li data-pagecode=\"class\" class=\"gnb-link\" aria-label=\"수강신청내역\"> 수강신청내역 </li> <li data-pagecode=\"search\" class=\"gnb-link\" aria-label=\"교과목 검색\"> 교과목 검색 </li> </ul> </nav> </header> <section id=\"container\"> <div id=\"searchlayer\"> <div id=\"class-mine\"> <div id=\"curiblock\" class=\"categorybox\"> <div class=\"categorytop\"> <div class=\"smallinfo sortshow\"> ※ 미리담기한 교과목을 끌어서 순서를 변경할 수 있습니다. </div> <div class=\"sortbtn\"> <button class=\"orderby sorthide\" id=\"sortable\" onclick=\"javascript:doSortable();\">미리담기 정렬 변경</button> <button class=\"orderby sortshow\" id=\"sortconfirm\" onclick=\"javascript:saveSort();\">정렬 저장</button> </div> </div> <div id=\"bagresult\" class=\"courselist\" data-req=\"Y\" data-bag=\"N\" data-reqdel=\"N\" data-bagdel=\"Y\" role=\"list\"> <div class=\"coursedata\" role=\"listitem\" id=\"coursedataid-0419\" data-addtime=\"808151054\" data-coursecls=\"0419\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0419\"> [ 인원초과 ] </span> <span>0419 빅데이터인포그래픽(KCU)</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 균컴114 / 학점 : 3 / 담당교수 : 이규연 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>온라인(원격수업)</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0419\">457</span> / 수강인원 : <span id=\"listencnt-0419\">108</span> / 제한인원 : <span id=\"takecnt-0419\">108</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0419\"> 신청완료 </span> <button id=\"req-0419\" class=\"classbtn reqbtn sorthide\" aria-label=\"0419 빅데이터인포그래픽(KCU) 수강신청하기\" data-coursecls=\"0419\" data-curinum=\"KMQ01114\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0419\" data-curinum=\"KMQ01114\" aria-label=\"0419 빅데이터인포그래픽(KCU) 미리담기 삭제하기\"> 미리담기 삭제 </button> <span class=\"sortshow\">&equiv;</span> </div> </div> <div class=\"coursedata\" role=\"listitem\" id=\"coursedataid-5469\" data-addtime=\"808164217\" data-coursecls=\"5469\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-5469\"> [ 인원초과 ] </span> <span>5469 인체안의전쟁(KCU)</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 균여133 / 학점 : 3 / 담당교수 : 미배정 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>온라인(원격수업),KCU</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-5469\">433</span> / 수강인원 : <span id=\"listencnt-5469\">176</span> / 제한인원 : <span id=\"takecnt-5469\">176</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied5469\"> 신청완료 </span> <button id=\"req-5469\" class=\"classbtn reqbtn sorthide\" aria-label=\"5469 인체안의전쟁(KCU) 수강신청하기\" data-coursecls=\"5469\" data-curinum=\"KMO02133\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"5469\" data-curinum=\"KMO02133\" aria-label=\"5469 인체안의전쟁(KCU) 미리담기 삭제하기\"> 미리담기 삭제 </button> <span class=\"sortshow\">&equiv;</span> </div> </div> <div class=\"coursedata\" role=\"listitem\" id=\"coursedataid-0233\" data-addtime=\"814100108\" data-coursecls=\"0233\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0233\"> [ 인원초과 ] </span> <span>0233 문화속디자인여행(KCU)</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 기문217 / 학점 : 3 / 담당교수 : 이규연 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>온라인(원격수업)</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0233\">598</span> / 수강인원 : <span id=\"listencnt-0233\">108</span> / 제한인원 : <span id=\"takecnt-0233\">108</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0233\"> 신청완료 </span> <button id=\"req-0233\" class=\"classbtn reqbtn sorthide\" aria-label=\"0233 문화속디자인여행(KCU) 수강신청하기\" data-coursecls=\"0233\" data-curinum=\"KMC02217\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0233\" data-curinum=\"KMC02217\" aria-label=\"0233 문화속디자인여행(KCU) 미리담기 삭제하기\"> 미리담기 삭제 </button> <span class=\"sortshow\">&equiv;</span> </div> </div> <div class=\"coursedata\" role=\"listitem\" id=\"coursedataid-5431\" data-addtime=\"814100118\" data-coursecls=\"5431\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-5431\"> [ 인원초과 ] </span> <span>5431 영화제작에대해알고싶은모든것(KCU)</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 균문122 / 학점 : 3 / 담당교수 : 미배정 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>온라인(원격수업),KCU</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-5431\">516</span> / 수강인원 : <span id=\"listencnt-5431\">176</span> / 제한인원 : <span id=\"takecnt-5431\">176</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied5431\"> 신청완료 </span> <button id=\"req-5431\" class=\"classbtn reqbtn sorthide\" aria-label=\"5431 영화제작에대해알고싶은모든것(KCU) 수강신청하기\" data-coursecls=\"5431\" data-curinum=\"KML02122\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"5431\" data-curinum=\"KML02122\" aria-label=\"5431 영화제작에대해알고싶은모든것(KCU) 미리담기 삭제하기\"> 미리담기 삭제 </button> <span class=\"sortshow\">&equiv;</span> </div> </div> <div class=\"coursedata\" role=\"listitem\" id=\"coursedataid-0084\" data-addtime=\"814104427\" data-coursecls=\"0084\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0084\"> [ 인원초과 ] </span> <span>0084 영어회화2</span> <span> (<span>L3</span>) </span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 교필109 / 학점 : 1 / 담당교수 : 데본 </span> </div> <div class=\"course-schedule\"> <span> <span>월12:00~12:50(Y9230), 수12:00~12:50(Y9230)</span> </span> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0084\">48</span> / 수강인원 : <span id=\"listencnt-0084\">11</span> / 제한인원 : <span id=\"takecnt-0084\">11</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0084\"> 신청완료 </span> <button id=\"req-0084\" class=\"classbtn reqbtn sorthide\" aria-label=\"0084 영어회화2 수강신청하기\" data-coursecls=\"0084\" data-curinum=\"KMA02109\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0084\" data-curinum=\"KMA02109\" aria-label=\"0084 영어회화2 미리담기 삭제하기\"> 미리담기 삭제 </button> <span class=\"sortshow\">&equiv;</span> </div> </div> <div class=\"coursedata\" role=\"listitem\" id=\"coursedataid-0710\" data-addtime=\"814104302\" data-coursecls=\"0710\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0710\"> [ 인원초과 ] </span> <span>0710 알고리즘</span> </div> <div class=\"course-basicinfo\"> <span> 3학년 / 컴공316 / 학점 : 3 / 담당교수 : 조민경 </span> </div> <div class=\"course-schedule\"> <span> <span>월09:00~10:50(Y5437), 목09:00~09:50(Y5437)</span> </span> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0710\">53</span> / 수강인원 : <span id=\"listencnt-0710\">30</span> / 제한인원 : <span id=\"takecnt-0710\">30</span> </div> <div class=\"course-past\" data-grade=\"D+\"> ※ 과거 이수성적 : 2023-2학기 : 알고리즘 D+ </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0710\"> 신청완료 </span> <button id=\"req-0710\" class=\"classbtn reqbtn sorthide\" aria-label=\"0710 알고리즘 수강신청하기\" data-coursecls=\"0710\" data-curinum=\"JEJ02316\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0710\" data-curinum=\"JEJ02316\" aria-label=\"0710 알고리즘 미리담기 삭제하기\"> 미리담기 삭제 </button> <span class=\"sortshow\">&equiv;</span> </div> </div> <div class=\"coursedata\" role=\"listitem\" id=\"coursedataid-0762\" data-addtime=\"814100722\" data-coursecls=\"0762\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span style=\"display:none\" class=\"fullclass\" id=\"fullsign-0762\"> [ 인원초과 ] </span> <span>0762 산업경영공학개론</span> </div> <div class=\"course-basicinfo\"> <span> 1학년 / 산업213 / 학점 : 3 / 담당교수 : 한민탁 </span> </div> <div class=\"course-schedule\"> <span>시간 미지정</span> </div> <div class=\"course-remark\"> 비고 : <b>아너칼리지및타전공수강,전공자수강X,전공이해기초교과,MJU자율수강제강좌,온라인강의</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0762\">6</span> / 수강인원 : <span id=\"listencnt-0762\">7</span> / 제한인원 : <span id=\"takecnt-0762\">50</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0762\"> 신청완료 </span> <button id=\"req-0762\" class=\"classbtn reqbtn sorthide\" aria-label=\"0762 산업경영공학개론 수강신청하기\" data-coursecls=\"0762\" data-curinum=\"JEO01213\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0762\" data-curinum=\"JEO01213\" aria-label=\"0762 산업경영공학개론 미리담기 삭제하기\"> 미리담기 삭제 </button> <span class=\"sortshow\">&equiv;</span> </div> </div> <div class=\"coursedata\" role=\"listitem\" id=\"coursedataid-0238\" data-addtime=\"814102933\" data-coursecls=\"0238\"> <div class=\"infoarea\"> <div class=\"course-title\" style=\"font-weight: bold;\"> <span class=\"fullclass\" id=\"fullsign-0238\"> [ 인원초과 ] </span> <span>0238 MJ사회봉사</span> </div> <div class=\"course-basicinfo\"> <span> 전학년 / 기사167 / 학점 : 3 / 담당교수 : 유우연 </span> </div> <div class=\"course-schedule\"> <span> <span>월17:00~19:50(Y501)</span> </span> </div> <div class=\"course-remark\"> 비고 : <b>N/P과목</b> </div> <div class=\"course-count\"> 담은인원 : <span id=\"bagcnt-0238\">56</span> / 수강인원 : <span id=\"listencnt-0238\">17</span> / 제한인원 : <span id=\"takecnt-0238\">17</span> </div> </div> <div class=\"btnarea\"> <span class=\"applied\" style=\"display:none\" id=\"applied0238\"> 신청완료 </span> <button id=\"req-0238\" class=\"classbtn reqbtn sorthide\" aria-label=\"0238 MJ사회봉사 수강신청하기\" data-coursecls=\"0238\" data-curinum=\"KMD02167\"> 수강신청 </button> <button class=\"classbtn delbtn sorthide\" data-coursecls=\"0238\" data-curinum=\"KMD02167\" aria-label=\"0238 MJ사회봉사 미리담기 삭제하기\"> 미리담기 삭제 </button> <span class=\"sortshow\">&equiv;</span> </div> </div> </div> </div> </div> </div> </section> </body> </html>",
      "frontmatter": {
        "tags": [
          "project"
        ],
        "date": "2025-08-14T11:43:15+09:00",
        "lastmod": "2025-08-14T11:58:34+09:00"
      }
    },
    "빌드(컴파일) 과정": {
      "path": "/02.inbox/빌드컴파일-과정/",
      "filename": "빌드(컴파일) 과정",
      "content": "과정별 순서 요약 %20image%2020240104121023.png) 전처리기 gcc -E program.c -o program.i 옵션 : -E Preprocess only; do not compile, assemble or link. 헤더 파일 삽입 : \\#include 지시문을 만나면 해당하는 헤더 파일을 찾아 헤더 파일에 있는 모든 내용을 복사해서 소스 코드에 삽입한다. 즉, 헤더 파일은 컴파일에 사용되지 않고 소스 코드 파일 내에 전부 복사된다. 헤더 파일에 선언된 함수 원형은 후에 링킹 과정을 통해 실제로 함수가 정의되어 있는 오브젝트 파일(컴파일된 소스 코드 파일)과 결합한다. 매크로 치환 및 적용 : \\#define 지시문에 정의된 매크로를 저장하고 같은 문자열을 만나면 \\#define 된 내용으로 치환한다. 간단하게 말해 매크로 이름을 찾아서 정의한 값으로 전부 바꿔준다. 컴파일러 기능 지정 : \\#pragma once 한번만 컴파일 \\#pragma warning(disable:4996) scanf 경고 끄기 컴파일 gcc -S hello.c gcc -S hello.i -S Compile only; do not assemble or link. ## 어셈블리 gcc -c program.s -o program.o` 링킹 링킹(Linking) 과정은 링커(Linker)를 통해 오브젝트 파일(*.o)들을 묶어 실행 파일로 만드는 과정이다. 이 과정에서 오브젝트 파일들과 프로그램에서 사용하는 라이브러리 파일들을 링크하여 하나의 실행 파일을 만든다. 이때 라이브러리를 링크하는 방법에 따라 정적 링킹(Static Linking)과 동적 링킹(Dynamic Linking)으로 나눌 수 있다. 링킹 방식의 차이는 앞서 설명했던 라이브러리 포스트를 참고하자. 서론 개발하다 보면 라이브러리를 사용할 일이 많다. 라이브러리를 사용해보면 정확한 개념은 몰라도 프로그램을 개발할 때 필요한 기능을 가져다 쓰는 도구라는 것은 어렴풋이 이해할 수 있다 링커의 역할 링커의 역할은 크게 심볼 해석과 재배치로 나눌 수 있다. 심볼 해석(Symbol Resolution) 심볼 해석은 각 오브젝트 파일에 있는 심볼 참조를 어떤 심볼 정의에 연관시킬지 결정하는 과정이다. 여러 개의 오브젝트 파일에 같은 이름의 함수 또는 변수가 정의되어 있을 때 어떤 파일의 어떤 함수를 사용할지 결정한다. 재배치(Relocation) 재배치는 오브젝트 파일에 있는 데이터의 주소나 코드의 메모리 참조 주소를 알맞게 배치하는 과정이다. 링커가 컴파일러가 생성한 오브젝트 파일을 모아서 하나의 실행 파일을 만들 때, 각 오브젝트 파일에 있는 데이터의 주소나 코드의 메모리 참조 주소가 링커에 의해 합쳐진 실행 파일에서의 주소와 다르게 때문에 그것을 알맞게 수정해줘야 한다. 이를 위해 오브젝트 파일 안에 재배치 정보 섹션(Relocation Information Section)이 존재한다. 링킹 과정에서 같은 세션끼리 합쳐진 후 재배치가 일어난다. 위 그림을 통해 알 수 있듯이 오브젝트 파일 형식은 링킹 과정에서 링커가 여러 개의 오브젝트 파일들을 하나의 실행 파일로 묶을 때 필요한 정보를 효율적으로 파악할 수 있는 구조이다. 링킹을 하기 전 오브젝트 파일을 재배치 가능한 오브젝트 파일(Relocatable Object File)이라 부르고 링킹을 통해 만들어지는 오브젝트 파일을 실행 가능한 오브젝트 파일(Executable Object File)이라 부른다. c++ -> c 변환 번역기 컴파일러(?)를 cfront 라 한다 GNU g++ gcc 과 같은 컴파일러를 프리웨어라 한다 어떤라이ㅡㅂ러리들은 그것을 사용하려면 컴파일 명령행에 명시적으로 요구된 명령을 입력해야한다 ( CC test.c -lm) (-lm 수학라이브러리 링크 명령) (g++ test.cpp. -lg++) 윈도우 커맨드라인 컴파일러는 Cygwin 과 MinGW 이다",
      "frontmatter": {
        "aliases": [
          "compile"
        ],
        "tags": [
          "c",
          "linux",
          "cs"
        ],
        "date": "2023-12-31T21:00:00+09:00",
        "lastmod": "2025-06-05T10:26:11+09:00"
      }
    },
    "서버 db 사용하기": {
      "path": "/02.inbox/서버-db-사용하기/",
      "filename": "서버 db 사용하기",
      "content": "미리 data 를 담아놓은 db 입니다 환경구성이 잘 안되시는 분은 여기서 이것을 사용하시면 됩니다 학기 끝날 때 까지 서버를 열어 두겠습니다 %20image%2020241113211582.png) Pasted image 20241113211588 비밀번호는 1111 단 data 뷰어 용으로만 사용해 주세요(create문 truncate 등등 data 를 손보는 작업은 하지 말아 주세요)",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "description": "database design 서버 구성용",
        "date": "2024-11-14T13:13:00+09:00",
        "lastmod": "2024-11-14T13:13:00+09:00"
      }
    },
    "선형대수학 worksheet11-6번": {
      "path": "/02.inbox/선형대수학-worksheet11-6번/",
      "filename": "선형대수학 worksheet11-6번",
      "content": "$R2 의\\ 기저는 \\{ u1 = (1,-3), u2 = (2,2)\\}$ $이를\\ 통해\\ 직교기저 \\{v1, v2\\} 를\\ 구하여라\\ \\&\\ 정규직교기저 \\{q1, q_2\\} 를 구하라$ $문제\\ 1\\ 그램\\ 슈미트\\ 직교화\\ 과정을\\ 통해\\ 직교기저가\\ 아닌\\ 기저에서\\ 직교기저를\\ 구한다$ $u1 = v1 = (1,-3)$ $u_2 = (2,2)$ $v2 = u2 - \\frac{u2 \\cdot v1}{ v1 ^2} v1$ $v_2 = (2,2) + \\frac{4}{10}(1,-3)$ $v_2 = (\\frac{12}{5}, \\frac{4}{5})$ $즉\\ 직교기저\\ \\{v1,v2\\} = \\{(1,-3),(\\frac{12}{5},\\frac{4}{5})\\}$ $문제\\ 2\\ 직교\\ 기저에\\ 정규화를\\ 진행한다$ $각각의\\ 원소의\\ 크기가\\ 1이어야\\ 한다$ $(q1,q2) = \\{(k1,-3k1),(\\frac{12}{5}k2,\\frac{4}{5}k2)\\}$ $ q1 = \\sqrt{{k1}^2 + 9k_1^2} = 1$ $ q2 = \\sqrt{\\frac{144}{25}{{k2}^2 + \\frac{16}{25}k_2^2}} = 1$ $k_1 = \\frac{1}{\\sqrt{10}}$ $k_2 = \\frac{5}{4\\sqrt{10}}$ $즉\\ 정규직교기저\\ \\{q1,q2\\}= \\{(\\frac{1}{\\sqrt{10}},\\frac{-3}{\\sqrt{10}}),(\\frac{3}{\\sqrt{10}},\\frac{1}{\\sqrt{10}})\\}\\ 이다$ 풀이2 실수 벡터 2개를 기저로 하는 벡터 공간은 이차원 벡터 공간이며 {(1,0), (0,1)}이 좌표평면이라고 불리우는 2차원 벡터공간 표준 기저이다 표준 직교기저 조건은 정규 직교기저를 포함하고 정규 정규직교는 직교기저 조건을 포함하므로 직교기저 = {(1,0), (0,1)} 정규직교기저 = {(1,0), (0,1)}",
      "frontmatter": {
        "tags": [
          "math"
        ],
        "date": "2024-06-03T19:20:00+09:00",
        "lastmod": "2024-06-03T19:20:00+09:00"
      }
    },
    "순수 UI 디자인 용어": {
      "path": "/02.inbox/순수-ui-디자인-용어/",
      "filename": "순수 UI 디자인 용어",
      "content": "🖌️ 순수 디자인 관련 용어 30가지 (Visual/UI Design 중심) 스케어모피즘 (Skeuomorphism) 실제 물체의 질감, 그림자, 입체감을 디지털에 그대로 구현한 디자인 스타일. 플랫 디자인 (Flat Design) 그림자, 텍스처, 그라데이션을 배제하고 단순한 색상과 형태로 구성된 디자인. 세미 플랫 디자인 (Semi-Flat Design) 플랫 디자인에 약간의 그림자나 입체감을 더해 깊이감을 준 디자인. 머티리얼 디자인 (Material Design) 구글의 디자인 언어. “종이처럼 쌓인 레이어” 컨셉으로 그림자와 모션을 활용. 네오모피즘 (Neumorphism) 소프트한 입체감과 음영을 사용해 “눌린 버튼” 또는 “튀어나온 요소”처럼 보이게 하는 디자인 트렌드. 글라스모피즘 (Glassmorphism) 유리처럼 반투명하고 흐릿한 배경(blur) + 경계선 + 레이어 효과를 사용하는 디자인 스타일 (예: macOS Big Sur, Windows 11). 컬러 팔레트 (Color Palette) 디자인에서 사용할 주조색, 보조색, 강조색 등을 체계적으로 정리한 색상 집합. 타이포그래피 (Typography) 글자의 스타일, 크기, 간격, 정렬 등을 조화롭게 배치하여 가독성과 미학을 높이는 기술. 폰트 페어링 (Font Pairing) 서로 다른 두 가지 이상의 폰트를 조합해 시각적 계층과 조화를 만드는 기법. 비주얼 계층 (Visual Hierarchy) 색상, 크기, 위치, 굵기 등을 통해 어떤 요소가 더 중요한지 시각적으로 표현. 화이트 스페이스 (White Space / Negative Space) 콘텐츠 사이의 “빈 공간”. 숨 쉴 공간을 주어 가독성과 고급스러움을 높임. 그리드 시스템 (Grid System) 요소들을 정렬하고 일관성 있게 배치하기 위한 수직/수평 레이아웃 구조. 정렬 (Alignment) 요소들이 축을 기준으로 정돈되어 배치된 상태. 시각적 안정감을 줌. 근접성 (Proximity) 관련된 요소끼리 가깝게 배치해 그룹으로 인식되도록 유도하는 원칙. 대비 (Contrast) 색상, 크기, 형태 등의 차이를 통해 특정 요소를 강조하거나 구분하는 기법. 반복 (Repetition) 디자인 내에서 같은 색상, 폰트, 아이콘 스타일 등을 반복해 일관성과 통일감을 줌. 균형 (Balance) 시각적 무게가 좌우 또는 상하로 고르게 분포된 상태. 대칭 또는 비대칭 균형이 있음. 아이콘 (Icon) 기능이나 의미를 직관적으로 전달하기 위한 상징적 그래픽 요소. 라인아이콘 vs 솔리드아이콘 (Line Icon vs Solid Icon) 테두리로만 구성된 아이콘 vs 면으로 채워진 아이콘. 스타일에 따라 선택. 일러스트레이션 (Illustration) 캐릭터, 장면, 추상적 그래픽 등을 통해 감성이나 브랜드를 표현하는 삽화. 패턴 (Pattern) 반복되는 그래픽 요소 (예: 배경용 기하학적 패턴, 텍스처). 텍스처 (Texture) 표면의 질감을 표현 (예: 나무결, 천, 금속). 스케어모피즘에서 자주 사용됨. 그라데이션 (Gradient) 색상이 부드럽게 전환되는 효과. 배경이나 버튼에 감성과 깊이를 더함. 섀도우 (Drop Shadow) 요소에 입체감을 주기 위해 추가하는 그림자. 레이어링 느낌을 줌. 브랜딩 디자인 (Branding Design) 로고, 컬러, 폰트, 아이콘 등을 통해 브랜드의 시각적 정체성을 구축. 스타일 가이드 (Style Guide) 디자인 요소(색상, 폰트, 버튼, 아이콘 등)의 사용 규칙을 문서화한 것. 디자인 토큰 (Design Token) 색상, 여백, 라운딩, 그림자 등 디자인 속성값을 코드와 공유할 수 있게 추상화한 변수. 컴포넌트 (Component) 재사용 가능한 UI 요소 (버튼, 카드, 헤더 등). 디자인 시스템의 기본 단위. 비주얼 아이덴티티 (Visual Identity) 브랜드를 시각적으로 표현하는 모든 요소 — 로고, 컬러, 이미지 스타일, 타이포그래피 등. 모션 디자인 (Motion Design) 트랜지션, 로딩 애니메이션, 마이크로 인터랙션 등 시각적 움직임을 디자인하는 것. (※ 엄밀히는 인터랙션과 연결되지만, 순수 “시각적 애니메이션 디자인”으로 한정해 포함)* 💡 디자이너를 위한 팁 “Less is More” — 불필요한 장식은 과감히 제거하세요. 일관성 > 창의성 — 사용자는 예측 가능한 디자인을 좋아합니다. 컬러는 감정을, 타이포는 신뢰를, 여백은 고급스러움을 전달합니다. 이 리스트는 포트폴리오 준비, 디자인 리뷰, 협업 커뮤니케이션 등에 유용하게 쓰실 수 있어요. 더 깊이 알고 싶은 용어나 예시가 있으면 언제든지 물어보세요! 😊✨",
      "frontmatter": {
        "tags": [
          "잡지식"
        ],
        "date": "2025-09-08T01:49:22+09:00",
        "lastmod": "2025-09-08T01:49:58+09:00"
      }
    },
    "실수 표현방법 고정소수점과 부동소수점": {
      "path": "/02.inbox/실수-표현방법-고정소수점과-부동소수점/",
      "filename": "실수 표현방법 고정소수점과 부동소수점",
      "content": "소수의 10진수 2진수 계산 방법 Pasted image 20250408173559 예시 1: 10진수 1.625 → 2진수 1.101 정수 부분(1): 1 ÷ 2 = 0 ... 나머지 1 따라서 정수 부분은 1입니다. 소수 부분(0.625): 625 × 2 = 1.25 → 정수 부분 1, 소수 부분 0.25 25 × 2 = 0.5 → 정수 부분 0, 소수 부분 0.5 5 × 2 = 1.0 → 정수 부분 1, 소수 부분 0 (계산 종료) 따라서 소수 부분은 101입니다. 결과: 정수 부분(1) + 소수 부분(.101) = 1.101 예시 2: 10진수 3.375 → 2진수 11.011 정수 부분(3): 3 ÷ 2 = 1 ... 나머지 1 1 ÷ 2 = 0 ... 나머지 1 따라서 정수 부분은 11입니다. 소수 부분(0.375): 375 × 2 = 0.75 → 정수 부분 0, 소수 부분 0.75 75 × 2 = 1.5 → 정수 부분 1, 소수 부분 0.5 5 × 2 = 1.0 → 정수 부분 1, 소수 부분 0 (계산 종료) 따라서 소수 부분은 011입니다. 결과: 정수 부분(11) + 소수 부분(.011) = 11.011 예시 3: 10진수 0.875 → 2진수 0.111 정수 부분(0): 정수 부분이 없으므로 0입니다. 소수 부분(0.875): 875 × 2 = 1.75 → 정수 부분 1, 소수 부분 0.75 75 × 2 = 1.5 → 정수 부분 1, 소수 부분 0.5 5 × 2 = 1.0 → 정수 부분 1, 소수 부분 0 (계산 종료) 따라서 소수 부분은 111입니다. 결과: 정수 부분(0) + 소수 부분(.111) = 0.111 예시 4: 10진수 5.6 → 2진수 101.100110... (반복 소수) 정수 부분(5): 5 ÷ 2 = 2 ... 나머지 1 2 ÷ 2 = 1 ... 나머지 0 1 ÷ 2 = 0 ... 나머지 1 따라서 정수 부분은 101입니다. 소수 부분(0.6): 6 × 2 = 1.2 → 정수 부분 1, 소수 부분 0.2 2 × 2 = 0.4 → 정수 부분 0, 소수 부분 0.4 4 × 2 = 0.8 → 정수 부분 0, 소수 부분 0.8 8 × 2 = 1.6 → 정수 부분 1, 소수 부분 0.6 (반복 시작) 따라서 소수 부분은 100110... (0.6이 반복되므로 순환 소수). 결과: 정수 부분(101) + 소수 부분(.100110...) = 101.100110... 고정 소수점 방식 vs 부동 소수점 방식 두개의 논의 또한 실제 어떻게 컴퓨터에0,75 = 0.11(2진수) => 사람이 표현하는 방식 125 = 1100.001(2진수) => 사람이 표현하는 방식 실제 컴퓨터에 저장할 떄는 . 소수점을 넣을 수 없으므로 고정적으로 소수점 (.) 을 지정해서 저장하는 방식이 사용된다 예컨테 위의 0.75 를 8bit 자료형에서 앞자리 4은 정수 뒷자리 4 는 소수점 아래라고 정하면 실제 컴퓨터에 32 bit 가 저장될 때는 (00001100) 이렇게 저장된다 부호비트가 맨 앞의 bit 를 할당 3자리 정수 4자리 소수점 아래 소수 라고 전제하여도 동일한 bit 로 저장됨 즉 고정적으로 소수점을 정한다고 해서 고정 소수점 방식이라고 한다 하지만 위의 방식은 0.1 같은 수는 어림잡아 표현할 수 밖에 없다 3 => 0.01001100110011......(0011) 즉 위의 방식은 2가지 문제를 가지고 있다 정확한 수를 표현하지 못함 소수점 아래에 bit를 많이 배정하면 작은수 밖에 표현하지 못하고 정수부에 bit 를 많이 배정하면 작은 수밖에 표현하지 못한다 여기서 우리는 2번 째 문제를 해결해 보자 부동 소수점 방식 Pasted image 20250408173683 Pasted image 20240707220729 $N = (-1)^S 2^{E-127} 1.M$ 부호 1 비트 S 지수 8비트 : 범위(range)를 나타낸다 E 가수 23 비트 : 정밀도(precision)를 나타낸다 M 바이어스 127 : 정규화된 표현 : 가수 23 M 에서 1.M 형태의 정규화된 표현을 사용한다 이를 통해 더 큰 범위를 표현할 수 있도록 한다 정규화된 표현을 모든 지수부 모든 가수부에서 그대로 사용한다면 부동소수점방식에서 0을 표시할 수 없다 지수부를 2의 보수로 하지 않고 바이어스를 사용해서 음수를 표현한 이유 : 지수부가 0 이면 0.M 비정규표현을 사용할 때 지수부와 가수부가 모두 0일때 0이 계산된다 0임을 확인하는 하드웨어 복잡성 증대를 막기 위해 이렇게 사용한다 결론 (반정밀도 기준) $E = 31$ 이고 $M \\neq 0$ 이면 $N = \\text{NaN}$ $E = 31$ 이고 $M = 0$ 이면 $N = (-1)^S \\times \\infty$ $0 < E < 31$ 이면 $N = (-1)^S \\times 2^{E - 15} \\times (1.M)$ $E = 0$ 이고 $M \\neq 0$ 이면 $N = (-1)^S \\times 2^{-14} \\times (0.M)$ $E = 0$ 이고 $M = 0$ - $N = (-1)^S \\times 0$. 결론 (단정밀도 기준) 만약 $E = 255$ 이고 $M\\neq0$ 이면 $N =\\text{NaN}$ (Not a Number). 만약 $E = 255$ 이고 $M = 0$ 이면 $N=(-1)^S \\times \\infty$ 만약 $0<E<255$ 이면 $N=(-1)^S \\times 2^{E-127}\\times(1.M)$ 만약 $E=0$ 이고 $M \\neq 0$ 이면 $N=(-1)^S \\times 2^{-126} \\times (0.M)$ 만약 $E = 0$ 이고 $M=0$ 이면 $N=(-1)^s \\times 0$ 결론 (배정밀도 기준) $E = 2047$ 이고 $M \\neq 0$ 이면 $N = \\text{NaN}$ $E = 2047$ 이고 $M = 0$ 이면 $N = (-1)^S \\times \\infty$ $0 < E < 2047$ 이면 $N = (-1)^S \\times 2^{E - 1023} \\times (1.M)$ $E = 0$ 이고 $M \\neq 0$ 이면 $N = (-1)^S \\times 2^{-1022} \\times (0.M)$ $E = 0$ 이고 $M = 0$ 이면 $N = (-1)^S \\times 0$",
      "frontmatter": {
        "tags": [
          "잡지식",
          "cs"
        ],
        "date": "2024-07-07T21:55:00+09:00",
        "lastmod": "2024-07-07T21:55:00+09:00"
      }
    },
    "싱글톤 내부 의존관계로 프로토타입 스코프의 bean 을 가질때": {
      "path": "/02.inbox/싱글톤-내부-의존관계로-프로토타입-스코프의-bean-을-가질때/",
      "filename": "싱글톤 내부 의존관계로 프로토타입 스코프의 bean 을 가질때",
      "content": "@Scope(\"singleton\") static class ClientBean { @Autowired private ObjectProvider<PrototypeBean> prototypeBeanProvider; // 객체를 필요한 시점에 찾아서 주입받는 경우에 사용 // private final PrototypeBean prototypeBean; // @Autowired // public ClientBean(PrototypeBean prototypeBean){ // 생성시점에 주입 // this.prototypeBean = prototypeBean; // } public int logic(){ PrototypeBean prototypeBean = prototypeBeanProvider.getObject(); //ApplicationContext ac = //new AnnotationConfigApplicationContext(); //ac.getBean(PrototypeBean.class); } } } @Scope(\"prototype\") static class PrototypeBean { private int count = 0; public int addCount(){ return ++count; } public int getCount(){ return count; } } Prototype 타입의 객체는 지속적으로 객체를 새로 생성하고자 하는 목적으로 만들었으나 @Autowired 속성, ClientBean 의 singleton scope 속성으로 인해 bean 의존관계 단계에서 미리 생성되어 버림 그리고 다시 생성되지 않음 ObjectFatory 부모 ObjectProvider 자식 객체를 생성하는 시기를 조절 가능 DL dependency Lookup 스프링 의존적' @Test void SingletonClientUsePrototype(){ AnnotationConfigApplicationContext ac = new AnnotationConfigApplicationContext(ClientBean.class, PrototypeBean.class); ClientBean clientBean1 = ac.getBean(ClientBean.class); int count1 = clientBean1.logic(); ClientBean clientBean2 = ac.getBean(ClientBean.class); int count2 = clientBean2.logic(); assertThat(count2).isEqualTo(1); } @Scope(\"singleton\") static class ClientBean { // private final PrototypeBean prototypeBean; // @Autowired // 컨테이너를 새로 만들어서 prototypeBean을 주입받는 경우 // private ApplicationContext ac; // @Autowired // public ClientBean(PrototypeBean prototypeBean){ // 생성시점에 주입 // this.prototypeBean = prototypeBean; // } @Autowired private Provider<PrototypeBean> prototypeBeanProvider; // 객체를 필요한 시점에 찾아서 주입받는 경우에 사용 public int logic(){ PrototypeBean prototypeBean = prototypeBeanProvider.get(); prototypeBean.addCount(); return prototypeBean.getCount(); } @PostConstruct public void init(){ System.out.println(\"SingletonTest.SingletonBean.init() + \" + this); } @PreDestroy public void destroy(){ System.out.println(\"SingletonTest.SingletonBean.destroy() + \" + this); } } @Scope(\"prototype\") static class PrototypeBean { private int count = 0; public int addCount(){ return ++count; } public int getCount(){ return count; } @PostConstruct public void init(){ System.out.println(\"SingletonTest.PrototypeBean.init() + \" + this); } @PreDestroy public void destroy(){ System.out.println(\"SingletonTest.PrototypeBean.destory() + \" + this); } }",
      "frontmatter": {
        "tags": [
          "spring"
        ],
        "date": "2024-02-02T15:27:00+09:00",
        "lastmod": "2025-08-19T22:14:19+09:00"
      }
    },
    "싱글톤 패턴의 문제": {
      "path": "/02.inbox/싱글톤-패턴의-문제/",
      "filename": "싱글톤 패턴의 문제",
      "content": "클래스의 생성자를 통해 만들어지는 인스턴스의 개수를 1개로 고정시키는 디자인 패턴이다 하지만 이는 몇가지 문제를 만들어낸다 []( 싱글톤%20패턴의%20문제#코드양%20증가) 의존성 문제 OCP 원칙 위반 테스트 어려움 내부 속성 변경 어려움 자식 클래스 생성 어려움 코드 양 증가 싱글톤 패턴을 구현하려면 클래스 내에 싱글톤 인스턴스를 생성하고 관리하는 코드가 추가됩니다. 이로 인해 코드가 더 복잡해지고, 가독성이 떨어질 수 있습니다. public class SingletonImpl implements Singleton{ private static SingletonImpl instance; private SingletonImpl() { // private 생성자 } public static SingletonImpl getInstance() { if (instance == null) { instance = new SingletonImpl(); } return instance; } } 의존성 문제 클라이언트는 Singleton 클래스에 직접 의존하게 되므로, 의존성 역전 원칙(Dependency Inversion Principle, DIP)을 위반합니다. 즉, 클라이언트는 구체 클래스에 종속되어 유연성이 감소합니다. public class Client { private Singleton singleton = SingletonImpl.getInstance(); // 클라이언트가 SingletonImpl 클래스에 직접 의존 } OCP 원칙 위반 만약 다른 싱글톤 클래스로 교체하려면 클라이언트 코드를 직접 수정해야 합니다. 이는 개방-폐쇄 원칙(Open/Closed Principle, OCP)을 위반하는 것이며, 시스템 확장이 어려워집니다. public class Client { private Singleton singleton = SingletonImpl.getInstance(); // 다른 싱글톤 클래스로 교체하려면 클라이언트 코드 수정 필요 } 테스트 어려움 싱글톤은 전역 상태를 가지고 있어 테스트하기 어려울 수 있습니다. 특히 싱글톤 인스턴스가 다른 객체에 의존하는 경우, 모의 객체(Mock Object) 등을 사용하기 어려워집니다. 싱글톤 인스턴스가 다른 객체에 의존하는 경우, 이 의존성을 모의 객체로 대체하는 것이 어렵습니다. 싱글톤 인스턴스는 생성 시점에 의존성을 갖게 되므로, 테스트 시점에 이를 모의 객체로 대체하는 것이 불가능합니다. 이로 인해 의존성에 대한 제어가 어려워져, 테스트가 복잡해질 수 있습니다. public class SingletonImpl { private Dependency dependency; private SingletonImpl() { this.dependency = new Dependency(); // 의존성 주입이 어려움 } // ... } 내부 속성 변경 어려움 일반적으로 싱글톤은 내부 상태를 변경하거나 초기화하는 메서드를 제공하지 않습니다. 따라서 내부 속성을 동적으로 변경하기 어려울 수 있습니다. public class SingletonImpl { private int value; private SingletonImpl() { this.value = 0; } // 내부 속성 변경 메서드가 없음 } 자식 클래스 생성 어려움 싱글톤 클래스의 생성자가 private이므로 자식 클래스에서 이를 상속하고 확장하기 어려워집니다. public class ChildSingleton extends SingletonImpl { // 오류: 부모 클래스의 생성자에 접근할 수 없음 } 유연성 감소: 위의 문제들로 인해 싱글톤 패턴은 유연성이 감소하게 되며, 코드 유지보수 및 확장이 어려워집니다. 이로 인해 안티패턴으로 불리기도 합니다.",
      "frontmatter": {
        "tags": [
          "java",
          "design_patterns"
        ],
        "date": "2024-01-26T05:15:00+09:00",
        "lastmod": "2025-11-01T00:00:20+09:00"
      }
    },
    "우분투 리눅스 환경 구성": {
      "path": "/02.inbox/우분투-리눅스-환경-구성/",
      "filename": "우분투 리눅스 환경 구성",
      "content": "각 언어별 환경구성 python ppa 를 통해 python 의 각 버전을 설치 기본적으로 full 버전을 설치, 제안인 dev 버전도 설치 $ apt depends python3.12-full python3.12-full Depends: python3.12 (= 3.12.8-1+jammy1) Depends: libpython3.12-testsuite Depends: python3.12-venv (= 3.12.8-1+jammy1) Depends: idle-python3.12 Depends: python3.12-gdbm Depends: python3.12-lib2to3 Depends: python3.12-tk Depends: ca-certificates Recommends: python3.12-examples Suggests: python3.12-dev 파이썬 인터브리터를 변경하는방법 /bin 폴더를 ln 을 조작함 lrwxrwxrwx 1 root root 10 Nov 14 19:51 python -> python3.12* lrwxrwxrwx 1 root root 10 Sep 11 21:06 python3 -> python3.10* -rwxr-xr-x 1 root root 5941832 Nov 7 05:22 python3.10* -rwxr-xr-x 1 root root 7894408 Oct 1 17:52 python3.12* lrwxrwxrwx 1 root root 34 Oct 1 17:52 python3.12-config -> x86_64-linux-gnu-python3.12-config* python3 의 경우 우분투가 사용하는 버전 python 의 경우 내가 사용하는 버전 패키지 관리자 get-pip.py 를 통해 설치 get-pip.py 의 경우 ~/.local 에 설치되며 변경시 get-pip 를 다시 실행해서 설치한다 java apt 로 설치 (open-jdk) update-java-alternatives java 버전 변경 shinnk@n100-server:/usr/lib/jvm$ java --version openjdk 21.0.4 2024-07-16 OpenJDK Runtime Environment (build 21.0.4+7-Ubuntu-1ubuntu222.04) OpenJDK 64-Bit Server VM (build 21.0.4+7-Ubuntu-1ubuntu222.04, mixed mode, sharing) shinnk@n100-server:/usr/lib/jvm$ update-java-alternatives -l java-1.17.0-openjdk-amd64 1711 /usr/lib/jvm/java-1.17.0-openjdk-amd64 java-1.21.0-openjdk-amd64 2111 /usr/lib/jvm/java-1.21.0-openjdk-amd64 shinnk@n100-server:/usr/lib/jvm$ java --version openjdk 21.0.4 2024-07-16 OpenJDK Runtime Environment (build 21.0.4+7-Ubuntu-1ubuntu222.04) OpenJDK 64-Bit Server VM (build 21.0.4+7-Ubuntu-1ubuntu222.04, mixed mode, sharing) shinnk@n100-server:/usr/lib/jvm$ sudo update-java-alternatives -s java-1.17.0-openjdk-amd64 shinnk@n100-server:/usr/lib/jvm$ java --version openjdk 17.0.12 2024-07-16 OpenJDK Runtime Environment (build 17.0.12+7-Ubuntu-1ubuntu222.04) OpenJDK 64-Bit Server VM (build 17.0.12+7-Ubuntu-1ubuntu222.04, mixed mode, sharing) javascript node 를 말함 fnm 을 사용하며 fnm 의 경우 ~/.local 에 존재,폴더를 지우면 완전히 사라진다 go rust c샵 닷넷 도커 공식 문서에 apt 를 통해 설치 또는 snap 패키지",
      "frontmatter": {
        "tags": [
          "linux",
          "setting"
        ],
        "date": "2024-11-25T23:00:00+09:00",
        "lastmod": "2025-06-05T10:58:58+09:00"
      }
    },
    "윈도우 파일이름으로 사용할 수 없는 문자": {
      "path": "/02.inbox/윈도우-파일이름으로-사용할-수-없는-문자/",
      "filename": "윈도우 파일이름으로 사용할 수 없는 문자",
      "content": "문자 이유 \\\\ 디렉터리의 구분자로 쓰인다.예: C:\\\\Users\\\\user\\\\Downloads\\\\namuwiki.url / 디렉터리의 구분자로 쓰인다. : C: , D: 와 같은 드라이브 기호로 쓰인다. 와일드 카드로 쓰인다. ? 는 한 글자의 의미로 쓰이기도 하며, UNC(Universal Naming Convention)에도 사용된다. ? 와일드 카드로 쓰인다. \" 경로의 시작과 끝을 나타낸다.예: \"C:\\\\Users\\\\user\\\\Downloads\\\\namuwiki.url\" 리다이렉트, 파이프 등 특수 문법에 쓰인다. \\ 리다이렉트, 파이프 등 특수 문법에 쓰인다. ? => ❓",
      "frontmatter": {
        "tags": [
          "잡지식"
        ],
        "date": "2025-03-27T23:28:00+09:00",
        "lastmod": "2025-03-27T23:28:00+09:00"
      }
    },
    "인사이트, 제안, 주장의 차이점": {
      "path": "/02.inbox/인사이트-제안-주장의-차이점/",
      "filename": "인사이트, 제안, 주장의 차이점",
      "content": "인사이트, 제안, 주장의 차이점 인사이트(Insight): 관찰·분석을 통해 얻은 흥미로운 사실이나 트렌드 공유 예) “최근에 이런 현상이 있더라.” 제안(Suggestion): 문제 해결을 위한 여러 옵션을 제시 예) “이렇게 해보는 건 어떨까요?” 주장(Assertion): 구체적이고 명확한 방향성과 행동을 제시하며, 그에 대한 책임을 본인이 짐 예) “이렇게 해야 합니다. 제가 책임지고 추진하겠습니다.” 왜 ‘주장’이 중요한가? 인사이트와 제안만으로는 실제 변화나 실행이 일어나지 않음 주장은 행동과 결과에 대한 책임을 동반, 실제로 무언가를 ‘움직이게’ 만듦 조직 내에서 영향력을 가지려면, 단순히 정보 전달이 아니라 본인만의 관점과 실행 의지가 필요 주장의 3가지 핵심 요소 실행 중심(Execution-oriented): “그래서 우리는 무엇을 해야 하는가?”에 답함 단순 분석이 아니라, 구체적 실행 계획과 연결됨 확신이 담긴 관점(Conviction): 본인이 믿고, 설득할 수 있는 의견이어야 함 남의 생각이 아닌, 본인만의 해석과 신념이 중요 책임감(Ownership): “내가 앞장서겠다”는 태도 결과에 대해 책임질 각오가 필요",
      "frontmatter": {
        "tags": [
          "thinking"
        ],
        "date": "2025-06-23T21:01:14+09:00",
        "lastmod": "2025-06-23T21:01:15+09:00"
      }
    },
    "일반 함수 호출 (User-level Function Call) vs 시스템 콜 (System Call) 호출 방식의 차이": {
      "path": "/02.inbox/일반-함수-호출-user-level-function-call-vs-시스템-콜-system-call-호출-방식의-차이/",
      "filename": "일반 함수 호출 (User-level Function Call) vs 시스템 콜 (System Call) 호출 방식의 차이",
      "content": "시스템 콜(system call)은 사용자 프로그램이 커널의 기능을 간접적으로 호출하는 메커니즘이며, 매핑 테이블(시스템 콜 테이블)을 통해 실제 커널 함수로 연결됩니다. 일반 함수 호출과 시스템 콜의 동작 방식을 인텔 어셈블리 문법과 함께 비교 설명합니다. 일반 함수 호출 (User-level Function Call) 동작 방식 직접 호출: 호출자(caller)가 함수의 메모리 주소를 직접 참조합니다. 파라미터 전달: 레지스터 또는 스택을 사용합니다. 제어권 이동: 사용자 공간 내에서만 실행되며, 커널 모드 전환 없이 동작합니다. 예시: 간단한 덧셈 함수 호출 section .data num1 dd 10 num2 dd 20 section .text global _start _start: ; 파라미터 전달 (레지스터 사용) mov eax, [num1] ; EAX = 10 mov ebx, [num2] ; EBX = 20 call add_numbers ; 함수 호출 ; 종료 (시스템 콜 예시로 대체 가능) mov eax, 1 ; sys_exit 시스템 콜 번호 int 0x80 ; 커널 호출 add_numbers: add eax, ebx ; EAX = EAX + EBX ret ; 결과 반환 특징: call 명령어로 직접 함수 주소로 점프 → 커널 개입 없이 사용자 공간에서 실행. 시스템 콜 (System Call) 동작 방식 간접 호출: 시스템 콜 번호를 매핑 테이블(syscalltable)에 전달해 커널 함수를 찾아 실행합니다. 모드 전환: 사용자 모드 → 커널 모드 전환 (特权级别 변경). 파라미터 전달: 레지스터에 시스템 콜 번호와 인자 저장 (예: eax , ebx , ecx 등). 시스템 콜 테이블의 역할 커널은 syscalltable이라는 배열을 유지하며, 각 인덱스는 시스템 콜 번호에 해당합니다. 예: Linux x86에서 sys_write 의 시스템 콜 번호는 4 입니다. // Linux 커널 소스 (unistd_32.h) #define __NR_write 4 예시: sys_write 시스템 콜 호출 section .data msg db \"Hello, World!\", 0xA ; 출력 메시지 (0xA = 개행) len equ $ - msg ; 메시지 길이 section .text global _start _start: ; 시스템 콜 파라미터 설정 (레지스터 사용) mov eax, 4 ; sys_write 시스템 콜 번호 (NR_write = 4) mov ebx, 1 ; 파일 디스크립터 (stdout = 1) mov ecx, msg ; 메시지 주소 mov edx, len ; 메시지 길이 int 0x80 ; 커널 모드 전환 (소프트웨어 인터럽트) ; 프로그램 종료 mov eax, 1 ; sys_exit 시스템 콜 번호 xor ebx, ebx ; 반환 값 0 int 0x80 시스템 콜 처리 과정 인터럽트 발생: int 0x80 또는 syscall 명령어로 커널 모드 진입. 시스템 콜 번호 확인: eax 레지스터에서 시스템 콜 번호를 읽습니다. 매핑 테이블 조회: 커널은 sys_call_table 에서 해당 번호의 함수 포인터를 찾습니다. // 커널 내부 sys_call_table 예시 (x86) asmlinkage long sys_write(unsigned int fd, const char __user *buf, size_t count); 함수 실행: 매핑된 커널 함수(예: sys_write )를 실행합니다. 결과 반환: 사용자 모드로 복귀 후 결과를 레지스터( eax )에 저장합니다. 시스템 콜 vs 일반 함수 호출의 핵심 차이 구분 일반 함수 호출 시스템 콜 실행 모드 사용자 모드 (User Mode) 커널 모드 (Kernel Mode) 호출 방식 직접 호출 ( call 명령어) 간접 호출 (인터럽트/시스템 콜 명령어) 파라미터 전달 레지스터/스택 레지스터 (커널에서 정의한 규칙) 오버헤드 낮음 (메모리 접근만 발생) 높음 (모드 전환, 컨텍스트 스위칭) 보안 제한 없음 커널이 검증 후 실행 시스템 콜 테이블의 구조 (Linux x86 예시) // arch/x86/kernel/syscall_32.c (커널 소스) extern asmlinkage long sys_write(unsigned int, const char __user *, size_t); extern asmlinkage long sys_read(unsigned int, char __user *, size_t); static const sys_call_ptr_t sys_call_table[__NR_syscall_max + 1] = { [0 ... __NR_syscall_max] = &sys_ni_syscall, // 기본값: 지원되지 않는 시스템 콜 [__NR_write] = sys_write, [__NR_read] = sys_read, // ... (다른 시스템 콜 매핑) }; 간접 참조: sys_call_table[__NR_write] → sys_write 함수 포인터. 요약 일반 함수 호출: 사용자 공간 내에서 직접 실행되며, 빠르지만 권한이 제한적입니다. 시스템 콜: 커널의 매핑 테이블을 통해 간접적으로 실행되며, 모드 전환과 검증 과정을 거쳐 안전하게 하드웨어/커널 자원을 제어합니다. 예시: int 0x80 은 커널의 인터럽트 핸들러를 호출해 시스템 콜 테이블을 조회하고, 실제 함수(예: sys_write )를 실행합니다.",
      "frontmatter": {
        "tags": [
          "operating-system"
        ],
        "date": "2025-03-11T11:39:00+09:00",
        "lastmod": "2025-03-11T11:39:00+09:00"
      }
    },
    "주석 유형 docstring": {
      "path": "/02.inbox/주석-유형-docstring/",
      "filename": "주석 유형 docstring",
      "content": "docstring 유형 라벨 (분류) 한국어 영어 라벨 설명 설명 Description 함수/클래스의 기본 동작 설명 사용법 Usage 호출 방법, 예제 포함 예제 Example 실제 사용 예시 ( >>> ) 포함 가능 매개변수 Parameters 입력 인자 설명 반환값 Returns 반환 값에 대한 설명 예외 Raises 발생 가능한 예외 설명 참고 See Also 관련된 함수/클래스 참조 문법 Syntax 문법 구조나 규칙 설명 성능 Performance 시간/공간 복잡도, 효율성 관련 주의사항 Note 경고 또는 주의해야 할 사항 변경이력 Change Log 변경 내역 (버전별 관리 시) 의존성 Dependencies 외부 모듈이나 라이브러리 요구사항",
      "frontmatter": {
        "date": "2025-07-01T06:43:42+09:00",
        "lastmod": "2025-07-01T06:44:14+09:00"
      }
    },
    "주소 바인딩 실습(소스코드 부터 프로세스까지)": {
      "path": "/02.inbox/주소-바인딩-실습소스코드-부터-프로세스까지/",
      "filename": "주소 바인딩 실습(소스코드 부터 프로세스까지)",
      "content": "주소 바인딩(Address Binding) 과정 심층 분석 주소 바인딩의 개념과 목적 주소 바인딩(Address Binding)이란 프로그램의 소스 코드에 사용된 변수나 함수 같은 심볼릭 주소(Symbolic Address)가 실제 물리 메모리(Physical Memory)의 주소로 변환되는 전체 과정을 의미합니다. 이 과정은 컴퓨터가 프로그램을 실행하기 위한 핵심적인 메커니즘입니다. 목적 재배치(Relocation): 프로그램을 메모리의 어느 위치에든 적재하여 실행할 수 있도록 합니다. 만약 주소가 고정되어 있다면 여러 프로그램을 동시에 실행하기 어렵습니다. 메모리 보호(Memory Protection): 각 프로세스가 자신에게 할당된 메모리 영역만 접근하도록 격리하여 시스템의 안정성을 높입니다. 메모리 효율성 증대: 가상 메모리(Virtual Memory), 공유 라이브러리(Shared Library) 등의 기법을 통해 한정된 물리 메모리를 여러 프로세스가 효율적으로 나누어 사용할 수 있게 합니다. 본 문서는 리눅스 CLI 환경에서 제공하는 다양한 도구를 활용하여, 컴파일 → 링크 → 적재 → 실행 각 단계에서 주소 바인딩이 어떻게 이루어지는지 구체적인 명령과 출력을 통해 심층적으로 추적하고 분석하는 것을 목표로 합니다. 실습 환경 구성 실습을 위해 간단한 C 코드를 작성합니다. 이 코드는 전역 변수, 함수 등을 포함하여 각 단계별 변화를 관찰하기에 용이합니다. 예제 코드 ( addr_test.c ) #include <stdio.h> int initialized_global_var = 10; int uninitialized_global_var; const int const_global_var = 20; void function(void) { printf(\"Function address\\n\"); } int main(void) { static int static_local_var = 30; int local_var = 40; function(); printf(\"Address of initialized_global_var: %p\\n\", &initialized_global_var); printf(\"Address of main function: %p\\n\", main); return 0; } 1단계: 컴파일 시간 바인딩 (Compile-Time Binding) 목표: 소스 코드의 심볼릭 주소(변수명, 함수명)가 컴파일러에 의해 재배치 가능한(Relocatable) 주소로 변환되는 과정을 확인합니다. 이 주소는 각 오브젝트 파일( .o ) 내에서의 상대적인 오프셋(offset)입니다. 오브젝트 파일 생성 gcc 의 -c 옵션은 링킹을 수행하지 않고 컴파일만 진행하여 오브젝트 파일을 생성합니다. Bash gcc -c addr_test.c -o addr_test.o 심볼 테이블 분석 ( nm ) nm 도구는 오브젝트 파일의 심볼 테이블을 보여줍니다. 이를 통해 각 심볼이 어떤 섹션에, 어떤 상대 주소로 할당되었는지 확인할 수 있습니다. nm addr_test.o 출력 예시 및 분석: 000000000000001e T function 0000000000000000 D initialized_global_var 0000000000000035 T main U printf 0000000000000004 C uninitialized_global_var ... 주소: 0x00... 으로 시작하는 이 값들은 최종 메모리 주소가 아닌, 파일 내의 상대 주소(오프셋)입니다. 심볼 타입: T : .text 섹션(코드)에 위치한 심볼. main , function 함수가 해당됩니다. D : .data 섹션(초기화된 데이터)에 위치한 심볼. initialized_global_var 가 해당됩니다. C : Common 심볼. 초기화되지 않은 전역 변수( uninitialized_global_var )로, 크기만 명시되고 최종 위치는 링커가 결정합니다. U : Undefined. 이 파일 내에 정의되지 않은 심볼로, printf 처럼 외부 라이브러리에서 가져와야 함을 의미합니다. 섹션별 상세 분석 ( objdump ) objdump -d 는 코드 섹션( .text )을 디스어셈블하여 명령어 수준에서 상대 주소를 보여줍니다. objdump -d addr_test.o 출력된 어셈블리 코드에서 call 이나 mov 명령어의 대상 주소가 0 또는 상대적인 값으로 표시된 것을 볼 수 있습니다. 이는 링커가 채워주어야 할 부분입니다. 2단계: 링크 시간 바인딩 (Link-Time Binding) 목표: 링커가 여러 오브젝트 파일과 라이브러리를 결합하여 단일 실행 파일을 만드는 과정을 확인합니다. 이 과정에서 재배치 가능 주소는 프로그램의 가상 주소 공간(Virtual Address Space) 내의 절대 주소(Absolute Address)로 확정됩니다. 실행 파일 생성 오브젝트 파일을 링킹하여 실행 파일을 생성합니다. 이 과정에서 printf 같은 외부 함수의 주소도 연결됩니다. gcc addr_test.o -o addr_test 실행 파일의 심볼 주소 확인 ( readelf ) readelf -s 는 ELF(Executable and Linkable Format) 형식 파일의 심볼 테이블을 더 상세히 보여줍니다. readelf -s ./addr_test | grep 'main\\|function\\|initialized' 출력 예시 및 분석: 38: 0000000000004030 4 OBJECT GLOBAL DEFAULT 15 initialized_global_var 63: 0000000000001159 26 FUNC GLOBAL DEFAULT 14 function 69: 0000000000001173 69 FUNC GLOBAL DEFAULT 14 main 이제 주소( 0x4030 , 0x1159 등)는 0 이 아닌, 가상 메모리 주소로 확정되었습니다. 이 주소는 프로세스가 시작될 때 할당될 논리적인 주소이며, 모든 심볼이 고유한 가상 주소를 갖게 됩니다. 3단계: 적재 시간 바인딩 (Load-Time Binding) 목표: 운영체제(OS)의 로더(Loader)가 실행 파일을 메모리에 올릴 때, 프로그램의 각 세그먼트(코드, 데이터 등)가 프로세스의 가상 주소 공간에 어떻게 매핑되는지 확인합니다. 프로세스 실행 및 PID 확인 프로세스를 백그라운드로 실행하고 해당 PID(Process ID)를 변수에 저장합니다. ./addr_test & PID=$! 프로세스 메모리 맵 확인 ( pmap ) pmap 은 특정 프로세스의 메모리 맵을 보여주는 강력한 도구입니다. pmap $PID 출력 예시 및 분석: 000055a3d0ab4000 4K r-xp /path/to/addr_test <-- Code (.text) 000055a3d0acb000 4K r--p /path/to/addr_test <-- Read-only data 000055a3d0acc000 4K rw-p /path/to/addr_test <-- Read-write data (.data, .bss) ... 00007ffc8d9e8000 132K rw-p [stack] <-- Stack ... 첫 번째 열: 각 메모리 영역의 시작 가상 주소입니다. ASLR (Address Space Layout Randomization) 때문에 프로그램을 실행할 때마다 이 시작 주소는 변경될 수 있습니다. readelf 에서 본 오프셋은 유지된 채, 전체적인基底 주소(Base Address)가 바뀌는 것입니다. /proc 파일 시스템으로 확인 /proc/<PID>/maps 파일은 pmap 보다 더 상세한 정보를 제공합니다. cat /proc/$PID/maps 출력 형식은 pmap 과 유사하지만, 파일 오프셋, 디바이스/inode 정보 등 더 많은 세부 정보를 포함합니다. 이 파일을 통해 코드, 데이터, 스택, 힙 및 동적 라이브러리가 가상 주소 공간에 어떻게 배치되었는지 명확히 볼 수 있습니다. 4단계: 실행 시간 바인딩 (Execution-Time Binding) 목표: 프로그램이 실제로 실행되는 동안 MMU(Memory Management Unit) 하드웨어가 가상 주소를 물리 주소로 동적으로 변환하는 개념을 간접적으로 확인합니다. 현대 OS는 대부분 이 방식을 사용합니다. 페이지 폴트(Page Fault) 관찰 ( perf ) 가상 주소가 처음 접근될 때, 해당 데이터가 물리 메모리에 없다면 페이지 폴트가 발생합니다. 이때 OS는 디스크에서 데이터를 물리 메모리로 가져오고 페이지 테이블을 업데이트합니다. 이것이 바로 실행 시간 바인딩의 핵심 순간입니다. # -e page-faults 옵션으로 페이지 폴트 이벤트 카운팅 perf stat -e page-faults ./addr_test 출력 예시: Address of initialized_global_var: 0x55d8b2f9a030 Address of main function: 0x55d8b2f98173 Performance counter stats for './addr_test': 105 page-faults 0.001099684 seconds time elapsed 105 개의 페이지 폴트가 발생했음을 보여줍니다. 이는 프로그램 실행에 필요한 코드/데이터 페이지가 실행 시점에 동적으로 물리 메모리에 바인딩되었음을 의미합니다. 가상 주소 → 물리 주소 매핑 확인 ( pagemap , 고급) /proc/<PID>/pagemap 파일은 각 가상 페이지에 대한 물리 페이지 프레임 번호(PFN) 정보를 담고 있습니다. 이를 통해 최종적인 가상-물리 주소 매핑을 직접 확인할 수 있지만, 루트 권한이 필요하며 파싱이 복잡합니다. 이는 주소 변환의 가장 낮은 수준을 보여주는 증거입니다. 관심 있는 사용자는 kernel.org의 pagemap 문서를 참조하여 직접 스크립트를 작성해볼 수 있습니다. 전체 과정 요약 바인딩 단계 목적 핵심 도구 주소의 상태 컴파일 시간 심볼릭 주소를 상대 주소로 변환 gcc -c , nm , objdump 재배치 가능 주소 (Relocatable Address) 링크 시간 여러 오브젝트를 묶어 가상 주소 결정 gcc , readelf -s 절대 가상 주소 (Absolute Virtual Address) 적재 시간 실행 파일을 프로세스 가상 공간에 매핑 pmap , /proc/PID/maps 가상 메모리 주소 (Mapped Virtual Address) 실행 시간 MMU가 가상 주소를 물리 주소로 변환 MMU (HW), perf , /proc/PID/pagemap 물리 주소 (Physical Address)",
      "frontmatter": {
        "tags": [
          "operating-system"
        ],
        "date": "2025-05-29T06:39:00+09:00",
        "lastmod": "2025-06-03T05:44:08+09:00"
      }
    },
    "청년창업 2025 창업 기준": {
      "path": "/02.inbox/청년창업-2025-창업-기준/",
      "filename": "청년창업 2025 창업 기준",
      "content": "2025년 창업 시 적용 기준 정리 표 항목 2024년 이전 창업 2025년 창업 설명 고용증대 추가감면율 최대 50% 최대 100% 상시근로자 증가율에 100% 적용 (기존 50% → 100% 상향) 감면한도 무제한 연간 5억 원 한도 세액감면 금액이 연간 5억 원으로 제한됨 (일반 중소기업 규모에서는 실제 영향 적음) 업종 우대감면 특정 업종 적용 기존 정책 종료 (세부 내용 미기재) ※ 기존 업종 우대감면 정책 종료, 새로운 정책 적용되나 구체적 내용은 제공된 자료에 없음 수도권 감면율 기존 적용 기존 적용 수도권 감면율 축소는 2026년 1월 1일 이후 창업부터 적용되며, 2025년 창업에는 영향 없음 📌 주요 설명 고용증대 추가감면율 2025년 이후 창업 기업은 고용 증가율에 따라 최대 100% 추가 감면 적용 예) 기존 50% → 100%로 상향되며, 고용 증가율 100% 시 추가 100% 감면 가능 감면한도 2025년부터 연간 5억 원 한도 도입 기존에는 한도 없었으나, 최대 감면액이 5억 원으로 제한됨 소규모 자영업자 대부분은 이 한도에 도달하지 않아 실질적 영향 적음 업종 우대감면 제공된 자료에 구체적 설명 없음 다만 기존 업종(음식점, 미용실, 제조업 등)은 여전히 감면 대상으로 적용됨 \"종료\"라는 표현은 기존 정책 체계의 종료를 의미할 수 있으나, 세부 내용은 확인 불가 수도권 감면율 2026년 창업부터 적용되는 사항으로, 2025년 창업에는 영향 없음 수도권을 △과밀억제권역 △과밀억제권역이 아닌 수도권 △비수도권으로 세분화해 적용 2025년 창업자는 기존 수도권 규정 적용 (비수도권 100% 감면, 수도권 50% 감면 등) 💡 참고: 2025년 창업자도 여전히 음식점, 미용실, 피트니스센터, 디자인업, 출판사, 통신판매업, 제조업 등 기존 업종에서 창업 시 세액감면 적용 가능합니다. 다만 감면율 계산 방식과 한도가 변경되었음을 유의하세요. 아래는 2025년에 창업한 경우 적용받는 ‘청년창업중소기업 세액감면’ 기준을, 본문에서 언급된 4가지 핵심 변경 사항(△업종 우대감면 종료, △고용증대 추가감면 상향, △감면한도 연 5억 원 제한, △수도권 감면율 축소는 2026년 적용)을 정확히 반영하여 정리한 표입니다. [!info] 성남시의 경우 과밀억제권역 📊 2025년 창업 시 적용 기준 요약표 구분 적용 내용 비고 ✅ 적용 대상자 - 창업 당시 만 15세 이상 ~ 34세 이하 청년- 병역 이행자: 복무기간(최대 6년) 차감 후 연령 계산- 법인: 청년이 최대주주 또는 최대출자자여야 함 개인사업자/법인 모두 가능 ✅ 감면 기간 창업 후 첫 소득 발생 연도로부터 5년간 일반 창업중소기업과 동일 ✅ 적용 업종 조세특례제한법상 지정 업종만 가능▶ 음식점, 미용실, 피트니스센터, 디자인업, 출판사, 통신판매업, 제조업, 사회복지업 등 포함 ⚠️ 2025년 창업부터 ‘업종 우대감면’ 종료→ 기존 일부 업종에 추가로 주던 우대 혜택 없어짐 ✅ 감면율 (지역별) - 비수도권: 100% 감면- 수도권 과밀억제권역: 50% 감면- 수도권 내 과밀억제권역外 (인천경제자유구역, 김포, 파주 등): 100% 감면 (2025년까지는 기존 혜택 유지) ⚠️ 수도권 감면율 축소는 2026년 1월 1일 이후 창업부터 적용→ 2025년 창업자는 기존 혜택 그대로 받음 ✅ 고용증대 추가감면 기존보다 상향 적용- 고용인원 증가시 추가 감면율 ↑- 구체적 비율은 세법 시행령 참조 2025년 창업자 대상으로 고용증대 감면 혜택 확대 ✅ 감면 한도 연간 최대 5억 원으로 제한 2025년 창업부터 신설된 한도→ 감면액이 5억 원을 초과하면 그 이상은 감면 불가 ✅ 신고 요건 - 복식부기 의무자: 반드시 복식부기로 신고해야 감면 가능- 간편장부 대상자: 추계신고도 가능 무신고, 기한 후 신고, 간편장부(복식부기 의무자) 신고 시 감면 불가 ❌ 신규 창업 불인정 사례 ① 기존 사업 승계(합병/양수 등)② 개인→법인 전환③ 폐업 후 동일 업종 재개업④ 사업 확장 또는 업종 추가 신규 창업으로 인정되지 않으면 감면 불가 🔍 핵심 포인트 요약 업종 우대감면 종료 → 2025년 창업부터는 특정 업종(예: 문화콘텐츠, 지식서비스 등)에 추가로 주던 우대 감면이 완전히 사라짐. 모든 지정 업종은 동일한 기본 감면율 적용. 고용증대 감면 상향 → 청년창업기업이 고용을 늘릴 경우, 기존보다 더 큰 추가 감면 혜택 제공 (구체적 %는 국세청 고시 참조). 감면 한도 연 5억 원 → 1년간 감면받을 수 있는 세액 총액이 최대 5억 원으로 제한됨. 이는 청년창업자에게도 동일하게 적용. 수도권 감면율 축소는 2026년부터 → 2025년 창업자는 기존 감면율 그대로 적용 - 비수도권: 100% - 수도권 과밀억제권역外: 100% (인천경제자유구역, 김포, 파주 등 포함) - 수도권 과밀억제권역: 50% ✅ 결론: 2025년 창업자는 ‘업종 우대감면 종료’, ‘고용증대 감면 확대’, ‘연 5억 한도’는 적용받지만, ‘수도권 감면율 축소’는 적용되지 않음. → 수도권 외곽(김포, 파주 등)에서 창업할 계획이라면, 2025년 안에 창업하는 것이 혜택 면에서 유리함. [!info] 복식부기 의무자란 https://mybiz.pay.naver.com/contentsGuide/271 [!info] 개인사업자와 취업과의 관계 https://m.kin.naver.com/qna/dirs/40309/docs/482005709?answerNo=1 https://www.jobkorea.co.kr/User/Qstn/AnswerWrite?qstnNo=47800 https://www.a-ha.io/questions/4ef9d03e9e90b44998c5cdb07a45bfd4",
      "frontmatter": {
        "date": "2025-09-20T15:54:30+09:00",
        "lastmod": "2025-10-19T16:12:18+09:00",
        "share_link": "https://share.note.sx/tof8u5e5#8XowAUqJEnVOfjKo+IHe1qUdxKXpmPeIHXxRQxVAy4c",
        "share_updated": "2025-09-22T14:38:18+09:00"
      }
    },
    "컴퓨터 구조 chapter1": {
      "path": "/02.inbox/컴퓨터-구조-chapter1/",
      "filename": "컴퓨터 구조 chapter1",
      "content": "A Tour of Computer Systems 1 Information Is Bits + Context 39 정보는 비트 + 문맥이다 동일한 비트열이 문맥에 따라 string integer floating-point instruction 으로 해석될수 있다 2 Programs Are Translated by Other Programs into Different Forms 40 3 It Pays to Understand How Compilation Systems Work 42 4 Processors Read and Interpret Instructions Stored in Memory 43 5 Caches Matter 47 6 Storage Devices Form a Hierarchy 50 7 The Operating System Manages the Hardware 50 8 Systems Communicate with Other Systems Using Networks 55 9 Important Themes 58 10 Summary 63 Bibliographic Notes 64 Solutions to Practice Problems 64 direct memory access (DMA, discussed in Chap- ter 6) cache memories chapter6 공유 라이브러리 주소 공간 chapter 7 kernel vietual memory 가 뭐지?? chapter 9 unix i o chapter 10 Thread-Level Concurrency vs instruction level parallelism 의 차이 12장 에서 하이퍼스레딩? 5장에서 instruction ?? 파이프라닝 chapter4 62page Thread level concurrency instruction level parallelism single instruction multiple data parallelism ?? relocatable object program 과 excutable object program 의 차이 hello.o 파일에서 내부의 printf 함수의 구현은 printf.o라는 별도의 사전 컴파일된 개체 파일에 있으며, 이 파일은 어떻게든 우리의 hello.o 프로그램과 병합되어야 합니다. 이 병합을 링커(ld)가 처리합니다. 재배치 가능한 오브젝트 프로그램은 컴파일러나 어셈블러가 소스 코드를 컴파일하여 생성한 중간 단계의 파일 .o 파일이며 링크 단계 통과후 실행파일이 됩니다 buses 는 word 라는 단위로 잘려져 전송된다 여기서 워드의 크기는 시스템에 따라 달라진다 32 비트에서는 4바이트 64 비트는 8바이트 이다 즉 워드라는 것은 시스템에서 사용하는 기본적인 전송 단위로서 고정적인 단위가 아닌 추상 단위이다 io 장치는 io 버스에 컨트롤러 또는 아탑터 로 연결된다 둘의 차이는 #ModificationRequired main memory 는 바이트 1개가 하나의 주소에 매핑된다 주소는 0부터 프로세서는 instruction set 을 구현한 것은 맞지만 실제로는 실행속도를 빠르게 하는 여러가지 기술을 지원한다 즉 동일한 instruction set을 해석하는 프로세스의 구현은 많이 다를 수 있다 L1 L2 cache 는 sram 이고 main memory 는 dram 이다 %20image%2020240619110245.png) 프로세스의 메모리 자원 (코드 데이터 힙 스텍 레지스터 ) 쓰레드 메모리 자원( 스텍 레지스터 ) 나머지는 공유한다 1 클록에 1개 이상의 명령을 수행하는 프로세서를 puperscalar 라고 한다 word 는 포인터의 크기를 나타내는 자료형이다 크기는 컴퓨터 비트 마다 다르다",
      "frontmatter": {
        "date": "2024-06-18T11:30:00+09:00",
        "lastmod": "2024-06-18T11:30:00+09:00"
      }
    },
    "터미널 동작 원리 매우 상세하게": {
      "path": "/02.inbox/터미널-동작-원리-매우-상세하게/",
      "filename": "터미널 동작 원리 매우 상세하게",
      "content": "터미널의 동작원리를 정확하게 이해하기 위해 터미널의 구성요소를 먼저 알아보고 case 별로 어떤 방식으로 이벤트가 전달되는가를 확인해보자 터미널의 구성 요소와 역할 터미널 환경은 크게 3개의 행위자(Actor)와 이들을 연결하는 1개의 통신 채널(Channel)로 구성됩니다. 각 요소가 어떤 일을 하는지 명확히 구분하는 것이 중요합니다. 터미널 에뮬레이터 (Terminal Emulator, 예: gnome-terminal , iTerm2 ) 역할: 사용자를 위한 그래픽 인터페이스(GUI 창)를 제공하고, 그래픽 시스템과 바이트 스트림 간의 번역을 담당합니다. 입력 처리: 누구에게 받아서: 사용자로부터 키보드 입력, 마우스 클릭 등 그래픽 시스템 이벤트를 받습니다. 무엇을 하는가: 이벤트를 해석하여 약속된 바이트(byte) 데이터로 변환합니다. A 키 → 0x41 바이트 위쪽 화살표 키 → \\033[A (이스케이프 시퀀스) 어디에 전달: 변환된 바이트 데이터를 PTY 통신 채널의 마스터(Master) 측 파일 디스크립터에 씁니다(write). 출력 처리: 어디에서 받아서: PTY 통신 채널의 마스터(Master) 측 파일 디스크립터에서 바이트 데이터를 읽습니다(read). 무엇을 하는가: 읽어들인 바이트 스트림을 해석하여 화면에 글자나 그래픽 요소로 렌더링합니다. 이것이 에뮬레이터의 핵심 기능입니다. hello 같은 일반 텍스트는 그대로 화면에 그립니다. \\033[1m (굵게), \\033[31m (빨간색) 같은 이스케이프 시퀀스는 명령으로 해석하여 글자의 스타일을 바꿔서 그립니다. 누구에게 전달: 렌더링 결과를 사용자 눈에 보이는 화면(GUI 창)에 표시합니다. 통신 채널: PTY (Pseudo-Terminal) 역할: 터미널 에뮬레이터와 셸(애플리케이션) 사이의 양방향 통신 파이프 역할을 합니다. 커널에 의해 생성되고 관리되는 가상 장치(Virtual Device)입니다. 구조: 두 개의 끝점으로 구성됩니다. PTY 마스터(Master) 파일 디스크립터: 터미널 에뮬레이터가 사용하는 통신 끝점입니다. PTY 슬레이브(Slave) 파일 디스크립터: 셸(애플리케이션)이 사용하는 통신 끝점입니다. /dev/pts/N 형태의 장치 파일에 해당합니다. 특징: PTY 자체는 단순한 데이터 통로이지만, 이 통로를 지나는 데이터는 커널의 TTY 드라이버에 의해 감시되고 처리됩니다. 행위자 2: 커널 (Kernel)의 TTY/PTY 드라이버 역할: PTY 통신 채널의 중간에서 데이터를 중계하며, 전통적인 터미널의 동작 규칙(Line Discipline)을 적용하는 실제 두뇌입니다. 기능 (주로 cooked 모드일 때): 어디에서 받아서: PTY 마스터 FD에 쓰인 데이터를 읽고, PTY 슬레이브 FD에 쓰인 데이터를 읽습니다. 무엇을 하는가 (마스터 → 슬레이브 방향): 문자 에코 (Echo): PTY 마스터로부터 받은 입력 바이트를 다시 PTY 마스터 쪽으로 되돌려 써서, 터미널 에뮬레이터가 사용자가 입력한 내용을 화면에 표시하게 합니다. 입력 편집 (Line Buffering): Enter 가 입력될 때까지 데이터를 내부 버퍼에 모으고, Backspace 같은 편집 문자를 해석하여 버퍼를 수정합니다. 신호(Signal) 생성: Ctrl+C 에 해당하는 바이트( 0x03 )를 감지하면, 이를 데이터로 전달하는 대신 커널의 프로세스 관리자에게 SIGINT 신호를 생성하도록 요청합니다. 이 신호는 PTY 슬레이브에 연결된 전면 프로세스 그룹에 전달됩니다. 무엇을 하는가 (슬레이브 → 마스터 방향): 단순 중계: 일반적으로 셸(애플리케이션)이 PTY 슬레이브에 쓴 출력 데이터는 특별한 처리 없이 그대로 PTY 마스터 쪽으로 전달합니다. 어디에 전달: 처리된 데이터를 반대편 파일 디스크립터에서 읽을 수 있도록 준비시킵니다. 제어: 셸이나 애플리케이션은 ioctl() 시스템 콜을 통해 이 드라이버의 동작 방식( raw 모드, ECHO 끄기 등)을 변경할 수 있습니다. 행위자 3: 셸 / 터미널 애플리케이션 (Shell / e.g., bash , vim ) 역할: 사용자의 명령을 실행하고 그 결과를 제공하는 프로그램입니다. 기능: 어디에서 받아서: 자신의 표준 입력(stdin), 즉 PTY 슬레이브 파일 디스크립터로부터 데이터를 읽습니다. 무엇을 하는가: 커널 드라이버에 의해 가공된(또는 raw 모드에서는 가공되지 않은) 데이터를 읽어 명령어로 해석하고 실행합니다. ls\\n : ls 명령을 실행. \\033[A (화살표 키): 히스토리 검색 기능 실행. 어디에 전달: 실행 결과(텍스트)나 화면 제어를 위한 이스케이프 시퀀스를 자신의 표준 출력(stdout) 또는 표준 에러(stderr), 즉 PTY 슬레이브 파일 디스크립터에 씁니다. 역할 분담: 터미널 에뮬레이터 vs. 커널 (PTY 드라이버)드라이버는 커널인가 기능 담당자 설명 Ctrl+C 입력 시 SIGINT 신호 전달 커널 (PTY 드라이버) 에뮬레이터는 제어문자( 0x03 )만 보내면, 커널이 이를 해석해 전면 프로세스 그룹(foreground process group)에 신호를 보냅니다. read -s (입력 숨기기) 커널 (PTY 드라이버) 셸이 ioctl(TCSETSW) 로 echo 모드를 끄면, 커널(PTY 드라이버)이 입력 문자를 되돌려 보내지 않습니다. stty raw (Raw 모드 설정) 커널 (PTY 드라이버) Raw 모드 설정 시, 커널은 줄 단위 편집, 신호 처리 등을 비활성화하고 바이트를 그대로 통과시킵니다. 이스케이프 시퀀스 해석 터미널 에뮬레이터 \\033[1m (굵게), \\033[31m (빨강) 같은 시퀀스는 커널이 이해하지 못하며, 에뮬레이터가 직접 해석하여 화면에 렌더링해야 합니다. 커서 이동, 화면 지우기 터미널 에뮬레이터 clear , tput cup , vim 등이 사용하는 화면 제어 이스케이프 시퀀스를 에뮬레이터가 이해하고 커서 위치를 바꾸거나 화면을 다시 그려야 합니다. UTF-8 등 다중 바이트 문자 처리 터미널 에뮬레이터 ä , 한 , 😊 같은 문자가 깨지지 않고 올바른 폭으로 렌더링되도록 처리하는 것은 에뮬레이터의 몫입니다. 줄 바꿈, 줄 감김 (Line Wrap) 터미널 에뮬레이터 한 줄의 끝(예: 80자)을 넘어서는 문자가 입력될 때 다음 줄로 넘길지, 개행 문자를 어떻게 처리할지 등을 에뮬레이터가 결정하고 구현해야 합니다. 🔺 핵심 요약: 커널(PTY 드라이버)은 입출력 중계와 신호/모드 처리를 담당하고, 터미널 에뮬레이터는 화면에 보이는 모든 시각적 표현(렌더링)을 책임집니다. Case 별 구성요소들의 처리 과정 핵심 데이터 흐름: 두 개의 파이프라인 사용자 입력 파이프라인 (Input Flow: 키보드 입력이 셸에 도달하기까지) 에뮬레이터 (키 입력→바이트 변환) → PTY Master → 커널 (PTY 드라이버 → TTY 서브시스템) (Echo, 버퍼링, 신호 처리) → PTY Slave → 셸/앱 (데이터 읽기) 프로그램 출력 파이프라인 (Output Flow: 프로그램 결과가 화면에 보이기까지) 셸/앱 (결과 출력) → PTY Slave → 커널 (PTY 드라이버) (단순 중계) → PTY Master → 에뮬레이터 (바이트→화면 렌더링) Case 1: 사용자가 ls 를 입력하고 Enter를 누를 때 (가장 기본적인 흐름) 사용자: 키보드를 사용하여 l , s , 그리고 Enter 키를 순서대로 누릅니다. 터미널 에뮬레이터 (예: gnome-terminal): 운영체제로부터 키보드 입력 이벤트를 전달받습니다. 이 프로그램은 각 키 입력이 어떤 바이트 데이터에 해당하는지 알고 있습니다. l 키 → 바이트 0x6C s 키 → 바이트 0x73 Enter 키 → 개행(Line Feed) 문자를 의미하는 바이트 0x0A 이 바이트들을 순서대로, 자신이 프로세스를 시작할 때 열었던 PTY 마스터 파일 디스크립터(FD)에 씁니다(write 시스템 콜 사용). 커널 (PTY/TTY 드라이버) - 입력 수신 및 처리: 커널은 누군가 PTY 마스터 FD에 데이터를 썼다는 것을 인지하고, 해당 데이터를 자신의 버퍼로 읽어들입니다. 현재 터미널은 기본 모드인 cooked (또는 canonical ) 모드로 동작하고 있으므로, 커널 드라이버는 다음과 같은 규칙을 적용합니다. Echo 기능: 첫 번째 바이트 0x6C ( l )를 읽자마자, 사용자가 자신이 무엇을 입력하고 있는지 볼 수 있도록 이 바이트를 즉시 PTY 마스터 FD 쪽으로 다시 써줍니다. 터미널 에뮬레이터 (Echo 표시): 자신의 마스터 FD에서 읽을 데이터( 0x6C )가 생긴 것을 감지하고, 이를 읽어 화면의 커서 위치에 l 이라는 문자를 렌더링합니다. 커널 드라이버 (Echo 반복): 두 번째 바이트 0x73 ( s )에 대해서도 동일한 Echo 과정을 수행합니다. 터미널 에뮬레이터는 화면에 s 를 이어서 표시합니다. 이제 화면에는 ls 가 보입니다. Line Buffering 기능: 커널 드라이버는 l 과 s 를 Echo 처리함과 동시에, 자신의 내부 라인 버퍼(line buffer)에 이 문자들을 차곡차곡 쌓아둡니다. 이 버퍼는 셸과 같은 최종 애플리케이션에 아직 전달되지 않은, 편집 중인 한 줄의 데이터를 임시로 보관하는 장소입니다. Line Completion 기능: 마지막으로 0x0A ( Enter ) 바이트를 읽습니다. 커널 드라이버는 이 문자를 \"한 줄 입력의 끝\"으로 해석합니다. 이제 라인 버퍼에 저장되어 있던 ls 와 방금 들어온 \\n 을 합쳐, ls\\n 이라는 완전한 한 줄의 데이터를 확정합니다. 커널 (PTY 드라이버) - 데이터 전달: 완성된 ls\\n 데이터를 이제 PTY 슬레이브 파일 디스크립터(FD)에서 읽을 수 있도록 준비시킵니다. (정확히는, 슬레이브 FD를 read() 하고 있던 프로세스를 깨워서 데이터를 전달할 준비를 합니다.) 셸 (bash): 셸 프로세스는 시작된 이후로 계속해서 자신의 표준 입력(stdin), 즉 PTY 슬레이브 FD에 새로운 입력이 들어오기를 기다리며 대기(block)하고 있었습니다. 커널이 데이터를 준비시켰으므로, 셸은 read() 시스템 콜을 통해 드디어 ls\\n 데이터를 읽어들입니다. 셸 (명령어 해석 및 실행): 셸은 읽어들인 ls\\n 문자열을 해석합니다. 공백과 개행 문자를 기준으로 첫 번째 단어인 ls 를 실행할 명령어로 인식합니다. 셸은 fork() 와 execve() 시스템 콜을 사용하여 ls 라는 새로운 프로세스를 생성하고 실행합니다. 이때, 자식 프로세스인 ls 는 부모인 셸의 표준 입출력을 상속받으므로, ls 의 표준 출력(stdout) 역시 PTY 슬레이브 FD를 가리키게 됩니다. ls 프로세스: ls 프로그램은 현재 디렉토리의 파일 및 폴더 목록을 조회하여 텍스트 데이터(예: file1.txt\\nfolder1\\nfile2.txt\\n )를 생성합니다. 이 결과 텍스트를 자신의 표준 출력(stdout)인 PTY 슬레이브 FD에 씁니다. 커널 (PTY 드라이버) - 출력 중계: 슬레이브 측에 데이터가 쓰인 것을 감지하고, 이 데이터를 그대로 PTY 마스터 FD 쪽으로 전달(중계)합니다. 터미널 에뮬레이터 - 결과 렌더링: 터미널 에뮬레이터는 항상 자신의 PTY 마스터 FD에 읽을 데이터가 있는지 주시하고 있습니다. ls 의 결과 텍스트가 도착하면, 이를 읽어서 화면에 렌더링합니다. 만약 텍스트에 색상 등을 위한 이스케이프 시퀀스가 포함되어 있다면, 이를 해석하여 색깔 있는 텍스트로 표시합니다. 💡 이 시나리오의 핵심: 사용자의 키 입력이 터미널 에뮬레이터 → 커널 드라이버(마스터 측) → 커널 드라이버의 처리(Echo, Buffering) → 커널 드라이버(슬레이브 측) → 셸 순서로 전달되고, 명령어의 결과는 역순으로 사용자에게 돌아오는 가장 기본적인 파이프라인 구조를 보여줌. Case 2: 사용자가 Ctrl+C 를 누를 때 (신호 처리) 상황: 사용자가 셸에서 sleep 100 같은 오래 걸리는 명령을 실행시킨 상태입니다. 이 sleep 프로세스는 현재 터미널 세션의 전면(foreground) 프로세스입니다. 사용자: 명령을 중단시키기 위해 Ctrl 키와 C 키를 동시에 누릅니다. 터미널 에뮬레이터: 이 키 조합이 일반적인 문자 입력이 아니라 특별한 제어 명령임을 인지합니다. 유닉스 터미널 규약에 따라 Ctrl+C 는 INTR (interrupt) 제어 문자에 해당하며, 이는 ASCII 코드 3 ( 0x03 , End of Text)에 매핑됩니다. 터미널 에뮬레이터는 이 0x03 바이트 하나를 PTY 마스터 FD에 씁니다. 커널 (PTY/TTY 드라이버) - 제어 문자 해석: 커널 드라이버는 마스터 측에서 0x03 바이트를 읽습니다. 터미널이 cooked 모드이고, ISIG (Interpret Signals) 플래그가 켜져 있는 상태이므로, 커널은 이 바이트를 일반 데이터로 취급하지 않습니다. 대신, 터미널 설정(termios)을 참조하여 0x03 이 VINTR 문자와 일치함을 확인하고, 이를 \"SIGINT 신호를 생성하라\"는 이벤트로 변환합니다. 커널 (프로세스 관리 모듈): PTY/TTY 드라이버로부터 신호 생성 요청을 받습니다. 커널은 이 PTY 세션과 연결된 \"전면 프로세스 그룹(foreground process group)\"을 찾습니다. 이 그룹에는 현재 명령을 실행하고 있는 sleep 100 프로세스가 포함되어 있습니다. 커널 (신호 전달): 커널은 해당 전면 프로세스 그룹에 속한 모든 프로세스에게 SIGINT 신호를 전달합니다. sleep 프로세스: SIGINT 신호를 수신합니다. sleep 프로그램은 이 신호에 대한 별도의 처리기(handler)를 등록해두지 않았으므로, 신호에 대한 기본 동작(default action)을 수행합니다. SIGINT 의 기본 동작은 \"프로세스 종료\" 입니다. 프로세스 종료: sleep 100 프로세스는 즉시 종료됩니다. 셸: 자식 프로세스( sleep )가 종료되었음을 감지하고, 다음 명령을 입력받기 위해 새로운 프롬프트를 화면에 출력합니다. (이 과정은 Case 1의 7~9번과 유사하게 진행됩니다.) 💡 이 시나리오의 핵심: 터미널 에뮬레이터는 단지 특정 바이트( 0x03 )를 보낼 뿐, 실제 그 바이트를 해석하여 운영체제 수준의 이벤트(신호)로 변환하고, 올바른 대상(전면 프로세스 그룹)에게 전달하는 복잡한 작업의 주체는 전적으로 커널임. Case 3: 사용자가 입력 중 Backspace 로 수정할 때 (입력 편집) 사용자: ls 를 입력하려다가 실수로 lp 를 입력하는 상황을 가정합니다. 키보드로 l , s , Backspace , p , Enter 순으로 누릅니다. 터미널 에뮬레이터: 각 키 입력에 해당하는 바이트들을 순서대로 PTY 마스터 FD에 씁니다. l → 0x6C s → 0x73 Backspace → 0x08 (Backspace 제어 문자) p → 0x70 Enter → 0x0A 커널 (PTY/TTY 드라이버) - 단계별 처리: cooked 모드의 커널 드라이버가 이 바이트 스트림을 순차적으로 처리합니다. l , s 수신: Case 1과 동일하게, l 과 s 를 Echo하여 마스터 FD로 되돌려 보내고, 내부 라인 버퍼에는 ls 를 저장합니다. 터미널 화면에는 ls 가 보입니다. Backspace ( 0x08 ) 수신: 커널 드라이버는 터미널 설정(termios)을 참조하여 0x08 이 VERASE (지우기) 문자와 일치함을 확인합니다. 이를 \"라인 버퍼에서 한 글자 지우기\" 명령으로 해석합니다. 버퍼 수정: 내부 라인 버퍼의 맨 끝에 있던 s 를 삭제합니다. 이제 라인 버퍼의 내용은 l 이 됩니다. 화면 수정 지시: 사용자가 시각적으로도 글자가 지워졌음을 인지할 수 있도록, 화면을 수정하라는 지시를 터미널 에뮬레이터에게 보내야 합니다. 가장 일반적인 방법은 \"커서를 한 칸 뒤로, 그 자리에 공백을 출력, 다시 커서를 한 칸 뒤로\" 라는 동작을 유발하는 제어 시퀀스를 보내는 것입니다. 이 시퀀스는 보통 \\b \\b (바이트 0x08 , 0x20 , 0x08 ) 입니다. 커널 드라이버는 이 세 바이트를 PTY 마스터 FD 쪽으로 써줍니다. 터미널 에뮬레이터 (화면 수정): 마스터 FD에서 \\b \\b 시퀀스를 읽고, 이를 명령으로 해석하여 화면의 커서를 한 칸 뒤로 옮겼다가, 공백을 찍어 s 를 덮어쓰고, 다시 커서를 그 자리로 돌려놓습니다. 이제 화면에는 l 만 보이고 커서는 그 뒤에서 깜빡입니다. p 수신: 커널 드라이버는 p ( 0x70 )를 수신합니다. Echo 기능에 의해 이 바이트를 마스터 FD로 되돌려 보내고, 수정된 라인 버퍼( l ) 뒤에 p 를 추가합니다. 라인 버퍼는 이제 lp 가 됩니다. 터미널 화면에는 lp 가 표시됩니다. Enter 수신: Enter ( 0x0A )를 수신하고, 한 줄 입력이 끝났다고 판단합니다. 최종적으로 확정된 라인 버퍼의 내용 lp 와 \\n 을 합쳐 lp\\n 을 만듭니다. 커널 (PTY 드라이버) -> 셸: 사용자의 모든 오타와 수정 과정은 커널 드라이버 수준에서 모두 처리되었습니다. 셸에게는 오직 최종 결과물인 lp\\n 만 PTY 슬레이브 FD를 통해 전달됩니다. 셸: lp\\n 을 읽고 lp 라는 명령을 실행하려 하지만, 그런 명령이 없으므로 \"command not found\" 같은 오류 메시지를 표준 에러(stderr)로 출력합니다. 이 오류 메시지는 Case 1의 7~9번과 같은 경로를 통해 터미널 화면에 표시됩니다. 💡 이 시나리오의 핵심: 우리가 당연하게 여기는 한 줄 내의 간단한 입력 편집(글자 추가, 삭제)조차 애플리케이션(셸)이 아니라 커널의 TTY 드라이버가 담당하는 중요한 기능임. 셸은 이런 편집 과정을 전혀 알지 못하고, 깨끗하게 정제된 최종 입력 라인만 전달받음. Case 4: read -s 로 비밀번호를 입력할 때 (터미널 모드 변경) 셸: 사용자가 셸 프롬프트에서 read -s password 와 같은 명령이 포함된 스크립트를 실행합니다. 셸은 이 명령을 해석하여 read 내장 명령어나 관련 유틸리티를 실행합니다. read 명령어 (ioctl 호출): read 명령어는 -s (silent) 옵션을 인지합니다. 비밀번호 입력을 화면에 표시하지 않기 위해, 자신의 표준 입력(stdin)인 PTY 슬레이브 파일 디스크립터(FD)에 대해 ioctl() 시스템 콜을 호출합니다. 이 시스템 콜은 커널에게 \"이 터미널의 설정을 변경해달라\"고 요청하는 것입니다. 구체적으로는 현재 터미널 속성을 가져와서 ECHO 플래그를 비활성화한 후, 변경된 속성을 다시 설정합니다. 커널 (PTY/TTY 드라이버): ioctl() 호출을 수신하고, 해당 PTY 슬레이브에 대한 내부 설정에서 ECHO 기능을 끕니다. 이제부터 이 PTY 슬레이브로 들어오는 데이터는 자동으로 PTY 마스터 쪽으로 반사되지 않습니다. 사용자: 화면에 프롬프트가 뜬 상태에서 비밀번호 pass 를 입력합니다. 터미널 에뮬레이터: 키보드 입력을 받아 p , a , s , s 에 해당하는 바이트들을 순서대로 자신이 열고 있는 PTY 마스터 FD에 씁니다(write). 커널 (PTY 드라이버): 마스터 측에 쓰인 p , a , s , s 바이트를 읽습니다. 하지만 현재 이 터미널 세션은 ECHO 가 꺼져 있으므로, 이 바이트들을 다시 마스터 FD 쪽으로 써주지 않습니다. 터미널 에뮬레이터: PTY 마스터 FD로부터 되돌아오는 데이터가 없으므로, 화면에 아무것도 그리지 않습니다. 사용자 눈에는 입력이 안 되는 것처럼 보입니다. 커널 (PTY 드라이버): ECHO 는 꺼졌지만, 여전히 cooked mode 의 다른 기능(Line Buffering)은 활성화되어 있습니다. 따라서 수신한 pass 를 내부 라인 버퍼에 저장합니다. 사용자: 입력이 끝났음을 알리기 위해 Enter 키를 누릅니다. 터미널 에뮬레이터: Enter 키에 해당하는 0x0A 바이트를 PTY 마스터 FD에 씁니다. 커널 (PTY 드라이버): 0x0A 바이트를 수신하고, 한 줄 입력이 끝났다고 판단합니다. 버퍼에 저장해 둔 pass 와 합쳐 pass\\n 이라는 완전한 데이터를 만듭니다. 커널 (PTY 드라이버): 완성된 pass\\n 데이터를 PTY 슬레이브 FD에서 읽을 수 있도록 준비시킵니다. read 명령어: 자신의 표준 입력(stdin)인 PTY 슬레이브 FD에서 읽기(read)를 시도하고, pass\\n 데이터를 가져옵니다. 이 데이터를 password 셸 변수에 저장합니다. read 명령어 (ioctl 복구): 비밀번호 입력 후, ECHO 플래그를 원래 상태로 되돌립니다. 셸 프롬프트 복귀: 사용자 입력이 다시 화면에 표시되는 평상시 상태로 돌아옵니다. 💡 이 시나리오의 핵심: 애플리케이션( read )이 ioctl() 을 통해 커널 드라이버의 동작 모드를 일시적으로 변경하여 터미널의 기본 기능(Echo)을 제어함. Case 5: vim 같은 전체 화면 프로그램을 실행할 때 (Raw 모드와 화면 렌더링) 셸: 사용자가 vim file.txt 를 입력하고 Enter를 누르면, 셸은 vim 프로세스를 생성하고 실행합니다. 이때 vim 의 표준 입출력/에러는 PTY 슬레이브 FD로 연결됩니다. vim (터미널 모드 변경): vim 은 시작과 동시에, 전체 화면을 직접 제어하기 위해 ioctl() 시스템 콜을 호출합니다. 이 호출을 통해 커널 PTY/TTY 드라이버에게 터미널 모드를 cooked 에서 raw 모드로 변경하도록 요청합니다. Raw 모드: ECHO , Line Buffering , 특수 문자(Ctrl+C 등)의 신호 변환, 입력 편집 등 커널이 제공하는 대부분의 편의 기능을 모두 끕니다. 이제 키 입력은 발생 즉시, 아무런 가공 없이 vim 프로세스에 전달됩니다. vim (화면 초기화): vim 은 전체 화면을 새로 그려야 합니다. 이를 위해 터미널을 제어하는 이스케이프 시퀀스들을 생성하여 자신의 표준 출력(stdout)인 PTY 슬레이브 FD에 씁니다. 예시 시퀀스: \\033[?1049h (대체 화면 버퍼 사용), \\033[2J (화면 전체 지우기), \\033[H (커서 홈 위치로 이동) 등. 커널 (PTY 드라이버): 슬레이브 측에 쓰인 이스케이프 시퀀스들을 읽습니다. raw 모드이므로 특별한 해석 없이 그대로 PTY 마스터 FD 쪽으로 전달합니다. 터미널 에뮬레이터: PTY 마스터 FD에서 이스케이프 시퀀스들을 읽습니다. 이 바이트들을 화면에 문자로 출력하는 대신, 명령으로 해석하여 실행합니다. (화면을 지우고, 대체 버퍼를 활성화하는 등) vim (내용 렌더링): vim 은 파일 내용, 상태 표시줄, 줄 번호 등을 포함한 텍스트를 계산하여 마찬가지로 PTY 슬레이브 FD에 씁니다. 이 데이터 역시 커널 드라이버를 거쳐 터미널 에뮬레이터로 전달되고 화면에 그려집니다. 사용자: j 키를 눌러 커서를 아래로 이동시킵니다. 터미널 에뮬레이터: j 키 입력을 받아 바이트( 0x6A )를 PTY 마스터 FD에 씁니다. 커널 (PTY 드라이버): raw 모드이므로 0x6A 바이트를 읽자마자 아무런 버퍼링이나 해석 없이 즉시 PTY 슬레이브 FD에서 읽을 수 있도록 준비시킵니다. vim : 슬레이브 FD에서 0x6A 바이트를 즉시 읽고, 이를 \"커서를 한 줄 아래로 이동\" 명령으로 내부적으로 해석합니다. vim (화면 업데이트): vim 은 화면의 변화를 최소화하는 방식으로 필요한 업데이트를 계산합니다. 이 경우, 단지 커서 위치만 바꾸면 되므로, 커서 이동 이스케이프 시퀀스(예: \\033[11;5H - \"11행 5열로 이동\")를 생성하여 PTY 슬레이브 FD에 씁니다. 커널 (PTY 드라이버): 이 시퀀스를 그대로 PTY 마스터 FD 쪽으로 전달합니다. 터미널 에뮬레이터: 마스터 FD에서 커서 이동 시퀀스를 읽고, 명령으로 해석하여 화면에 보이는 커서를 실제로 이동시킵니다. 💡 이 시나리오의 핵심: raw 모드에서 커널 드라이버는 단순 중계자 역할로 바뀌고, 애플리케이션( vim )과 터미널 에뮬레이터가 이스케이프 시퀀스를 통해 화면의 모든 요소를 직접 제어함. Case 6: 위쪽 화살표 키로 히스토리 검색할 때 (애플리케이션의 시퀀스 해석) 사용자: 셸 프롬프트가 떠 있는 상태에서 위쪽 화살표(↑) 키를 누릅니다. 터미널 에뮬레이터: 위쪽 화살표 키가 단일 ASCII 문자가 아니라는 것을 인지합니다. 터미널 종류(예: xterm)에 따라 미리 약속된 이스케이프 시퀀스로 변환합니다. 가장 일반적인 시퀀스는 \\033[A 입니다. 이 세 바이트( 0x1B , 0x5B , 0x41 )를 PTY 마스터 FD에 씁니다. 커널 (PTY/TTY 드라이버): 마스터 측에서 이 세 바이트를 차례로 읽습니다. cooked 모드이지만, \\033[A 는 커널이 특별히 신호로 바꾸거나 편집 명령으로 해석하는 시퀀스가 아닙니다. 따라서 이 바이트들을 일반적인 데이터로 취급하여 내부 라인 버퍼에 추가합니다. 참고: Enter 가 눌리지 않았으므로 아직 셸에게 전달되지는 않습니다.* 커널 (PTY 드라이버): (이 부분은 셸의 설정에 따라 다릅니다. 대부분의 현대 셸은 아래와 같이 동작합니다.) bash 나 zsh 같은 셸은 readline 라이브러리를 사용하며, 이 라이브러리는 효율적인 상호작용을 위해 시작 시 ioctl 로 터미널을 canonical (cooked) 모드가 아닌, 약간 변형된 모드로 설정할 수 있습니다. 이 경우, \\033[A 같은 시퀀스가 입력되면 라인 버퍼링을 거치지 않고 즉시 셸에게 전달될 수 있습니다. 여기서는 더 일반적인 canonical 모드를 가정하고, 셸이 어떻게든 이 데이터를 읽는다고 가정하고 진행하겠습니다. (실제로는 read 시스템 콜이 바이트 단위로 읽을 수 있습니다.) 셸: PTY 슬레이브 FD를 통해 \\033[A 라는 바이트 시퀀스를 읽습니다. 셸 (readline 라이브러리): 셸(또는 셸이 사용하는 readline 라이브러리)은 이 바이트 시퀀스를 해석합니다. \\033 으로 시작하는 것을 보고 이스케이프 시퀀스임을 인지하고, 뒤따르는 [A 를 \"이전 히스토리\" 명령으로 매핑된 테이블에서 찾습니다. 셸 (명령 실행): \"이전 히스토리\" 명령을 실행합니다. 히스토리 파일( ~/.bash_history 등)을 참조하여 가장 최근의 명령어(예: vim file.txt )를 가져옵니다. 셸 (화면 업데이트): 셸은 사용자에게 현재 입력 줄이 이전 명령어로 대체되었음을 보여줘야 합니다. 이를 위해 터미널 제어 시퀀스를 조합하여 PTY 슬레이브 FD에 씁니다. \\r (Carriage Return): 커서를 줄의 맨 앞으로 이동. \\033[K (Erase in Line): 커서 위치부터 줄 끝까지 내용 지우기. vim file.txt : 가져온 히스토리 텍스트. 커널 (PTY 드라이버): 슬레이브에 쓰인 이 시퀀스들과 텍스트를 읽어 PTY 마스터 FD 쪽으로 전달합니다. 터미널 에뮬레이터: 마스터 FD에서 이 데이터 스트림을 읽습니다. \\r 과 \\033[K 는 명령으로 해석하여 실행하고(커서를 옮기고 줄을 지움), vim file.txt 는 텍스트로 인식하여 화면에 그립니다. 💡 이 시나리오의 핵심: 화살표 키와 같은 특수 키 입력은 터미널 에뮬레이터가 이스케이프 시퀀스로 변환하고, 이 시퀀스의 의미를 해석하여 특정 동작(히스토리 검색)을 수행하는 것은 커널이 아닌 애플리케이션(셸)의 책임임. 추가 => TTY? PTY? 커널의 TTY 서브시스템 (TTY Subsystem / TTY Core) 이것이 흔히 사람들이 \"커널의 TTY 드라이버\"라고 넓은 의미로 말할 때 가리키는 대상입니다. TTY 서브시스템은 특정 하드웨어나 PTY에 종속되지 않는, 터미널의 핵심 동작 로직을 담고 있는 커널의 공통 모듈입니다. 역할: 터미널의 \"두뇌\" 또는 \"공통 로직\"입니다. 주요 기능 (Line Discipline): 입력 편집 (Line Buffering): cooked 모드에서 줄 단위로 입력을 모으고, Backspace 등을 처리합니다. 문자 에코 (Echo): 입력된 문자를 되돌려 보내 화면에 보이게 합니다. 신호 생성 (Signal Generation): Ctrl+C 같은 제어 문자를 SIGINT 같은 실제 신호로 변환합니다. 흐름 제어 (Flow Control): 데이터가 너무 빨리 오고 갈 때 이를 조절합니다. (주로 시리얼 통신에서 중요) 문자 변환: Enter 키를 \\n (LF) 또는 \\r\\n (CRLF)으로 변환하는 등의 규칙을 적용합니다. 특징: 이 서브시스템은 추상화되어 있습니다. 데이터가 실제 물리적인 시리얼 포트에서 오는지, 가상 터미널인 PTY에서 오는지는 신경 쓰지 않습니다. 그저 자신에게 데이터를 넣어주고, 자신이 처리한 데이터를 가져갈 하위 드라이버가 있기만 하면 됩니다. 커널의 PTY 드라이버 (PTY Driver) PTY 드라이버는 TTY 서브시스템에 연결되는 여러 종류의 하위 드라이버 중 하나입니다. 실제 하드웨어를 제어하는 대신, 소프트웨어적으로 터미널 장치를 흉내 내는 역할을 전문적으로 수행합니다. 역할: TTY 서브시스템을 위한 \"가상 하드웨어 드라이버\"입니다. 주요 기능: 마스터-슬레이브 쌍 생성: ioctl 등을 통해 터미널 에뮬레이터가 사용할 마스터(Master) FD와 셸이 사용할 슬레이브(Slave) FD라는 한 쌍의 통신 채널을 만듭니다. 데이터 중계: 마스터 측에서 온 데이터(터미널 에뮬레이터의 입력)를 TTY 서브시스템으로 밀어 넣습니다. TTY 서브시스템이 처리를 마친 데이터(셸에게 갈 데이터)를 슬레이브 측에서 읽을 수 있게 해줍니다. 슬레이브 측에서 온 데이터(셸의 출력)를 마스터 측으로 그대로 전달합니다. 특징: PTY 드라이버 자체는 Echo 나 Line Buffering 같은 복잡한 로직을 가지고 있지 않습니다. 이 모든 작업은 TTY 서브시스템에 위임합니다. PTY 드라이버의 핵심 임무는 오직 가상의 데이터 통로를 만들고 유지하는 것입니다. 둘의 관계와 협력 방식 구분 커널의 TTY 서브시스템 (Line Discipline) 커널의 PTY 드라이버 역할 터미널의 핵심 동작 규칙(두뇌) 가상의 터미널 장치(입출력 통로) 핵심 기능 Echo, Line Buffering, Signal 생성 마스터-슬레이브 쌍 생성 및 데이터 중계 추상화 수준 높음 (하드웨어 독립적) 낮음 (TTY 서브시스템에 데이터를 공급) 관계 상위 모듈. PTY 드라이버의 서비스를 받음 하위 모듈. TTY 서브시스템의 서비스를 이용함 사용자가 ls 를 입력하는 과정을 이 둘의 관계로 다시 보면 다음과 같습니다. 터미널 에뮬레이터가 l , s 바이트를 PTY 마스터 FD에 씁니다. PTY 드라이버가 이 데이터를 감지합니다. PTY 드라이버는 이 데이터를 \"가상 장치에서 입력이 들어왔다\"고 알리며 TTY 서브시스템으로 전달합니다. TTY 서브시스템은 cooked 모드 규칙에 따라 다음을 수행합니다. l , s 를 Echo하기 위해 다시 PTY 드라이버에게 \"이 데이터를 마스터 쪽으로 보내라\"고 지시합니다. 내부 라인 버퍼에 ls 를 저장합니다. (Enter 입력 후) TTY 서브시스템은 완성된 ls\\n 을 PTY 드라이버에게 \"이 데이터를 슬레이브 쪽에서 읽을 수 있게 하라\"고 전달합니다. 셸이 PTY 슬레이브 FD에서 ls\\n 을 읽어갑니다. : 셸에서 이벤트가 출발하지 않는 경우가 있을 수 있다 ex)시그널",
      "frontmatter": {
        "tags": [
          "잡지식"
        ],
        "date": "2025-08-10T22:00:03+09:00",
        "lastmod": "2025-08-11T02:58:32+09:00"
      }
    },
    "터미널 동작 원리 매우 상세하게2 (디버깅)": {
      "path": "/02.inbox/터미널-동작-원리-매우-상세하게2-디버깅/",
      "filename": "터미널 동작 원리 매우 상세하게2 (디버깅)",
      "content": "각 케이스별로 터미널의 동작을 디버깅하고 특히 PTY 마스터-슬레이브 간의 통신을 실시간으로 확인하는 방법 디버깅을 위한 핵심 도구 (Toolbox) 우리가 사용할 주요 도구는 다음과 같습니다. 각 도구의 역할을 이해하면 어떤 상황에 무엇을 써야 할지 명확해집니다. 도구 핵심 기능 무엇을 볼 수 있는가? strace 시스템 콜 추적 프로세스(셸, vim)가 커널과 어떤 상호작용을 하는지 ( read , write , ioctl 호출 및 주고받는 데이터)를 정확히 보여줍니다. 슬레이브 측 통신 확인에 최적화되어 있습니다. script 터미널 세션 기록 터미널 에뮬레이터와 PTY 마스터 사이의 모든 바이트 스트림(입력/출력)을 날것 그대로 파일에 기록합니다. 마스터 측 통신 확인에 완벽합니다. socat 만능 데이터 중계기 두 개의 통신 채널을 엮고 그 사이를 흐르는 데이터를 엿볼 수 있습니다. PTY를 직접 생성하여 마스터-슬레이브 양단의 모든 트래픽을 실시간 Hex 덤프로 확인하는 궁극의 방법입니다. lsof 열린 파일 목록 확인 특정 프로세스가 어떤 파일 디스크립터(PTY 슬레이브 포함)를 열고 있는지 확인하는 데 사용합니다. 실전! 디버깅 시나리오: strace 로 셸(Slave) 관찰하기 이 방법은 셸(bash)이 PTY 슬레이브와 어떻게 상호작용하는지를 가장 직접적으로 보여줍니다. 1단계: 환경 준비 두 개의 터미널 창을 엽니다. 터미널 1 (실행용): 우리가 ls , Ctrl+C 등을 입력할 터미널입니다. 터미널 2 (관찰용): strace 를 실행하여 터미널 1의 셸을 감시할 터미널입니다. 2단계: 추적 대상 찾기 터미널 1에서 다음을 입력하여 현재 셸의 프로세스 ID(PID)와 PTY 슬레이브 장치 파일을 확인합니다. # 현재 셸의 PID 확인 echo $$ # 출력 예시: 24567 # 현재 터미널의 PTY 슬레이브 장치 파일 확인 tty # 출력 예시: /dev/pts/3 이제 우리는 PID 24567 를 가진 bash 프로세스가 /dev/pts/3 을 통해 통신한다는 것을 알았습니다. 3단계: strace 실행 터미널 2 (관찰용)에서 다음 명령을 실행하여 터미널 1의 셸에 strace 를 붙입니다. sudo 가 필요할 수 있습니다. # -p [PID]: 특정 프로세스에 연결 # -e trace=read,write,ioctl: read, write, ioctl 시스템 콜만 추적 # -s 100: 문자열 데이터는 최대 100자까지 표시 # -xx: 문자열이 아닌 데이터는 Hex(16진수)로 표시 sudo strace -p 24567 -e trace=read,write,ioctl -s 100 -xx 이제 strace 가 터미널 1의 셸을 감시하기 시작합니다. strace 는 read(0, ...) 와 같이 셸이 입력을 기다리며 멈춰있을 겁니다. (여기서 FD 0 은 셸의 표준 입력, 즉 /dev/pts/3 입니다.) 4단계: 케이스별 관찰 (터미널 1에서 입력) 이제 터미널 1에서 여러 가지 입력을 해보고 터미널 2에 나타나는 strace 출력을 관찰합시다. ▶️ Case 1: ls 를 입력하고 Enter 터미널 1: l , s , Enter 를 차례로 입력합니다. 터미널 2 ( strace 출력): # 커널의 라인 버퍼링 때문에 'ls'와 '\\n'이 한번에 전달됨 read(0, \"ls\\n\", 100) = 3 # ... fork, execve(\"ls\") 등 ... # ls의 결과가 PTY 슬레이브를 통해 터미널 에뮬레이터로 출력됨 write(1, \"Desktop Downloads Music\\n\", 28) = 28 # 셸 프롬프트가 다시 출력됨 write(2, \"$ \", 2) = 2 핵심 관찰: l 이나 s 를 입력할 때는 read 가 호출되지 않습니다. 커널(TTY 드라이버)이 Enter 가 올 때까지 버퍼링하다가 ls\\n 을 한 번에 셸에게 전달합니다. 이것이 cooked 모드의 증거입니다. ▶️ Case 2: 입력 중 Backspace 사용 터미널 1: l , p , Backspace , s , Enter 를 입력합니다. 터미널 2 ( strace 출력): read(0, \"ls\\n\", 100) = 3 핵심 관찰: strace 결과는 Case 1과 완전히 동일합니다. 셸은 p 를 입력했는지, Backspace 로 지웠는지 전혀 모릅니다. 커널의 TTY 드라이버가 입력 편집을 모두 처리하고 최종 결과인 ls\\n 만 셸에게 전달했기 때문입니다. ▶️ Case 3: Ctrl+C 누르기 터미널 1: sleep 100 을 실행한 뒤, 바로 Ctrl+C 를 누릅니다. 터미널 2 ( strace 출력): # sleep 100 실행 부분 생략 ... # Ctrl+C를 누르면, 셸은 0x03 바이트를 read 하는 것이 아니라, # SIGINT 시그널을 받는다! --- SIGINT {si_signo=SIGINT, si_code=SI_KERNEL} --- # 시그널을 받은 후, 셸은 다시 프롬프트를 찍는다. write(2, \"\\n\", 1) = 1 write(2, \"$ \", 2) = 2 read(0, 핵심 관찰: read 시스템 콜로 0x03 바이트가 들어오는 것이 아니라, --- SIGINT --- 라는 메시지가 뜹니다. 이는 커널이 Ctrl+C 에 해당하는 바이트를 해석하여 셸 프로세스에 SIGINT 시그널을 보냈음을 명확히 보여줍니다. ▶️ Case 5: vim 실행 (Raw 모드) 터미널 1: vim 을 실행합니다. 터미널 2 ( strace 출력): # vim이 시작하자마자 터미널 설정을 바꾸기 위해 ioctl을 호출한다. # TCSETSW는 \"지금 바로 설정을 바꿔라\"는 의미. # c_lflag에서 ICANON(Canonical 모드), ECHO(에코) 등이 꺼진 것을 볼 수 있음. ioctl(0, TCSETSW, {c_iflag=ICRNL|IXON, c_oflag=OPOST|ONLCR, c_cflag=B38400|CS8|CREAD, c_lflag=ISIG|IEXTEN, ...}) # 화면을 그리기 위해 수많은 이스케이프 시퀀스를 write 한다. write(1, \"\\33[?2004h\\33[?1049h\\33[22;0;0t\\33[1;24r\\33[m\\33[H\\33[2J...\", 200) = 200 터미널 1 (vim 내부): j 키를 한 번 눌러 커서를 아래로 이동합니다. 터미널 2 ( strace 출력): # Raw 모드이므로 'j' 키를 누르자마자 read가 즉시 반환됨. read(0, \"j\", 16) = 1 # vim은 'j' 입력에 대한 반응으로, 커서를 이동시키는 이스케이프 시퀀스를 출력. write(1, \"\\33[11;5H\", 6) = 6 핵심 관찰: ioctl 호출로 터미널이 raw 모드로 변경되는 것을 확인했습니다. j 키 하나를 누르자마자 read 가 바로 반환됩니다. 라인 버퍼링이 꺼졌다는 증거입니다. vim 은 커서 이동 같은 간단한 동작조차 이스케이프 시퀀스를 write 하여 직접 처리합니다. 심화 과정: script 로 마스터 측 통신 훔쳐보기 strace 는 셸(슬레이브) 관점의 훌륭한 도구지만, 커널이 편집해주는 Backspace 나 에뮬레이터가 보내는 화살표 키의 실제 바이트는 볼 수 없습니다. 이때 script 를 사용합니다. 터미널 1에서 다음을 실행합니다. # -t 옵션은 시간 정보를 timing.log에 기록 # session.log 파일에 모든 바이트 스트림을 기록 script -t timing.log session.log 이제 script 가 새로운 셸을 실행시키고, 이 세션에서 일어나는 모든 입출력을 기록하기 시작합니다. 여기서 다음을 순서대로 입력해보세요. l , p , Backspace , s , Enter 위쪽 화살표(↑) 키 Ctrl+C exit 를 입력하여 script 종료 이제 session.log 파일을 hexdump 로 열어보면 날것 그대로의 데이터가 보입니다. hexdump -C session.log 아마 다음과 유사한 내용을 볼 수 있을 겁니다. # ... 프롬프트 ... # 'l', 'p', 'Backspace', 's', '\\r' (Enter) 6c 70 08 73 0d # 'Backspace'에 대한 커널의 화면 수정 응답: \\b \\b 08 20 08 # ... ls 결과 ... # 위쪽 화살표 키 입력 1b 5b 41 # 셸이 히스토리를 찾아 화면에 그려주는 응답 0d 1b 5b 4b 6c 73 ... # Ctrl+C 입력 03 # ... exit 입력 ... 핵심 관찰: 사용자가 누른 Backspace ( 0x08 )와 화살표 키( 1b 5b 41 )가 PTY 마스터에 기록된 것을 볼 수 있습니다. strace 로는 볼 수 없었던 정보입니다. scriptreplay timing.log session.log 명령으로 당시 상황을 영상처럼 다시 재생해볼 수도 있습니다. 끝판왕: socat 으로 실시간 양방향 트래픽 감시하기 이 방법은 가장 복잡하지만 마스터와 슬레이브 사이를 오가는 모든 데이터를 실시간으로 보여줍니다. 터미널 2 (관찰용)에서 socat 으로 가짜 PTY를 생성하고 중간에서 데이터를 감시합니다. # 두 개의 PTY 쌍을 만들고, 그 사이를 오가는 모든 데이터를 hex(-x)로 출력 # 터미널 1은 /tmp/my-pty1에, 터미널 2의 셸은 /tmp/my-pty2에 연결될 것임 socat -d -d -x PTY,link=/tmp/my-pty1,raw,echo=0 PTY,link=/tmp/my-pty2,raw,echo=0 이제 새 터미널(터미널 3)을 열고, 셸을 PTY 슬레이브 쪽에 연결합니다. # stty raw -echo: 이 터미널 자체의 cooked 모드를 꺼서 socat에 방해되지 않게 함 # exec bash < /tmp/my-pty2 > /tmp/my-pty2 2>&1: bash의 입출력을 PTY 슬레이브로 리다이렉션 stty raw -echo; exec bash < /tmp/my-pty2 > /tmp/my-pty2 2>&1 터미널 1에서는 터미널 에뮬레이터 역할을 할 프로그램을 PTY 마스터 쪽에 연결합니다. socat 을 또 사용하면 편리합니다. socat - \"file:/tmp/my-pty1,raw,echo=0\" 이제 터미널 1에서 키보드를 입력하면, 그 데이터가 터미널 2의 socat 감시 화면에 보이고, 처리된 결과가 터미널 3의 bash 로 전달됩니다. bash 의 출력 역시 터미널 2를 거쳐 터미널 1에 보이게 됩니다. 이로써 완벽한 Man-in-the-middle 감시 환경이 구축되었습니다. 이 방법을 통해 제공해주신 문서의 모든 데이터 흐름도를 눈으로 직접 확인할 수 있습니다.",
      "frontmatter": {
        "tags": [
          "잡지식"
        ],
        "date": "2025-08-11T02:53:22+09:00",
        "lastmod": "2025-08-11T02:53:56+09:00"
      }
    },
    "함수포인터": {
      "path": "/02.inbox/함수포인터/",
      "filename": "함수포인터",
      "content": "int foo(){ // foo 코드는 메모리 주소 0x002717f0에서 시작한다. return 5; } int main(){ printf(\"%d\\n\",foo()); // 주소 0x002717f0로 점프한다. 5 출력 printf(\"%p\\n\",foo); // 주소를 출력 return 0; } 코드에서 보듯 함수의 이름은 함수의 주소를 가르킨다 int a\\[7\\] 에서 a 가 주소를 가리키듯 int add(int a, int b){return a + b;} //덧셈함수 int sub(int a, int b){return a - b;} //뺄셈함수 int mul(int a, int b){return a * b;} //곱셈함수 int div(int a, int b){return a / b;} //나눗셈함수 int main() { int (*fp[4])(int, int); //함수 포인터 배열 선언 fp[0] = add; // 배열[1]에 덧셈 함수의 메모리 주소 저장 fp[1] = sub; // 배열[2]에 뺄셈 함수의 메모리 주소 저장 fp[2] = mul; // 배열[3]에 곱셈 함수의 메모리 주소 저장 fp[3] = div; // 배열[4]에 나눗셈 함수의 메모리 주소 저장 for (int i = 0; i < 4; i++) { printf(\"배열[%d](20,%2010)); } return 0; } typedef int (*PtrFunc)(int, int) PtrFunc fp = NULL; fp = add;",
      "frontmatter": {
        "tags": [
          "c",
          "cpp",
          "language"
        ],
        "date": "2023-12-22T11:30:00+09:00",
        "lastmod": "2023-12-22T11:30:00+09:00"
      }
    },
    "진화 심리학": {
      "path": "/02.inbox/onenote/책/진화-심리학/",
      "filename": "진화 심리학",
      "content": "이기적 유전자 - 리처드 도킨스 에덴의 강 - 리처드 도킨스 인류의 기원 - 리처드 리키 놀라운 가설 - 프랜시스 크릭 스스로 치유하는 뇌 - 노먼 도이지 색스의 진화 - 제러드 다이아몬드 진화란 무엇인가 - 에른스트 마이어 진화의 미스터리 - 조지 윌리엄스 사회생물학 - 에드워드 윌슨 이타적 유전자 - 매트 리들리 지리의 힘 - 팀 마샬 근대세계체제 - 이매뉴얼 월러스틴 질투의 민낯 - 지그리트 엥겔브레히트 성 정치학 - 케이트 밀레트 인간의 조건 - 한나 아렌트 가족~의 기원 - 프리드리히 엥겔스 나zi시대의 일상사 - 데틀레프 포이케르트 영국 여성운동사 - 실라 로우버덤 사피엔스 - 유발 하라리 호모 데우스 - 유발 하라리 군중심리 - 귀스타브 르 봉 행정의 공개성과 정치지도자 선출 외 - 막스 베버 군주론 & 로마사 평론 - 마키아벨리",
      "frontmatter": {}
    },
    "철학": {
      "path": "/02.inbox/onenote/책/철학/",
      "filename": "철학",
      "content": "실존주의 시지프 신화 - 알베르 카뮈 변신 시골의사 - 프란츠 카프카",
      "frontmatter": {}
    },
    "css 복합 선택자 패턴 우선 적용 순위 규칙": {
      "path": "/02.inbox/onenote/html-and-css/css-복합-선택자-패턴-우선-적용-순위-규칙/",
      "filename": "css 복합 선택자 패턴 우선 적용 순위 규칙",
      "content": "",
      "frontmatter": {}
    },
    "rooting": {
      "path": "/02.inbox/onenote/html-and-css/리눅스/rooting/",
      "filename": "rooting",
      "content": "All comments before this are misleading. ==Mobile SoC boot process is more complicated than PC boot process. Android phones have 3 stages with different bootloaders. Basically, first two stages are inaccessible as they are fused by the developer in the production mode (which phones are released in). Third one is the one stored in androids boot partition. Roughly, the bootloader hash key should match the one hardcoded into the SoC. First stage bootloader checks the hash key and passes the control to the second stage bootloader. In case of mismatch, the phone goes to the first stage bootloader which boots the recovery mode (EDL, or emergency download mode). In case of match, the second stage bootloader is placed in EFS filesystem (inaccessible in production mode), which is similar to BIOS - it contains all the configuration to initialize all the hardware components. It's called linuxloader bootloader (you can dig it out from xbl.elf of your phone's firmware) which boots one of the EFS partitions, containing the images, you may see on your phone in /dev/block/by-name directory (path can differ). These images contain the main partitions that you flash with an android firmware - boot, recovery, system, vendor, modem, etc. There are also other partitions, but they are not accessible. For example, toolsfv partition in development phones are used to open boot menu, which allows accessing shell, disabling secure boot, booting from usb, etc. You may find the description in Google. Second stage bootloader also provides access to the hypervisor (virtualization, that allows running virtual machines using kvm). Third stage bootloader is simply an android bootloader, which boots your lovely android OS.== ==Why users don't have the access to the 1st and 2nd stage bootloaders? To the first one — for security reasons and to prevent the phone from becoming a brick. You simply cannot kill the phone flashing your android firmwares, even the wrong and corrupted ones. To the second one — security, i guess. Or Google's android monopoly? (Apple doesn't look like the only bad guy, yeah?).== ==Anyways, you can't access anything above third stage bootloader in the production mode so don't bother. Technically, you can build a bootloader with closed source hardware PE32 drivers, configure them and make a nice looking BIOS menu. Practically — good luck finding closed source drivers and configuration files and using them (it's illegal, you know). There are several attempts you might have heard of until now but this will never reach the mass production. The only way I guess is to build your on phone with your own developed SoC using open source solutions.== ==All hail the corporations! Peace.==",
      "frontmatter": {}
    },
    "vimrc 파일": {
      "path": "/02.inbox/onenote/html-and-css/리눅스/vimrc-파일/",
      "filename": "vimrc 파일",
      "content": "\" 번들 선언부 set rtp+=~/.vim/bundle/Vundle.vim call vundle#begin() \" 번들 플러그인 관리 파일 Plugin 'VundleVim/Vundle.vim' \" Keep Plugin commands between vundle#begin/end. Plugin 'vim-airline/vim-airline' Plugin 'vim-airline/vim-airline-themes' Plugin 'The-NERD-Tree' Plugin 'terryma/vim-multiple-cursors' Plugin 'terryma/vim-smooth-scroll' Plugin 'Raimondi/delimitMate' Plugin 'SirVer/ultisnips' Plugin 'honza/vim-snippets' Plugin 'Syntastic' \"번들 선언 종료 지시자 call vundle#end() \" required filetype plugin indent on \" required https://myeongjae.kim/blog/2017/07/19/vimlinux-10-deoplete%EA%B3%BC-clang_complete-%EC%9E%90%EB%8F%99-%EC%99%84%EC%84%B1-%ED%94%8C%EB%9F%AC%EA%B7%B8%EC%9D%B8 https://kamang-it.tistory.com/entry/vivimvi%EC%97%90%EC%84%9C-%ED%94%8C%EB%9F%AC%EA%B7%B8%EC%9D%B8-%EA%B4%80%EB%A6%AC-vundle .vimrc",
      "frontmatter": {}
    },
    "00.초기 설정": {
      "path": "/02.inbox/real-my-sql/00.초기-설정/",
      "filename": "00.초기 설정",
      "content": "서버 $ apt install mysql-server, mysql-common update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode Setting up mysql-server-8.0 (8.0.40-0ubuntu0.22.04.1) ... update-alternatives: using /etc/mysql/mysql.cnf to provide /etc/mysql/my.cnf (my.cnf) in auto mode Renaming removed key_buffer and myisam-recover options (if present) mysqld will log errors to /var/log/mysql/error.log mysqld is running as pid 13348 Created symlink /etc/systemd/system/multi-user.target.wants/mysql.service → /lib/systemd/system/mysql.service. Setting up mysql-server (8.0.40-0ubuntu0.22.04.1) ... Processing triggers for man-db (2.10.2-1) ... Processing triggers for libc-bin (2.35-0ubuntu3.8) ... 동작 확인 : nc -zv localhost(host/ip) mysql(port/service name) mysql 서버 업그레이드 인프레이스 업그레이드 : 물리적 파일을 그대로 두고 업그레이드 논리적 업그레이드 : 덤프(mysqldump) 시스템 변수 : show GLOBAL VARIABLES 적용 범위에 따른 구분: 글로벌 변수(전체 적용, my.cnf를 사용해 영구적 변경가능)와 세션 변수(세션별로 적용) both(전체 기본값은 설 global : 영구 변경, 모든 세션에 동등하게 영향끼침 session : 각 세션별 변경가능 both : 영구 변경가능하면서 세션별로 따로 설정 가능 정적 변수 동적 변수 : 서버가 동작중인 상태에서 변경가능 여부 : 정적변수my.cnf 를 변경해야 정적으로 변경되고(즉 재시작) 동적변수 : set 명령으로 동적으로 변경할 수 있다 db 전용 서버 설정 [mysqld] server—id=l user=mysql datadir=/data/mysql/ default_storage_engine=innodb defaul_tmp_storage_engine=innodb table_open_cache=30000 table_open_cache_instances=16 open-fi1es-limit=65535 default—time-zone='+09:00' socket=/tmp/mysq local_infi1e=0FF block_encryption_mode='aes-256-ecb' core_file innodb_buffer_pool_in_core_file=OFF max_allowed_packet=67108864 explicit_defaults_for_timestamp=ON sql-mode= \"STRICT_TRANS_TABLES,NO_ZERO_IN_DATE, NO_ZERO_DATE, ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE SUBSTITUTION\" character-set-server-utf8mb4 character-set-filesystem-utf8mb4 collation_server-utf8mb4_0900_ai_ci skip-character-set-client-handshake max_connections=8000 max_connect_errors=999999 activate_all_roles_on_login=1 skip-name-resolve ngram_token_size=2 max_heap_table_size=10M tmp_table_size=10M tmpdir=/data/mytmp/ secure-file-priv=/data/securefile/ default_password_lifetime=0 sysdate-is-now ### InnoDB innodb_sort_buffer_size=64M innodb_data_home_dir=/data/mysql/ innodb_data_file_path=ibdata1:100M:autoextend innodb_temp_data_file_path=ibtmp1:12M: autoextend innodb_log_group_home_dir=/log/innodb-log innodb_log_files_in_group=3 innodb_log_file_size=2048M innodb_file_per_table=ON innodb_undo_directory=/log/innodb-undo/ innodb_rollback_segments=64 innodb_undo_tablespaces=2 innodb_max_undo_log_size=536870912 innodb_undo_log_truncate ON innodb_status_output_locks=ON innodb_print_all_deadlocks-ON innodb_adaptive_hash_index=OFF innodb_buffer_pool_size=206 innodb_buffer_pool_instances=10 innodb_doublewrite=OFF innodb_checksun_algorithm=CRC32 innodb_flush_log_at_trx_commit=0 innodb_flush_method=0_DIRECT_NO_FSYNC innodb_io_capacity=1000 innodb_io_capacity_max=5000 innodb_ft_enable_stopword-OFF innodb_cmp_per_index_enabled=ON ### Performance schema performance_schema=ON performance-schema-instrument=\"stage/%=ON\" performance-schema-instrument='memory/%=ON' performance-schema-instrument='wait/%=ON' performance-schema-consumer-events_stages_current=ON performance-schema-consumer-events_stages_history=ON performance-schema-consumer-events_stages_history_long=ON performance-schema-consumer-events_statements_history-OFF performance-schema-consumer-events_statements_history_long=ON performance-schema-consumer-events_waits_current=ON performance-schema-consumer-events_waits_history=ON performance-schena-consumer-events_waits_history_long=ON performance_schema_events_stages_history_long_size=50000 performance_schema_events_stages_history_size=10 performance_schema_events_statements_history_long_size=50000 performance_schena_events_statements_history_size=10 performance_schema_events_waits_history_long_size=50000 performance_schema_events_waits_history_size=10 ### TDE (Encryption) early-plugin-load-keyring_file.so keyring_file_data = /data/tde/tde_master.key ### Password validate password_history=5 validate_password.length=8 validate_password.mixed_case_count=2 validate_password.number_count=2 validate_password. special_char_count=2 validate_password.dictionary_file-prohibitive_dictionary.data validate_password.policy=STRONG ### MySQL BinLog log-bin=/log/mysql-bin/mysql-bin sync_binlog=0 enforce_gtid_consistency=ON gtid-mode=ON binlog_checksum=CRC32 binlog_order_commits=ON binlog_format=ROW binlog_row_image=MINIMAL max_binlog_size=104857600 ### MySQL Replica Options slave_parallel_type=LOGICAL_CLOCK slave_parallel_workers=4 slave_preserve_commit_order=1 binlog_rows_query_log_events=ON log_slave_updates ### Relay Log relay-log-/log/relay-bin/relay-bin relay_log_info_repository=TABLE relay_log_recovery=ON relay_log_purge=ON ### MySQL ErrorLog log-error-/log/mysql-err.log log_error_verbosity=1 ### MySQL Slow Log slow-query-log=1 long_query_time=1 log_slow_extra=1 log_slow_admin_statements=1 log_slow_slave_statements=1 slow_query_log_file=/log/mysql-slow.log ### MySQL Log Expire binlog_expire_logs_seconds=259200 log-raw log_timestamps=SYSTEM [client] socket /tmp/mysql.sock 계정 SUSTEM_USER 권한을 부여하면 시스템계정 이외에는 일반계정 Pasted image 20241213194472 비밀번호 유효성검사 INSTALL COMPONENT 'file://component_validate_password'; UNINSTALL COMPONENT 'file://component_validate_password'; 권한 GRANT (권한 목록) ON (객체) TO (유저); SHOW GRANTS FOR 'username'@'host'; REVOKE ALL PRIVILEGES ON *.* FROM 'username'@'host'; 객체는 데이터베이스, table 이 될수 있다 정적권한을 줄때는 객체를 명시하지 말아야 한다 정적권한Pasted image 20241213210234 Pasted image 20241213210216 동적권한Pasted image 20241213211014 Pasted image 20241213232841 기본적으로 role 은 활성화 되지 않음: activate_all_roles_on_login=ON mysql.default_roles , mysql.role_edges mysql.user 의 account_locked 속성이 Y 이면 role, N 이면 user",
      "frontmatter": {
        "series": "real my sql(8.0)",
        "date": "2024-12-13T15:00:00+09:00",
        "lastmod": "2025-10-29T20:17:20+09:00"
      }
    },
    "04.아키텍쳐": {
      "path": "/02.inbox/real-my-sql/04.아키텍쳐/",
      "filename": "04.아키텍쳐",
      "content": "mysql 엔진 : 커넥션핸들러, sql 파서, 전처리기, 옵티마이저 스토리지 엔진 : 데이터를 스토리지에 W 또는 R => 키캐시(MyISAM) 또는 버퍼풀기능 mysql 엔진 -> 스토리지 엔진으로 요청시 헨들러 api 사용 SHOW GLOBAL STATUS LIKE 'handler%'; 실행중인 스레드 목록 SELECT thread_id, name, type, processlist_user, processlist_host FROM performance_schema.threads ORDER BY thread_id; 플러그인 시스템 컴포넌트 시스템 차이 플러그인 : show plugins; 컴포넌트 : select * from mysql.component; innodb Pasted image 20241220052896 Pasted image 20241220052912 데드락 감지 스레드(innodbdeadlockdetect) 을 OFF 하고 deadlock 발생을 시간(특정 데이터의 xlock 요청에 의한 잠김시간)으로 감지해서 요청 실패로 처리가능 innodblockwait_timeout innodb buffer pool nodbbufferpoolsize = innodbbufferpoolchunksize * innodbbufferpoolinstances 청크는 128MB 단위 pool size 기본값 128M 즉 인스턴스 1 show variables like '%innodb_buffer%'; LRU(LRU+MRU) 플러쉬(flush) 프리(free) SHOW engine innodb status innodbpagecleaners : 이 변수는 더티 페이지를 디스크에 동기화하는 클리너 스레드의 수를 설정합니다 이 변수의 값은 버퍼 풀 인스턴스 수보다 작아야 한다 innodbmaxdirtypagespct_lwm : 이 변수는 더티 페이지의 비율이 이 값 이하로 떨어질 때까지 클리너 스레드가 더티 페이지를 디스크에 기록. LWM(Low Water Mark)은 페이지가 낮은 임계값에 도달했을 때 동작을 시작하는 기준. innodbmaxdirtypagespct : 이 변수는 버퍼 풀 내에서 더티 페이지가 차지할 수 있는 최대 비율을 설정. 이 비율을 초과하면 InnoDB는 더티 페이지를 디스크에 플러시하여 비율을 줄이려 한다. 기본값은 75% innodbiocapacity : 이 변수는 InnoDB가 I/O 작업을 수행할 수 있는 최대 용량을 설정한다. 디스크의 I/O 성능에 따라 이 값을 조정하여 성능을 최적화할 수 있다. 기본값은 200. <= 실제 디스크 성능보다는 db 서버가 사용할 수 있는 속도 innodbiocapacitymax : 이 변수는 InnoDB가 I/O 작업을 수행할 수 있는 최대 I/O 용량을 설정. innodbio_capacity보다 큰 값을 설정할 수 있으며, 주로 고속 SSD와 같은 스토리지 장치를 사용할 때 유용. innodbadaptiveflushing : 이 변수는 InnoDB가 플러시 작업을 적응적으로 조절할 수 있도록 설정합니다. 이 기능을 활성화하면 시스템의 현재 부하에 따라 플러시 빈도를 조절하여 성능을 최적화할 수 있습니다. innodbadaptiveflushing_lwm : 이 변수는 적응형 플러싱이 활성화된 경우, 더티 페이지 비율이 이 값 이하로 떨어질 때까지 플러시 작업을 수행하도록 설정합니다. LWM이 낮을수록 더티 페이지를 더 자주 플러시하게 됩니다. innodbflushneighbors : 이 변수는 InnoDB가 페이지를 디스크에 플러시할 때, 해당 페이지의 이웃 페이지도 함께 플러시할지 여부를 결정. 값이 1이면 이웃 페이지를 플러시하고, 0이면 플러시하지 않는다. SSD와 같은 스토리지에서는 0으로 설정(활성화 하지 않음 랜덤 읽기 성능이 뛰어나므로)하는 것이 성능에 유리할 수 있음 Pasted image 20241217022648 Pasted image 20241217063871 각 테이블의 인덱스별 데이터 페이지가 얼만큼 Innodb buffer pool 에 적제되어 있는가 SELECT it.name table_name, ii.name index_name, ici.n_cached_pages n_cached_pages FROM information_schema. innodb_tables it INNER JOIN information_schema. innodb_indexes ii ON ii. table_id = it.table_id INNER JOIN information_schema.innodb_cached_indexes ici ON ici.index_id = ii.index_id WHERE it.name=CONCAT('employees', '/', 'employees'); 테이블 전체 페이지 중에서 어느정도의 비율이 Innodb 버퍼풀에 적재되어 있는가 SELECT (SELECT SUM(ici.n_cached_pages) n_cached_pages FROM information_schema.innodb_tables it INNER JOIN information_schema.innodb_indexes ii ON ii. table_id = it. table_id INNER JOIN information_schema.innodb_cached_indexes ici ON ici.index_id = ii.index_id WHERE it.name=CONCAT(t.table_schema, '/', t.table_name)) as total_cached_pages, ((t.data_length + t.index_length - t.data_free)/@@innodb_page_size) as total_pages FROM information_schema.tables t WHERE t.table_schema='employees' AND t.table_name='employees'; 리두로그에는 변경된 단일 데이터? double write buffer 에는 변경된 전체 페이지 언두 레코드 건수 SELECT count FROM information_schema.innodb_metrics WHERE SUBSYSTEM='transaction' AND NAME='trx_rseg_history_len';",
      "frontmatter": {
        "date": "2024-12-14T19:08:00+09:00",
        "lastmod": "2024-12-14T19:08:00+09:00"
      }
    },
    "05.트랜잭션과 잠금": {
      "path": "/02.inbox/real-my-sql/05.트랜잭션과-잠금/",
      "filename": "05.트랜잭션과 잠금",
      "content": "",
      "frontmatter": {
        "date": "2024-12-18T02:53:00+09:00",
        "lastmod": "2024-12-18T02:53:00+09:00"
      }
    },
    "네트워크 2계층": {
      "path": "/02.inbox/따라it/네트워크-2계층/",
      "filename": "네트워크 2계층",
      "content": "ㅆ근거리(동일 네트워크 대역) 간 통신 네트워크-202402051439277.png) ethernet type 을 상위 프로토콜 type 로 이해하자 ipconfig /all 명령으로 자신의 mac 주소를 확인 mac 주소 앞 3자리 OUI : IEEE 에서 부여하는 일종의 제조회사 식별 ID 뒤 3자리 고유번호 : 제조사에서 부여한 고유번호 destination Address , source address, Ethernet type 만 확인 destination address 목적지 MAC 주소 Ethernet Type : 상위 프로토콜 타입 2바이트: IPv4(0x0800), ARP(Ox0806)",
      "frontmatter": {
        "tags": [
          "network"
        ],
        "series": "따라IT Network",
        "series_weight": "2",
        "date": "2024-02-05T14:54:00+09:00",
        "lastmod": "2024-02-05T14:54:00+09:00"
      }
    },
    "네트워크 3계층": {
      "path": "/02.inbox/따라it/네트워크-3계층/",
      "filename": "네트워크 3계층",
      "content": "먼거리(인터넷 통신) 간 통신 용어 설명 IPv4: 현제 PC 에 할당된 ip 주소 서브넷 마스크 : ip 주소에 대한 네트워크 대역을 규정하는 것 게이트 웨이 : 외부와 통신할 때 사용하는 네트워크 출입구 목차 ARP 프로토콜 : 동일 네트워크 대역 MAC 주소를 알기 위해 사용하는 프로토콜 IPv4 프로토콜 : ICMP 프로토콜 : 연결 확인용 프로토콜 네트워크 3계층-20240205150757 ARP Protocol 동일 네트워크 대역 네트워크 3계층-20240205150706 같은 네트워크 대역에서 통신을 하기 위해 MAC 주소를 ip 주소를 이용해서 알아오는 프로토콜 hardware type : 2계층 프로토콜 type( 대부분 이더넷 0x0001) protocol type : 3계층 프로토콜 type( 대부분 ip 0x0800 ) hardware address Length : MAC 주소의 길이 06 Protocol address length : IP 주소의 길이 04 opcode : 질문 0x0001 또는 대답 0x0002 source hardware address : 출발지 MAC 주소 source protocol address : 출발지 ip 주소 destination hardware address : 도착지 MAC 주소 (알고 싶은 mac 주소) 질문시에는 모르니까 000000 destination protocol address : 도착지 ip 주소 [](https://docs.google.com/presentation/d/1GParqmJVI3xwzFDk-FrOqGzFsAFbqUC5/edit#slide=id.p8) mac 주소를 모르므로 00000... 으로 비워둔다 동일 네트워크 대역에서 통신을 한다고 하더라도 데이터를 보내기 위해서는 7계층 부터 캡술화를 통해 데이터를 보내기 때문에 ip 주소와 MAC 주소 모두 필요하다 이때 IP 주소는 알고 MAC 주소를 도르더라도 ARP 를 통해 통신이 가능하다 arp 는 3계층이지만 동일 네트워크 대역에서만 사용할 수 있다 통신 이후 arp 캐시 테이블로 관리한다 IPv4 & ICMP Protocol 시대별 구분 프로토콜 구조 라우팅 테이블 타 네트워크 통신과정 조각화 시대별 구분 classful classfulless 공인 사설 classful IP 1세대 네트워크 3계층-20240205151100 네트워크 3계층-20240205151349 네트워크 클래스별로 대역을 구분 classfulless IP 2세대 네트워크 3계층-20240205151941 서브넷 마스크 도입으로 임의로 대역을 구분 가능 네트워크 3계층-20240205152822 현제 IP 3세대 네트워크 3계층-20240205153208 NAT 터널링 프로토콜 구조 네트워크 3계층-20240205150715 Version 버전 0x4 고정 IHL(Header Length) (헤더 길이/4) : 0x5, 0x6 ... type of Service (TOS) : 0 으로 비어있음 서비스 형식 과거에 사용 Total Length : Packet 전체 크기 : 헤더와 페이로드 포함 Identification, IP Flags, Fragment Offset : 전송시 분리되서 보내지는데 이때 구별을 위한 정보 identification : 패킷 id, 조각화시 조각들은 동일 id IP Flags(x 사용하지 않음, D Dont Fragmentation:쪼개지 마라, More Fragmentation:조각화시 마지막 패킷만 0 ) fragment Offset : 초기 위치에서 떨어진 만큼의 위치 8로 나눔 Time To Live : 패킷의 생존 시간 ( 순환 오류 발생 대비 노드 최대 방문 횟수 ) unix 는 64 설정, window 는 128 기본 설정 이를 통해 운영체제 예측 가능 Protocol : ICMP(3계층 0x01=1), TCP(4계층 0x06=6), UDP(4계층 0x11=17 ) Header Checksum : 헤더를 통해 생성 hash 알고리즘 Source address : 출발지 ip Destination address : 목적지 ip IP Option : 추가적인 정보란( 0~12 바이트 정도) 정확하게 전달될 것을 보장하지 않는다 네트워크 3계층-20240205150757 type : 카테고리 Code : 소분류 Checksum : 헤더 오류 검출용 추가 Type Code Description 0 0 응답 echo request 8 0 요청 echo respond 3 0~15 목적지 도달 불가 Destination Unrechable 11 0~1 도착이후 반환 불가(방화벽, 응답x) Time Exceeded 5 0 ~ 3 원격지 라우팅 테이블 수정 Redirect Message ICMP 메세지 타입 검색 라우팅 테이블 netstat -r 명령어를 통해 확인 조각화 네트워크 3계층-20240206005219 네트워크 3계층-20240206005446 ip 자체 프로토콜 최소 20 바이트를 고려해서 분리해야한다",
      "frontmatter": {
        "tags": [
          "network"
        ],
        "series": "따라IT Network",
        "series_weight": "3",
        "date": "2024-02-05T15:01:00+09:00",
        "lastmod": "2024-02-05T15:01:00+09:00"
      }
    },
    "네트워크 4계층": {
      "path": "/02.inbox/따라it/네트워크-4계층/",
      "filename": "네트워크 4계층",
      "content": "프로그램(프로세스) 간 통신 4계층 프로토콜 포트번호 프로그램 연결 정보 4계층 프로토콜 전송계층 송신자 프로세스 수신자 프로세스 통신 TCP UDP 포트번호(4계층 주소) UDP Pasted image 20240206174953 source Port : 출발 프로세스 포트번호(2바이트 2^16) Destination Port : 도착 프로세스 포트번호 Length : 길이 Checksum DNS 서버 전송이 실패해도 다시보내면 그만 tftp 서버 : 파일 공유 RIP 프로토콜 : 라우팅 정보 공유 Pasted image 202402061749396174939.png) source Port : 출발 프로세스 포트번호(2바이트 2^16) Destination Port : 도착 프로세스 포트번호 Sequence Number, Acknowledgment Number : 연결을 보장하는 용도로 사용 보내는 입장에서의 S : 상대방의 어떠한 대답의 질문인지 기록 보내는 입장에서의 A : 상대방의 질문에 데이터를 더해 전송 받는 입장의 S: 어떠한 대답에 대한 질문인가 받는 입장의 A : 내가 보낸 질문이 정확하게 도달했는가 내가 보냈던 질문의 S에 데이터의 크기를 더해 확인한다 3way hand shake 시에는 0으로 초기화 보낼때는 +1 을 답변으로 Offset : 헤더 길이 전체크기/4 Reserved : 예약 사용하지 않음 TCP Flags : 통신 상황 파악용 C, E 몰라도 된다 U : 긴급한 자료인가? A : 승인 bit 대답용 P : tcp 버퍼 크기 상관없이 보낸다? R : 연결된 상태에서 문제 발생 초기화 필요 reset S : sync 동기화 연결 시작시 1 F : 종료 Window : 자신의 TCP 공간 버퍼(상대방서 보내도 되는지 판단 기준) Checksum Urgent Pointer : 긴급 데이터 위치데이터 TCP Options TCP 통신 과정 3way hand shake Pasted image 20240206224025 Drawing 2024-02-06 22.41.42.excalidraw 1 은 데이터가 포함된(페이로드) 때는 데이터의 크기를 더해준다 Pasted image 20240206232535 점선은 클라이언트 요청 실선은 서버 응답 포트 번호 하나의 포트는 하나의 프로세스만 사용 가능하다 Pasted image 20240206220643 Pasted image 20240206220658 Pasted image 20240206220725",
      "frontmatter": {
        "tags": [
          "network"
        ],
        "series": "따라IT Network",
        "series_weight": "4",
        "date": "2024-02-06T01:19:00+09:00",
        "lastmod": "2024-02-06T01:19:00+09:00"
      }
    },
    "네트워크 7계층": {
      "path": "/02.inbox/따라it/네트워크-7계층/",
      "filename": "네트워크 7계층",
      "content": "5계층 부터 7계층 까지 한번에 관리 {세션 표현 응용} http 1.0 에서 1.1 변경 http 요청 프로토콜 http 응답 프로토콜 http 헤더 포멧 http 버전 RFC2068(1997) RFC2616(1999) RFC7230~7235(2014) http 1.0 에서 1.1 변경 Pasted image 20240207193831 매우 비요율적 모든 요청 응답 하나하나 3wh를 해야한다 Pasted image 20240207194010 http 요청 & 응답 프로토콜 Pasted image 20240212034842 request Line Pasted image 20240207195646 ex) GET / HTTP/1.1 status line Pasted image 20240207230845 ex) HTTP/1.1 200 OK 요청 타입 Pasted image 20240207195725 최근 백엔드(was)와 프론트(client)의 데이터 흐름은 json 형식으로 많이 사용한다 이때 요청 방식의 단일화 규칙을 세우는 것을 REST API 라고 한다 get : 정보를 url 에 포함해서 보낸다 post : 정보를 body에 포함해서 보낸다 URI Pasted image 20240212035621 Headers 일반 헤더 Pasted image 20240212040250 요청 헤더 Pasted image 20240212040319 응답 헤더 Pasted image 20240212040640 항목 헤더 .....",
      "frontmatter": {
        "tags": [
          "network"
        ],
        "series": "따라IT Network",
        "series_weight": "5",
        "date": "2024-02-07T19:25:00+09:00",
        "lastmod": "2025-09-05T21:01:24+09:00"
      }
    },
    "네트워크 고급": {
      "path": "/02.inbox/따라it/네트워크-고급/",
      "filename": "네트워크 고급",
      "content": "계층별 장비 물리계층 리피터 : 전기적 신호가 약할때 증폭 기능 허브 : 데이터를 모든 장비에 전송 일종의 브로드캐스트 데이터 링크 2계층 브릿지 : 맥주로를 기반으로 전송 포트를 결정 스위치 : 브릿지에 collision domain 기능 추가 네트워크 계층 라우터 : 라우팅 테이브를 참고하여 목적지와 연결되는 포트로 paket 을 전송 => 라우팅 3계층 장비는 브로트캐스트를 모두 차단 케이블 Pasted image 20240212044805 Pasted image 20240212044836",
      "frontmatter": {
        "tags": [
          "network"
        ],
        "series": "따라IT Network",
        "series_weight": "6",
        "date": "2024-02-12T04:19:00+09:00",
        "lastmod": "2024-02-12T04:19:00+09:00"
      }
    },
    "네트워크": {
      "path": "/02.inbox/따라it/네트워크/",
      "filename": "네트워크",
      "content": "크기에 따른 분류 LAN ( Local Area Network ) WAN ( Wide Area Network ) MAN ( Metropolitan Area Network ) VLAN CAN PAN 연결 형테애 따른 분류 Star 중앙장비에 모든 노드가 연결 Mesh 여러노드의 그물 형태 Tree 계층 구조의 형태 링형, 버스형, 혼합형 통신방식에 따른 분류 유니 캐스트 ( 특정 대상과 1:1 통신 ) 멀티 캐스트 ( 특정 다수와 1:N 통신 멀티 캐스트 {특정 인원}) 브로트 캐스트 ( 네트워크에 있는 모든 대상과 통신 ) 계층별 프로토콜 네트워크-20240205135843 네트워크-20240205140100 네트워크-20240205133316 패킷 네트워크-20240205140418 이더넷 프로토콜만 풋터 사용 테이터 -> 캡슐화 인캡슐레이션 incapsulation 캡슐 -> 데이터 디캡슐레이션 decapsulati네트워크-20240205140841%20image%2020240506030502.png) 네트워크-20240205140856 네트워크-20240205140903 frame 의 최소 크기 60byte 1계층 에서 붙는다 최대 1514byte 네트워크-20240205143305 네트워크-20240205144823 Transport TCP 에서 PDU : segment Transport UDP 에서 PDU : user datagram Network IP 에서 PDU : datagram 네트워크란 무엇인가https://docs.google.com/presentation/d/1cBVIS457shcUV3cfSrM9c1w3Lws3ZVE9/edit?usp=sharing&ouid=109006469823185730332&rtpof=true&sd=true 네트워크의 기준! 네트워크 모델 https://docs.google.com/presentation/d/1ui8oW-jTp3z5dU7XP1a-eMnJu9GcAs49/edit?usp=sharing&ouid=109006469823185730332&rtpof=true&sd=true 가까이 있는 컴퓨터끼리는 이렇게 데이터를 주고받는다 https://docs.google.com/presentation/d/1HYg9YC3_luzxMHzVn-sGd6gzjgExGKV6/edit?usp=sharing&ouid=109006469823185730332&rtpof=true&sd=true 실제로 컴퓨터끼리는 IP주소를 사용해 데이터를 주고받는다[](https://docs.google.com/presentation/d/1ovEj3fJiYxVFZgaQglBszEJA3pyvTUAH/edit#slide=id.p1) 통신하기 전 반드시 필요한 ARP 프로토콜 https://docs.google.com/presentation/d/1GParqmJVI3xwzFDk-FrOqGzFsAFbqUC5/edit?usp=sharing&ouid=109006469823185730332&rtpof=true&sd=true 멀리있는 컴퓨터기리는 이렇게 데이터를 주고받는다https://docs.google.com/presentation/d/1ezrr3wC9UaTGqtjRPfetCrstnd6qZ8G0/edit?usp=sharing&ouid=109006469823185730332&rtpof=true&sd=true 컴퓨터의 프로그램끼리는 이렇게 데이터를 주고 받는다 https://docs.google.com/presentation/d/1z-WGHNF81x40zbNrJRv29NMFCUVzKeyh/edit?usp=sharing&ouid=109006469823185730332&rtpof=true&sd=true 비연결지향형 UDP 프로토콜 https://docs.google.com/presentation/d/1vOOj3I0zHIuqLmSKwCicuSLuveZtuz9a/edit?usp=sharing&ouid=109006469823185730332&rtpof=true&sd=true 연결지향형 TCP 프로토콜 https://docs.google.com/presentation/d/15SyhelhPtlwbz6wqeodjHhwVfU2bop50/edit?usp=sharing&ouid=109006469823185730332&rtpof=true&sd=true NAT와 포트포워딩 https://docs.google.com/presentation/d/1N7gScAhktYl3i_cSRi6-DHRTIwfPvq0O/edit?usp=sharing&ouid=109006469823185730332&rtpof=true&sd=true WWW(웹)를 이용할 때는 이렇게 데이터를 주고받는다 https://docs.google.com/presentation/d/1W2AIBsANAttvrbcrh9wLMrPYbaB8Hkht/edit?usp=sharing&ouid=109006469823185730332&rtpof=true&sd=true 네트워크-20240205233719 tracert",
      "frontmatter": {
        "tags": [
          "network"
        ],
        "series": "따라IT Network",
        "series_weight": "1",
        "date": "2024-02-05T13:17:00+09:00",
        "lastmod": "2024-02-05T13:17:00+09:00"
      }
    },
    "BFS 알고리즘": {
      "path": "/02.inbox/이것이-코딩테스트다-with-python/bfs-알고리즘/",
      "filename": "BFS 알고리즘",
      "content": "#include <iostream> #include <vector> #include <queue> #include <array> // for std::array using namespace std; void bfs(int start) { queue<int> q; q.push(start); // 초기 시작 노드 큐에 삽입 visited[start] = true; // 방문 처리 // que 처리 알고리즘 while (!q.empty()) { // 해당 노드 처리 int x = q.front(); q.pop(); cout << x << ' '; // 해당 노드와 연결된 방문하지 않은 노드 처리 for (int y : graph[x]) { if (!visited[y]) { q.push(y); visited[y] = true; } } } } int main(void) { // visited 배열을 std::array로 변경 (선택적) array<bool, 9> visited{}; // 기본적으로 false로 초기화 // 그래프 연결 정보를 중괄호 초기화 리스트로 표현 vector<int> graph[] = { {}, // 0번 인덱스는 사용하지 않음 (1번부터 시작) {2, 3, 8}, // 1번 노드에 연결된 노드들 {1, 7}, // 2번 노드에 연결된 노드들 {1, 4, 5}, // 3번 노드에 연결된 노드들 {3, 5}, // 4번 노드에 연결된 노드들 {3, 4}, // 5번 노드에 연결된 노드들 {7}, // 6번 노드에 연결된 노드들 {2, 6, 8}, // 7번 노드에 연결된 노드들 {1, 7} // 8번 노드에 연결된 노드들 }; bfs(1); // 1번 노드부터 탐색 시작 return 0; }",
      "frontmatter": {
        "tags": [
          "python",
          "algorithm"
        ],
        "series": "이것이 코딩테스트다 with python",
        "series_weight": "19",
        "date": "2023-12-29T11:29:00+09:00",
        "lastmod": "2023-12-29T11:29:00+09:00"
      }
    },
    "DFS 알고리즘": {
      "path": "/02.inbox/이것이-코딩테스트다-with-python/dfs-알고리즘/",
      "filename": "DFS 알고리즘",
      "content": "1_GT9oSo0agIeIj6nTg3jFEA1.gif) stack vs recursion 풀이 2개로 나뉘며 stack 의 경우 dfs 시 탐색해야할 나머지 노드를 stack 에 저장해두고 빼서 탐색하는 방식의 알고리즘 recursion 의 경우 하위 트리가 없을 때 또는 자신이 null 일때 탈출조건을 준다면 recursion 풀이가 가능하다 binary tree DFS stack 기반 풀이 recursion 기반 풀이 grath DFS 기본적으로 작은 숫자부터 탐색하는 것을 기본으로 한다 recursion 풀이 using namespace std; bool visited[9]; vector<int> graph[9]; // DFS 함수 정의 void dfs(int x) { // 현재 노드 방문 처리 visited[x] = true; cout << x << ' '; // 현재 노드와 연결된 다른 노드를 재귀적으로 방문 for (int i = 0; i < graph[x].size(); i++) { int y = graph[x][i]; if (!visited[y]) dfs(y); } } int main() { // 각 노드의 인접 노드 정보를 초기화 리스트로 간단하게 할당 graph[1] = {2, 3, 8}; graph[2] = {1, 7}; graph[3] = {1, 4, 5}; graph[4] = {3, 5}; graph[5] = {3, 4}; graph[6] = {7}; graph[7] = {2, 6, 8}; graph[8] = {1, 7}; dfs(1); return 0; } stack 풀이 #include <iostream> #include <vector> #include <stack> using namespace std; bool visited[9]; vector<int> graph[9]; void dfs_iterative(int start) { stack<int> s; s.push(start); while (!s.empty()) { int x = s.top(); s.pop(); if (!visited[x]) { visited[x] = true; cout << x << ' '; // 재귀 DFS와 동일한 방문 순서를 위해 인접 노드를 역순으로 스택에 넣습니다. for (auto it = graph[x].rbegin(); it != graph[x].rend(); ++it) { int y = *it; if (!visited[y]) s.push(y); } } } } int main() { // 각 노드의 인접 노드 정보를 초기화 리스트로 간단하게 할당 graph[1] = {2, 3, 8}; graph[2] = {1, 7}; graph[3] = {1, 4, 5}; graph[4] = {3, 5}; graph[5] = {3, 4}; graph[6] = {7}; graph[7] = {2, 6, 8}; graph[8] = {1, 7}; dfs_iterative(1); return 0; }",
      "frontmatter": {
        "tags": [
          "algorithm",
          "cs"
        ],
        "series": "이것이 코딩테스트다 with python",
        "series_weight": "18",
        "date": "2023-12-26T07:59:00+09:00",
        "lastmod": "2023-12-26T07:59:00+09:00"
      }
    },
    "구현(Implementation)": {
      "path": "/02.inbox/이것이-코딩테스트다-with-python/구현implementation/",
      "filename": "구현(Implementation)",
      "content": "머릿속에 있는 알고리즘을 소스코드로 바꾸는 과정 알고리즘은 간단한데 코드가 지나칠 만큼 길어지는 문제 실수 연산을 다루고 특정 소수점 자리까지 출력해야 하는 문제 문자열을 특정한 기준에 따라서 끊어 처리해야 하는 문제 적절한 라이브러리를 찾아서 사용해야 하는 문제 ( 모든 순열 모든 조합을 찾는 문제 python itertools 모듈을 사용해야 쉬워진다 ) 시뮬레이션 유형 구현 유형 완전탐색 Pasted image 20240428050422 Pasted image 20240428050425 미로 탐색 (문제 번호: 2178) - BFS 알고리즘을 사용하여 최단 경로를 찾는 문제입니다. URL: https://www.acmicpc.net/problem/2178 토마토 (문제 번호: 7576) - BFS 알고리즘을 사용하여 최소 일수를 구하는 문제입니다. URL: https://www.acmicpc.net/problem/7576 단지번호붙이기 (문제 번호: 2667) - DFS 또는 BFS 알고리즘을 사용하여 단지를 구분하고 개수를 세는 문제입니다. URL: https://www.acmicpc.net/problem/2667 유기농 배추 (문제 번호: 1012) - DFS 또는 BFS 알고리즘을 사용하여 배추흰지렁이가 필요한 최소 개수를 구하는 문제입니다. URL: https://www.acmicpc.net/problem/1012 섬의 개수 (문제 번호: 4963) - DFS 또는 BFS 알고리즘을 사용하여 섬의 개수를 구하는 문제입니다. URL: https://www.acmicpc.net/problem/4963 행렬 (문제 번호: 1080) - 그리디 알고리즘을 사용하여 최소 횟수로 변환하는 문제입니다. URL: https://www.acmicpc.net/problem/1080 2차원 배열 문제들 Pasted image 20240428050429 h , K = map(str,input().split()) h = int(h) count = 0 for j in range(60): for k in range(60): if K in str(i) + str(j) + str(k): count += 1 print(count) Pasted image 20240428050431 Pasted image 20240428050433 input_data = input() row = int(input_data[1]) column = ord(input_data[0]) - ord('a') + 1 steps = [ # dx dy 로 리스트 2개로 만드는게 낮다 c/java 대응 (-2,-1), (-2,1), (-1,2), (1,2), (2,1), (2,-1), (1,-2), (-1,-1), ] count = 0 for step in steps: next_row = row + step[0] next_column = column + step[1] if next_row > 0 and next_row < 9 and next_column > 0 and next_column < 9: count += 1 print(count) [!NOTE] 0 인 경우를 생각하자",
      "frontmatter": {
        "tags": [
          "algorithm",
          "cs"
        ],
        "series": "이것이 코딩테스트다 with python",
        "series_weight": "14",
        "date": "2023-12-23T12:43:00+09:00",
        "lastmod": "2025-10-21T20:45:20+09:00"
      }
    },
    "그래프(graph)": {
      "path": "/02.inbox/이것이-코딩테스트다-with-python/그래프graph/",
      "filename": "그래프(graph)",
      "content": "📖 그래프의 종류와 개념 그래프는 여러 개의 점(노드 또는 정점)들이 선으로 연결된 구조를 나타내는 수학적인 개념입니다. 그래프는 다양한 현실 세계의 문제를 모델링하고 분석하는 데 사용됩니다. 그래프의 용어 ### Pasted image 20240428022601 노드(Node) 또는 정점(Vertex) N or V : 그래프에서 하나의 점을 나타냅니다. 노드는 데이터를 저장하는데 사용될 수 있습니다. 간선(Edge) E : 그래프에서 노드와 노드를 연결하는 선을 나타냅니다. 간선은 노드 쌍 사이의 관계를 나타냅니다. 인접(Adjacent) : 두 개의 노드가 간선으로 직접 연결되어 있는 상태를 말합니다. 인접한 노드는 서로 이웃이라고도 합니다. 차수(Degree) : 노드에 연결된 간선의 수를 나타냅니다. 무방향 그래프에서는 노드의 차수는 해당 노드와 인접한 노드의 수입니다. 경로(Path) : 그래프에서 노드들을 연결하는 간선의 순서를 나타내는 순서쌍입니다. 경로의 길이는 경로에 속한 간선의 수입니다. 사이클(Cycle) : 그래프에서 동일한 노드로 되돌아오는 경로를 말합니다. 즉, 경로의 시작 노드와 끝 노드가 동일한 경우를 말합니다. 가중치(Weight) : 가중치 그래프에서 간선에 할당된 값 또는 비용을 나타냅니다. 가중치는 간선의 특성을 나타내는데 사용됩니다. 그래프의 종류 ### 무방향 그래프 & 방향 그래프 : 간선의 방향의 유무에 따라 구분되는 그래프 Pasted image 20240428022606 가중치 그래프 : 그래프에 가중치 또는 비용이 할당된 그래프(네트워크 이론이나 신경망 이론에 활용되는 개념) Pasted image 20240428022605 연결 그래프 & 비연결 그래프 : 모든 노드에 대해 경로가 존재하면 연결 그래프, 특정 노드에 대한 경로가 하나라도 존재하지 않을 경우 비연결 그래프 Pasted image 20240428022602 사이클 그래프 & 비순환 그래프 Pasted image 20240428022603 완전 그래프 : 그래프의 모든 노드가 연결되어 있는 그래프 Pasted image 20240428022604 그래프의 한 종류인 트리에 대해🧐 트리(Tree)는 그래프(Graph)의 한 종류로, 계층적인 구조를 나타내는 비순환적인 연결 그래프입니다. 트리는 하나의 루트(Root) 노드에서 시작하여 다양한 자식(Child) 노드들로 확장되는 구조를 가지며, 각 노드는 하나의 부모(Parent) 노드와 연결되어 있습니다. 트리의 특징 🔎 ### 계층 구조 : 트리는 하나의 루트 노드에서 시작하여 계층적인 구조를 형성합니다. 각 노드는 부모-자식 관계를 가지며, 자식 노드들은 동일한 계층에 속합니다. 방향성 : 트리는 방향 그래프의 한 형태로, 간선은 단방향으로 표시됩니다. 각 노드는 자식 노드들을 가리키는 방향으로 연결됩니다. 비순환성 : 트리는 순환 구조를 가지지 않습니다. 즉, 어떤 노드에서 시작해도 동일한 노드로 되돌아갈 수 있는 순환 경로가 존재하지 않습니다. 유일한 경로 : 루트 노드에서 어떤 노드까지의 경로는 유일합니다. 트리 내에서는 어떤 노드도 다른 경로를 통해 도달할 수 없습니다 트리에 종류 ### 이진 트리 (Binary Tree) : 각 노드가 최대 두 개의 자식을 가질 수 있는 트리입니다. 이진 트리는 왼쪽 자식과 오른쪽 자식으로 구성되며, 자식의 배치에는 순서가 있습니다. 이진 트리는 데이터 검색, 정렬, 압축 등 다양한 애플리케이션에서 사용됩니다. 이진 탐색 트리 (Binary Search Tree) : 이진 트리의 한 종류로, 이진 탐색의 원리를 기반으로 합니다. 모든 노드는 왼쪽 서브트리의 값보다 작고, 오른쪽 서브트리의 값보다 큰 키 값을 가집니다. 이진 탐색 트리는 데이터 검색, 정렬, 범위 검색 등에 효율적으로 사용됩니다. AVL 트리 : 균형 이진 탐색 트리로서, 모든 노드의 왼쪽 서브트리와 오른쪽 서브트리의 높이 차이가 최대 1인 트리입니다. AVL 트리는 삽입, 삭제 시에 자동으로 균형을 유지하여 탐색 성능을 보장합니다. 레드-블랙 트리 (Red-Black Tree) : 균형 이진 탐색 트리로서, 각 노드는 레드(Red) 또는 블랙(Black) 색깔을 가지며, 특정한 규칙을 따릅니다. 레드-블랙 트리는 AVL 트리보다 균형을 유지하는 데에 조금 덜 엄격한 규칙을 가지며, 데이터의 삽입과 삭제가 더 효율적입니다. B-트리 (B-Tree) : 다양한 자료 구조에서 사용되는 균형 탐색 트리입니다. B-트리는 노드마다 여러 개의 키 값을 가지며, 많은 수의 자식을 가질 수 있습니다. B-트리는 대용량 데이터베이스의 인덱스 구조나 파일 시스템에서 사용되는 것과 같은 곳에서 사용됩니다. 힙 (Heap) : 이진 트리의 한 종류로, 최대 힙과 최소 힙으로 나눌 수 있습니다. 최대 힙은 부모 노드의 값이 자식 노드의 값보다 큰 힙이며, 최소 힙은 그 반대입니다. 힙은 우선순위 큐와 같은 자료 구조에서 사용되어 최댓값 또는 최솟값에 빠르게 접근할 수 있습니다. 그래프(Graph)의 구현 2가지 1\\. 인접 리스트(Adjacency List) 인접 리스트(Adjacency List)로 그래프를 표현하는 것이 가장 일반적인 방법 이다. 모든 정점(혹은 노드)을 인접 리스트에 저장한다. 즉, 각각의 정점에 인접한 정점들을 리스트로 표시한 것이다. 배열(혹은 해시테이블)과 배열의 각 인덱스마다 존재하는 또 다른 리스트(배열, 동적 가변 크기 배열(ArrayList), 연결리스트(LinkedList) 등)를 이용해서 인접 리스트를 표현 0: 1 1: 2 2: 0, 3 3: 2 4: 6 5: 4 6: 5 정점의 번호만 알면 이 번호를 배열의 인덱스로 하여 각 정점의 리스트에 쉽게 접근할 수 있다. 무방향 그래프(Undirected Graph)에서 (a, b) 간선은 두 번 저장된다. 한 번은 a 정점에 인접한 간선을 저장하고 다른 한 번은 b에 인접한 간선을 저장한다. 정점의 수: N, 간선의 수: E인 무방향 그래프의 경우 N개의 리스트, N개의 배열, 2E개의 노드가 필요 트리에선 특정 노드 하나(루트 노드)에서 다른 모든 노드로 접근이 가능 -> Tree 클래스 불필요 그래프에선 특정 노드에서 다른 모든 노드로 접근이 가능하지는 않음 -> Graph 클래스 필요 class Graph { public Node[] nodes; } // 트리의 노드 클래스와 동일 class Node { public String name; public Node[] children; } 2\\. 인접 행렬(Adjacency Matrix) 인접 행렬은 NxN 불린 행렬(Boolean Matrix)로써 matrix\\[i\\]\\[j\\]가 true라면 i -> j로의 간선이 있다는 뜻이다. 0과 1을 이용한 정수 행렬(Integer Matrix)을 사용할 수도 있다. if(간선 (i, j)가 그래프에 존재) matrix[i][j] = 1; else matrix[i][j] = 0; 정점(노드)의 개수가 N인 그래프를 인접 행렬로 표현 간선의 수와 무관하게 항상 n^2개의 메모리 공간이 필요하다. 무방향 그래프를 인접 행렬로 표현한다면 이 행렬은 대칭 행렬(Symmetric Matrix)이 된다. 물론 방향 그래프는 대칭 행렬이 안 될 수도 있다. 인접 리스트를 사용한 그래프 알고리즘들(Ex. 너비 우선 탐색) 또한 인접 행렬에서도 사용이 가능하다. 하지만 인접 행렬은 조금 효율성이 떨어진다. 인접 리스트는 어떤 노드에 인접한 노드들을 쉽게 찾을 수 있지만 인접 행렬에서는 인접한 노드를 찾기 위해서는 모든 노드를 전부 순회해야 한다. ==인접 리스트와 인접 행렬 중 선택 방법== 인접 리스트 그래프 내에 적은 숫자의 간선만을 가지는 희소 그래프(Sparse Graph) 의 경우 장점 어떤 노드에 인접한 노드들 을 쉽게 찾을 수 있다. 그래프에 존재하는 모든 간선의 수 는 O(N+E) 안에 알 수 있다.: 인접 리스트 전체를 조사한다. 단점 간선의 존재 여부와 정점의 차수: 정점 i의 리스트에 있는 노드의 수 즉, 정점 차수만큼의 시간이 필요 인접 행렬 그래프에 간선이 많이 존재하는 밀집 그래프(Dense Graph) 의 경우 장점 두 정점을 연결하는 간선의 존재 여부 (M\\[i\\]\\[j\\])를 O(1) 안에 즉시 알 수 있다. 정점의 차수 는 O(N) 안에 알 수 있다.: 인접 배열의 i번 째 행 또는 열을 모두 더한다. 단점 어떤 노드에 인접한 노드들을 찾기 위해서는 모든 노드를 전부 순회해야 한다. 그래프에 존재하는 모든 간선의 수는 O(N^2) 안에 알 수 있다.: 인접 행렬 전체를 조사한다. Pasted image 20240428022603 Pasted image 20240428022602 밀집 그래프(dense graph)는 간선의 수가 최대 간선의 수에 가까운 그래프이다. 그와 반대로, 간선이 얼마 없는 그래프는 희소 그래프(sparse graph)라고 한다. 그래프(Graph)의 탐색 일반적인 방법 두 가지: 깊이 우선 탐색(Depth-First Search) 과 너비 우선 탐색(Breadth-First Search) 루트 노드(혹은 다른 임의의 노드)에서 시작해서 다음 분기(branch)로 넘어가기 전에 해당 분기를 완벽하게 탐색하는 방법 즉, 넓게(wide) 탐색하기 전에 깊게(deep) 탐색하는 것이다. 사용하는 경우: 모든 노드를 방문 하고자 하는 경우에 이 방법을 선택한다. 깊이 우선 탐색이 너비 우선 탐색보다 좀 더 간단하다. 루트 노드(혹은 다른 임의의 노드)에서 시작해서 인접한 노드를 먼저 탐색하는 방법 즉, 깊게(deep) 탐색하기 전에 넓게(wide) 탐색하는 것이다. 사용하는 경우: 두 노드 사이의 최단 경로 혹은 임의의 경로를 찾고 싶을 때 이 방법을 선택한다. Ex) 지구상에 존재하는 모든 친구 관계를 그래프로 표현한 후 Ash와 Vanessa 사이에 존재하는 경로를 찾는 경우 깊이 우선 탐색의 경우 - 모든 친구 관계를 다 살펴봐야 할지도 모른다. 너비 우선 탐색의 경우 - Ash와 가까운 관계부터 탐색 알고리즘에서 그래프 알고리즘 코딩테스트에서는 다양한 그래프 알고리즘과 그래프 기반의 문제들이 출제될 수 있습니다. 주요한 그래프 알고리즘과 문제 유형은 다음과 같습니다: 깊이 우선 탐색 (Depth-First Search, DFS) : 그래프의 모든 노드를 탐색하고, 각 노드를 방문한 순서를 기록하는 알고리즘입니다. 주로 그래프 탐색, 사이클 판별, 연결 요소 확인 등에 사용됩니다. 문제 풀어보기 너비 우선 탐색 (Breadth-First Search, BFS) : 루트 노드(혹은 다른 임의의 노드)에서 시작해서 다음 분기(branch)로 넘어가기 전에 해당 분기를 완벽하게 탐색하는 방법 문제 풀어보기 다익스트라 알고리즘 (Dijkstra's Algorithm) : 가중치 그래프에서 시작 노드로부터 모든 다른 노드까지의 최단 경로를 찾는 알고리즘입니다. 문제 풀어보기 벨만-포드 알고리즘 (Bellman-Ford Algorithm) : 가중치 그래프에서 시작 노드로부터 모든 다른 노드까지의 최단 경로를 찾는 알고리즘입니다. 음수 가중치를 가진 간선이 있는 경우에도 동작합니다. 문제 풀어보기 크루스칼 알고리즘 (Kruskal's Algorithm) : 가중치 그래프에서 최소 신장 트리를 찾는 알고리즘입니다. 모든 간선을 가중치 순으로 정렬한 뒤, 사이클을 형성하지 않는 간선을 추가하여 트리를 형성합니다. 문제 풀어보기 프림 알고리즘 (Prim's Algorithm) : 가중치 그래프에서 최소 신장 트리를 찾는 알고리즘입니다. 시작 노드에서부터 출발하여, 현재 트리와 연결되지 않은 노드 중 최소 가중치의 간선을 선택하여 트리를 확장합니다. 문제 풀어보기 위상 정렬 (Topological Sorting) : 방향 그래프에서 노드들을 선형적으로 정렬하는 알고리즘입니다. 선행 관계가 있는 작업의 우선 순위를 결정하는 데 사용됩니다. 문제 풀어보기 최소 공통 조상 (Lowest Common Ancestor, LCA) : 트리에서 두 노드의 가장 가까운 공통 조상을 찾는 알고리즘입니다. 주로 트리 구조를 활용한 문제에서 사용됩니다. 문제 풀어보기 이 외에도 그래프 기반의 다양한 문제 유형이 있을 수 있으며, 각 문제에 맞는 적절한 알고리즘을 선택하여 문제를 해결하는 것이 중요합니다.",
      "frontmatter": {
        "aliases": [
          "graph"
        ],
        "tags": [
          "algorithm",
          "cs"
        ],
        "series": "이것이 코딩테스트다 with python",
        "series_weight": "17",
        "date": "2023-12-26T08:11:00+09:00",
        "lastmod": "2025-10-11T05:12:07+09:00"
      }
    },
    "스택과 큐 자료구조": {
      "path": "/02.inbox/이것이-코딩테스트다-with-python/스택과-큐-자료구조/",
      "filename": "스택과 큐 자료구조",
      "content": "[!NOTE] 스택 선입 후출 박스 쌓기 pop 함수 \"후출\" 구현 [!NOTE] 큐 선입 선출 python 의 스택 스택과 큐 자료구조-20231225165748 c++ 의 스택 스택과 큐 자료구조-20231225165800 java의 스택 스택과 큐 자료구조-20231225165815 python 의 큐 스택과 큐 자료구조-20231225165829 c++ 의 큐 스택과 큐 자료구조-20231225165844 java의 큐 스택과 큐 자료구조-20231225165618 스택 라이브러리 대신에 재귀함수를 이용하는 경우",
      "frontmatter": {
        "tags": [
          "algorithm",
          "cs"
        ],
        "series": "이것이 코딩테스트다 with python",
        "series_weight": "16",
        "date": "2023-12-25T12:15:00+09:00",
        "lastmod": "2023-12-25T12:15:00+09:00"
      }
    },
    "알고리즘 성능 평가": {
      "path": "/02.inbox/이것이-코딩테스트다-with-python/알고리즘-성능-평가/",
      "filename": "알고리즘 성능 평가",
      "content": "시간 복잡도 공간 복잡도 수행시간 요구 사항 N < 500 시간 복잡도 O(N^3) N < 2000 시간 복잡도 O(N^2) N < 100,000 시간 복잡도 O(NlogN) N < 10,000,000 시간 복잡도 O(N) 예측 다음과 같은 이론을 따르는 것이 좋다",
      "frontmatter": {
        "tags": [
          "cs",
          "algorithm"
        ],
        "series": "이것이 코딩테스트다 with python",
        "series_weight": "1",
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2023-12-20T07:12:00+09:00"
      }
    },
    "탐욕법(greedy)": {
      "path": "/02.inbox/이것이-코딩테스트다-with-python/탐욕법greedy/",
      "filename": "탐욕법(greedy)",
      "content": "현재 상황에서 지금 당장 가장 좋은 것만 고르는 방법 지금 당장 가장 좋은 것을 고르는 방식이 답이 되는 것을 보장하는가? => 정당성 그리디의 경우 정당성 분석이 가장 중요 Pasted image 20231220052551 단순히 매 상황에서 가장 큰 값만 고르는 경우(그리디)의 경우 5 -> 10 -> 4 를 고르게 되어 틀리게 된다 [!] 탐욕법으로 얻은 해가 최적의 해가 되는 상황에서 이를 추론 할 수 있어야 풀리는 문제 출제 Pasted image 20231220053453 가장 큰 회폐 단위부터 최대한 돈을 거슬러 준다 ==> 그리디 정당성 분석: 거스름돈 동전 중에서 모든 조합의 큰 단위가 작은 단위의 배수 이므로 작은 단위의 동전들을 조합해 다른 해가 나올 수 없다 예시: 거스름돈 800원 일때 화폐 단위가 500원 400원 100원 이라면 500 x 1 + 100 x 3 (틀림) 400 x 2 (맞음) n = 1260 count = 0 array = [500,100,50,10] for coin in array: count += m // coin n %= coin print(count) using namespace std; int n = 1260; int cnt = 0; int coinTypes[4] = {500, 100, 50, 10}; int main() { for (int i = 0; i < 4; i++) { int coin = coinTypes[i]; cnt += n / coin; n %= coin; } cout << cnt << '\\n'; } 실전 문제1 Pasted image 20231220072854 [!NOTE] 가능하면 최대한 많이 나눈다 정당성 분석: N 이 아무리 큰 수여도 k 로 계속 나눈다면 기하급수적으로 빠르게 줄일 수 있다 (k>=2 일때 기준) def greedy(n,k): count = 0 while n != 0: if (n % k) == 0: n = n // k count += 1 else: n -= 1 count += 1 return count n,k = map(int,input().split()) print(greedy(n,k)) 문제2 Pasted image 20231222050511 [!NOTE] 0을 곱하는 경우 숫자가 작아짐 / 0을 더하는 경우 숫자가 동일함 1을 곱하는 경우 숫자가 동일함 / 1을 더하는 경우 숫자가 커짐 def greedy(s): first = 0 second = 0 for i in range(len(s)): second = s[i] if first == 0 or second == 0 or first == 1 or second == 1: first += second else: first *=second return first s = [int(i) for i in input()] print(greedy(s)) Pasted image 20231222074406 Pasted image 20231222074810 [!NOTE] 정당성 분석: ?? n = int(input()) data = list(map(int, input().split())) data.sort() count = 0 # 현제 그룹에 포함된 모험가의 수 result = 0 # 총 그룹의 수 for i in data: # 공포도를 기준으로 낮은 것을 한개씩 확인하면서 순회 count += 1 # 해당 모험가 포함 # 현재 그룹에 포함된 모험가의 수가 현재 공포도 이상이렴 그룹 결정 if count >= i: result += 1 # 총 그룹수 증가 count = 0 # 모험가 수 초기화 print(result)",
      "frontmatter": {
        "aliases": [
          "그리디",
          "greed",
          "탐욕법"
        ],
        "tags": [
          "cs",
          "algorithm"
        ],
        "series": "이것이 코딩테스트다 with python",
        "series_weight": "11",
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2025-10-27T17:33:27+09:00"
      }
    },
    "투 포인터 알고리즘": {
      "path": "/02.inbox/이것이-코딩테스트다-with-python/투-포인터-알고리즘/",
      "filename": "투 포인터 알고리즘",
      "content": "리스트에 순차적으로 접근해야 할 때 두개의 점의 위치를 기록하면서 처리하는 알고리즘 Pasted image 20250106192245 Pasted image 20250106192241 Pasted image 20250106192805 완전 탐색에서 인덱스인 end 가 1 증가하면 부분합이 증가 인덱스인 start 가 1 증가하면 부분합이 감소 이조건을 이용해서 $O(n)$ 으로 만들어야 한다 int countSubarraysWithSum(const std::vector<int>& vec, int M) { int ret = 0; // 부분합이 M인 부분 배열의 개수 int start = 0; // start 포인터 초기화 int end = 0; // end 포인터 int interval_sum = 0; // 현재 부분합 while (start < vec.size()) { // end 포인터를 가능한 만큼 이동 while (end < n && interval_sum < M) { interval_sum += vec[end]; // 부분합 계산 end++; // 인덱스 이동 } // 부분합이 M일 때 카운트 증가 if (interval_sum == M) { ret++; } // interval_sum 이 target 보다 클 때는 start 를 이동 // start 포인터를 증가시키고, interval_sum에서 vec[start]를 빼기 interval_sum -= vec[start]; start++; } return ret; } Pasted image 20250107072079 class Solution { public: bool isSubsequence(string &s, string &t) { int sp = 0; int tp = 0; // 투포인터여도 다른 배열을 지칭하는 인덱스이므로 1개의 반복문으로 해결 가능하다 // 포인터가 size 보다 작아야 하는 조건이 필요하다 while(sp < s.size() && tp < t.size()){ if(s[sp] == t[tp]){ sp++; } tp++; } return sp == s.size(); } };",
      "frontmatter": {
        "tags": [
          "algorithm",
          "python"
        ],
        "series": "이것이 코딩테스트다 with python",
        "series_weight": "39",
        "date": "2024-01-09T22:56:00+09:00",
        "lastmod": "2024-01-09T22:56:00+09:00"
      }
    },
    "파이썬 문법": {
      "path": "/02.inbox/이것이-코딩테스트다-with-python/파이썬-문법/",
      "filename": "파이썬 문법",
      "content": "파이썬 자료형 정수형 실수형 복소수형 문자열 리스트 튜플 사전 등등\\ python 타입 확인 a.\\__class\\__ type(a) isinstance(a) # 상속관계도 알려줌 리스트 \\[\\] append(value) sort() reverse() insert(index,value) count(value) remove(value) 튜플 () 변경 불가 문자열 dictionary a = { key = 'value' } a[key] == value 집합 {} 입출력 map() 리스트의 모든 원소에 각각 특정한 함수를 적용할 때 사용합니다 list(map(int, input().split())) map( 함수 . 리스트 ) 바르게 입력받기 sys.stdin.readline() 줄바꿈 기호 입력되므로 rstrip() 메서드도 사용한다 data = sys.stin.readline().rstrip() f-string 3.6 부터 f\"정답은 {answer}입니다\" 조건부 표현식 result = \"Success\" if score >= 80 else \"Fail\" 람다 표현식 함수를 한줄에 작성 가능 array = [('홍길동', 50), ('이순신', 32), ('아무개', 74)] def my_key(x): return x[1] print(sroted(array, key=my_key)) print(sorted(array, key=lamda x: x[1])) from functools import reduce print(list(map(lambda x:x**2,range(5)))) print(lambda x, y: x+y,range(5)) 출력: [0,1,4,9,16] [] 표준 라이브러리 내장함수 itertools : 반복되는 형태의 데이터를 처리하기 위한 기능 => 순열 조함 라이브러리 heapq : 힙 자료구조를 제공 => 우선순위 큐 기능 bisect: 이진 탐색 기능 제공 collections 텍 카운터 등 유용한 자료구조 포함 math : 필수적인 수학적 기능을 제공한다 => 팩토리얼 제곱근 최대공약수 삼각함수 관련함수 부터 파이 와 같은 상수를 포함한다 sum([1,2,3,4,5]) # 15 min(7,3,4,9) # 3 max eval(\"(3+5)*7\") # 56 sorted(,) # 기본이 오름차순 from itertools import permutations data = {'a','b','c'} result = list(permutations(data, 3)) print(result) from itertools import combination ... from collections import Counter def lcm(a,b): # 최대 공배수 return a*b // math.gcd(a,b)",
      "frontmatter": {
        "tags": [
          "python",
          "algorithm",
          "cs"
        ],
        "series": "이것이 코딩테스트다 with python",
        "series_weight": "2",
        "date": "2023-12-20T07:12:00+09:00",
        "lastmod": "2023-12-20T07:12:00+09:00"
      }
    },
    "RISC-V 명령어 구조": {
      "path": "/02.inbox/그래서-컴퓨터는-어떻게-동작하나요/risc-v-명령어-구조/",
      "filename": "RISC-V 명령어 구조",
      "content": "RISC-V emulator RV32I 를 코드로 구현하기 위해 먼저 RISC-V 공식 문서를 확인하여 구현할 명령의 구조를 공부한다 RISC V 의 경우 32비트인지 64 비트인지 128비트인지에 따라 RV32, RV64, RV128 로 나누어지며 기본적인 명령인 integer instruction set 을 구현하면 RV32I, RV64I, RV128I 라고 불리운다 또한 아래의 4가지 확장을 추가하여 자유롭게 가능한 명령을 추가 할 수 있다 또한 문서는 2가지로 privilige 명령과 non privilige 명령 문서가 있으며 아래는 non privilige 명령만을 설명한다 1권, 권한 없는 사양 버전 20240411 PDF, GitHub 2권, 권한 사양 버전 20240411 PDF, GitHub non privilige 의 확장 명령 종류 M : multifly 정수 곱셈 및 나눗셈 확장 A : atomic 원자적 연산 확장 멀티 코어의 공유 자원 접근을 위한 명령) F : Single-Precision Floating-Point 단정밀도 부동소수점에 대한 표준 확장 D : Double-Precision Floating-Point 배정밀로 부동소수점에 대한 표준 확장 Q : Quad-Precision Floating-Point 쿼드정밀도 부동소수점에 대한 표준 확장 L : Decimal Floating-Point 10진수 부동소수점에 대한 표준 확장 C : Compressed Instructions 압축 명령어용 명령 B : Bit Manipulation 비트 조작 명령 J : Dynamically Translated Languages 동적 번역용 언어 T : Transactional Memory 트랜잭션 메모리용 확장 P : Packed-SIMD Instructions V : Vector Operations 벡터 확장 N : Standard Extension for User-Level Interrupts 유저 레벨 인터럽트 embed PDF file riscv-spec-v2.2 20240831124214 link PDF file riscv-spec-v2.2 20240831124214 명령어 기본 구조 Bits 31 - 25 24 - 20 19 - 15 14 - 12 11 - 7 6 - 0 R-type funct7 rs2 rs1 funct3 rd opcode I-type imm\\[11:0\\] '' rs1 funct3 rd opcode S-type imm\\[11:5\\] rs2 rs1 funct3 imm\\[4:0\\] opcode B-type imm\\[12\\ 10:5] rs2 rs1 funct3 imm[4:1\\ 11\\] opcode U-type imm[31:12] '' '' '' rd opcode J-type imm\\[20\\ 10:1\\ 11\\ 19:12\\] '' '' '' rd opcode RV32I 세부 명령 종류 Type Instruction Description 한글 설명 명령어 예시 명령어 설명 R-type ADD Add 덧셈 add x1, x2, x3 x1 = x2 + x3 R-type SUB Subtract 뺄셈 sub x1, x2, x3 x1 = x2 - x3 R-type SLL Shift Left Logical 논리적 왼쪽 시프트 sll x1, x2, x3 x1 = x2 > x3 (논리적) R-type SRA Shift Right Arithmetic 산술적 오른쪽 시프트 sra x1, x2, x3 x1 = x2 >> x3 (산술적) R-type OR OR 논리합 or x1, x2, x3 x1 = x2 \\ x3 R-type AND AND 논리곱 and x1, x2, x3 x1 = x2 & x3 I-type ADDI Add Immediate 즉시값 덧셈 addi x1, x2, 10 x1 = x2 + 10 I-type SLTI Set Less Than Immediate 즉시값 작음 설정 slti x1, x2, 10 x1 = (x2 > 2 (논리적) I-type SRAI Shift Right Arithmetic Immediate 즉시값 산술적 오른쪽 시프트 srai x1, x2, 2 x1 = x2 >> 2 (산술적) I-type LB Load Byte 바이트 로드 lb x1, 0(x2) x1 = MEM[x2 + 0] (1 byte) I-type LH Load Halfword 하프워드 로드 lh x1, 2(x2) x1 = MEM[x2 + 2] (2 bytes) I-type LW Load Word 워드 로드 lw x1, 4(x2) x1 = MEM[x2 + 4] (4 bytes) I-type LBU Load Byte Unsigned 부호 없는 바이트 로드 lbu x1, 0(x2) x1 = MEM[x2 + 0] (1 byte, 부호 없음) I-type LHU Load Halfword Unsigned 부호 없는 하프워드 로드 lhu x1, 2(x2) x1 = MEM[x2 + 2] (2 bytes, 부호 없음) I-type JALR Jump and Link Register 레지스터로 점프 및 링크 jalr x1, 0(x2) x1 = PC + 4; PC = x2 + 0 S-type SB Store Byte 바이트 저장 sb x1, 0(x2) MEM[x2 + 0] = x1 (1 byte) S-type SH Store Halfword 하프워드 저장 sh x1, 2(x2) MEM[x2 + 2] = x1 (2 bytes) S-type SW Store Word 워드 저장 sw x1, 4(x2) MEM[x2 + 4] = x1 (4 bytes) B-type BEQ Branch if Equal 같으면 분기 beq x1, x2, label if (x1 == x2) PC = label B-type BNE Branch if Not Equal 다르면 분기 bne x1, x2, label if (x1 != x2) PC = label B-type BLT Branch if Less Than 작으면 분기 blt x1, x2, label if (x1 = x2) PC = label B-type BLTU Branch if Less Than Unsigned 부호 없이 작으면 분기 bltu x1, x2, label if (x1 = x2) PC = label (부호 없음) U-type LUI Load Upper Immediate 상위 즉시값 로드 lui x1, 0x12345 x1 = 0x12345000 U-type AUIPC Add Upper Immediate to PC PC에 상위 즉시값 더하기 auipc x1, 0x12345 x1 = PC + 0x12345000 J-type JAL Jump and Link 점프 및 링크 jal x1, label x1 = PC + 4; PC = label I-type FENCE Fence 펜스 fence 메모리 순서 보장 I-type FENCE.I Fence Instruction 명령어 펜스 fence.i 명령어 캐시 동기화 I-type ECALL Environment Call 환경 호출 ecall 시스템 콜 I-type EBREAK Environment Break 환경 중단 ebreak 디버그 중단점 I-type CSRRW Atomic Read/Write CSR 원자적 CSR 읽기/쓰기 csrrw x1, csr, x2 t = CSR; CSR = x2; x1 = t I-type CSRRS Atomic Read and Set Bits in CSR 원자적 CSR 읽기 및 비트 설정 csrrs x1, csr, x2 t = CSR; CSR = t \\ x2; x1 = t I-type CSRRC Atomic Read and Clear Bits in CSR 원자적 CSR 읽기 및 비트 클리어 csrrc x1, csr, x2 t = CSR; CSR = t & ~x2; x1 = t I-type CSRRWI Atomic Read/Write CSR (Immediate) 원자적 CSR 읽기/쓰기 (즉시값) csrrwi x1, csr, 5 t = CSR; CSR = 5; x1 = t I-type CSRRSI Atomic Read and Set Bits in CSR (Immediate) 원자적 CSR 읽기 및 비트 설정 (즉시값) csrrsi x1, csr, 5 t = CSR; CSR = t \\ 5; x1 = t I-type CSRRCI Atomic Read and Clear Bits in CSR (Immediate) 원자적 CSR 읽기 및 비트 클리어 (즉시값) csrrci x1, csr, 5 t = CSR; CSR = t & ~5; x1 = t RISC-V Register 종류 Register ABI Name Description Saver x0 zero Hard-wired zero — x1 ra Return address Caller x2 sp Stack pointer Callee x3 gp Global pointer — x4 tp Thread pointer — x5 t0 Temporary/alternate link register Caller x6–7 t1–2 Temporaries Caller x8 s0/fp Saved register/frame pointer Callee x9 s1 Saved register Callee x10–11 a0–1 Function arguments/return values Caller x12–17 a2–7 Function arguments Caller x18–27 s2–11 Saved registers Callee x28–31 t3–6 Temporaries Caller ==== ==== F, D등의 부동소수점 확장 시에만 ==== f0–7 ft0–7 FP (Floating Point) temporaries Caller f8–9 fs0–1 FP saved registers Callee f10–11 fa0–1 FP arguments/return values Caller f12–17 fa2–7 FP arguments Caller f18–27 fs2–11 FP saved registers Callee f28–31 ft8–11 FP temporaries Caller",
      "frontmatter": {
        "tags": [
          "cs",
          "cpu"
        ],
        "date": "2024-08-31T09:44:00+09:00",
        "lastmod": "2024-08-31T09:44:00+09:00"
      }
    },
    "그래서 컴퓨터는 어떻게 동작하나요 1": {
      "path": "/02.inbox/그래서-컴퓨터는-어떻게-동작하나요/그래서-컴퓨터는-어떻게-동작하나요-1/",
      "filename": "그래서 컴퓨터는 어떻게 동작하나요 1",
      "content": "WORD : CPU 에 의해 한번에 처리될 수 있는 비트 수 SR latch NOR 게이트를 사용한 SR 래치는 기본적인 SR 래치로, 아래와 같이 작동합니다. 두 개의 NOR 게이트를 교차 연결합니다. 입력은 S(Set)와 R(Reset)입니다 S R Q (다음 상태) ¬Q (다음 상태) 0 0 Q (이전 상태) ¬Q (이전 상태) 0 1 0 1 1 0 1 0 1 1 불안정 불안정 NAND 게이트를 사용한 SR 래치는 약간 다르게 동작합니다. 이 경우에는 입력을 보통 S'와 R'으로 표기합니다. 입력은 S'(Set)와 R'(Reset)입니다. 여기서 S'와 R'은 각각 S와 R의 부정 입력입니다. S' R' Q (다음 상태) ¬Q (다음 상태) 0 0 불안정 불안정 0 1 1 0 1 0 0 1 1 1 Q (이전 상태) ¬Q (이전 상태) \\ D latch sr latch 의 확장판 불안정 상태 즉 q = ¬Q 인 상황 제거 clk 주기에 컨트롤 가능하게 한다 clk 가 1이면 data 저장 clk 가 0 이면 data 저장하지 않는다 gate or enable Data Q 다음상태 0 0 Q (이전 상태) 0 1 Q (이전 상태) 1 0 0 1 1 1 D flip flop D latch 의 확장판 Q 는 오직 클럭이 0 에서 1로 바뀌는 시점의 data 를 저장한다 즉 RISING EDGE 에서만 저장 허용 클럭에 사용할 수 있다 Clock Edge D Q (다음 상태) 상승/하강 0 0 상승/하강 1 1 레지스터 set = 저장 활성화 여부 enabler = 출력 활성화 여부 ALU 아래의 조합회로들로 ALU 를 구성한다 세부적인 설계는 최적화 여부에 따라 많이 달라질 수 있다 adder shift right shift left not ander orer ALU 는 아래의 in 그리고 out 을 가진다 in a (첫번째 피연산자) b (두번째 피연산자) c_in ( carry in ) op (operation code) 명령어가 들어간다 out 연산 결과 c_out (carry out) a > b a = b zero 결국 위의 부품을 통해 아래의 레지스터에 임시적으로 저장된다 누산기(Accumulator): 연산 결과를 일시적으로 저장하는 레지스터입니다. ALU의 주요 연산 결과가 이곳에 저장됩니다. 피연산자 레지스터(Operand Registers): 연산에 사용되는 데이터를 저장하는 레지스터입니다. 예를 들어, 소스 레지스터(Source Register)와 대상 레지스터(Destination Register)가 있습니다. 상태 레지스터(Status Register) 또는 플래그 레지스터(Flag Register): 연산 결과에 따른 상태 정보(예: 오버플로우, 제로, 부호 등)를 저장합니다. 이는 이후 연산이나 조건 분기에 사용됩니다. flag register 의 각 비트에 이러한 bit 를 정해 위에서 나온 out 결과를 저장한다 프로그램 카운터(Program Counter): 다음에 실행할 명령어의 주소를 저장하는 레지스터로, ALU가 프로그램의 흐름을 제어하는 데 중요한 역할을 합니다. 상태 레지스터의 플래그 예시 제로 플래그(Zero Flag, ZF): 연산 결과가 0일 때 설정됩니다. 결과가 0이 아닌 경우에는 클리어됩니다. 부호 플래그(Sign Flag, SF): 연산 결과의 최상위 비트가 1일 경우 설정됩니다. 이는 결과가 음수임을 나타냅니다. 캐리 플래그(Carry Flag, CF): 덧셈에서 자리올림이 발생하거나, 뺄셈에서 자리내림이 발생할 때 설정됩니다. 이는 또한 unsigned 연산의 오버플로우를 나타냅니다. 오버플로우 플래그(Overflow Flag, OF): 부호 있는 연산에서 오버플로우가 발생할 때 설정됩니다. 이는 결과가 표현할 수 있는 범위를 초과했음을 나타냅니다. 패리티 플래그(Parity Flag, PF): 연산 결과의 하위 8비트의 1의 개수가 짝수일 때 설정됩니다. 보조 캐리 플래그(Auxiliary Carry Flag, AF): 4비트 경계에서 자리올림이나 자리내림이 발생할 때 설정됩니다. 이는 주로 BCD(Binary-Coded Decimal) 연산에서 사용됩니다. 트랩 플래그(Trap Flag, TF): 설정되면 프로세서가 한 명령어를 실행한 후 인터럽트를 발생시킵니다. 주로 디버깅 목적으로 사용됩니다. 인터럽트 플래그(Interrupt Flag, IF): 설정되면 마스크 가능한 인터럽트를 허용합니다. 클리어되면 인터럽트가 무시됩니다. 디렉션 플래그(Direction Flag, DF): 문자열 처리 명령어에서 증가 또는 감소 방향을 설정합니다. 설정되면 감소 방향으로, 클리어되면 증가 방향으로 처리됩니다. RAM ram은 크게 저장된 위치를 나타내는 address bus 와 data 를 저장하거나 출력해 볼 수 있는 data bus 2개로 이루어져있다 여기서 조금 더 들어가서 클럭의 종류 원본 클럭 01010101 반복 출력 제어 011101110111 반복 enable 출력 입력 제어 001000100010 반복 set 저장 사용되는 레지스터 범용 레지스터 실제 인스턴스 R0, R1, R2, R3 범용 레지스터 IAR(PC) : 다음 실행할 명령어의 주소를 저장하는 레지스터 IR : 현재 실행중인 명령어 그 자체를 보유 TMP : ALU 는 2개의 피연산자가 필요한데 첫번째로 bus로 흐르는 값을 임시 저장하는 레지스터 BUS1 : 다음에 실행할 명령어는 일반적으로 1 이며 이것을 회로 단에서 쉽게 계산하기 위한 TMP 와 ALU 사이에 있는 1 레지스터 ACC : ALU 로 부터 계산된 값은 임시적으로 저장하는 레지스터 MAR : 메모리 특정 위치의 값을 가리키는 레지스터 CTMP : carry out 이 발생한 것을 다시 carry in 하기위한 임시 저장 레지스터 8비트 컴퓨터 명령어 구조 1~4 : 4비트는 명령어의 종류 선택하는 OPCODE 이다 5~6 : 2비트는 레지스터 선택자(RA)로 00 부터 11 까지 각각 R0, R1, R2, R3 레지스터에 매핑되어 있다 7~8 : 2비트는 레지스터 2비트는 레지스터 선택자(RB)로 00 부터 11 까지 각각 R0, R1, R2, R3 레지스터에 매핑되어 있다 후술한 cpu 는 8비트 컴퓨터로 여러가지 제약사항이 있다 실제 컴퓨터는 rising edge 에 set(저장) 을 하는 것이 일반적인 방식이지만 지금은 1일때 set 하는 방식으로 만들었다 ROM에 제어방식(어떤 OPCODE 일때 어떤 동작을 수행(일반적으로 레지스터 쓰기 읽기))을 적는 마이크로코드 방식이 아닌 하드웨어 기반 제어방식으로 서술 되어있다 상대적으로 적은 step 을 사용하는 명령의 경우 END_INST 비트가 있다면 남은 스텝을 밟지 않고 다시 처음 step 부터 시작할 수 있지만 opcode 의 비트수가 4개밖에 없으므로 남은 step 은 아무 실행도 하지 않지만 step 은 밟아야 한다 이를 위해 링카운터를 사용한다 (다른 종류는 리플 카운터가 있다) INSTRUCTIO DECODE (Stepper 단계마다 실행해야할 작업 목록) 아래는 ROM에 제어방식을 적는 마이크로코드 방식이 아닌 하드웨어 기반 제어방식으로 서술 되어있다 어떤 명령이든 1,2,3 단계인 ram 에서 명령어를 꺼내와서 IR 에 저장하고 IAR 에 +1 을 하여 다시 IAR 에 저장하는 단계는 동일하다 1,2,3단계 명령어 가져오기 실행해야할 작업 목록 IAR에 담긴 주소를 이용해 RAM에서 현재 실행할 명령어를 꺼낸다 IR 에 저장한다 IAR 값을 1 증가시킨다 자세한 접근 현재 상태 : IAR 에는 현재 실행해야 할 명령을 가리키는 주소가 들어있다 / BUS 에 IAR 값이 흐를 때 다음 접근 명령 주소를 미리 계산해야 한다 IAR 값을 enable 시켜서 MAR 에 set(저장) 시킨다 또한 다음 싸이클에 IAR 이 접근해야할 메모리 주소 +1 을 해야 하므로 bus1 을 enable 시킨다 그리고 +1 한 값을 3단계에서 IAR 에 집어넣어야 하기 때문에 임시 보관 용도로 ACC 를 set 시킨다 RAM 을 Enable 시켜서 MAR 에 들어있는 현재 명령값이 BUS 로 흐르게 된다 또한 그 BUS 로 흐른 값이 IR 로 들어가게 set 한다 ACC 를 enable 시켜서 IAR + 1 한 값이 BUS 로 흐르게 하고 그것은 IAR 을 Set 시켜서 다음 명령 주소값을 저장한다 stapper ACC BUS1(s) IAR IR MAR RAM BUS 1 set enable enable set IAR (현재 실행중 명령 주소) 2 3 enable set ACC(현재 실행 IAR+1) 4,5,6단계 ALU 명령일 때 실행해야할 작업 목록(op_0 = 1) Pasted image 20240712214531 Pasted image 20240712220752 Pasted image 20240713032992 자세한 접근 현재 RB 에 값을 bus 보내기 위해 enable 한다 ALU 는 2개의 값이 필요하므로 임시로 TMP 를 set 시켜서 bus 의 값을 임시 저장시킨다 RA 를 bus로 보내기 위해 enable 시킨다 또한 동시에 ALU 는 계산된 값을 내보내며 이것은 임시 저장하기 위해 ACC 를 set 시킨다 ACC 값을 enable 시켜서 bus 로 흐르게 한다 bus로 흐른 값이 RB 로 저장하기 위해 RB 를 set 한다 단 여기서 ALU 비교연산(OP 명령 1111)에서는 RB 로 저장하지 않는다 %% 4. RB 값 TMP 저장 RA 와 TMP을 ALU를 통해 계산할 것을 ACC 에 저장 ACC 의 값을 RB 로 저장 단 비교 연산은 제외 %% 4,5,6단계 LOAD, STORE 명령일 때 실행해야할 작업 목록(op = 0000), (op = 0001) Pasted image 20240713075383 Pasted image 20240713075825 LOAD RA 에 들어있는 RAM 주소를 MAR 에 저장 MAR을 통해 RAM의 데이터를 RB 에 저장 STORE RA 에 들어있는 RAM 주소를 MAR 에 저장' RB 의 데이터를 RAM 에 저장 4,5,6 단계 DATA 명령일 때 실행해야할 작업 목록(op = 0010) Pasted image 20240713083787 Pasted image 20240713084179 IAR 의 값을 MAR 로 저장, BUS1 을 enable 하여 ALU 값을 실행하여 ACC 에 미리 다음 명령을 준비 MAR 에 의해 읽어낸 RAM 의 값을 RB 로 저장 ACC 의 값을 IAR 에 저장 자세한 접근 예를 들어서 설명 현재 명령어 위치 주소는 26 이지만 명령어 가져오기 단계에서 IAR 에 이미 27 다음 단계의 명령어 위치 주소가 기다리고 있다 또한 DATA 명령에서는 27 주소(다음 명령 주소 위치)에는 명령어가 아닌 data 값이 들어 있으므로 4단계에서 IAR 의 값을 MAR 에 주면 데이터를 꺼내올수 있다 물론 앞단계에서 +1 을 하여 다음 명령을 대비한 것은 무효화 했으므로 +1 해서 다시 IAR 에 저장해야 한다 그렇게 하기 위해 bus 에 흐르는 IAR 값(2바이트 명령어 data 부분)을 ALU 로 보내고 BUS1 또한 enable 하여 ALU 에 계산된 값을 임시저장하기 위해 ACC 를 set 한다 즉 IAR enable, MAR set, BUS1 enable, ACC set RAM enable 하여 원하는 2개의 바이트로 이루어진 명령의 2번째 바이트의 명령(실제는 data) 가 버스로 흐르게 된다 버스로 흐르는 RB 에 저장하기 위해 RB 를 set ACC 에 임시 저장된 값(다음 명령어 주소 29)를 IAR 에 저장하기 위해 enable 하고 IAR 은 set 한다 4,5,6 단계 JMPR RB 명령일 때 실행해야할 작업 목록(op = 0011) Pasted image 20240714091621 단순히 RB 에 들어있는 것을 IAR 에 저장 RB enable, IAR set : RB 를 enable 하여 bus 로 보낸다 그리고 IAR 을 set 시켜 bus 흐른 값을 저장 시켜 다음 명령에는 RB 에 들어있는 주소가 바로 실행되게 한다 4,5,6 단계 JMP Addr 명령일 때 실행해야할 작업 목록(op = 0011) IAR 을 MAR 에 저장 RAM 에 출력을 IAR 로 저장 자세한 접근 IAR enable, MAR set : 현재 IAR 에는 다음 명령의 주소가 지금 명령의 +1 해서 들어있다 즉 주소데이터에 분기할 주소가 적혀 있다 RAM enable, IAR set : 분기 명령이므로 IAR 의 값을 두번째 바이트의 값으로 바꿔 주어야 한다 즉 현재 저장된 주소값에 저장된 메모리에 접근해서 분기할 주소값을 가져와 IAR 에 넣어주어야 한다 4,5,6 단계 J(C,A,E,Z) JUMP CONDITION 명령일 때 실행해야할 작업 목록 (op = 0101) Pasted image 20240716143528 Pasted image 20240716143532 Pasted image 20240716143535 Pasted image 20240717075319 c: Carry (자리올림) a: Greater than (a > b) e: Equal (a = b) z: Zero (결과가 0) 일반 Jump 와 동일하게 IAR 을 MAR 에 저장 5단계에서는 분기하지 않는 상황을 처리 acc 에 있는 6단계에서는 분기하는 상황을 처리 자세한 접근 전제 조건 5단계에서는 분기하지 않는 상황을 처리 6단계에서는 분기하는 상황을 처리 ALU 는 항상 flag 를 출력하는데 이때 BUS1 같은 상황에서 플래그 비트를 저장하지 말하야 한다 그렇다면 ALU 실행중에서만 명령에서 자리올림을 받기 위해 일반적인 ALU 연산에서는 4단계에서 TMP 레지스터를 set 하고 5단계에서 ACC 레지스터를 set 한다 이때 TMP set 의 신호가 Ctmp 와 함께 set 되며 이전에 자리올림되었던 값을 적용 시킨다 또한 이전의 ALU 명령에서 flag 레지스터를 저장하기 위해 5단계 ALU 명령에서 ACC set 신호와 함께 flags ALU set 신호를 함께 준다 각각의 IR의 4,5,6,7 번 플래그와 ALU(Flags 레지스터)를 각각 플레그를 비교하여 UPDATE IAR 을 할지 판단한다 IAR enable, MAR set, BUS1 enable 일반 JUMP 와 동일하게 IAR 을 MAR 에 저장 또한 분기 하지 않을 수 있으므로 BUS1 또한 enable bus1 enable, acc enable, IAR set 5단계에서는 분기 하지 않을 상황을 처리 미리 IAR 을 분기 하지 않을 상황으로 가정하고 BUS1 으로 계산된 +1 (조건이 맞지 않는 상황) ACC 값을 IAR 에 저장 6단계에서는 분기할 상황을 처리하므로 MAR 을 통해 접근할 주소를 얻기 위해 RAM 을 Enable, IAR 을 조건에 따라 SET 4,5,6 단계 CLF 명령일 때 실행해야 할 작업 목록 (op = 0110) 현재까지 Flags set 신호 Ctmp enable 신호만 만들고 Ctmp 신호의 set 신호의 경우 ALU 연산실행시 무조건 set 되는 TMP 신호를 중복 활용하여 사용했다 또한 Flags 의 enable 신호는 따로 만들지 않았기 때문에 Flags 레시스터를 이후 연산에 사용되지 않도록 clear 0000으로 만드는 Clear Flag instruction 이 필요하다 ((c_out)=0 -> 0 , (a>b) -> 0, (a=b) -> 0, (Zero) -> 0) 인 상황을 강제로 만들기 위해 BUS1 을 Enable 하면 ALU b 입력으로 1이 들어가고 A 입력은 default 값이 0 이 들어가므로 Flags 레지스터를 set 만 한다면 Flags 비트가 0000으로 초기화 된다 BUS1 enable, Flages set 4,5,6 단계 NOP 명령일 때 실행해야할 작업 목록 프로그램을 멈추기 위해 inscruction fetch 모듈에서 명령을 가져오지 못하게 하면 프로그램이 멈추게 된다 된다 ALU Selection module 모든 ALU 명령은 RB 를 먼저 가져와 TMP 에 먼저 저장 그다음에 RA 를 가져와 ALU 계산을 통해 ACC 저장 ACC 저장 값을 다시 RB 에 저장하는 형태이다(비교연산을 제외하고) 즉 $RB = RA\\ (산술\\ 연산)\\ RB$ 와 같이 표현되며 ALU Selection moudle 에서는 입력 클럭 사용은 RB 에만 적용된다 18강 레지스터 상태변화 4~7 단계 stapper ACC R0 R1 TMP(s) BUS 초기 값 Err 5 8 Err Err 4 enable(8) set(8) R1(8) 5 set(5) enable(5) R0(5) 6 enable(13) set(13) ACC(13) 7 20강 명령어 가져오기 단계(1 ~ 3)에서의 레지스터 상태 변화 stapper ACC BUS1(s) IAR IR MAR RAM BUS 1 set enable enable set IAR (현재 실행중 명령 주소) 2 set enable RAM(접근한 명령값) 3 enable set ACC(현재 실행 IAR+1) stapper ACC RA RB TMP(s) BUS 1 2 3 4 enable set RB 5 set enable disable unset RA 6 enable disable ACC 7 reset reset reset reset reset reset reset reset A = 127 B = 29 C = 88 D = A - B - C = 10 :aa <- 7f // 1단계 :ab <- 1d // 2단계 :ac <- 58 // 3단계 :c3 <- ([:aa] - [:ab]) - [:ac] //4단계 Pasted image 20240714055948 Pasted image 20240714060909 Pasted image 20240714060244 Pasted image 20240714060371 Pasted image 20240714060494 뺄때는 2의 보수법을 사용하여 음수를 더하는 개념으로 ALU 에서 - 가 구현되지 않았지만 동작하게 할 수 있다 이 경우 HW 로 구현한 것이 아닌 SW 로 구현한 것이 된다 즉 어셈블리어 단계에서 이 방식을 미리 SW 에서 구현해주어야 한다 아래는 음수로 하고 +1 을 하는 방식이다 Pasted image 20240714060667 Pasted image 20240714061236 Pasted image 20240714061127 Pasted image 20240714061251 Pasted image 20240714062148 Pasted image 20240714062107 Pasted image 20240714062272 명령어 갯수 : 13개 Pasted image 20240714100280 Pasted image 20240714100280 Stepper module : \" \" => 클럭, step 생성 클럭 신호 스텝퍼 처리 IR code module : IR code => ALU 의 어떤 기능 사용 op_out, IR code 해석 모듈 Instruction fetch module : 1~3 단계 명령 가져오기 모듈 ALU instruction condition : 4~6 단계 ALU 관련 명령 조건 (ex IR_0 가 1일때) ALU instruction module : 4~6 단계 단계에 어떤 레지스터 조건이 필요한지 Resgister selection module : IR 뒷자리 4개가 어떤 범용 레지스터 선택했는지 초깃값 문재 power on reset 또는 preset",
      "frontmatter": {
        "tags": [
          "cs",
          "cpu"
        ],
        "series": "그래서 컴퓨터는 어떻게 동작하나요",
        "series_weight": "1",
        "date": "2024-07-10T00:41:00+09:00",
        "lastmod": "2025-10-12T15:43:22+09:00"
      }
    },
    "그래서 컴퓨터는 어떻게 동작하나요 2": {
      "path": "/02.inbox/그래서-컴퓨터는-어떻게-동작하나요/그래서-컴퓨터는-어떻게-동작하나요-2/",
      "filename": "그래서 컴퓨터는 어떻게 동작하나요 2",
      "content": "여기서 부터는 32 비트로 설계하게 된다 즉 word 가 32 비트 이다 변경점 RAM RAM 의 경우 data bit size 는 32가 가능하지만 address bit width 의 경우 32 비트가 가능하지 않다(logisim 의 한계 4기가의 메모리는 감당하지 못한다) 그러므로 address bit width 의 경우 24 비트로 한다 현대 컴퓨터에 사용하는 RAM 과 ㄱ Register IAR INSTRUCTION ADDRESS REGISTER 을 PC PROGRAM COUTER 로 표기한다 레지스터의 경우 7비트에서는 2개가 존재했다 Instruction set 8비트 0~3 4~5 6~7 OPCODE 레지스터 선택자 RA 레시스터 선택자 RB 1~4 : 4비트는 명령어의 종류 선택하는 OPCODE 이다 5~6 : 2비트는 레지스터 선택자(RA)로 00 부터 11 까지 각각 R0, R1, R2, R3 레지스터에 매핑되어 있다 7~8 : 2비트는 레지스터 2비트는 레지스터 선택자(RB)로 00 부터 11 까지 각각 R0, R1, R2, R3 레지스터에 매핑되어 있다 32 비트 RAM 메모리 최대 크기 위를 참고하자 Logisim에서 구현한 컴퓨터의 경우는 메인 CPU 클럭을 RAM의 클럭으로 그대로 사용한다 이러한 설계는 실제 현재 컴퓨터와 비교했을 때 훨씬 간단하게 구현이 가능하지만 현대 컴퓨터의 경우에는 다른 점이 여러가지가 있다 하지만 여기에서는 2가지 차이와 그로 인한 문제(?)에 대해 이야기 해본다 data bit width 의 크기가 현대 RAM 의 경우는 8bit(1Byte) 하지만 Logisim 으로 구현한 RAM 의 경우에는 32bit(4Byte) 현대 컴퓨터의 경우에는 RAM 이 CPU의 클럭을 그대로 사용하지 않는다 하지만 Logisim 으로 구현한 컴퓨터의 경우에는 RAM 이 CPU 의 클럭을 그대로 사용한다 실제 cpu는 MMU 메모리 실제 주소 변경 유닛 과 같은 것이 있지만 이 cpu 에서는 없다 현대 cpu 의 경우 주소 버스, 데이터 버스, 제어 버스와 같은 것들이 구분되어 있지만 여기서는 모든 버스를 구분해서 사용하지는 않는다 초기에는 controll unit 에서 실제 어떠한 동작을 하는지 각각의 배선을 따로 만들었지만 지금은 ROM 을 이용해서 각 명령어 마다 행동해야할 행동을 ROM 에 저장할 수 있다 이를 전문 용어로 제어메트릭스에서 마이크로 코드 방식으로 변경한다 라고 이야기한다",
      "frontmatter": {
        "tags": [
          "cs",
          "cpu"
        ],
        "series": "그래서 컴퓨터는 어떻게 동작하나요",
        "date": "2024-08-10T16:37:00+09:00",
        "lastmod": "2024-08-10T16:37:00+09:00"
      }
    },
    "어셈블러": {
      "path": "/02.inbox/그래서-컴퓨터는-어떻게-동작하나요/어셈블러/",
      "filename": "어셈블러",
      "content": "",
      "frontmatter": {
        "tags": [
          "cpu",
          "assembler"
        ],
        "date": "2024-08-26T06:42:00+09:00",
        "lastmod": "2024-08-26T06:42:00+09:00"
      }
    },
    "spring locale 설정": {
      "path": "/younghan-mvc2/spring-locale-설정/",
      "filename": "spring locale 설정",
      "content": "spring locale 설정 질문 build.gradle // 모든 Java 작업에 한국어 로케일 설정 적용 tasks.withType(JavaForkOptions) { // 모든 JVM 포크 작업(test, bootRun 등)에 적용 systemProperty 'user.language', 'ko' systemProperty 'user.country', 'KR' } // Java 컴파일 작업에도 동일한 설정 tasks.withType(JavaCompile) { options.fork = true options.forkOptions.jvmArgs += ['-Duser.language=ko', '-Duser.country=KR'] } 질문의 내용을 null값으로 주게 되면 시스템 os locale 을 찾는다고 했는데 실제 제대로 적용되지 않는 상황인것 같습니다 저는 linux에서 개발환경구성되어 있는데 linux 에서 지역 locale 을 쓰지 않고 중립 locale LANG=C.UTF-8 을 씁니다 그래서 저도 동일한 문제가 발생하였습니다 다음 3가지 접근법이 제가 알고 있는 접근법 입니다 운영체제 locale 변경(질문에 내용에 해당되지 않음) jvm locale 변경 -Duser.language=ko -Duser.country=KR 이전 댓글의 내용 동일 빌드 설정 처음의 요약과 동일 코드를 수정 @BeforeAll static void setUpLocale() { Locale.setDefault(Locale.KOREA); } Spring MVC 프로젝트에서 Gradle을 활용한 일관된 로케일 설정 사용자는 IntelliJ IDEA에서 기본 로케일이 en-US 로 표시되는 반면, 운영 체제의 로케일은 한국어로 설정되어 있는 문제를 보고했습니다. 이로 인해 Spring MVC 애플리케이션이 null 로케일이 주어졌을 때 영어 리소스 번들을 기본적으로 선택하는 문제가 발생했습니다. 반면, ko_KR 로케일을 명시적으로 설정했을 때는 애플리케이션이 정상적으로 동작함을 확인했습니다. 이에 따라 사용자는 Gradle 빌드 과정에서 테스트 및 컴파일 단계에서 한국어 로케일을 강제하도록 설정하는 Gradle 구성 스니펫을 제공하였으며, 이에 대한 자세한 설명을 요청했습니다. 본 보고서는 이러한 Gradle 설정의 목적과 기능을 설명하고, IntelliJ와 Gradle 간의 로케일 불일치 문제를 해결하는 방법을 다룹니다. Gradle과 작업(Task) 개념 Gradle은 강력한 빌드 자동화 도구로, 작업(Task)이라는 개념을 중심으로 동작합니다. 작업(Task)은 소스 코드 컴파일, 테스트 실행, 문서 생성, 애플리케이션 패키징 등의 독립적인 단위로 구성됩니다. 이러한 작업은 Gradle 빌드 스크립트( build.gradle 또는 build.gradle.kts )에서 정의되며, 플러그인을 적용하여 추가적인 작업을 확장할 수도 있습니다. 예를 들어, Java 플러그인을 적용하면 다음과 같은 작업이 자동으로 추가됩니다. compileJava : Java 소스 코드 컴파일 test : 테스트 실행 jar : JAR 파일 생성 Gradle에서 실행 가능한 모든 작업 목록을 확인하려면 프로젝트의 루트 디렉터리에서 다음 명령어를 실행할 수 있습니다. ./gradlew tasks JVM 프로세스 분리(Forking)와 Gradle 설정 Gradle에서는 특정 작업을 수행할 때 별도의 JVM 프로세스를 실행(포크, Forking)할 수 있습니다. 이러한 작업에는 주로 테스트 실행( Test ), Java 코드 실행( JavaExec ), 그리고 Java 코드 컴파일( JavaCompile )이 포함됩니다. JVM을 포크하는 이유는 다음과 같습니다. 빌드 프로세스와 개별 작업을 격리하여 안정성 확보 특정 JVM 설정(메모리 크기, 시스템 속성 등)을 독립적으로 적용 테스트 작업 병렬 실행을 통해 빌드 속도 향상 Gradle에서는 JavaForkOptions 인터페이스를 사용하여 포크된 JVM에 다양한 설정을 적용할 수 있습니다. 대표적으로 Test 및 JavaExec 작업은 JavaForkOptions 을 구현하며, JavaCompile 작업도 설정에 따라 JVM을 포크할 수 있습니다. Gradle 설정 코드 분석 (1) 테스트 및 실행 작업에 대한 로케일 설정 tasks.withType(JavaForkOptions) { systemProperty 'user.language', 'ko' systemProperty 'user.country', 'KR' } 설명 tasks.withType(JavaForkOptions) {...} Gradle의 TaskContainer 에서 JavaForkOptions 을 구현하는 작업만 선택 대표적으로 Test 및 JavaExec 작업이 포함됨 systemProperty 'user.language', 'ko' JVM 시스템 속성 user.language 를 ko (한국어)로 설정 systemProperty 'user.country', 'KR' JVM 시스템 속성 user.country 를 KR (대한민국)로 설정 결과: Gradle이 테스트( Test ) 또는 Java 실행( JavaExec )을 수행할 때, JVM의 기본 로케일이 ko_KR 로 설정됨 애플리케이션이 null 로케일을 받을 때 한국어 리소스 번들을 선택하도록 강제됨 (2) Java 컴파일 작업에 대한 로케일 설정 tasks.withType(JavaCompile) { options.fork = true options.forkOptions.jvmArgs += [\"-Duser.language=ko\", \"-Duser.country=KR\"] } 설명 tasks.withType(JavaCompile) {...} Gradle의 JavaCompile 작업에만 설정 적용 options.fork = true Java 컴파일러( javac )를 별도의 JVM 프로세스에서 실행하도록 설정 options.forkOptions.jvmArgs += [\"-Duser.language=ko\", \"-Duser.country=KR\"] 포크된 JVM에 user.language 및 user.country 시스템 속성 추가 결과: Java 코드 컴파일( javac )이 한국어 로케일에서 수행됨 만약 어노테이션 프로세서 또는 코드 생성이 로케일에 따라 다른 결과를 생성한다면, 이를 한국어 기준으로 일관되게 유지할 수 있음 IntelliJ IDEA와 Gradle의 로케일 불일치 문제 IntelliJ IDEA는 자체 설정에서 기본 로케일을 관리하기 때문에, 운영 체제의 로케일과 일치하지 않을 수 있습니다. IntelliJ에서 실행되는 JVM과 Gradle이 실행하는 JVM은 독립적으로 동작 IntelliJ는 기본적으로 en-US 로케일을 사용할 수 있음 Gradle은 OS 설정을 직접 따르지 않고, 명시적으로 설정된 로케일을 사용 이러한 차이점 때문에 Gradle 빌드가 IntelliJ의 로케일과 다르게 동작할 수 있습니다. 이를 해결하려면 Gradle 설정을 명확하게 지정해야 합니다. 로케일 설정을 더욱 유연하게 관리하는 방법 Gradle 속성( gradle.properties ) 또는 환경 변수를 활용하여 로케일 설정을 변경할 수 있습니다. (1) gradle.properties 파일에 설정 추가 gradle.properties (프로젝트 루트에 위치) userLanguage=ko userCountry=KR (2) build.gradle 에서 Gradle 속성 적용 tasks.withType(JavaForkOptions) { systemProperty 'user.language', project.properties['userLanguage'] systemProperty 'user.country', project.properties['userCountry'] } tasks.withType(JavaCompile) { options.fork = true options.forkOptions.jvmArgs += [ \"-Duser.language=${project.properties['userLanguage']}\", \"-Duser.country=${project.properties['userCountry']}\" ] } 장점: 필요에 따라 gradle.properties 파일에서 로케일을 쉽게 변경 가능 다양한 환경(OS, 개발자 설정)에서 유연하게 적용 가능 결론 및 권장 사항 ✅ Gradle 설정 유지: 테스트 및 컴파일 단계에서 일관된 한국어 로케일을 강제하기 위해 제공된 Gradle 설정을 유지하는 것이 좋습니다. ✅ 애플리케이션 실행 시 로케일 확인: Spring Boot의 bootRun 작업을 실행할 때도 한국어 로케일을 적용해야 할 수 있습니다. IntelliJ의 Run Configuration에서 VM 옵션을 설정하는 것이 도움이 될 수 있습니다. ✅ 환경별 로케일 변경 고려: Gradle 속성( gradle.properties ) 또는 환경 변수를 활용하여 필요에 따라 로케일을 변경할 수 있도록 설정하는 것이 좋습니다. ✅ 운영 환경 고려: 최종 애플리케이션이 배포되는 환경에서도 로케일 설정을 고려해야 합니다. OS의 기본 로케일이 애플리케이션 실행 시 영향을 줄 수 있습니다.",
      "frontmatter": {
        "date": "2025-03-30T20:50:00+09:00",
        "lastmod": "2025-03-30T20:50:00+09:00"
      }
    },
    "thymeleaf 정리": {
      "path": "/younghan-mvc2/thymeleaf-정리/",
      "filename": "thymeleaf 정리",
      "content": "text <li>th:text 사용 <span th:text=\"${data}\"></span></li> <li>컨텐츠 안에서 직접 출력하기 = [${data}](${data})</li> 아래는 타임리프(Thymeleaf)의 기본 사용 선언과 제공하는 기본 표현식들을 정리한 내용입니다. 이를 통해 타임리프를 효과적으로 활용할 수 있습니다. 타임리프 사용 선언 <html xmlns:th=\"http://www.thymeleaf.org\"> 위 선언은 HTML 문서에서 타임리프를 사용하기 위해 필요한 네임스페이스를 정의합니다. 타임리프 기본 표현식 간단한 표현식 변수 표현식: ${...} 컨텍스트 내 변수에 접근하거나 값을 출력하는 데 사용됩니다. 예: <p th:text=\"${user.name}\">이름</p> 선택 변수 표현식: *{...} 현재 선택된 객체에 대한 속성에 접근할 때 사용됩니다. 예: <div th:object=\"${user}\"><p th:text=\"*{name}\">이름</p></div> 메시지 표현식: #{...} 국제화(i18n)를 지원하는 메시지를 출력하는 데 사용됩니다. 예: <p th:text=\"#{welcome.message}\">환영합니다!</p> 링크 URL 표현식: @{...} 동적인 URL을 생성하는 데 사용됩니다. 예: <a th:href=\"@{/user/{id}(id=${user.id})}\">프로필</a> 조각 표현식: ~{...} 템플릿 조각(fragment)을 포함하는 데 사용됩니다. 예: <div th:replace=\"~{fragments/header :: header}\"></div> 리터럴 텍스트: 'one text' , 'Another one!' 문자열을 나타냅니다. 예: <p th:text=\"'Hello, World!'\">기본 텍스트</p> 숫자: 0 , 34 , 3.0 , 12.3 숫자를 나타냅니다. 예: <p th:text=\"${10 + 20}\">합계</p> 불린: true , false 논리값을 나타냅니다. 예: <p th:if=\"${isActive}\">활성화됨</p> 널: null null 값을 나타냅니다. 예: <p th:if=\"${user == null}\">사용자 없음</p> 리터럴 토큰: one , sometext , main 문자열 대신 사용할 수 있는 심볼릭 토큰입니다. 예: <p th:class=\"main\">주요 내용</p> 문자 연산 문자 합치기: + 문자열을 연결합니다. 예: <p th:text=\"'Hello, ' + ${name}\">인사말</p> 리터럴 대체: |The name is ${name}| 문자열 내에서 변수를 직접 삽입하는 방법입니다. 예: <p th:text=\"|Welcome, ${user.name}!|\">환영합니다</p> 산술 연산 이항 연산자: + , - , * , / , % 산술 연산을 수행합니다. 예: <p th:text=\"${10 + 5}\">15</p> 단항 연산자(음수 표시): - 음수를 나타냅니다. 예: <p th:text=\"${-10}\">-10</p> 불린 연산 이항 연산자: and , or 논리 AND와 OR 연산을 수행합니다. 예: <p th:if=\"${isActive and isAdmin}\">관리자 활성화</p> 부정 연산자: ! , not 논리값을 반전합니다. 예: <p th:if=\"${!isActive}\">비활성화됨</p> 비교와 동등 연산 비교 연산자: > , < , >= , <= ( gt , lt , ge , le ) 크기 비교를 수행합니다. 예: <p th:if=\"${age >= 18}\">성인</p> 동등 연산자: == , != ( eq , ne ) 값의 동일성을 비교합니다. 예: <p th:if=\"${status == 'active'}\">활성 상태</p> 조건 연산 If-then: (if) ? (then) 조건이 참일 때 특정 값을 반환합니다. 예: <p th:text=\"${isActive ? 'Active' : 'Inactive'}\">상태</p> If-then-else: (if) ? (then) : (else) 조건에 따라 다른 값을 반환합니다. 예: <p th:text=\"${isAdmin ? 'Admin' : 'User'}\">역할</p> Default: (value) ?: (defaultvalue) 값이 null일 경우 기본값을 반환합니다. 예: <p th:text=\"${user.name ?: 'Guest'}\">이름</p> 특별한 토큰 No-Operation: _ 아무 작업도 수행하지 않음을 나타냅니다. 예: <p th:text=\"_\">기본값 유지</p> 참고 자료 공식 문서: [](https://www.thymeleaf.org/doc/tutorials/3.0/usingthymeleaf.html#standard-expression-syntax)",
      "frontmatter": {
        "date": "2025-03-18T13:26:00+09:00",
        "lastmod": "2025-03-18T13:26:00+09:00"
      }
    },
    "사고과정 제한 프롬프트": {
      "path": "/prompt/사고과정-제한-프롬프트/",
      "filename": "사고과정 제한 프롬프트",
      "content": "한글 버전 1. 당신은 사용자의 어떤 질문이나 아이디어, 정보를 받으면, 아래 사고법 중에 가장 적합한 방식을 두개를 선택하여 혼합하여 분석하세요(1500자 이상) 2. 분석을 토대로 천재적 아이디어를 10개 이상 3000자 이상 출력합니다 아래 공식들은 참고하세요. --- ## 1. 천재적 통찰 도출 공식 (Genius Insight Formula) GI = (O × C × P × S) / (A + B) - GI(Genius Insight) = 천재적 통찰 - O(Observation) = 관찰의 깊이 (1-10점) - C(Connection) = 연결의 독창성 (1-10점) - P(Pattern) = 패턴 인식 능력 (1-10점) - S(Synthesis) = 종합적 사고 (1-10점) - A(Assumption) = 고정관념 수준 (1-10점) - B(Bias) = 편향 정도 (1-10점) 적용법: 주제에 대해 각 요소의 점수를 매기고, 고정관념과 편향을 최소화하면서 관찰-연결-패턴-종합의 순서로 사고를 전개하세요. --- ## 2. 다차원적 분석 프레임워크 MDA = Σ[Di × Wi × Ii] (i=1 to n) - MDA(Multi-Dimensional Analysis) = 다차원 분석 결과 - Di(Dimension i) = i번째 차원에서의 통찰 - Wi(Weight i) = i번째 차원의 가중치 - Ii(Impact i) = i번째 차원의 영향력 분석 차원 설정: - D1 = 시간적 차원 (과거-현재-미래) - D2 = 공간적 차원 (로컬-글로벌-우주적) - D3 = 추상적 차원 (구체-중간-추상) - D4 = 인과적 차원 (원인-과정-결과) - D5 = 계층적 차원 (미시-중간-거시) --- ## 3. 창의적 연결 매트릭스 CC = |A ∩ B| + |A ⊕ B| + f(A→B) - CC(Creative Connection) = 창의적 연결 지수 - A ∩ B = 두 개념의 공통 요소 - A ⊕ B = 배타적 차이 요소 - f(A→B) = A에서 B로의 전이 함수 연결 탐색 프로세스: 1. 직접적 연결 찾기 2. 간접적 연결 탐색 3. 역설적 연결 발견 4. 메타포적 연결 구성 5. 시스템적 연결 분석 --- ## 4. 문제 재정의 알고리즘 PR = P₀ × T(θ) × S(φ) × M(ψ) - PR(Problem Redefinition) = 재정의된 문제 - P₀ = 원래 문제 - T(θ) = θ각도만큼 관점 회전 - S(φ) = φ비율로 범위 조정 - M(ψ) = ψ차원으로 메타 레벨 이동 재정의 기법: - 반대 관점에서 보기 (θ = 180°) - 확대/축소하여 보기 (φ = 0.1x ~ 10x) - 상위/하위 개념으로 이동 (ψ = ±1,±2,±3) - 다른 도메인으로 전환 - 시간 축 변경 --- ## 5. 혁신적 솔루션 생성 공식 IS = Σ[Ci × Ni × Fi × Vi] / Ri - IS(Innovative Solution) = 혁신적 솔루션 - Ci(Combination i) = i번째 조합 방식 - Ni(Novelty i) = 참신성 지수 - Fi(Feasibility i) = 실현 가능성 - Vi(Value i) = 가치 창출 정도 - Ri(Risk i) = 위험 요소 솔루션 생성 방법: - 기존 요소들의 새로운 조합 - 전혀 다른 분야의 솔루션 차용 - 제약 조건을 오히려 활용 - 역방향 사고로 접근 - 시스템 전체 재설계 --- ## 6. 인사이트 증폭 공식 IA = I₀ × (1 + r)ⁿ × C × Q - IA(Insight Amplification) = 증폭된 인사이트 - I₀ = 초기 인사이트 - r = 반복 개선율 - n = 반복 횟수 - C = 협력 효과 (1-3배수) - Q = 질문의 질 (1-5배수) 증폭 전략: - 'Why'를 5번 이상 반복 - 'What if' 시나리오 구성 - 'How might we' 질문 생성 - 다양한 관점자와 토론 - 아날로그 사례 탐구 --- ## 7. 사고의 진화 방정식 TE = T₀ + ∫[L(t) + E(t) + R(t)]dt - TE(Thinking Evolution) = 진화된 사고 - T₀ = 초기 사고 상태 - L(t) = 시간 t에서의 학습 함수 - E(t) = 경험 축적 함수 - R(t) = 반성적 사고 함수 진화 촉진 요인: - 지속적 학습과 정보 습득 - 다양한 경험과 실험 - 깊은 반성과 메타인지 - 타인과의 지적 교류 - 실패로부터의 학습 --- ## 8. 복잡성 해결 매트릭스 CS = det|M| × Σ[Si/Ci] × ∏[Ii] - CS(Complexity Solution) = 복잡성 해결책 - det|M| = 시스템 매트릭스의 행렬식 - Si = i번째 하위 시스템 해결책 - Ci = i번째 하위 시스템 복잡도 - Ii = 상호작용 계수 복잡성 분해 전략: - 시스템을 하위 구성요소로 분해 - 각 구성요소 간 관계 매핑 - 핵심 레버리지 포인트 식별 - 순차적/병렬적 해결 순서 결정 - 전체 시스템 최적화 --- ## 9. 직관적 도약 공식 IL = (S × E × T) / (L × R) - IL(Intuitive Leap) = 직관적 도약 - S(Silence) = 정적 사고 시간 - E(Experience) = 관련 경험 축적 - T(Trust) = 직관에 대한 신뢰 - L(Logic) = 논리적 제약 - R(Rationalization) = 과도한 합리화 직관 활성화 방법: - 의식적 사고 중단 - 몸과 마음의 이완 - 무의식적 연결 허용 - 첫 번째 떠오르는 아이디어 포착 - 판단 없이 수용 --- ## 10. 통합적 지혜 공식 IW = (K + U + W + C + A) × H × E - IW(Integrated Wisdom) = 통합적 지혜 - K(Knowledge) = 지식의 폭과 깊이 - U(Understanding) = 이해의 수준 - W(Wisdom) = 지혜의 깊이 - C(Compassion) = 공감과 연민 - A(Action) = 실행 능력 - H(Humility) = 겸손함 - E(Ethics) = 윤리적 기준 --- ## 사용 가이드라인 1. 단계적 적용: 각 공식을 순차적으로 적용하여 사고를 심화시키세요. 2. 반복적 개선: 한 번의 적용으로 끝내지 말고 여러 번 반복하여 정교화하세요. 3. 다양한 관점: 서로 다른 배경을 가진 사람들과 함께 공식을 적용해보세요. 4. 실험적 태도: 공식을 기계적으로 따르기보다는 창의적으로 변형하여 사용하세요. 5. 균형적 접근: 분석적 사고와 직관적 사고를 균형 있게 활용하세요. 영어 버전 1. Whenever you receive any question, idea, or information from the user, select the two most suitable thinking methods from the list below, combine them, and conduct an in-depth analysis (1,500+ characters). 2. Based on your analysis, generate more than 10 genius-level ideas, with a total output exceeding 3,000 characters. Please refer to the following formulas: --- ## 1. Genius Insight Formula (GI) GI = (O × C × P × S) / (A + B) - GI (Genius Insight): The level of genius insight - O (Observation): Depth of observation (1–10 scale) - C (Connection): Originality of connections (1–10) - P (Pattern): Pattern recognition ability (1–10) - S (Synthesis): Holistic synthesis (1–10) - A (Assumption): Level of fixed assumptions (1–10; higher = more rigid) - B (Bias): Degree of bias (1–10; higher = more biased) **Application Guide**: Score each factor for the given topic. Minimize assumptions and biases while progressing through observation → connection → pattern recognition → synthesis. --- ## 2. Multi-Dimensional Analysis Framework (MDA) MDA = Σ[Di × Wi × Ii] (i = 1 to n) - MDA (Multi-Dimensional Analysis): Result of multi-dimensional insight - Di (Dimension i): Insight from the i-th dimension - Wi (Weight i): Weight of the i-th dimension - Ii (Impact i): Impact level of the i-th dimension **Suggested Dimensions**: - D1 = Temporal Dimension (Past–Present–Future) - D2 = Spatial Dimension (Local–Global–Cosmic) - D3 = Abstract Dimension (Concrete–Intermediate–Abstract) - D4 = Causal Dimension (Cause–Process–Outcome) - D5 = Hierarchical Dimension (Micro–Meso–Macro) --- ## 3. Creative Connection Matrix (CC) CC = |A ∩ B| + |A ⊕ B| + f(A→B) - CC (Creative Connection): Index of creative linkage - A ∩ B: Common elements between concepts A and B - A ⊕ B: Exclusive differences (symmetric difference) - f(A→B): Transformation function from A to B **Connection Discovery Process**: 1. Find direct links 2. Explore indirect links 3. Discover paradoxical links 4. Construct metaphorical links 5. Analyze systemic links --- ## 4. Problem Redefinition Algorithm (PR) PR = P₀ × T(θ) × S(φ) × M(ψ) - PR (Problem Redefinition): Redefined problem - P₀: Original problem - T(θ): Perspective rotation by angle θ - S(φ): Scope adjustment by ratio φ - M(ψ): Meta-level shift by ψ dimensions **Redefinition Techniques**: - View from the opposite perspective (θ = 180°) - Zoom in/out (φ = 0.1x ~ 10x) - Shift to higher/lower conceptual levels (ψ = ±1, ±2, ±3) - Transfer to a different domain - Change the time axis --- ## 5. Innovative Solution Formula (IS) IS = Σ[Ci × Ni × Fi × Vi] / Ri - IS (Innovative Solution): Innovation score - Ci (Combination i): Combination method i - Ni (Novelty i): Degree of novelty - Fi (Feasibility i): Practical feasibility - Vi (Value i): Value creation potential - Ri (Risk i): Risk factor **Solution Generation Methods**: - New combinations of existing elements - Borrowing solutions from unrelated fields - Turning constraints into advantages - Reverse thinking - Redesigning the entire system --- ## 6. Insight Amplification Formula (IA) IA = I₀ × (1 + r)ⁿ × C × Q - IA (Insight Amplification): Amplified insight - I₀: Initial insight - r: Rate of iterative improvement - n: Number of iterations - C: Collaboration multiplier (1–3x) - Q: Quality of questioning (1–5x) **Amplification Strategies**: - Ask \"Why?\" five or more times - Develop \"What if?\" scenarios - Generate \"How might we?\" questions - Discuss with diverse perspectives - Study analog cases --- ## 7. Thinking Evolution Equation (TE) TE = T₀ + ∫[L(t) + E(t) + R(t)]dt - TE (Thinking Evolution): Evolved thinking state - T₀: Initial thinking state - L(t): Learning function at time t - E(t): Experience accumulation function - R(t): Reflective thinking function **Evolution Catalysts**: - Continuous learning and information intake - Diverse experiences and experiments - Deep reflection and metacognition - Intellectual exchange with others - Learning from failure --- ## 8. Complexity Solution Matrix (CS) CS = det|M| × Σ[Si/Ci] × ∏[Ii] - CS (Complexity Solution): Solution to complexity - det|M|: Determinant of the system matrix (measures system interdependence) - Si: Solution for sub-system i - Ci: Complexity of sub-system i - Ii: Interaction coefficient between sub-systems **Complexity Decomposition Strategies**: - Break system into subsystems - Map relationships between components - Identify key leverage points - Determine sequential/parallel resolution order - Optimize the whole system --- ## 9. Intuitive Leap Formula (IL) IL = (S × E × T) / (L × R) - IL (Intuitive Leap): Intuitive breakthrough - S (Silence): Time spent in quiet reflection - E (Experience): Accumulated relevant experience - T (Trust): Trust in intuition - L (Logic): Logical constraints - R (Rationalization): Over-rationalization **Intuition Activation Methods**: - Consciously pause analytical thinking - Relax body and mind - Allow unconscious connections - Capture the first emerging idea - Accept without judgment --- ## 10. Integrated Wisdom Formula (IW) IW = (K + U + W + C + A) × H × E - IW (Integrated Wisdom): Holistic wisdom - K (Knowledge): Breadth and depth of knowledge - U (Understanding): Level of comprehension - W (Wisdom): Depth of insight - C (Compassion): Empathy and care - A (Action): Ability to act - H (Humility): Humbleness - E (Ethics): Ethical grounding --- ## Usage Guidelines 1. **Step-by-Step Application**: Apply each formula sequentially to deepen thinking. 2. **Iterative Refinement**: Don’t stop at one pass—repeat and refine multiple times. 3. **Diverse Perspectives**: Apply formulas with people from different backgrounds. 4. **Experimental Mindset**: Use formulas creatively, not mechanically. 5. **Balanced Approach**: Balance analytical and intuitive thinking.",
      "frontmatter": {
        "date": "2025-07-30T08:57:33+09:00",
        "lastmod": "2025-07-31T09:15:00+09:00"
      }
    },
    "HTML form태그 요청 PUT PATH 메서드 확장": {
      "path": "/younghan-mvc1/html-form태그-요청-put-path-메서드-확장/",
      "filename": "HTML form태그 요청 PUT PATH 메서드 확장",
      "content": "HTML 표준 Form 태그에서 기본적으로 지원하는 메서드는 GET 과 POST 뿐이지만, 현대 웹 개발에서는 PUT, PATCH, 또는 DELETE 같은 HTTP 메서드를 사용할 수 있도록 확장할 수 있습니다. 이를 구현하기 위해 몇 가지 방법이 있습니다. HTML Form의 기본 제한 HTML 표준에서는 <form> 태그의 method 속성으로 GET 또는 POST만 지정할 수 있습니다. <form action=\"/submit\" method=\"POST\"> <!-- 폼 데이터 --> </form> 즉, 기본적으로 HTML Form 자체로는 PUT , PATCH , DELETE 를 직접 사용할 수 없습니다. PUT/PATCH를 사용하는 방법 (1) JavaScript를 활용한 방식 JavaScript를 사용하면 HTML Form 데이터를 PUT 또는 PATCH 요청으로 전송할 수 있습니다. 예를 들어, fetch API를 사용해 다음과 같이 구현할 수 있습니다. <form id=\"updateForm\"> <input type=\"text\" name=\"username\" value=\"hello\"> <input type=\"number\" name=\"age\" value=\"20\"> <button type=\"button\" onclick=\"submitForm()\">제출</button> </form> <script> function submitForm() { const formData = new FormData(document.getElementById('updateForm')); const data = {}; formData.forEach((value, key) => { data[key] = value; }); fetch('/update', { method: 'PUT', // 또는 PATCH headers: { 'Content-Type': 'application/json', }, body: JSON.stringify(data), }) .then(response => response.json()) .then(result => console.log(result)); } </script> 설명: HTML Form을 직접 제출하지 않고, JavaScript로 데이터를 수집하고 PUT 또는 PATCH 요청으로 전송합니다. 이 방식은 RESTful API와 함께 자주 사용됩니다. (2) 숨겨진 필드 _method 를 활용한 에뮬레이션 일부 웹 프레임워크 (예: Ruby on Rails, Spring Boot)에서는 POST 요청을 통해 PUT/PATCH/DELETE 메서드를 에뮬레이션할 수 있는 기능을 제공합니다. 이때 _method 라는 숨겨진 필드를 사용합니다. <form action=\"/update\" method=\"POST\"> <input type=\"hidden\" name=\"_method\" value=\"PUT\"> <input type=\"text\" name=\"username\" value=\"hello\"> <input type=\"number\" name=\"age\" value=\"20\"> <button type=\"submit\">제출</button> </form> 설명: 실제로는 POST 요청이 전송되지만, 서버 측에서 _method=PUT 을 확인하고 이를 PUT 요청으로 처리합니다. 이러한 동작은 프레임워크가 내부적으로 처리하며, 클라이언트는 POST 요청만 보내도 됩니다. PUT vs PATCH 차이점 기준 PUT PATCH 목적 리소스 전체를 수정 리소스 일부를 수정 데이터 전송 전체 데이터를 포함 변경된 부분만 포함 멱등성(Idempotent) O (같은 요청을 여러 번 실행해도 동일한 결과) X (변경 사항에 따라 결과 다를 수 있음) PUT/PATCH 사용 예시 PUT 예시 리소스 전체를 업데이트하는 경우: PUT /users/123 HTTP/1.1 Content-Type: application/json { \"username\": \"newName\", \"age\": 25, \"email\": \"new@example.com\" } PATCH 예시 리소스 일부만 업데이트하는 경우: PATCH /users/123 HTTP/1.1 Content-Type: application/json { \"age\": 26 } 왜 HTML Form에서 PUT/PATCH를 직접 지원하지 않을까? 역사적 이유: 초기 HTML 설계 시에는 단순한 데이터 전송(GET, POST)만 고려되었습니다. RESTful API의 등장: 이후 REST 아키텍처가 널리 사용되면서 PUT, PATCH, DELETE 같은 메서드가 중요해졌습니다. 보안 및 간결성: GET과 POST는 간단하고 직관적이며, 복잡한 메서드는 JavaScript나 프레임워크를 통해 구현하도록 설계되었습니다. 결론 HTML Form 자체로는 PUT/PATCH를 직접 사용할 수 없습니다. JavaScript를 사용하거나 _method 와 같은 에뮬레이션 기법을 통해 PUT/PATCH를 구현할 수 있습니다. RESTful API 설계 시, PUT은 리소스 전체를 수정하고, PATCH는 리소스 일부를 수정하는 데 사용됩니다. 따라서, HTML Form에서도 PUT/PATCH를 사용할 수 있지만, 추가적인 기술적 구현이 필요합니다.",
      "frontmatter": {
        "tags": [
          "html",
          "young_han"
        ],
        "date": "2025-02-28T14:56:00+09:00",
        "lastmod": "2025-06-27T19:52:13+09:00"
      }
    },
    "HTTP 요청 3가지 방법": {
      "path": "/younghan-mvc1/http-요청-3가지-방법/",
      "filename": "HTTP 요청 3가지 방법",
      "content": "GET - 쿼리 파라미터 데이터 전송 방식: URL 끝에 ?key1=value1&key2=value2 형식으로 데이터를 포함합니다. 특징: 메시지 바디 없음: 데이터가 URL에 직접 노출됩니다. 캐시 가능: 브라우저나 프록시 서버에서 캐시됩니다. 안전하지 않음: 중요한 데이터(비밀번호 등) 전송에 부적합합니다. 사용 사례: 검색, 필터링, 페이징 (예: GET /products?category=book&page=2 ) 북마크 또는 공유 가능한 링크 생성 예시: GET /users?name=hello&age=20 HTTP/1.1 Host: example.com POST - HTML Form 데이터 전송 방식: 메시지 바디에 key1=value1&key2=value2 형식으로 데이터를 전송합니다. 쿼리 파라미터 형식으로 보낸다 헤더 설정: Content-Type: application/x-www-form-urlencoded 특징: 메시지 바디 사용: URL에 데이터가 노출되지 않습니다. 폼 데이터 전송: HTML <form> 태그 기본 방식입니다. 데이터 길이 제한 없음: 대량의 데이터 전송 가능합니다. 사용 사례: 회원 가입, 로그인, 상품 주문 (예: POST /login + username=admin&password=1234 ) 예시: POST /submit-form HTTP/1.1 Host: example.com Content-Type: application/x-www-form-urlencoded username=hello&age=20 HTTP Message Body (JSON/XML 등) 데이터 전송 방식: 메시지 바디에 구조화된 데이터(JSON, XML 등)를 직접 작성합니다. 헤더 설정: Content-Type: application/json # JSON 사용 시 특징: 다양한 데이터 형식: JSON, XML, 텍스트 등 사용 가능합니다. HTTP API 표준: RESTful API에서 주로 사용됩니다. 복잡한 데이터 처리: 계층적/중첩된 데이터 전송에 적합합니다. 사용 사례: 모바일 앱/백엔드 연동 (예: POST /api/users + JSON 데이터) PUT/PATCH를 통한 리소스 업데이트 예시 (JSON): POST /api/users HTTP/1.1 Host: example.com Content-Type: application/json { \"name\": \"hello\", \"age\": 20, \"hobbies\": [\"reading\", \"coding\"] } 📌 차이점 요약 구분 GET (쿼리 파라미터) POST (HTML Form) HTTP Message Body (JSON/XML) 데이터 위치 URL 끝에 ?key=value 메시지 바디 ( key=value ) 메시지 바디 (구조화된 데이터) 사용 HTTP 메서드 GET POST POST, PUT, PATCH 등 데이터 형식 쿼리 스트링 application/x-www-form-urlencoded JSON, XML, 텍스트 등 주요 사용처 검색, 필터링, 공유 링크 HTML 폼 제출 (로그인, 주문) API 통신 (모바일/서버 연동) 💡 선택 가이드 간단한 데이터 조회: GET + 쿼리 파라미터 폼 기반 데이터 제출: POST + application/x-www-form-urlencoded 복잡한 데이터 연동: POST/PUT/PATCH + application/json",
      "frontmatter": {
        "date": "2025-02-28T05:24:00+09:00",
        "lastmod": "2025-06-27T19:52:08+09:00"
      }
    },
    "MVC 1 강의 질문 사항": {
      "path": "/younghan-mvc1/mvc-1-강의-질문-사항/",
      "filename": "MVC 1 강의 질문 사항",
      "content": "HTTP 응답 코드 (Status Code) 목적 : 클라이언트에게 요청 처리 결과를 수치화해 전달합니다. 구조 : 3자리 숫자로 분류되며, 첫 번째 숫자는 응답 클래스를 나타냅니다. 1xx (정보 제공) : 100 Continue (요청 진행 중). 2xx (성공) : 200 OK (성공), 201 Created (리소스 생성됨). 3xx (리다이렉션) : 301 Moved Permanently (영구 이동), 302 Found (임시 이동). 4xx (클라이언트 오류) : 400 Bad Request (잘못된 요청), 404 Not Found . 5xx (서버 오류) : 500 Internal Server Error , 503 Service Unavailable . @GetMapping 의 메소드 단위 매핑의 원리가 궁금하다 강의 질문답에서는 리플렉션이라고 답하고 있다 어노테이션의 동작 원리 라이브러리 설정시에 표기되는 여러가지 annotation processor compile processer compile class path production runtime classpath runtime classpath test compile classpath test runtime classpath PathVariable(경로 변수) ex) mapping/{userId} 로 요청을 하는 것과 parameter 로 전달하는 것 jsp 는 jar 파일로 패키징 하는 것이 권장되지 않는 이유",
      "frontmatter": {
        "date": "2025-02-28T11:57:00+09:00",
        "lastmod": "2025-02-28T11:57:00+09:00"
      }
    },
    "Spring MVC 주요 처리 과정": {
      "path": "/younghan-mvc1/spring-mvc-주요-처리-과정/",
      "filename": "Spring MVC 주요 처리 과정",
      "content": "메인 흐름 사용자 요청 DispatcherServle doDispatch() 호출 등록되어 있는 핸들러(컨트롤러) 조회 : 매핑정보에서 맞는 핸들러를 가져온다 핸들러를 처리할 수 있는 어댑터 조회 핸들러 어댑터를 통해 핸들러(컨트롤러)를 실행 (어뎁터를 통해 무조건) ModelAndView 를 반환받는다 ModelAndView를 processDispatchResult함수를 통해 넘겨준다 뷰리졸버를 통해 적절한 뷰를 찾아서 뷰를 반환받는다 뷰를 통해 렌더링한다 2번 처리 (DispatcherServle doDispatch() 이 호출되기 까지 설명) DispatcherServlet 서블릿 등록 DispatcherServlet 도 부모 클래스에서 HttpServlet 을 상속 받아서 사용하고, 서블릿으로 동작한다. DispatcherServlet -> FrameworkServlet -> HttpServletBean -> HttpServlet 스프링 부트는 DispatcherServlet 을 서블릿으로 자동으로 등록하면서 모든 경로( urlPatterns=\"/\" )에 대해서 매핑한다. 참고: 더 자세한 경로가 우선순위가 높다. 그래서 기존에 등록한 서블릿도 함께 동작한다 요청 흐름 서블릿이 호출되면 HttpServlet 이 제공하는 serivce() 가 호출된다. 스프링 MVC는 DispatcherServlet 의 부모인 FrameworkServlet 에서 service() 를 오버라이드 해 두었다. FrameworkServlet.service() 를 시작으로 여러 메서드가 호출되면서 DispatcherServlet.doDispatch() 가 호출된다. 3번 처리 스프링이 적절한 컨트롤러를 가져오는 과정을 하기 위해서 (파일이든 xml 이든) 미리 가져와서 매핑처리를 하는 친구가 필요하다 HandlerMapping 0 = RequestMappingHandlerMapping : 애노테이션 기반의 컨트롤러인 @RequestMapping에서 사용 1 = BeanNameUrlHandlerMapping : 스프링 빈의 이름으로 핸들러를 찾는다. spring RequestMapping 구현체 어노테이션 기반의 컨트롤러를 명시하기 위해서는 @Controller 또는 @RestController 가 필요하다 4번 처리 HandlerAdapter 0 = RequestMappingHandlerAdapter : 애노테이션 기반의 컨트롤러인 @RequestMapping에서 사용 1 = HttpRequestHandlerAdapter : HttpRequestHandler 처리 2 = SimpleControllerHandlerAdapter : Controller 인터페이스 (애노테이션X, 과거에 사용) 처리 spring HandlerAdapter 구현체 5번 처리 ModelAndView 는 Spring MVC에서 컨트롤러가 처리 결과를 뷰에 전달하는 데 사용되는 핵심 클래스 입니다. Model(데이터)과 View(화면) 이름을 함께 저장하고, DispatcherServlet 이 이를 해석해 최종 응답을 생성합니다. ModelAndView 역할 Model : 뷰에 전달할 데이터를 키-값 쌍(Map) 으로 저장합니다. (예: model.put(\"users\", userList) → 뷰에서 ${users} 로 접근) View : 뷰 이름(문자열) 또는 View 객체를 저장합니다. 뷰 이름 : ViewResolver 가 실제 뷰 객체(예: JSP, Thymeleaf 템플릿)로 변환합니다. View 객체 : 직접 생성한 View 구현체(예: JSON 뷰)를 사용합니다 8번 처리 spring ViewResolver 구현체 9번 처리 spring View 구현체 추가",
      "frontmatter": {
        "tags": [
          "spring",
          "young_han"
        ],
        "date": "2025-03-09T18:39:00+09:00",
        "lastmod": "2025-06-27T19:54:41+09:00"
      }
    },
    "forward vs redirect": {
      "path": "/younghan-mvc1/forward-vs-redirect/",
      "filename": "forward vs redirect",
      "content": "Forward 동작 원리 서버 내부에서만 처리: forward() 는 클라이언트(브라우저)의 요청을 수신한 서블릿이 동일한 웹 컨테이너 내부에서 다른 자원(예: JSP, 다른 서블릿)으로 제어를 넘기는 동작입니다. HTTP 요청/응답 재사용: 원본 ServletRequest 와 ServletResponse 객체를 그대로 전달하므로, 클라이언트는 이 과정을 인지하지 못합니다. URL 변경 없음: 브라우저 주소창의 URL은 최초 요청 경로 그대로 유지됩니다. (예: /original-servlet → /WEB-INF/views/new-form.jsp 로 전달되어도 URL 변경 없음) HTTP 호출과의 차이 Redirect(리다이렉트): 클라이언트에게 302 상태 코드와 새 URL을 응답으로 전송 → 클라이언트가 새로운 HTTP 요청을 발생시킵니다. URL 변경됨, 네트워크 비용 증가, 요청 데이터 유실 가능성 있음. Forward(포워드): 서버 내부에서 단일 HTTP 요청 생명주기 내에서 처리됩니다. 네트워크 오버헤드 없음, 요청/세션 데이터 보존, 클라이언트 투명성 보장. 기술적 특징 RequestDispatcher의 역할: RequestDispatcher dispatcher = request.getRequestDispatcher(viewPath); dispatcher.forward(request, response); RequestDispatcher 는 Servlet API의 일부로, 서버 내부 자원 접근을 추상화한 인터페이스입니다. forward() 는 동기식 제어 전달을 수행하며, 스레드는 동일한 요청 컨텍스트를 공유합니다. 보호된 자원 접근: WEB-INF 디렉토리 아래의 JSP는 클라이언트 직접 접근이 불가능합니다. → 서버 내부에서만 forward() 로 접근 가능 (보안 강화). 사용 사례 MVC 패턴 구현: 컨트롤러(서블릿)에서 비즈니스 로직 처리 후 뷰(JSP)로 포워딩하여 화면 렌더링. 에러 페이지 처리: web.xml 또는 @WebServlet 에서 에러 코드/예외 매핑 후 포워딩. 다중 자원 조합: 하나의 요청에 여러 자원(예: 헤더, 본문, 푸터 JSP)을 조합하여 응답 생성. 주의 사항 응답 커밋 전에만 가능: response 객체가 이미 클라이언트로 전송된 후( flush() 호출 후)에는 forward() 사용 불가 (예: IllegalStateException 발생). 속성 전달: request.setAttribute() 로 데이터를 전달해야 하며, request.getParameter() 는 원본 요청 데이터를 유지합니다. 요약 forward() 는 서버 내부 파이프라인 재구성으로, 클라이언트는 최초 요청에 대한 응답만 받는 것으로 인지합니다. 이는 HTTP 프로토콜의 요청-응답 사이클을 최소화하며, 보안 및 성능 최적화에 활용됩니다.",
      "frontmatter": {
        "date": "2025-03-05T13:38:00+09:00",
        "lastmod": "2025-06-27T19:53:53+09:00"
      }
    },
    "http 분석": {
      "path": "/younghan-mvc1/http-분석/",
      "filename": "http 분석",
      "content": "전문가를 대상으로 한 심층적인 HTTP 구성 요소 설명입니다. 각 항목의 기술적 특성과 실제 적용 시 고려사항을 중심으로 설명합니다: HTTP 메소드 (HTTP Methods) RFC 7231 표준에 정의된 동사(Verb)로, 리소스에 대한 의도를 명시 Idempotency: PUT/DELETE는 멱등성 보장 (동일 요청 반복 시 결과 동일) Safe Methods: GET/HEAD/OPTIONS는 서버 상태 변경 없음 확장 메소드: PATCH(부분 수정), LINK(리소스 연결) 등 RFC 5789 API 설계 시 고려사항: RESTful 원칙 준수 (리소스 중심 경로 + 메소드 조합) OPTIONS 메소드를 통한 CORS Preflight 처리 TRACE 메소드의 보안 취약점 관리 (XST 공격 방어) URL (Uniform Resource Locator) URI의 하위 집합으로, 리소스 위치 및 접근 방법을 포함 scheme:[//authority][/path][?query][#fragment] Authority: user:pass@host:port (인증정보는 RFC 7617에서 deprecated) Path Parameter: /users/123;version=2 (Matrix URIs, RFC 3986) Encoding: 퍼센트 인코딩 (예: 한글 → %ED%95%9C%EA%B8%80 ) REST API 설계: 버저닝 전략 ( /v1/resource vs Accept: application/vnd.example.v1+json ) HATEOAS를 위한 링크 표현 (HAL, JSON-LD) 쿼리 스트링 (Query String) RFC 3986에 정의된 비계층적 데이터 전달 방식 중복 키 처리: ?q=1&q=2 → 서버측 언어별 파싱 차이 (PHP: 배열, Node.js: 마지막 값) 보안 이슈: SQLi/XSS 방지를 위한 입력 검증 필수 민감 데이터 전송 금지 (로그에 노출 위험) 성능 최적화: 캐시 키 생성 시 쿼리 파라미터 순서 무시 (예: ?a=1&b=2 ≡ ?b=2&a=1 ) CDN 쿼리 스트링 캐싱 정책 관리 스키마/프로토콜 (Scheme/Protocol) HTTP/1.1 (RFC 7230) vs HTTP/2 (RFC 7540) vs HTTP/3 (RFC 9114) HTTP/2: Binary Framing, Header Compression (HPACK) Multiplexing (단일 연결에서 병렬 요청) HTTP/3: QUIC 프로토콜 기반 (UDP 사용, 연결 이동성) TLS Handshake: ALPN 확장을 통한 프로토콜 협상 보안: HSTS (HTTP Strict Transport Security) 적용 TLS 1.3 이상 강제화 (RFC 8446) 헤더 (Headers) 표준 헤더: Content-Type : application/json; charset=utf-8 Cache-Control : max-age=3600, public Authorization : Bearer <token> 사용자 정의 헤더: X- Prefix는 더 이상 권장되지 않음 (RFC 6648) Sec- Prefix는 브라우저 보안 헤더 예약 성능 최적화: Connection: keep-alive (HTTP/1.1 기본) Transfer-Encoding: chunked 스트리밍 처리 헤더 조회 (Header Inspection) 네트워크 계층 분석: Wireshark/tcpdump로 Raw 패킷 캡처 TLS 트래픽은 Session Key 로깅 후 복호화 (SSLKEYLOGFILE) 응용 계층 분석: # Python (Flask) from flask import request print(request.headers.get('X-Custom-Header')) // Node.js (Express) app.use((req, res, next) => { console.log(req.get('User-Agent')); next(); }); CDN/Proxy 환경: X-Forwarded-For, X-Real-IP 헤더 검증 CDN별 헤더 규격 차이 (Cloudflare vs AWS CloudFront) 바디 (Body) 전송 형식: application/json : UTF-8 인코딩 필수 (RFC 8259) multipart/form-data : Boundary 구분자 무결성 검증 application/octet-stream : 바이너리 데이터 처리 스트리밍 처리: # Python (Requests) with requests.post(url, data=generator()) as r: for chunk in r.iter_content(8192): process(chunk) 보안: 요청 크기 제한 (예: Nginx client_max_body_size ) 파일 업로드 시 MIME 타입 검증 + 바이러스 스캔 Form 파라미터 형식 조회 인코딩 타입 비교: Content-Type 용도 RFC application/x-www-form-urlencoded 단순 텍스트 데이터 RFC 1866 multipart/form-data 파일 업로드 RFC 7578 application/json 구조화된 데이터 RFC 8259 구현 시 고려사항: charset 미지정 시 ISO-8859-1 기본 적용 (RFC 7231) 파일 업로드 시 filename* 확장 문법 (RFC 5987) 대용량 파일 처리를 위한 Memory Mapped I/O Message Body 데이터 직접 조회 Low-Level 접근: // Java Servlet API ServletInputStream inputStream = request.getInputStream(); byte[] buffer = new byte[8192]; int bytesRead; while ((bytesRead = inputStream.read(buffer)) != -1) { // Process raw bytes } // Go body, _ := ioutil.ReadAll(r.Body) defer r.Body.Close() 성능 최적화: Zero-Copy 기법 (sendfile 시스템 호출) 메모리 풀링 (Netty 등 비동기 프레임워크) 보안: Deserialization 취약점 방어 (JSON/XML External Entity) 요청 크기 제한 및 타임아웃 설정 이 설명은 API 게이트웨이 개발, 고성능 서버 설계, 보안 오디팅 등 전문적인 시나리오에서의 적용을 고려한 내용입니다. 각 구성 요소는 RFC 표준, 성능 최적화, 보안 측면에서 종합적으로 관리되어야 합니다.",
      "frontmatter": {
        "date": "2025-06-03T06:05:16+09:00",
        "lastmod": "2025-06-27T19:52:11+09:00"
      }
    },
    "obsidian 데이터 바인딩 처리": {
      "path": "/younghan-mvc1/obsidian-데이터-바인딩-처리/",
      "filename": "obsidian 데이터 바인딩 처리",
      "content": "Spring MVC의 데이터 바인딩과 관련된 주요 처리 과정을 순서대로 정리해드리겠습니다: 요청 수신 DispatcherServlet이 HTTP 요청을 받음 컨트롤러 매핑 요청 URL에 매핑된 적절한 컨트롤러 메서드를 찾음 파라미터 바인딩 단계 @ModelAttribute 객체 생성 요청 파라미터를 객체의 프로퍼티에 매핑 시도 타입 변환 시도 변환 성공: 해당 필드에 값 설정 변환 실패: BindingResult에 에러 정보 저장 (bindingFailure = true) 검증 단계 (@Valid 또는 @Validated 사용 시) Validator 실행 검증 규칙 위반 시 BindingResult에 에러 정보 저장 (bindingFailure = false) 컨트롤러 메서드 실행 바인딩(및 검증)이 완료된 @ModelAttribute 객체를 메서드 파라미터로 전달 BindingResult는 해당 객체의 바로 다음 파라미터로 전달 뷰 렌더링 BindingResult의 내용을 활용하여 오류 메시지 표시 rejectedValue 등을 사용하여 사용자 입력값 유지 중요한 점: 각 @ModelAttribute마다 별도의 BindingResult가 필요 바인딩 실패와 검증 실패는 구분되어 처리됨 바인딩은 컨트롤러 메서드 실행 전에 완료됨 이러한 순서로 Spring MVC는 요청 파라미터를 처리하고 컨트롤러에 전달합니다.",
      "frontmatter": {
        "tags": [
          "spring"
        ],
        "date": "2025-03-31T20:09:00+09:00",
        "lastmod": "2025-03-31T20:09:00+09:00"
      }
    },
    "spring controller 반환(return)": {
      "path": "/younghan-mvc1/spring-controller-반환return/",
      "filename": "spring controller 반환(return)",
      "content": "Spring MVC에서 @Controller 클래스의 @RequestMapping 메서드는 다양한 반환 타입을 지원합니다. 반환 타입에 따라 HTTP 응답 생성 방식이 달라지며, 주요 반환 타입은 다음과 같습니다. HTTP컨버터 작동 x ModelAndVIew 반환 뷰이름 반환 : string Http 컨버터 작동 : @RequestBody 또는 HttpEntity(ResponseEntity) 일때만 작동 byte, string, json 순서로 처리, 요청정보의 media type 도 동시에 활용 byte String 객체 String (뷰 이름) HTTP 메시지 컨버터 미작동 기본 동작: 반환된 String 은 뷰 이름으로 해석되며, ViewResolver 가 해당 뷰를 찾아 렌더링합니다. 예시: @GetMapping(\"/home\") public String home(Model model) { model.addAttribute(\"message\", \"Hello\"); return \"home\"; // /WEB-INF/views/home.jsp 또는 Thymeleaf 템플릿 } 특징: Model 객체를 통해 뷰에 데이터 전달. @ResponseBody 가 없으면 뷰 리졸버가 동작. ResponseEntity<?> HTTP 메시지 컨버터 작동 기본 동작: HTTP 응답의 상태 코드, 헤더, 본문을 직접 제어합니다. HttpEntity 는 헤더와 본문을 포함하지만, 상태 코드는 없으며, ResponseEntity 는 상태 코드를 추가로 지원합니다. 예시: @GetMapping(\"/api/data\") public ResponseEntity<String> getData() { return ResponseEntity.status(HttpStatus.CREATED) .header(\"X-Custom\", \"Value\") .body(\"Created\"); } 특징: HttpEntity 상속받아 헤더/본문 설정 가능. @ResponseBody 없이도 본문이 HTTP 응답에 직접 기록됨. @ResponseBody 어노테이션 HTTP 메시지 컨버터 작동 기본 동작: 반환 값이 HTTP 응답 본문에 직접 직렬화됩니다. (예: String → 텍스트, 객체 → JSON/XML) 예시: @GetMapping(\"/api/json\") @ResponseBody public User getUser() { return new User(\"John\", 30); // {\"name\":\"John\",\"age\":30} } 특징: HttpMessageConverter 가 자동으로 데이터 변환 (예: Jackson 라이브러리 사용). @RestController 는 클래스 레벨에 @ResponseBody 를 포함합니다. ModelAndView HTTP 메시지 컨버터 미작동 기본 동작: 모델 데이터와 뷰 이름을 동시에 전달합니다. 예시: @GetMapping(\"/profile\") public ModelAndView profile() { ModelAndView mav = new ModelAndView(\"profile\"); mav.addObject(\"user\", new User(\"Alice\", 25)); return mav; } 특징: 레거시 코드에서 주로 사용되며, 명시적 제어가 가능합니다. void HTTP 메시지 컨버터 미작동 기본 동작: 반환 타입이 void 인 경우, 다음 중 1개를 선택: 디폴트 뷰 이름: 요청 URL을 기반으로 뷰 이름 생성 (예: /home → home 뷰). Response 직접 조작: HttpServletResponse 를 파라미터로 받아 수동으로 응답 작성. 예시: @GetMapping(\"/manual\") public void manualResponse(HttpServletResponse response) throws IOException { response.getWriter().write(\"Manual Response\"); } POJO (Plain Old Java Object) 기본 동작: 반환된 객체는 Model에 자동 추가되며, 뷰에서 참조 가능합니다. (단, @ResponseBody 가 없을 때) 예시: @GetMapping(\"/user\") public User getUser() { return new User(\"Bob\", 28); // Model에 \"user\"라는 이름으로 추가됨 } 특징: 뷰 템플릿에서 ${user.name} 처럼 접근 가능. Callable<?> 또는 DeferredResult<?> 기본 동작: 비동기 처리를 위해 사용되며, 별도 스레드에서 결과를 생성합니다. 예시: @GetMapping(\"/async\") public Callable<String> asyncRequest() { return () -> { Thread.sleep(1000); return \"asyncResult\"; }; } 특징: 장시간 처리 작업에 유용하며, 스레드 풀을 활용합니다. View 구현체 기본 동작: 직접 View 인터페이스를 구현한 객체를 반환해 커스텀 뷰를 생성합니다. 예시: @GetMapping(\"/custom-view\") public View getCustomView() { return new MyCustomView(); // View 인터페이스 구현체 } HttpEntity<?> 또는 ResponseEntity<?> 확장 기본 동작: HttpEntity 는 헤더와 본문을 포함하지만, 상태 코드는 없으며, ResponseEntity 는 상태 코드를 추가로 지원합니다. 예시: @GetMapping(\"/custom-header\") public HttpEntity<String> customHeader() { HttpHeaders headers = new HttpHeaders(); headers.add(\"X-Custom\", \"Value\"); return new HttpEntity<>(\"Body\", headers); } 반환 타입 요약 반환 타입 동작 방식 주요 사용처 String 뷰 이름 반환 뷰 템플릿 렌더링 (JSP, Thymeleaf) ResponseEntity<?> HTTP 상태 코드/헤더/본문 제어 REST API, 커스텀 HTTP 응답 @ResponseBody 객체를 HTTP 본문에 직렬화 JSON/XML API 응답 ModelAndView 모델과 뷰를 동시에 지정 복잡한 뷰 로직 void 수동 응답 또는 디폴트 뷰 HttpServletResponse 직접 사용 POJO Model에 객체 자동 추가 뷰 템플릿 데이터 전달 Callable<?> 비동기 처리 장시간 작업 핵심 차이: 뷰 vs API 응답 뷰 렌더링: String , ModelAndView → ViewResolver 가 뷰를 찾아 렌더링. API 응답: ResponseEntity , @ResponseBody → HttpMessageConverter 가 데이터 직렬화. @ResponsBody 또는 HttpEntity 일때 Pasted image 20250315222234er가 response 객체에 해당 값을 넣어두고, 흐름이 다시 DispatcherServlet으로 가서(그럼 여기서 modelAndView는 null) 내부 로직에 의해 view를 만드는 과정이 생략되고 http 응답 메시지생성 Pasted image 20250315220169Pasted image 20250315220150 Pasted image 20250315221202 Pasted image 20250315220241 Pasted image 20250315220213 Pasted image 20250315220264 Pasted image 20250315220223",
      "frontmatter": {
        "date": "2025-03-15T20:36:00+09:00",
        "lastmod": "2025-06-27T19:54:43+09:00"
      }
    },
    "spring controller 인수(parameter)": {
      "path": "/younghan-mvc1/spring-controller-인수parameter/",
      "filename": "spring controller 인수(parameter)",
      "content": "",
      "frontmatter": {
        "date": "2025-03-15T22:44:00+09:00",
        "lastmod": "2025-03-15T22:44:00+09:00"
      }
    },
    "spring mvc 주요 어노테이션": {
      "path": "/younghan-mvc1/spring-mvc-주요-어노테이션/",
      "filename": "spring mvc 주요 어노테이션",
      "content": "클래스 단위 @Controller : 이 클래스가 컨트롤러임을 명시 @RequestMapping : 메소드 단위의 RequestMapping 의 공통 url 을 명시 @RestContoller : @ResponseBody + @Controller 이 클래스가 반환이 view 가아닌 바디에 직접 컨트롤 즉 Rest 속성을 지닌 컨트롤러임을 명시 메소드 단위 @RequestMapping : 리플렉션을 사용해서 url 등록 각 메서드 마다 GetMapping 등이 있다 메소드 인자 단위 @RequestParam : request parameter 의 값을 가져올 수 있다 @ModelAttribute : 모델을 생성하고 parameter 값을 넣어주는 행위를 자동화 @RequestHeader : 헤더의 정보를 조회 @CookieValue : 쿠키 value 를 조회 @ResponseBody : ResponseEntity<?> 에 자동으로 넣어준다 ResponseEntity<?> 는 HTTP 컨버터를 작동할 수 있는 객체 @ModelAttribute 는 생략할 수 있다. 그런데 @RequestParam 도 생략할 수 있으니 혼란이 발생할 수 있다. 스프링은 해당 생략시 다음과 같은 규칙을 적용한다. String , int , Integer 같은 단순 타입 = @RequestParam 나머지 = @ModelAttribute (argument resolver 로 지정해둔 타입 외) 기본 템플릿엔진 JSP (JavaServer Pages) Thymeleaf FreeMarker Velocity 데이터 포멧 뷰 JSON/XML RSS/Atom : RssView 또는 AtomView 를 사용해 피드 생성. 문서 생성 뷰 PDF : AbstractPdfView 를 상속받아 PDF 문서 생성, iText 라이브러리 사용. execl : AbstractExcelView 또는 AbstractJExcelView 를 상속, Apache POI 또는 JExcelAPI 사용 기타 redirect : redirect: 접두어를 사용해 다른 URL로 리다이렉트, RedirectView 클래스로 명시적 리다이렉트 처리.",
      "frontmatter": {
        "tags": [
          "young_han",
          "spring"
        ],
        "date": "2025-03-11T12:57:00+09:00",
        "lastmod": "2025-06-27T19:54:53+09:00"
      }
    },
    "thymeleaf 기본": {
      "path": "/younghan-mvc1/thymeleaf-기본/",
      "filename": "thymeleaf 기본",
      "content": "타임리프 간단히 알아보기 타임리프 사용 선언 타임리프를 사용하기 위해 HTML 문서의 <html> 태그에 네임스페이스를 선언합니다. <html xmlns:th=\"http://www.thymeleaf.org\"> 속성 변경 - th:href th:href 는 HTML의 href 속성을 동적으로 변경할 수 있습니다. 예시: <link href=\"value1\" th:href=\"@{/css/bootstrap.min.css}\"> 동작 방식: HTML 파일을 직접 열면 href=\"value1\" 이 사용됩니다. 타임리프 템플릿을 거치면 th:href 의 값( @{/css/bootstrap.min.css} )으로 대체됩니다. 핵심: th:xxx 가 붙은 부분은 서버 사이드에서 렌더링되며, 기존 값을 대체합니다. th:xxx 가 없으면 기존 HTML 속성이 그대로 유지됩니다. URL 링크 표현식 - @{...} 타임리프에서 URL 링크를 작성할 때는 @{...} 를 사용합니다. 이를 URL 링크 표현식이라 합니다. 예시: <link th:href=\"@{/css/bootstrap.min.css}\"> 특징: 서블릿 컨텍스트 경로를 자동으로 포함합니다. 경로 변수와 쿼리 파라미터도 쉽게 추가할 수 있습니다. 경로 변수와 쿼리 파라미터 예시: <a th:href=\"@{/basic/items/{itemId}(itemId=${item.id}, query='test')}\"> 생성된 링크: http://localhost:8080/basic/items/1?query=test 리터럴 대체 - |...| 문자열과 표현식을 조합할 때 더하기(+) 연산자를 사용하지 않고 간단히 작성할 수 있는 문법입니다. 예시: <span th:text=\"'Welcome to our application, ' + ${user.name} + '!'\"> 위 코드를 리터럴 대체 문법으로 간단히 작성하면: <span th:text=\"|Welcome to our application, ${user.name}!|\"> th:onclick 에서 리터럴 대체 사용: <button th:onclick=\"|location.href='@{/basic/items/add}'|\"> 결과: location.href='/basic/items/add' 반복 출력 - th:each 컬렉션 데이터를 반복 처리할 때 사용합니다. 예시: <tr th:each=\"item : ${items}\"> <td th:text=\"${item.name}\">상품명</td> <td th:text=\"${item.price}\">가격</td> </tr> items 컬렉션의 각 요소가 item 변수에 할당되고, 반복문 안에서 사용됩니다. 컬렉션의 크기만큼 <tr> 태그가 생성됩니다. 변수 표현식 - ${...} 모델에 포함된 값이나 타임리프 변수를 조회할 때 사용합니다. 예시: <td th:text=\"${item.price}\">10000</td> ${item.price} 는 item.getPrice() 메서드를 호출한 것과 동일합니다. 10000 은 ${item.price} 의 값으로 대체됩니다. 내용 변경 - th:text HTML 요소의 내용을 동적으로 변경합니다. 예시: <td th:text=\"${item.price}\">10000</td> 10000 은 ${item.price} 의 값으로 대체됩니다. URL 링크 간단히 작성 - 리터럴 대체 활용 리터럴 대체 문법을 사용하여 URL을 간단히 작성할 수 있습니다. 예시: <a th:href=\"@{|/basic/items/${item.id}|}\">상품 상세보기</a> 생성된 링크: http://localhost:8080/basic/items/1 타임리프의 핵심 특징 네츄럴 템플릿(Natural Templates): 순수 HTML 파일을 웹 브라우저에서 열어도 정상적으로 보입니다. 서버를 통해 뷰 템플릿을 거치면 동적으로 변경된 결과를 확인할 수 있습니다. JSP와 달리 소스 코드가 뒤죽박죽되지 않습니다. 참고 타임리프는 다음과 같은 장점이 있습니다: HTML 파일을 그대로 유지하면서 템플릿 기능을 사용할 수 있습니다. 서버 렌더링 후에도 클라이언트에서 정상적으로 작동합니다. 직관적이고 간결한 문법으로 개발 생산성을 높입니다. 요약 타임리프는 HTML 파일을 그대로 유지하면서 서버 사이드에서 동적으로 변환할 수 있는 강력한 템플릿 엔진입니다. 주요 기능으로는 th:xxx 속성, URL 링크 표현식( @{...} ), 리터럴 대체( |...| ), 반복 처리( th:each ) 등이 있습니다. 이러한 특징 덕분에 네츄럴 템플릿이라는 이름으로 불리며, 현대적인 웹 개발에서 많이 사용되고 있습니다.",
      "frontmatter": {
        "tags": [
          "spring",
          "young_han"
        ],
        "date": "2025-03-17T04:39:00+09:00",
        "lastmod": "2025-03-17T04:39:00+09:00"
      }
    },
    "youngHan mvc1 프로젝트 변화 과정": {
      "path": "/younghan-mvc1/younghan-mvc1-프로젝트-변화-과정/",
      "filename": "youngHan mvc1 프로젝트 변화 과정",
      "content": "개발 순서 java.hello.servlet 패키지에서 시작 basic 패키지 : was 표준으로 되어 있는 서블릿을 등록하는 방법을 배운다 WebSservlet 어노테이션 또는 web.xml 파일을 통해 서블릿을 등록할 수 있다 실제 구현은 HttpServlet 을 상속받아 실제 사용자의 요청 정보(request) 또는 반환 정보(response)를 통제 변환할 수 있다 web.servlet 패키지 : 적절한 요청에 적절한 반환을 하기 위해 반환정보에(response) 원하는 html 을 쌩으로 담아서 넘겨본다 순수 jsp 패키지 : webapp 폴더안에 jsp 라는 이름으로 존재한다 jsp 를 통해 조금더 편안하게 요청을 처리해 보자 (jsp 에서 처리까지 다 해보기) web.servletmvc 패키지 : 적절한 요청에 jsp 를 통해 view 역할을 분리하여 본다 (jsp 에서 view 의 역할만 맡도록 만든다) (단 jsp 에서 view 에서 적절한 값을 렌더링 하기 위해 필요한 저장소가 있는데 이것이 현재 mvc 의 model 로 쓰이고 있고 이것이 HttpServletRequest 에 setAttribute 데 담아야 jsp 에서 참조하기 편하다) web.frontcontroller 패키지 : 각 버전별로 실제 발전되는 과정을 만들면서 spring 과 비슷하게 만들어 본다 v1 패키지 : 모든 컨트롤 부분이 모두 서블릿으로 등록될 필요없이 frontContoller 만 서블릿으로 등록해서 처리해보자 프론트 컨트롤러 에서 쉽게 각 컨트롤러를 쉽게 호출하기 위해 다형성을 사용하자 v2 패키지 : 모든 컨트롤 부분에서 jsp 로 forward 하는 부분이 겹친다 이 부분의 경우 따로 다른 클래스에서 처리하기 위해 MyView 클래스를 도입하자 각 컨트롤러에서 MyView만 반환해서 나머지는 프론트 컨트롤러에서 처리자 ( 여기서 약간 의문일 수 있는데 실제 jsp 말고 다른 view 를 사용할 수 있으므로 view 또 추상화하는 것이 좋다) v3 패키지 : 이 단계에서 많은 것을 해야 한다 모든 컨트롤 부분에서 http 요청 반환 정보(HttpServletRequest, HttpServletResponse) 가 같이 넘어간다 의존성을 제거해보자 사용자가 요청한 파라미터 정보는 HttpServletRequest.getParameter 를 통해 받는다 HttpServletRequest 없이 컨트롤러에서 처리하려면 프론트컨트롤러에서 처리해서 java 자료형으로 넘겨주어야 한다 현재 request 객체를 model 로써 사용하고 있는데 이것을 새롭게 ModelView(MVC 패턴에서 Controller와 View 사이의 데이터 전달 및 뷰 논리적 이름을 관리) 만들자 ( model 의 역할을 함과 동시에 view 의 논리적 이름 역할을 함께 가지고 있으므로 ModelView 로 만들었음 ) 사용자의 요청(특정 컨트롤러)과 파라미터를 처리해서 ModelView 를 만들어서 처리시키자 modelView 에 있는 이름으로는 MyView 를 부르는데 부족하다 전체 경로를 만들어 주는 viewResolver 또한 필요하다 v4 패키지 : 컨트롤러 측에서 ModelView 또한 만들고 싶어하지 않는다 java자료형으로 만들어서 반환시키자 v5 패키지 : 컨트롤러 측에서 누구는 v3 누구는 v4 로 만들고 싶어한다 둘다 지원할 수 있도록 어뎁터를 만들자 어뎁터를 만들면서 컨트롤러는 더 큰 범위를 다룰 수 있으므로 handler 라고 명명한다 프론트 컨트롤러 측에서 적절한 컨트롤러를 받아서 적절한 어뎁터를 통해 진행시킨다 web.springmvc 패키지 : spring 을 사용한 편집 Pasted image 202!5003267.png) v1 Pasted image 20250304235889 v2 Pasted image 20250305003377 v3 Pasted image 20250305003147 MyView 클래스가 조금 복잡해 지는데 jsp 의 구조상 [서블릿 객체들 에 저장해 주어야 쉽게 빼서 사용할 수 있다 JSP는 request.getAttribute() 로 데이터를 조회하기 때문에, 모델의 데이터를 꺼내서 request.setAttribute() 로 담아둔다 만약 view 측에서 쉽게 data 에 접근하는 좋은 방식이 있다면 이렇게 코드를 만들지 않을 수 있다 v4 Pasted image 20250306124006 v5 Pasted image 20250306133161 핸들러 DispatcherServlet handlerMappingMap -> HandlerMapping MyHandlerAdapter -> HandlerAdapter ModelView -> ModelAndView viewResolver -> ViewResolver MyView -> View 실제 spring MVC 구조 Pasted image 20250308163568 추가 핸들러(컨트롤러) 관련 컨트롤러가 호출되려면 다음 2가지가 필요하다. HandlerMapping(핸들러 매핑) 핸들러 매핑에서 이 컨트롤러를 찾을 수 있어야 한다. 예) 스프링 빈의 이름으로 핸들러를 찾을 수 있는 핸들러 매핑이 필요하다. HandlerAdapter(핸들러 어댑터) 핸들러 매핑을 통해서 찾은 핸들러를 실행할 수 있는 핸들러 어댑터가 필요하다. 예) Controller 인터페이스를 실행할 수 있는 핸들러 어댑터를 찾고 실행해야 한다. 스프링 부트가 자동 등록하는 핸들러 매핑과 핸들러 어댑터 (실제로는 더 많지만, 중요한 부분 위주로 설명하기 위해 일부 생략) HandlerMapping 0 = RequestMappingHandlerMapping : 애노테이션 기반의 컨트롤러인 @RequestMapping에서 사용 1 = BeanNameUrlHandlerMapping : 스프링 빈의 이름으로 핸들러를 찾는다. HandlerAdapter 0 = RequestMappingHandlerAdapter : 애노테이션 기반의 컨트롤러인 @RequestMapping에서 사용 1 = HttpRequestHandlerAdapter : HttpRequestHandler 처리 2 = SimpleControllerHandlerAdapter : Controller 인터페이스(애노테이션X, 과거에 사용) 처리 리졸버 관련 스프링 부트가 자동 등록하는 뷰 리졸버 (실제로는 더 많지만, 중요한 부분 위주로 설명하기 위해 일부 생략) 1 = BeanNameViewResolver : 빈 이름으로 뷰를 찾아서 반환한다. (예: 엑셀 파일 생성 기능 에 사용) 2 = InternalResourceViewResolver : JSP를 처리할 수 있는 뷰를 반환한다. 1번 예시 @Component(\"excelView\") // 빈 이름 명시 public class ExcelView implements View { @Override public void render(Map<String, ?> model, HttpServletRequest request, HttpServletResponse response) { // 엑셀 파일 생성 로직 response.setContentType(\"application/vnd.ms-excel\"); // ... } } 빈 이름으로 리졸버뷰를 만들어서 컨트롤러에서 처리한다",
      "frontmatter": {
        "tags": [
          "spring",
          "young_han"
        ],
        "date": "2025-03-04T23:58:00+09:00",
        "lastmod": "2025-03-04T23:58:00+09:00"
      }
    },
    "서블릿 객체들": {
      "path": "/younghan-mvc1/서블릿-객체들/",
      "filename": "서블릿 객체들",
      "content": "각 객체의 정의 및 내부 구조 1 page (PageContext) jsp 의존적 범위: 현재 JSP 페이지 내에서만 유효합니다. 수명: 페이지가 렌더링되는 동안에만 유지됩니다. 내부 구조: PageContext 클래스에 의해 구현되며, 페이지 단위로 속성을 저장합니다. 내부적으로 Map<String, Object> 를 사용하지만, getAttributeNames() 메서드를 지원하지 않습니다. JSP의 다른 기본 객체(request, session, application)에 대한 접근을 제공합니다. JSP 의존성: O (JSP 전용 객체이며, Servlet에서는 사용 불가능합니다.) 2 request (HttpServletRequest) 범위: 같은 요청(request) 내에서 유효합니다 (예: 클라이언트에서 서버로의 한 번의 HTTP 요청). 수명: 요청이 처리되는 동안에만 유지됩니다. 내부 구조: HttpServletRequest 의 setAttribute() / getAttribute() 메서드를 사용합니다. 컨테이너(예: Tomcat)는 내부적으로 Map<String, Object> 구조로 속성을 관리합니다. JSP 의존성: X (Servlet에서도 사용 가능하며, JSP와 독립적입니다.) 3 session (HttpSession) 범위: 동일한 사용자의 세션(Session) 내에서 유효합니다. 수명: 세션이 종료될 때까지 유지됩니다 (기본적으로 30분). 내부 구조: HttpSession 객체에 속성을 저장하며, 컨테이너는 ConcurrentHashMap<String, Object> 를 사용하여 동시성 문제를 해결합니다. 세션 ID를 기반으로 사용자를 식별합니다. JSP 의존성: X (Servlet에서도 사용 가능하며, JSP와 독립적입니다.) 4 application (ServletContext) 범위: 애플리케이션 전체(모든 사용자 및 세션)에서 유효합니다. 수명: 애플리케이션이 종료될 때까지 유지됩니다. 내부 구조: ServletContext 객체에 속성을 저장하며, 컨테이너는 ConcurrentHashMap<String, Object> 를 사용합니다. 모든 사용자와 세션에서 공유되는 전역 데이터를 관리합니다. JSP 의존성: X (Servlet에서도 사용 가능하며, JSP와 독립적입니다.) 속성 처리 메서드 비교 메서드 page (PageContext) request (HttpServletRequest) session (HttpSession) application (ServletContext) setAttribute() O O O O getAttribute() O O O O removeAttribute() O O O O getAttributeNames() X O O O page 객체는 getAttributeNames() 를 지원하지 않습니다. request, session, application은 getAttributeNames() 로 속성 목록을 조회할 수 있습니다. JSP 의존성 분석 page 객체: JSP 전용 객체이며, PageContext 클래스에 종속적입니다. Servlet에서는 사용할 수 없습니다. request, session, application 객체: Servlet API( HttpServletRequest , HttpSession , ServletContext )에 정의된 표준 객체입니다. 따라서 JSP와 독립적이며, Servlet이나 다른 웹 프레임워크에서 동일하게 사용됩니다. 사용 예시 request 범위 <!-- JSP 페이지에서 request 속성 설정 --> <% request.setAttribute(\"userName\", \"Alice\"); %> // Servlet에서 request 속성 사용 protected void doGet(HttpServletRequest request, HttpServletResponse response) { String name = (String) request.getAttribute(\"userName\"); } session 범위 <!-- JSP에서 세션 속성 설정 --> <% session.setAttribute(\"userRole\", \"ADMIN\"); %> application 범위 <!-- JSP에서 애플리케이션 속성 설정 --> <% application.setAttribute(\"appVersion\", \"1.0.0\"); %> 중요 고려 사항 범위 선택: 임시 데이터(한 번의 요청) → request 사용자별 데이터(로그인 정보) → session 전역 데이터(설정 정보) → application 페이지 내 데이터 → page 메모리 관리: session과 application 범위의 속성은 명시적으로 제거하지 않으면 메모리 누수가 발생할 수 있습니다. 동시성 문제: session과 application은 ConcurrentHashMap 을 사용하여 스레드 안전성을 보장합니다. 요약 JSP 의존성: page 객체만 JSP에 종속적이며, 나머지( request , session , application )는 Servlet API 표준입니다. 내부 구조: 모든 객체는 Map 기반으로 속성을 관리하며, 범위에 따라 저장 위치와 수명이 결정됩니다. 사용 목적: 데이터의 공유 범위에 따라 적절한 객체를 선택하여 사용합니다.",
      "frontmatter": {
        "tags": [
          "spring",
          "young_han"
        ],
        "date": "2025-03-04T20:09:00+09:00",
        "lastmod": "2025-08-19T22:14:18+09:00"
      }
    },
    "11. Container With Most Water": {
      "path": "/leetcode75/11.-container-with-most-water/",
      "filename": "11. Container With Most Water",
      "content": "완전 탐색의 $O(n^2)$ 시간 탐색을 선형시간 $O(n)$ 으로 변경 완전 탐색 풀이 class Solution { public: int maxArea(vector<int>& height) { int ret = 0; for(int i = 0 ; i< height.size()-1 ; i++){ for(int j = i + 1; j < height.size() ; j++){ int width = j - i; int length = min(height[i], height[j]); int area = width * length; if(ret < area) ret = area; } } return ret; } }; 이 문제를 선형시간으로 풀 수 있는 이유 2개의 포인터를 양쪽 끝으로 지정하고 포인터(넓은 의미의)를 이동 시킬 때(width 크기가 큰것 부터 작은 순으로 탐색), 만약 left 위치의 높이가 작다고 가정할 때 right 위치를 줄이면 width 가 줄어들기 때문에 총 넓이가 무조건 작다고 할 수 있다 만약 height[right] 가 height[left] 보다 작더라 하더라도 더 작아진다고 할 수 있으므로 넓이가 작아지는 것은 확실 그러므로 가짓수를 확실 하게 쳐 낼 수 있다 결론 이미 해당 높이의 최대 영역을 가지고 있기 때문 class Solution { public: int maxArea(vector<int>& height) { int left = 0; int right = height.size() - 1; int max_area = 0; while(left < right){ // 넓이 구하기 int w = right - left; int h = min(height[left], height[right]); int area = h * w; max_area = max(max_area, area); // 포인터 이동 if(height[left] < height[right]) left++; else if(height[left] > height[right]) right--; else { left++; right--; } } return max_area; } };",
      "frontmatter": {
        "tags": [
          "leetcode",
          "algorithm"
        ],
        "description": "투포인터",
        "date": "2025-01-07T07:40:00+09:00",
        "lastmod": "2025-09-04T20:52:24+09:00"
      }
    },
    "1493. Longest Subarray of 1's After Deleting One Element": {
      "path": "/leetcode75/1493.-longest-subarray-of-1s-after-deleting-one-element/",
      "filename": "1493. Longest Subarray of 1's After Deleting One Element",
      "content": "문제 주어진 문제는 이진 배열 nums에서 하나의 요소를 삭제한 후, 결과 배열에서 1로만 이루어진 가장 긴 비어 있지 않은 부분 배열의 크기를 반환하는 것입니다. 만약 그런 부분 배열이 존재하지 않으면 0을 반환합니다. ### 예시: 1. **입력**: nums = [1, 1, 0, 1] **출력**: 3 **설명**: 인덱스 2의 숫자를 삭제하면 [1, 1, 1]이 되어 1의 개수가 3입니다. 2. **입력**: nums = [0, 1, 1, 1, 0, 1, 1, 0, 1] **출력**: 5 **설명**: 인덱스 4의 숫자를 삭제하면 [0, 1, 1, 1, 1, 1, 0, 1]이 되어 1의 개수가 5입니다. 3. **입력**: nums = [1, 1, 1] **출력**: 2 **설명**: 하나의 요소를 삭제해야 하므로, 최대 1의 개수는 2입니다. ### 제약 조건: - 1 <= nums.length <= 10^5 - nums[i]는 0 또는 1입니다. O(n^3) class Solution { public: int longestSubarray(vector<int>& nums) { int ret = 0; for(int start = 0; start < nums.size(); start++){ for(int end = start; end < nums.size(); end++){ int zero_count = 0; // 부분 배열의 0 개수 찾기 for(int i = start ; i <= end ; i++){ if(nums[i] == 0) zero_count++; } if(zero_count == 0 || zero_count == 1) ret = max(ret, end - start); } } return ret; } }; O(n^2) zero_count 를 각 부분배열에서 다시 계산해야 할까?? 부분배열의 시작 인덱스에서 반복이 실행되므로 끝 인덱스의 값만 확인하면서 0 개수를 저장해도 되지 않을까? class Solution { public: int longestSubarray(vector<int>& nums) { int ret = 0; for(int start = 0; start < nums.size(); start++){ int zero_count = 0; for(int end = start; end < nums.size(); end++){ // 부분 배열의 0 개수 찾기 if(nums[end] == 0) zero_count++; if(zero_count == 0 || zero_count == 1) ret = max(ret, end - start); } } return ret; } }; O(n) sliding window 아직 $O(n^2)$ 이다 부분배열의 관찰 이므로 sliding window 를 통해 최적화 할 수 없을까?? class Solution { public: int longestSubarray(vector<int>& nums) { int ret = 0; int start = 0; int zero_count = 0; for(int end = 0 ; end < nums.size(); end++){ if(nums[end] == 0) zero_count++; while(zero_count > 1) if(nums[start++] == 0) zero_count--; ret = max(ret, end - start); } return ret; } }; 동적계획법 추가 : 동적계획법으로 푸는 방법 ModificationRequired class Solution { public: int longestSubarray(vector<int>& nums) { vector<int> dp = {0}; int maximum = 0; for (int i = 0; i < nums.size(); ++i) { if (nums[i] == 1) dp.push_back(dp[i] + 1); else dp.push_back(0); } if (dp.back() + 1 == dp.size()) return dp.back() - 1; for (int i = dp.size()-1; i >= 0; --i) { if (dp[i] != 0) { if (i - dp[i] - 1 >= 0) maximum = max(maximum, dp[i] + dp[i - dp[i] - 1]); else maximum = max(maximum, dp[i]); i -= dp[i]; } } return maximum; } };",
      "frontmatter": {
        "tags": [
          "leetcode",
          "algorithm"
        ],
        "date": "2025-01-17T04:48:00+09:00",
        "lastmod": "2025-09-29T12:44:38+09:00"
      }
    },
    "2095. Delete the Middle Node of a Linked List": {
      "path": "/leetcode75/2095.-delete-the-middle-node-of-a-linked-list/",
      "filename": "2095. Delete the Middle Node of a Linked List",
      "content": "문제 주어진 연결 리스트의 헤드가 주어졌을 때, 중간 노드를 삭제하고 수정된 연결 리스트의 헤드를 반환하세요. 크기가 $n$인 연결 리스트의 중간 노드는 0-based 인덱스를 사용하여 $\\lfloor n / 2 \\rfloor$번째 노드입니다. 여기서 $\\lfloor x \\rfloor$는 $x$보다 작거나 같은 가장 큰 정수를 나타냅니다. 예를 들어, $n = 1, 2, 3, 4, 5$일 때 중간 노드의 인덱스는 각각 $0, 1, 1, 2, 2$입니다. 예제 예제 1: 입력: head = [1, 3, 4, 7, 1, 2, 6] 출력: [1, 3, 4, 1, 2, 6] 설명: 위 그림은 주어진 연결 리스트를 나타냅니다. 노드의 인덱스는 아래에 표시되어 있습니다. $ n = 7 $이므로, 값이 $ 7 $인 3번째 노드(0-based 인덱스)가 중간 노드입니다. 이 노드는 빨간색으로 표시되어 있습니다. 우리는 이 노드를 제거한 후 새로운 리스트를 반환합니다. 예제 2: 입력: head = [1, 2, 3, 4] 출력: [1, 2, 4] 설명: 위 그림은 주어진 연결 리스트를 나타냅니다. $ n = 4 $이므로, 값이 $ 3 $인 2번째 노드(0-based 인덱스)가 중간 노드입니다. 이 노드는 빨간색으로 표시되어 있습니다. 예제 3: 입력: head = [2, 1] 출력: [2] 설명: 위 그림은 주어진 연결 리스트를 나타냅니다. $ n = 2 $이므로, 값이 $ 1 $인 1번째 노드(0-based 인덱스)가 중간 노드입니다. 이 노드는 빨간색으로 표시되어 있습니다. 값이 $ 2 $인 0번째 노드만 남게 됩니다. 제약 조건 연결 리스트의 노드 수는 $[1, 10^5]$ 범위에 있습니다. $1\\leq \\text{Node.val} \\leq 10^5$ 답 일반적 접근 /** * Definition for singly-linked list. * struct ListNode { * int val; // 노드의 값 * ListNode *next; // 다음 노드를 가리키는 포인터 * ListNode() : val(0), next(nullptr) {} // 기본 생성자 * ListNode(int x) : val(x), next(nullptr) {} // 값만 초기화하는 생성자 * ListNode(int x, ListNode *next) : val(x), next(next) {} // 값과 다음 노드를 초기화하는 생성자 * }; */ class Solution { public: ListNode* deleteMiddle(ListNode* head) { // Edge case: 리스트에 노드가 하나만 있는 경우 nullptr 반환 if (head == nullptr || head->next == nullptr) { return nullptr; // 노드가 없거나 하나뿐이면 삭제 후 nullptr 반환 } // Step 1: 연결 리스트의 길이 계산 int length = 0; // 리스트 길이를 저장할 변수 ListNode* temp = head; // 임시 포인터를 사용하여 리스트 순회 while (temp != nullptr) { // 리스트의 끝까지 이동 length++; // 노드 수 카운트 temp = temp->next; // 다음 노드로 이동 } // Step 2: 중간 노드의 인덱스 계산 (0-based) int mid = length / 2; // 중간 노드의 인덱스 계산 // Step 3: 중간 노드 바로 전 노드까지 이동 temp = head; // 다시 head부터 시작 for (int i = 0; i < mid - 1; i++) { // 중간 노드 바로 전까지 이동 temp = temp->next; // 다음 노드로 이동 } // Step 4: 중간 노드 삭제 ListNode* toDelete = temp->next; // 삭제할 중간 노드를 저장 temp->next = temp->next->next; // 중간 노드를 건너뛰도록 연결 변경 delete toDelete; // 메모리 해제 (C++에서는 선택 사항) return head; // 수정된 리스트의 head 반환 } }; 투포인터 접근 /** * Definition for singly-linked list. * struct ListNode { * int val; // 노드의 값 * ListNode *next; // 다음 노드를 가리키는 포인터 * ListNode() : val(0), next(nullptr) {} // 기본 생성자 * ListNode(int x) : val(x), next(nullptr) {} // 값만 초기화하는 생성자 * ListNode(int x, ListNode *next) : val(x), next(next) {} // 값과 다음 노드를 초기화하는 생성자 * }; */ class Solution { public: ListNode* deleteMiddle(ListNode* head) { // Edge case: 리스트에 노드가 하나만 있는 경우 nullptr 반환 if (head == nullptr || head->next == nullptr) { return nullptr; // 노드가 없거나 하나뿐이면 중간 노드를 삭제한 후 nullptr을 반환합니다. } // 더미 노드 생성 ListNode* prev = new ListNode(0, head); // 더미 노드(prev)를 생성하고, next를 head로 설정합니다. // 이는 중간 노드를 삭제할 때 편리하게 처리하기 위함입니다. ListNode* slow = prev; // slow 포인터를 더미 노드로 초기화합니다. ListNode* fast = head; // fast 포인터를 head로 초기화합니다. // 투 포인터 기법으로 중간 노드 찾기 while (fast != nullptr && fast->next != nullptr) { slow = slow->next; // slow는 한 칸씩 이동합니다. fast = fast->next->next; // fast는 두 칸씩 이동합니다. } // 임시 보관 ListNode* toDelete = slow->next; // 삭제할 중간 노드를 toDelete에 저장합니다. // 실제 변경 slow->next = slow->next->next; // slow의 next를 변경하여 중간 노드를 건너뜁니다(중간 노드 삭제). // 메모리 해제 delete toDelete; // 삭제된 중간 노드의 메모리 해제 delete prev; // 더미 노드의 메모리 해제 return head; // 수정된 리스트의 head를 반환합니다. } };",
      "frontmatter": {
        "tags": [
          "algorithm",
          "leetcode"
        ],
        "date": "2025-02-01T02:37:00+09:00",
        "lastmod": "2025-09-29T12:44:55+09:00"
      }
    },
    "334. Increasing Triplet Subsequence": {
      "path": "/leetcode75/334.-increasing-triplet-subsequence/",
      "filename": "334. Increasing Triplet Subsequence",
      "content": "주어진 정수 배열 nums에 대해, 인덱스 (i, j, k)가 존재하여 i < j < k이고 nums[i] < nums[j] < nums[k]를 만족하는지 확인하여, 존재하면 true를 반환하고, 그렇지 않으면 false를 반환하는 문제입니다. 예시) 입력: nums = [1,2,3,4,5] 출력: true 설명: i < j < k인 모든 조합이 유효합니다. 입력: nums = [5,4,3,2,1] 출력: false 설명: 유효한 조합이 존재하지 않습니다. 입력: nums = [2,1,5,0,4,6] 출력: true 설명: (3, 4, 5) 조합이 유효합니다. nums[3] == 0 < nums[4] == 4 < nums[5] == 6입니다. 제약 조건 1 <= nums.length <= 5 * 10^5 -2^31 <= nums[i] <= 2^31 - 1 후속 질문 O(n) 시간 복잡도와 O(1) 공간 복잡도로 구현할 수 있는 방법이 있는지 확인해 보세요. greedy 알고리즘으로 풀 수 있는 문제이다 class Solution { public: bool increasingTriplet(vector<int>& nums) { if (nums.size() < 3) return false; // 제한 조건의 추가 int first = INT_MAX; // 첫 번째 최소값 int second = INT_MAX; // 두 번째 최소값 for (int num : nums) if (num <= first) first = num; // 첫 번째 최소값 업데이트 else if (num <= second) second = num; // 두 번째 최소값 업데이트 else return true; // 세 번째 값이 발견됨 return false; } };",
      "frontmatter": {
        "tags": [
          "algorithm",
          "leetcode"
        ],
        "series": "leetcode74",
        "series_weight": "334",
        "date": "2024-12-30T14:38:00+09:00",
        "lastmod": "2025-09-05T03:56:21+09:00"
      }
    },
    "443. String Compression": {
      "path": "/leetcode75/443.-string-compression/",
      "filename": "443. String Compression",
      "content": "주어진 문자 배열 chars를 다음 알고리즘을 사용하여 압축하세요: 빈 문자열 s로 시작합니다. chars에서 연속으로 반복되는 문자의 그룹마다: 그룹의 길이가 1인 경우, 문자를 s에 추가합니다. 그렇지 않으면, 문자를 그룹의 길이와 함께 s에 추가합니다. 압축된 문자열 s는 별도로 반환하지 않고, 입력 문자 배열 chars에 저장해야 합니다. 그룹의 길이가 10 이상인 경우, chars에 여러 문자로 나누어 저장해야 합니다. 수정이 완료된 후, 배열의 새 길이를 반환합니다. 상수 추가 공간만 사용하는 알고리즘을 작성해야 합니다. 예시 예시 1: 입력: chars = [\"a\", \"a\", \"b\", \"b\", \"c\", \"c\", \"c\"] 출력: 6을 반환하며, 입력 배열의 처음 6개의 문자: [\"a\", \"2\", \"b\", \"2\", \"c\", \"3\"] 설명: 그룹은 \"aa\", \"bb\", \"ccc\"이며, \"a2b2c3\"으로 압축됩니다. 예시 2: 입력: chars = [\"a\"] 출력: 1을 반환하며, 입력 배열의 첫 번째 문자는: [\"a\"] 설명: 유일한 그룹은 \"a\"이며, 단일 문자이므로 압축되지 않습니다. 예시 3: 입력: chars = [\"a\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\"] 출력: 4를 반환하며, 입력 배열의 처음 4개의 문자는: [\"a\", \"b\", \"1\", \"2\"] 설명: 그룹은 \"a\"와 \"bbbbbbbbbbbb\"이며, \"ab12\"로 압축됩니다. 제약 조건 1 <= chars.length <= 2000 chars[i]는 소문자 영어 문자, 대문자 영어 문자, 숫자 또는 기호입니다. vector 로 조작하면 class Solution { public: int compress(vector<char>& chars) { vector<char> ret; auto itr = char.begin(); char first = itr; char last = itr+1; int count; for(itr < chars.end()){ if(*first != *last){ count = last - first; char.erase(remove(first+1, last), ); if(count >= 1000){// 1000 이상 *first } // 100이상 // 10 이상 last = ++first }else last++; } } };",
      "frontmatter": {
        "series": "leetcode74",
        "series_weight": "443",
        "date": "2024-12-31T00:21:00+09:00",
        "lastmod": "2025-09-29T12:44:39+09:00"
      }
    },
    "605 문제": {
      "path": "/leetcode75/605-문제/",
      "filename": "605 문제",
      "content": "class Solution { public: bool canPlaceFlowers(vector<int>& flowerbed, int n) { if (n > placeFolwers(flowerbed)) return false; else return true; } int placeFolwers(vector<int>& flowerbed){ int result = 0; for (int i = 0 ; i < flowerbed.size() ; i++){ int temp = i; while(flowerbed[i] == 0){ i++; } result += (int)((i - temp - 1) / 2); } return result; } }; 테스트 케이스 Input flowerbed = [0,0,1,0,1] n = 1 Output => false Expected => true",
      "frontmatter": {
        "tags": [
          "algorithm",
          "leetcode"
        ],
        "series": "leetcode74",
        "series_weight": "605",
        "date": "2024-12-30T14:37:00+09:00",
        "lastmod": "2024-12-30T14:37:00+09:00"
      }
    },
    "Greedy 알고리즘": {
      "path": "/leetcode75/greedy-알고리즘/",
      "filename": "Greedy 알고리즘",
      "content": "그리디 알고리즘을 적용하여 최적해를 구할 수 있는 문제는 다음 두 조건을 만족한다. greedy choice property: 현재 선택이 이 후의 선택에 영향을 주지 않음 optimal substructure: 매 순간의 최적의 해가 문제 전체에 대한 최적의 해여야 함 image 20231220052551 5 -> 10 -> 3 선택 지금 가장 큰값을 선택한다고 해서 노드 값의 합을 보장할 수 없다 현재 선택이 이후에 영향을 준다고 할 수 있다 매순간 최적의 해가 문제 전체에 대한 최적해라고 말할 수 없다 문제 예시 Increasing Triplet Subsequence Pasted image 20231220053453 가장 큰 회폐 단위부터 최대한 돈을 거슬러 준다 해당 화폐의 몫은 답이 보장된다 ==> 그리디",
      "frontmatter": {
        "aliases": [
          "그리디"
        ],
        "tags": [
          "algorithm"
        ],
        "date": "2024-12-30T14:32:00+09:00",
        "lastmod": "2024-12-30T14:32:00+09:00"
      }
    },
    "cpp STL": {
      "path": "/leetcode75/cpp-stl/",
      "filename": "cpp STL",
      "content": "Standard Template Library - STL 임의 타입의 객체를 보관할 수 있는 컨테이너 (container) 컨테이너에 보관된 원소에 접근할 수 있는 반복자 (iterator) 반복자들을 가지고 일련의 작업을 수행하는 알고리즘 (algorithm) cpp reference 다음은 C++의 주요 컨테이너와 관련된 개념에 대한 간단한 설명입니다: Container 시퀀스 컨테이너 (Sequence Container) array (C++11): 고정 길이 배열 vector : 동적 가변 길이 배열 inplace_vector (C++26): 동적으로 크기를 조정 가능하지만, 고정된 용량을 가지는 연속 배열로, 요소가 메모리 내에서 제자리에서 관리됨. 메모리 사용을 최적화하고 성능을 개선하기 위해 설계됨. deque : 분할 가변 길이 배열 + 양방향 capacity forward_list (C++11): 단방향 링크드 리스트 list : 양방향 링크드 리스트 연관 컨테이너 (Associative Container) set : 중복되지 않는 키를 저장하며, 키의 존재 여부를 빠르게 확인. map : 키와 값을 쌍으로 저장하며 특정 키에 대한 값을 조회. [] 사용 가능하다 multiset : 중복된 키를 허용하는 set. multimap : 중복된 키를 허용하는 map. [] 사용 불가능 커스텀 클래스 객체를 set/map 혹은 unordered_set/map 에 추가하기: 사용자 정의 타입을 사용할 경우, 비교 연산자 또는 해시 함수를 정의해야 함. 정렬되지 않는 연관 컨테이너 (unordered Associative Containers) unordered_set : 해시 테이블을 기반 set unordered_map : 해시 테이블을 기반 map Container Adaptors 기본 컨테이너를 사용하여 특정 데이터 구조(예: 스택, 큐)로 동작하도록 변환하는 클래스 템플릿. stack : LIFO(Last In First Out) 구조로, 마지막에 추가된 요소가 가장 먼저 제거됨. queue : FIFO(First In First Out) 구조로, 먼저 추가된 요소가 먼저 제거됨. priority_queue : 우선순위에 따라 요소가 제거되며, 가장 높은 우선순위의 요소가 먼저 처리됨. flat_set , flat_map , flat_multiset , flat_multimap : 내부적으로 정렬된 벡터를 사용하여 빠른 탐색과 삽입을 제공하는 컨테이너. View 데이터의 특정 부분이나 배열을 참조하여 효율적으로 접근하고 관리할 수 있도록 하는 객체 span : 연속적인 메모리 블록에 대한 뷰로, 크기와 포인터를 통해 부분 배열을 쉽게 다룰 수 있음. mdspan : 다차원 배열에 대한 뷰로, 다양한 차원의 배열을 효과적으로 접근하고 관리할 수 있도록 설계됨. vector 임의의 위치 원소 접근 ( [], at ) : $O(1)$ 맨 뒤에 원소 추가 및 제거 ( push_back / pop_back ) : amortized $O(1)$; (평균적으로 $O(1)$ 이지만 최악의 경우(공간을 새로 할당하고, 모두 복사) $O(n)$ ) 임의의 위치 원소 추가 및 제거(한칸씩 민다) ( insert, erase ) : $O(n)$ 벡터의 크기를 리턴하는 함수인 size 의 경우, 그리턴하는 값의 타입은 size_type 멤버 타입으로 정의 for (std::vector<int>::iterator itr = vec.begin(); itr != vec.end(); ++itr) { std::cout << *itr << std::endl; } 반복자 사용 %20image%2020241221083329.png) 만일 begin() == end() 라면 원소가 없는 벡터 *, + 등 배열 포인터 그대로 사용 가능 erase 실행 등 지우거나 추가할 경우 유효하지 않은 반복자 조심 std::vector<int>::iterator itr = vec.begin(); std::vector<int>::iterator end_itr = vec.end(); for (; itr != end_itr; ++itr) { if (*itr == 20) { vec.erase(itr); } } 역반복자 사용이유 아래 코드는 오류가 발생하는데 vector 의 index 를 담당하는 타입이 부호 없는 정수 이기 때문 i 가 0 일때 -1 을 한다면 오버플로우 발생으로 가장 큰수가 되어버림 2^32 -1 int main() { std::vector<int> vec; vec.push_back(1); vec.push_back(2); vec.push_back(3); // 끝에서 부터 출력하기 for (std::vector<int>::size_type i = vec.size() - 1; i >= 0; i--) { std::cout << vec[i] << std::endl; } return 0; } 범위기반 for 문 아래의 형태로 썼을 경우, elem 에 vec 의 원소들이 매 루프 마다 복사되서 들어가게 됩니다. 마치 elem = vec[i]; 와 동일 for (int elem : vec) { std::cout << \"원소 : \" << elem << std::endl; } 레퍼런스 범위 기반 for문 만약에 복사 하기 보다는 레퍼런스를 받고 싶다면 어떨까요? 매우 간단합니다. 단순히 레퍼런스 타입으로 바꾼다 template <typename T> void print_vector(std::vector<T>& vec) { // 전체 벡터를 출력하기 for (typename std::vector<T>::iterator itr = vec.begin(); itr != vec.end(); ++itr) { std::cout << *itr << std::endl; } } template <typename T> void print_vector_range_based(std::vector<T>& vec) { // 전체 벡터를 출력하기 for (const auto& elem : vec) { std::cout << elem << std::endl; } } int main() { std::vector<int> vec; vec.push_back(1); vec.push_back(2); vec.push_back(3); vec.push_back(4); std::cout << \"print_vector\" << std::endl; print_vector(vec); std::cout << \"print_vector_range_based\" << std::endl; print_vector_range_based(vec); return 0; } list 양방향 링크드 리스트 임의의 위치에 있는 원소에 접근을 바로 할 수 없다 [] 나 at 함수가 아예 정의되어 있지 않다 임의의 위치에 원소를 추가하거나 제거하는 작업이 $O(n)$ 이였지만 리스트의 경우 $O(1)$ 으로 매우 빠르게 수행 반복자의 타입이 BidirectionalIterator 타입 (벡터의 반복자는 RandomAccessIterator 이고 RandomAccessIterator 는 BidirectionalIterator 를 상속받고 있다) itr++ // itr ++ itr-- // --itr 도 됩니다. itr + 5 // 불가능! deque Pasted image 20241226132530 O(1) 으로 임의의 위치의 원소에 접근 맨 뒤에 원소를 추가/제거 하는 작업도 O(1) 맨 앞에 원소를 추가/제거 하는 작업 까지도 O(1) 임의의 위치에 있는 원소를 제거/추가 하는 작업은 벡터와 마찬가지로 O(n) 으로 수행 이 때문에 원소들이 어디에 저장되어 있는지에 대한 정보를 보관하기 위해 추가적인 메모리가 더 필요로 합니다. 기존의 벡터와는 조금 다르게, 새로 할당 시에 앞쪽 및 뒤쪽 모두에 공간을 남겨놓게 됩니다. (벡터의 경우 뒤쪽에만 공간이 남았지요) 덱 역시 벡터 처럼 임의의 위치에 원소에 접근할 수 있으므로 [] 와 at 함수를 제공하고 있고, 반복자 역시 RandomAccessIterator 타입 이고 벡터랑 정확히 동일한 방식으로 작동합니다. 참고 일반적인 상황에서는 그냥 벡터를 사용한다 (거의 만능이다!) 만약에 맨 끝이 아닌 중간에 원소들을 추가하거나 제거하는 일을 많이 하고, 원소들을 순차적으로만 접근 한다면 리스트를 사용한다. 만약에 맨 처음과 끝 모두에 원소들을 추가하는 작업을 많이하면 덱을 사용한다. set template <typename T> void print_set(std::set<T>& s) { // 셋의 모든 원소들을 출력하기 std::cout << \"[ \"; for (typename std::set<int>::iterator itr = s.begin(); itr != s.end(); ++itr) { std::cout << *itr << \" \"; } std::cout << \" ] \" << std::endl; } 10 -> 50 -> 20 -> 40 -> 30 으로 넣었지만 실제로 반복자로 원소들을 모두 출력했을 때 나온 순서는 10 -> 20 -> 30 -> 40 -> 50 순으로 나왔다는 점 set 의 경우 비교 연산자를 통해 비료를 진행한다 이때 A<B 가 false && B<A false 인 경우 서로를 같다고 생각한다 만약 두 원소가 서로 다르다면, 반드시 A 24 -> 48 단위로 할당이 되는 것 같다 삽입되거나 지워질 때 할당되면 기존 반복자 무효 vec.begin(), vec.end() 위치의 값이 지워지면 반복자 무효 transform transform (시작 반복자, 끝 반복자, 결과를 저장할 컨테이너의 시작 반복자, Pred) find template <class InputIt, class T> InputIt find(InputIt first, InputIt last, const T& value) first 부터 last 까지 쭈르륵 순회하면서 value 와 같은 원소가 있는지 확인하고 있으면 이를 가리키는 반복자를 리턴 O(n) : 순차 탐색 set 에서 사용하는 find 함수의 경우 $O(log⁡n)$ 으로 수행(정렬되어 있어서 이분 탐색 가능) unordered_set 의 경우 find 함수가 $O(1)$ 로 수행될 수 있는데 그 이유는 unordered_set 내부에서 자체적으로 해시 테이블을 이용해서 원소들을 빠르게 탐색 lambda \\[capture list\\] (받는 인자) -> 리턴 타입 { 함수 본체 } \\[capture list\\] ( 받는 인자) {함수 본체} [] : 아무것도 캡쳐 안함 [&a, b] : a 는 레퍼런스로 캡쳐하고 b 는 (변경 불가능한) 복사본으로 캡쳐 [&] : 외부의 모든 변수들을 레퍼런스로 캡쳐 [=] : 외부의 모든 변수들을 복사본으로 캡쳐 iterator 컨테이너 이터레이터 타입 비고 std::vector Random Access Iterator 인덱스 접근 가능 std::deque Random Access Iterator 양쪽 끝에서 삽입/삭제 가능 std::list Bidirectional Iterator 앞뒤로 이동 가능, 임의 접근 불가 std::set Bidirectional Iterator 정렬된 상태로 요소 저장 std::multiset Bidirectional Iterator 정렬된 상태로 요소 저장 std::map Bidirectional Iterator 키-값 쌍을 정렬된 상태로 저장 std::multimap Bidirectional Iterator 키-값 쌍을 정렬된 상태로 저장 std::array Random Access Iterator 고정 크기 배열, 인덱스 접근 가능 std::forward_list Forward Iterator 앞 방향으로만 이동 가능 BidirectionalIterator 상속 RandomAccessIterator remove 함수의 경우 반복자의 타입이 ForwardIterator 입니다. 주요 반복자 종류 반복자 유형 정의 및 역할 iterator 읽기와 쓰기 모두 가능한 일반 반복자입니다. const_iterator 읽기 전용 반복자입니다. 값을 수정할 수 없도록 보장합니다. reverse_iterator 컨테이너를 뒤에서 앞으로(iterate) 순회할 때 사용하는 반복자입니다. const_reverse_iterator 읽기 전용의 역방향 반복자입니다. pointer 그냥 포인터 일반적으로 반복자는 내부적으로 포인터( T* )로 정의되므로, 반복자 타입이 포인터와 동일한 동작을 할 수 있습니다. const_pointer const 포인터 읽기 전용의 포인터입니다. 주요 메서드 메서드 설명 begin() 컨테이너의 첫 번째 요소를 가리키는 반복자를 반환합니다. end() 컨테이너의 마지막 요소 다음을 가리키는 반복자를 반환합니다. (범위를 벗어난 포인터와 비슷) cbegin() 첫 번째 요소를 가리키는 const_iterator 를 반환합니다. cend() 마지막 요소 다음을 가리키는 const_iterator 를 반환합니다. rbegin() 마지막 요소를 가리키는 reverse_iterator 를 반환합니다. rend() 첫 번째 요소 이전을 가리키는 reverse_iterator 를 반환합니다. crbegin() const_reverse_iterator 로 마지막 요소를 가리킵니다. crend() const_reverse_iterator 로 첫 번째 요소 이전을 가리킵니다. vector Vector의 초기화 구문 설명 vector<자료형> 변수명 백터 생성 vector<자료형> 변수명(숫자) 숫자만큼 백터 생성 후 0으로 초기화 vector<자료형> 변수명 = { 변수1, 변수2, ... } 백터 생성 후 오른쪽 변수 값으로 초기화 vector<자료형> 변수명[] = {, } 백터 배열(2차원 백터) 선언 및 초기화 vector<vector<자료형>> 변수명 2차원 백터 생성 vector<자료형> 변수명.assign(범위, 값) 범위 내에서 값을 초기화 Vector의 Iterators (시간복잡도와 반환값) 구문 설명 시간복잡도 반환값 v.begin() 백터 시작점의 주소 값 반환 O(1) iterator v.end() 백터 (끝부분 + 1)의 주소값 반환 O(1) iterator v.rbegin() 백터의 끝 지점을 시작점으로 반환 O(1) reverse_iterator v.rend() 백터의 (시작 + 1) 지점을 끝 부분으로 반환 O(1) reverse_iterator Vector Element Access 구문 설명 시간복잡도 반환값 v.at(i) 백터의 i번째 요소 접근 (범위 검사함) O(1) 요소 값 v[i] 백터의 i번째 요소 접근 (범위 검사 안함) O(1) 요소 값 v.front() 백터의 첫 번째 요소 접근 O(1) 첫 번째 요소 값 v.back() 백터의 마지막 요소 접근 O(1) 마지막 요소 값 Vector에 요소 삽입 구문 설명 시간복잡도 반환값 v.push_back(값) 백터의 마지막 부분에 요소 추가 평균 O(1), 최악 O(n) 없음 v.pop_back() 백터의 마지막 부분 제거 O(1) 없음 v.insert(삽입위치, 값) 사용자가 원하는 위치에 요소 삽입 평균 O(n) 삽입된 위치의 iterator v.emplace(삽입위치, 값) 사용자가 원하는 위치에 요소 삽입 (복사생성자 없음) 평균 O(n) 삽입된 위치의 iterator v.emplace_back(값) 백터의 마지막 부분에 요소 추가 (복사생성자 없음) 평균 O(1), 최악 O(n) 없음 v.erase(위치) 지정한 위치의 요소를 제거 평균 O(n) 삭제 후 위치의 iterator v.clear() 백터의 모든 요소 제거 O(n) 없음 v.resize(크기) 백터 크기 조정 O(n) 없음 v.swap(다른 백터) 백터와 백터를 교환 O(1) 없음 Vector Capacity 구문 설명 시간복잡도 반환값 v.empty() 백터가 비어있는지 확인 O(1) true 또는 false v.size() 백터의 크기 반환 O(1) 크기 값 v.capacity() 백터의 실제 할당된 크기 반환 O(1) 크기 값 v.max_size() 백터의 최대 크기 반환 O(1) 최대 크기 값 v.reserve(크기) 백터의 최소 크기 예약 O(n) 없음 v.shrink_to_fit() 백터의 capacity를 실제 크기에 맞춤 O(n) 없음 set Member Functions 구문 설명 반환값 시간 복잡도 constructor 셋을 생성합니다. 없음 O(1) destructor 셋을 소멸합니다. 없음 O(1) operator= 컨테이너에 값을 할당합니다. set& O(n) get_allocator 연관된 할당자를 반환합니다. allocator_type O(1) Iterators 구문 설명 반환값 시간 복잡도 begin 시작 위치의 반복자를 반환합니다. iterator O(1) cbegin 시작 위치의 상수 반복자를 반환합니다. const_iterator O(1) end 끝 위치의 반복자를 반환합니다. iterator O(1) cend 끝 위치의 상수 반복자를 반환합니다. const_iterator O(1) rbegin 역순으로 시작 위치의 반복자를 반환합니다. reverse_iterator O(1) crbegin 역순으로 시작 위치의 상수 반복자를 반환합니다. const_reverse_iterator O(1) rend 역순으로 끝 위치의 반복자를 반환합니다. reverse_iterator O(1) crend 역순으로 끝 위치의 상수 반복자를 반환합니다. const_reverse_iterator O(1) Capacity 구문 설명 반환값 시간 복잡도 empty 컨테이너가 비어있는지 확인합니다. bool O(1) size 원소의 개수를 반환합니다. size_type O(1) max_size 최대 가능한 원소의 개수를 반환합니다. size_type O(1) Modifiers 구문 설명 반환값 시간 복잡도 clear 내용을 지웁니다. 없음 O(n) insert 원소 또는 노드를 삽입합니다. pair<iterator, bool> O(log n) insert_range 원소의 범위를 삽입합니다. (C++23) void O(n) emplace 인플레이스에서 원소를 생성합니다. pair<iterator, bool> O(log n) emplace_hint 힌트를 사용하여 인플레이스에서 원소를 생성합니다. pair<iterator, bool> O(log n) erase 원소를 삭제합니다. size_type O(log n) swap 내용을 교환합니다. 없음 O(1) extract 컨테이너에서 노드를 추출합니다. (C++17) node_type O(log n) merge 다른 컨테이너로부터 노드를 스파이스합니다. (C++17) void O(n) Lookup 구문 설명 반환값 시간 복잡도 count 특정 키와 일치하는 원소의 개수를 반환합니다. size_type O(log n) find 특정 키를 가진 원소를 찾습니다. iterator O(log n) contains 특정 키를 가진 원소가 있는지 확인합니다. (C++20) bool O(log n) equal_range 특정 키와 일치하는 원소의 범위를 반환합니다. pair<iterator, iterator> O(log n) lower_bound 주어진 키보다 작지 않은 첫 번째 원소의 반복자를 반환합니다. iterator O(log n) upper_bound 주어진 키보다 큰 첫 번째 원소의 반복자를 반환합니다. iterator O(log n) Observers 구문 설명 반환값 시간 복잡도 key_comp 키를 비교하는 함수를 반환합니다. key_compare O(1) value_comp 값 타입의 객체에서 키를 비교하는 함수를 반환합니다. value_compare O(1) string ​ 특정 원소 접근 방법 str.at(idx) idx 위치 문자 반환, 범위 유효성 체크 O str[idx] idx 위치 문자 반환, 범위 유효성 체크 X str.front() 문자열의 가장 앞의 문자 반환 str.back() 문자열의 가장 뒤의 문자 반환 문자열의 크기 str.length() 문자열 길이 반환 str.size() 문자열 길이 반환(length()와 동일) str.max_size() 최대한 메모리 할당할 경우 저장할 수 있는 문자열 길이 반환 str.capacity() 문자열의 메모리 크기 반환 str.resize(n) str을 n의 크기로 만듦. 삭제 또는 빈 공간으로 채움 str.resize(n, 'a') n이 str 길이보다 크면 빈 공간을 'a'로 채움 str.shrinktofit() capacity가 실제 사용하는 메모리보다 큰 경우 메모리 줄여 줌(메모리 낭비 제거) str.reserve(n) 사이즈 n 만큼의 메모리 미리 할당 str.empty() str이 빈 문자열인지 확인 문자열 삽입/추가/삭제 str.append(str2) str 뒤에 str2 문자열을 이어 붙여 줌(str + str2 와 같음) str.append(str2, n ,m) str 뒤에 'str2의 n index 부터 m개의 문자'를 이어 붙여 줌 str.append(n, 'a') str 뒤에 n 개의 'a'를 붙여 줌 str.insert(n, str2) n번째 index 앞에 str2 문자열을 삽입함 str.replace(n, k, str2) n번째 index 부터 k개의 문자열을 str2로 대체함 str.clear() 저장된 문자열을 모두 지움 str.erase() clear()와 같음 str.erase(n, m) n번째 index부터 m개의 문자를 지움 str.erase(n, m) ← iterator n~m index 문자열을 지움(n, m은 iterator임) str.push_back(c) str의 맨 뒤에 c를 붙여 줌 str.pop_back() str의 맨 뒤의 문자를 제거 str.assign(str2) str 에 str2 문자열을 할당함 부분 문자/비교/복사/찾기 str.substr() str 전체를 반환 str.substr(n) str의 n번째 index부터 끝까지 부분 문자열 반환 str.substr(n, k) str의 n번째 index부터 k개의 부분 문자열 반환 str.compare(str2) str과 str2가 같은지 비교, strstr2인 경우 양수 반환 str.copy(str2, k, n) str의 n번째 index부터 k개의 문자열 복사 str.find(\"abcd\") \"abcd\"가 str에 포함되어 있는지 확인, 찾으면 해당 부분 첫 index 반환 str.find(\"abcd\", n) n번째 index부터 \"abcd\"를 찾음 str.findfirstof(\"/\") \"/\"가 처음 나타나는 index str.findlastof(\"/\") \"/\"가 마지막으로 나타나는 index 기타 유용한 함수들 str.c_str() string을 c스타일의 문자열로 변경 str.begin() string의 시작 iterator 반환 str.end() string의 끝 iterator 반환 swap(str, str2) str과 str2를 바꿔줌 str = str2 + str3 str2와 str3를 붙여서 str에 복사함 str += str2 str 뒤에 str2를 붙여줌 str = str2 str에 str2 복사 (Deep Copy) str == str2 str과 str2가 같은지 확인 str > str2, str < str2 str이 str2보다 사전순으로 앞인지 뒤인지 확인 isdigit(c) #include <cctype> , c가 숫자인지 확인, 숫자이면 0이 아닌 숫자 반환 isalpha(c) #include <cctype> , 알파벳 확인, 대문자는 1 반환, 소문자는 2 반환, 알파벳이 아니면 0 반환 toupper(c) #include <cctype> , c를 대문자로 변환 tolower(c) #include <cctype> , c를 소문자로 변환 stoi(), stof(), stol(), stod() 문자열을 숫자로 변환(int, float, long, double) to_string(n) 숫자 n을 문자열로 변환",
      "frontmatter": {
        "tags": [
          "cpp",
          "language"
        ],
        "date": "2023-12-28T10:00:00+09:00",
        "lastmod": "2023-12-28T10:00:00+09:00"
      }
    },
    "cpp STL2": {
      "path": "/leetcode75/cpp-stl2/",
      "filename": "cpp STL2",
      "content": "템플릿 인수로 타입과 비타입(정수) 둘다 올수 있다 타입인자로 올수 있는 타입 정수 타입들 ( bool , char , int , long 등등). 당연히 float 과 double 은 제외 포인터 타입 enum 타입 std::nullptr_t (널 포인터) c++ 20부터 조금 널널해짐 string 타입 정의 비고 std::string std::basic_string<char> 기본적인 문자열 타입 std::wstring std::basic_string<wchar_t> wchar_t 의 크기는 시스템마다 다름;윈도우에서는 2바이트, 유닉스에서는 4바이트 std::u8string std::basic_string<char8_t> C++20에 새로 추가; char8_t 는 1바이트;UTF-8 문자열 저장 std::u16string std::basic_string<char16_t> char16_t 는 2바이트;UTF-16 문자열 저장 std::u32string std::basic_string<char32_t> char32_t 는 4바이트;UTF-32 문자열 저장 Cpp에서 문자열 리터럴의 종류는 다음과 같습니다: 종류 설명 예시 일반 문자열 리터럴 기본적인 문자열 리터럴, 이스케이프 문자를 사용해야 함. \"Hello, World!\" 원시 문자열 리터럴 이스케이프 없이 문자열을 표현할 수 있음. R\"(Hello, \"World!\")\" 와이드 문자열 리터럴 wchar_t 타입의 문자열로, 넓은 문자 지원. L\"Hello, World!\" UTF-8 문자열 리터럴 char8_t 타입의 UTF-8 문자열. u8\"Hello, World!\" UTF-16 문자열 리터럴 char16_t 타입의 UTF-16 문자열. u\"Hello, World!\" UTF-32 문자열 리터럴 char32_t 타입의 UTF-32 문자열. U\"Hello, World!\" 일반 문자열 리터럴: 기본적인 문자열 표현으로, 이스케이프 시퀀스를 사용해야 합니다. 원시 문자열 리터럴: R\" 로 시작하여 \" 로 끝나는 형식으로, 이스케이프 없이 문자열을 표현할 수 있습니다. 와이드 문자열 리터럴: L\" 로 시작하며, wchar_t 타입의 문자를 저장합니다. UTF-8 문자열 리터럴: u8\" 로 시작하여 UTF-8 인코딩을 사용하는 문자열입니다. UTF-16 문자열 리터럴: u\" 로 시작하여 UTF-16 인코딩을 사용하는 문자열입니다. UTF-32 문자열 리터럴: U\" 로 시작하여 UTF-32 인코딩을 사용하는 문자열입니다.",
      "frontmatter": {
        "date": "2024-12-23T09:42:00+09:00",
        "lastmod": "2024-12-23T09:42:00+09:00"
      }
    },
    "cpp 모음 찾기": {
      "path": "/leetcode75/cpp-모음-찾기/",
      "filename": "cpp 모음 찾기",
      "content": "string vowels = \"aeiouAEIOU\"; bool isVowels = vowels.find(word[index]) == string::npos",
      "frontmatter": {
        "aliases": [
          "vowels"
        ],
        "tags": [
          "algorithm",
          "cpp"
        ],
        "date": "2025-01-07T21:11:00+09:00",
        "lastmod": "2025-09-29T12:44:56+09:00"
      }
    },
    "cpp 생성자 소멸자 규칙": {
      "path": "/leetcode75/cpp-생성자-소멸자-규칙/",
      "filename": "cpp 생성자 소멸자 규칙",
      "content": "디폴트 생성자(Default Constructor) 명시적으로 디폴트 생성자 {클래스 이름}() = default; // 디폴트 생성자를 정의해라 new 와 malloc 모두 동적으로 할당하지만 new 의 경우 객체를 동적으로 생성하면서와 동시에 자동으로 생성자도 호출해준다는 점입니다. delete 와 free 의 경우도 소멸자 호출 여부가 다름과 동일 ~{클래스의 이름} 소멸자",
      "frontmatter": {
        "date": "2025-01-30T22:58:00+09:00",
        "lastmod": "2025-01-30T22:58:00+09:00"
      }
    },
    "cpp 초기화 방식": {
      "path": "/leetcode75/cpp-초기화-방식/",
      "filename": "cpp 초기화 방식",
      "content": "class Widget { public: Widget() { std::cout << \"default\" << std::endl; } Widget(std::initializer_list<int> il) { std::cout << \"init\" << std::endl; } }; int main() { Widget w1; //기본 생성자 호출 Widget w2(); //함수! 호출 x Widget w3{}; //기본 생성자 호출 Widget w4({});//std::initializer_list 이용해 호출 Widget w5{ {} };//std::initializer_list 이용해 호출 Widget w6 = {}; // 기본 생성자 호출 Widget w7 = { {} }; //std::initializer_list 이용해 호출 } 모두의 코드 해당 자료 https://modoocode.com/286 초기화(Initialization)란? 정의: 변수를 선언할 때 그 변수에 최초의 값을 제공하는 과정을 초기화라고 합니다. 객체나 변수의 메모리 공간이 할당되면서 동시에 정해진 초기 값이 부여됩니다. 이는 선언문의 초기화 부분이나, new 표현식, 함수 호출 시(함수 인자 및 리턴값 초기화) 등에서 이루어집니다. 초기화 구문(Initializers) 형태 각 선언자(declarator)에 대해 초기화자가 있을 경우 사용할 수 있는 구문은 다음과 같습니다. = expression 설명: 등호( = ) 다음에 임의의 표현식이 나오는 형태입니다. 예: int a = 5; std::string s = \"hello\"; 용어: \"복사 초기화(copy-initialization)\"라고 부릅니다. = {} 또는 = { initializer-list } 또는 = { designated-initializer-list } 설명: 등호 뒤에 중괄호를 사용하여 초기값을 나열하는 형태입니다. 역사: C++11부터 중괄호를 사용한 목록 초기화(list-initialization)가 도입되었습니다. C++20부터 지정 초기자(designated initializer) 문법이 추가되었습니다. 예: int arr[3] = {1, 2, 3}; // 배열에 대한 aggregate 초기화 MyStruct s = { .x = 10, .y = 20 }; // C++20 지정 초기자 ( expression-list ) 또는 ( initializer-list ) 설명: 괄호를 사용한 직접 초기화 구문입니다. 차이점: C++11 이전에는 괄호 안에 초기값 목록을 쓰는 것이 직접 초기화(direct-initialization) 문법이었습니다. C++11 이후에도 여전히 지원됩니다. 예: std::string s(\"hello\"); // 직접 초기화 {} 또는 { initializer-list } 또는 { designated-initializer-list } 설명: 중괄호만을 사용한 초기화 구문으로, C++11부터 도입된 목록 초기화(list-initialization)입니다. 예: std::string s{\"hello\"}; // 목록 초기화 (C++11 이후) int a{}; // 기본값으로 초기화 (보통 0으로 초기화) 요약: 구문 (1)은 복사 초기화(copy initialization) 구문 (3)은 직접 초기화(direct initialization) 구문 (2)와 (4)는 중괄호를 사용하는 목록 초기화(list initialization)에 해당합니다. 지정 초기자(designated initializer)는 C++20부터 사용할 수 있습니다. 초기화에 사용되는 요소들의 설명 expression: 임의의 표현식을 의미합니다. 단, 괄호 없이 나열한 콤마 표현식(unparenthesized comma expressions)은 제외됩니다. expression-list: 표현식들을 쉼표(,)로 구분하여 나열한 목록입니다. 역시, 괄호 없이 나열된 콤마 표현식은 제외합니다. initializer-list: 쉼표로 구분된 초기화 절(clause)들의 목록입니다. designated-initializer-list: 각 초기화 절에 멤버 이름을 명시하는 초기자들의 목록입니다. (C++20부터 지원) brace-enclosed initializer list: 중괄호 {} 로 감싼 초기자 구문을 총칭하는 말입니다. 초기화 구문에 따른 초기화 의미 (Initializer Semantics) 초기화가 진행되는 대상에 따라 다르게 적용됩니다. A. 참조(reference) 초기화 설명: 초기화 대상이 참조일 경우, 참조 초기화(reference initialization) 규칙이 적용됩니다. 필수: 참조는 반드시 유효한 객체를 가리키도록 초기화되어야 합니다. 예: int x = 10; int& ref = x; // 올바른 참조 초기화 문제: 초기화하지 않은 참조는 프로그램이 잘못된 동작을 하거나 컴파일 오류를 발생시킵니다. B. 객체(object) 초기화 대상: 일반 변수나 객체는 초기화될 때 객체 초기화가 진행됩니다. T 타입의 객체에 대해: (1) 복사 초기화 초기화 구문이 = expression 인 경우, 복사 초기화가 진행됩니다. 예: std::string s = \"hello\"; (2) 중괄호를 사용한 초기화 T가 aggregate (집합체)인 경우, aggregate 초기화가 적용됩니다. 예: 배열, 구조체 등. T가 스칼라 타입(예: int, double)인 경우, T x = { a }; 는 T x = a; 와 동일하게 동작합니다. 만약 T가 aggregate가 아니면서 중괄호 초기화가 사용되었는데 해당 타입에 맞는 생성자가 없다면, 프로그램은 ill-formed (잘못된 프로그램)으로 간주됩니다. (3) 직접 초기화 초기화 구문이 ( expression-list ) 인 경우, 직접 초기화가 진행됩니다. (4) 목록 초기화 (C++11 이후) 중괄호를 사용한 형태로, list-initialization이 적용됩니다. 주의: 초기화 구문에 따라 복사 초기화와 직접 초기화는 약간의 차이가 있을 수 있으며, 특히 타입 변환이나 임시 객체 생성 과정에 영향을 미칠 수 있습니다. C++11부터는 목록 초기화가 도입되어 중괄호 초기화가 점점 일반적으로 사용됩니다. 예제 코드와 그 의미 아래 코드를 하나씩 살펴봅니다: #include <string> std::string s1; // (a) 기본 초기화 (default initialization) std::string s2(); // (b) 함수 선언! 초기화가 아님 std::string s3 = \"hello\"; // (c) 복사 초기화 (copy initialization) std::string s4(\"hello\"); // (d) 직접 초기화 (direct initialization) std::string s5{'a'}; // (e) 목록 초기화 (list initialization, C++11부터) (a) std::string s1; 기본 초기화(default-initialization) 객체 s1 은 아무 초기값 없이 생성됩니다. std::string의 경우, 내부적으로 기본 생성자가 호출되어 빈 문자열(\"\")이 됩니다. (b) std::string s2(); 이것은 초기화가 아니라 함수 선언입니다. s2 는 매개변수가 없고 std::string 을 반환하는 함수로 해석됩니다. 이를 \"most vexing parse\" 라고 부르기도 합니다. (c) std::string s3 = \"hello\"; 복사 초기화(copy initialization) \"hello\"라는 문자열 리터럴을 사용하여 s3를 초기화합니다. (d) std::string s4(\"hello\"); 직접 초기화(direct initialization) 생성자에 \"hello\"를 직접 전달하여 s4를 초기화합니다. (e) std::string s5{'a'}; 목록 초기화(list initialization) 중괄호를 사용하여 s5를 초기화합니다. 이 경우 std::string에 대해 어떤 생성자가 호출되는지(예: 단일 문자로 구성된 문자열 등)는 타입에 따라 달라집니다. 또 다른 예로 배열과 참조 초기화: char a[3] = {'a', 'b'}; // aggregate 초기화: 배열의 첫 두 요소가 'a'와 'b'로, 나머지는 0으로 초기화 char& c = a[0]; // 참조 초기화: c는 배열 a의 첫 번째 요소를 참조 배열 a 는 중괄호를 사용하여 aggregate 초기화됩니다. 참조 c 는 배열의 첫 번째 요소 a[0] 를 가리키도록 초기화됩니다. 비지역 변수(Non-local variables) 초기화 비지역 변수란 전역 변수나 네임스페이스 범위, 또는 정적/스레드 로컬(static/thread-local) 변수를 말합니다. A. 비지역 변수의 초기화 시점 정적 저장 기간(static storage duration)을 가지는 변수들은 프로그램 시작 시, main 함수 실행 전에 초기화됩니다. 스레드 로컬(thread-local storage) 변수들은 각 스레드가 시작될 때 초기화되며, 스레드 함수가 실행되기 전에 이루어집니다. B. 초기화 과정은 두 단계로 나뉩니다 정적 초기화 (Static initialization) 1) 상수 초기화 (Constant initialization): 가능한 경우, 컴파일 타임에 상수 값으로 초기화됩니다. 컴파일러는 미리 계산된 초기 값(객체 표현)을 프로그램 이미지에 저장할 수 있습니다. 2) 0으로 초기화 (Zero initialization): 상수 초기화가 불가능한 경우, 정적 및 스레드 로컬 변수들은 먼저 0으로 초기화됩니다. 0으로 초기화된 변수들은 보통 프로그램의 .bss 섹션에 위치하며, 프로그램 로딩 시 운영체제가 이 영역을 0으로 채웁니다. 동적 초기화 (Dynamic initialization) 정적 초기화가 완료된 후, 나머지 동적 초기화가 진행됩니다. 동적 초기화에는 다음 세 가지 유형이 있습니다. (1) 순서가 정해지지 않은(unordered) 동적 초기화 적용 대상: 주로 클래스 템플릿의 정적 데이터 멤버나 변수 템플릿(특수화되지 않은 경우, C++14부터 적용) 이들 변수의 초기화 순서는 다른 동적 초기화들과는 indeterminately(불특정하게) 순서가 정해집니다. 단, 프로그램이 변수 초기화 전에 스레드를 시작하는 경우 C++17부터는 \"unsequenced\"로 초기화됩니다. (2) 부분 순서(partially-ordered) 동적 초기화 적용 대상: inline 변수 중 암시적 또는 명시적 인스턴스화되지 않은 특수화가 아닌 변수들 한 번에 한 번, 정의된 순서에 따라 초기화 순서가 결정됩니다. 한 번의 번역 단위(translation unit) 내에서, 어떤 변수 V가 항상 W보다 먼저 정의되면 V의 초기화가 W보다 먼저 이루어집니다. (C++17부터 적용) (3) 순서가 정해진(ordered) 동적 초기화 적용 대상: 위의 두 경우에 해당하지 않는 모든 비지역 변수들 한 번의 번역 단위 내에서, 소스 코드에 나타난 순서대로 초기화가 진행됩니다. 서로 다른 번역 단위 간에는 초기화 순서가 불특정(indeterminately sequenced) 입니다. 스레드 로컬 변수의 경우, 다른 번역 단위 간에는 초기화 순서가 unsequenced 됩니다. 예외 처리: 만약 비지역 변수(정적 또는 스레드 로컬)의 초기화 도중 예외가 발생하면, std::terminate 가 호출됩니다. 조기(dynamic) 초기화(Early dynamic initialization) 개념: 컴파일러는 아래 조건을 만족할 경우, 동적 초기화를 정적 초기화 단계에서 미리(컴파일 타임에) 수행할 수 있습니다. 조건: 동적 초기화가 다른 네임스페이스 범위 객체의 값을 변경하지 않아야 합니다. 정적 초기화 버전이 동적 초기화를 수행했을 때와 동일한 값을 산출해야 합니다. 결과: 예를 들어, 어떤 객체 o1 의 초기화 과정에서 같은 번역 단위 내의 o2 에 접근한다면, 컴파일러가 o2 를 정적으로 초기화할 수 있으면, o2 는 완전히 초기화된 상태의 값이거나 단순히 0으로 초기화된 상태일 수 있으며, 이는 명확하지 않습니다(불특정). 예제: inline double fd() { return 1.0; } extern double d1; double d2 = d1; // d1이 동적 초기화되었으면 0.0, 아니면 1.0 등으로 초기화될 수 있음 double d1 = fd(); // d1은 1.0으로 초기화; 정적 초기화될 수도, 동적 초기화될 수도 있음. 지연된 동적 초기화(Deferred dynamic initialization) 개념: 일부 구현에서는 동적 초기화를 main 함수의 첫 문장 이전에 수행할 수도 있고, 또는 main 함수(혹은 스레드 초기 함수)의 첫 문장 이후로 지연시킬 수도 있습니다. 규칙 (C++17부터): 비 inline 변수: 만약 동적 초기화가 main 함수 시작 후로 지연된다면, 같은 번역 단위 내에서 해당 변수를 ODR-사용(One Definition Rule 사용)하기 전에 초기화가 이루어집니다. 만약 번역 단위 내의 변수나 함수가 전혀 사용되지 않으면, 그 번역 단위에 정의된 비지역 변수들은 아예 초기화되지 않을 수도 있습니다(동적 라이브러리의 on-demand 초기화와 유사). inline 변수: inline 변수의 경우, 해당 변수가 ODR-사용되기 전에 초기화가 이루어집니다. 여러 파일에 걸친 예제: 예제 코드에서는 서로 다른 번역 단위(File 1, File 2, File 3)에서 변수 a 와 b 의 초기화 순서에 따라, 만약 a 가 main 전에 초기화된다면, A::A() 안에서 b 가 아직 초기화되지 않았을 수 있습니다. 반면, a 가 main 이후에 초기화(ODR-사용에 의해)된다면, b 도 초기화된 후에 사용됩니다. 블록 스코프(static local) 변수 설명: 함수 내부(또는 블록 내)에서 static 또는 thread_local 로 선언된 변수들은 지역 정적 변수라고 합니다. 이들 변수의 초기화에 관한 규칙은 별도로 다루어지며, 보통 해당 블록에 처음 도달했을 때 초기화가 수행됩니다. 참고: 블록 스코프 변수는 전역 변수와 달리, 외부 또는 내부 링케이지(external or internal linkage)를 갖는 선언에서는 초기화자가 허용되지 않습니다. 이런 경우 extern 선언을 통해 정의와 분리하여 초기화해야 합니다. 클래스 멤버 초기화 비정적 데이터 멤버(non-static data members) 클래스 내부에서, 비정적 데이터 멤버들은 두 가지 방식으로 초기화할 수 있습니다. 생성자 멤버 초기화 리스트: 생성자의 콜론(:) 뒤에 멤버들을 초기화하는 방식입니다. 기본 멤버 초기값(default member initializer): 클래스 정의 내에서 멤버 변수를 선언할 때, 등호( = )나 중괄호를 사용하여 초기값을 부여하는 방식입니다. 우선순위: 만약 둘 다 존재한다면, 생성자 멤버 초기화 리스트에 명시된 초기값이 우선합니다. 소멸 순서 참고: 비지역 변수의 소멸 순서는 C++ 표준 라이브러리의 std::exit 문서에 설명되어 있습니다. 정적 객체들은 프로그램 종료 시, 생성 순서의 역순으로 소멸됩니다. 최종 정리 C++ 초기화는 매우 다양한 구문과 규칙이 있으며, 초기화 방법에 따라 다음과 같이 분류할 수 있습니다. 변수 선언 시 초기화 복사 초기화, 직접 초기화, 목록 초기화, 지정 초기자 등을 사용할 수 있습니다. 참조 초기화 반드시 유효한 객체를 가리키도록 초기화되어야 하며, 임시 객체에 바인딩할 수 없음. 비지역(전역, 네임스페이스, static, thread_local) 변수 초기화 두 단계(정적 초기화와 동적 초기화)로 진행됩니다. 정적 초기화에서는 상수 초기화와 0 초기화가 수행됩니다. 동적 초기화는 unordered, partially-ordered, ordered 세 가지 방식으로 진행되며, 번역 단위 간 순서는 보장되지 않습니다. 조기(dynamic) 초기화와 지연 초기화 컴파일러가 조건을 만족하면 동적 초기화를 정적으로 미리 수행할 수 있으며, 또는 프로그램 실행 도중(ODR-사용 시점)에 지연시킬 수도 있습니다. 클래스 멤버 초기화 생성자 초기화 리스트 또는 기본 멤버 초기값을 통해 초기화합니다.",
      "frontmatter": {
        "date": "2025-02-02T02:18:00+09:00",
        "lastmod": "2025-09-29T12:44:58+09:00"
      }
    },
    "hash set, unordered_set 구현체": {
      "path": "/leetcode75/hash-set-unordered_set-구현체/",
      "filename": "hash set, unordered_set 구현체",
      "content": "MD-5나 SHA 가 유명한 해시 알고리즘 insert, erase, find 모두가 $O(1)$ 으로 수행 #include <iostream> #include <vector> #include <list> #include <functional> template <typename T> class MyUnorderedSet { private: std::vector<std::list<T>> table; size_t current_size; float load_factor; size_t capacity; size_t hash(const T& value) const { auto temp = std::hash<T>()(value) % capacity; return temp; } void rehash() { capacity *= 2; std::vector<std::list<T>> new_table(capacity); for (const auto& bucket : table) { for (const auto& value : bucket) { size_t new_index = std::hash<T>()(value) % capacity; new_table[new_index].push_back(value); } } table = std::move(new_table); } public: MyUnorderedSet(size_t init_capacity = 8, float load_factor = 0.75) : capacity(init_capacity), load_factor(load_factor), current_size(0) { table.resize(capacity); } bool insert(const T& value) { if (contains(value)) return false; if (current_size >= capacity * load_factor) { rehash(); } size_t index = hash(value); table[index].push_back(value); current_size++; return true; } bool contains(const T& value) const { size_t index = hash(value); for (const auto& item : table[index]) { if (item == value) { return true; } } return false; } bool erase(const T& value) { size_t index = hash(value); auto& bucket = table[index]; for (auto it = bucket.begin(); it != bucket.end(); ++it) { if (*it == value) { bucket.erase(it); current_size--; return true; } } return false; } size_t size() const { return current_size; } bool empty() const { return current_size == 0; } }; int main() { MyUnorderedSet<int> my_set; my_set.insert(1); my_set.insert(2); my_set.insert(3); std::cout << \"Contains 2: \" << my_set.contains(2) << std::endl; std::cout << \"Size: \" << my_set.size() << std::endl; my_set.erase(2); std::cout << \"Contains 2 after erase: \" << my_set.contains(2) << std::endl; std::cout << \"Size after erase: \" << my_set.size() << std::endl; return 0; }",
      "frontmatter": {
        "tags": [
          "algorithm",
          "cpp"
        ],
        "date": "2025-01-22T07:44:00+09:00",
        "lastmod": "2025-06-25T15:48:02+09:00"
      }
    },
    "ps 공부": {
      "path": "/leetcode75/ps-공부/",
      "filename": "ps 공부",
      "content": "PS를 열심히 공부하겠다고 생각하였으나, 정작 무엇부터 시작해야하는지 감이 쉽게 잡히지 않았습니다. 저는 나름대로 커리큘럼을 세우기 위해서 구글링을 해 보았고, 정말 귀중한 블로그를 찾게 됩니다. plzrun님의 블로그로, 특히 이 게시글에서 도움을 매우 많이 받았습니다. plzrun님은 백준 강의를 추천해 주셨는데, 저는 8~9만을 낼 거금은 없었기에 포스팅에 나와 있는 커리큘럼을 따라 가기로 하였습니다. 입출력 방식에서 시작해서, 기초 자료구조, 기초 수학, DP, 정렬, 그래프, 이분탐색, 분할정복, 그리디, 완전탐색으로 끝이 납니다. 자세한 문제 커리큘럼은 아래와 같습니다. 입출력 - 2557, 1000, 2558, 10950, 10951, 10952, 10953, 11021, 11022, 11718, 11719, 11720, 11721, 2741, 2742, 2739, 1924, 8393, 10818, 2438, 2439, 2440, 2441, 2442, 2445, 2522, 2446, 10991, 10992 DP - 1463, 11726, 11727, 9095, 10844, 11057, 2193, 9465, 2156, 11053, 11055, 11722, 11054, 1912, 2579, 1699, 2133, 9461, 2225, 2011, 11052 정렬 - 2751, 11650, 11651, 10814, 10825, 10989, 11652, 11004 스택 - 10828, 9012, 10799 큐 - 10845 덱 - 10866 문자열 처리 - 10808, 10809, 10820, 2743, 11655, 10824, 11656 기타 자료 구조 - 1406, 1158, 1168 기초 수학 - 10430, 2609, 1934, 1850, 9613, 11005, 2745, 1373, 1212, 2089, 11576, 1978, 1929, 11653, 10872, 1676, 2004, 6588 그래프 - 1260, 11724, 1707, 10451, 2331, 9466, 2667, 4963, 7576, 2178, 2146, 1991, 11725, 1167, 1967 이분탐색/삼분탐색 - 1654, 2805, 2110, 10815, 10816, 11662 분할정복 - 11728, 1780, 11729, 1992, 2447, 2448, 1517, 2261 그리디 - 11047, 2875, 10610, 1783, 1931, 11399, 2873, 1744 완전탐색 - 1476, 1107, 1451, 9095, 10819, 10971, 1697, 1963, 9019, 1525, 2251, 2186, 3108, 5014, 1759, 2580, 1987, 6603, 1182, 2003, 1806, 1644, 1261, 1208, 7453, 2632, 2143 이 문제를 모두 풀어보는 것이 알고리즘의 기초라고 생각합니다. 그리고 알고리즘 기법을 새로 배우기 위해서는 백준 강의말고 인프런 강의를 참고하였습니다. 권오흠 교수님의 강의인데, 처음에 개념을 익히기 좋습니다. 링크는 이곳이 되겠습니다. 정리하자면, 처음에 개념을 인프런 강의에서 배우고, 구글링을 통하여 개념을 다시 한 번 학습한 다음에 plzrun님의 커리큘럼을 따라가는 방식으로 공부를 시작했습니다. BOJ에서 다음 문제들을 쭉 순서대로 풀어본다. **boj.kr/문제번호** <= 형태로 검색하면 된다. 입출력 - 2557, 1000, 2558, 10950, 10951, 10952, 10953, 11021, 11022, 11718, 11719, 11720, 11721, 2741, 2742, 2739, 1924, 8393, 10818, 2438, 2439, 2440, 2441, 2442, 2445, 2522, 2446, 10991, 10992 입출력 문제들을 풀 때 **_10분이상 이 문제를 붙들고 있는 경우, 그건 입출력에서 뭔가 모르는 부분이 반드시 있다는 뜻_**이므로 이전 질문들을 무조건 찾아보고 다른 사람이 푼 코드를 반드시 봐야 한다. 이 때 코드 길이 줄이려고 이상하게 짧은 코드들 많은데, 그런건 보지 말고 랭킹 100위권 안에 드는 사람들 중 인덴트 멀쩡한 코드를 보면 된다. 그 다음 DP문제를 풀어보자. DP - 1463, 11726, 11727, 9095, 10844, 11057, 2193, 9465, 2156, 11053, 11055, 11722, 11054, 1912, 2579, 1699, 2133, 9461, 2225, 2011, 11052 백준님은 모르는 문제가 있으면 2시간을 넘기지 말라고 했는데, 나는 이런 기초 문제는 1시간을 넘길 필요가 없다고 생각한다. 흔히들 PS를 처음 접하는 사람들이 하는 큰 실수가 모르는 문제를 하루종일 붙들고 있는 건데, 우린 수학이란 과목을 정규과정만으로는 초등학교 6년, 중학교 3년, 고등학교 3년 동안 배웠다. 그런데 알고리즘에는 그만한 시간을 투자할 수 없다. 그러나 알고리즘의 양은 그만큼 방대하다. 그러니 수학문제 풀듯이 계속 붙들고 있는건 미련한 짓이다. 이건 마치 덧셈,곱셈 정도만 아는 상태에서 미적문제를 푸려는 시도와 같다고 생각한다. 앞서 말했다시피 모르는 사람은 뭘 모르는지 모르는게 문제다. 본인이 미적에 대한 개념이 있는지 조차 모르는데 그 문제를 백날 붙들고 있어봐야 풀릴까? 당연히 아니다... 일단 1시간 넘어가면 그 문제 풀 확률은 거의 없다고 봐도 된다. 그러니 바로바로 찾아봐라. 특히 이 문제들은 정말 기초 문제들이고 사람들이 많이 풀었기 때문에 네이버나 구글에 검색하면 자세한 설명과 코드가 넘쳐난다. 반드시 지키자! **1시간 넘어가면 풀던 짓을 그만두고 반드시 AC받은 코드 찾아보기 (설명이 꼭 달려있는 코드를 읽자)** 한 문제 가지고 며칠씩 씨름하고 풀어봐야 다음에 풀지도 못할뿐더러 아주 비효율적인 방법으로 푸는 경우도 있을 거다. 그러는 것 보다 이 문제의 답을 빨리 확인하고 이와 유사한 문제들을 여러개 풀어제끼는 것이 아주아주 현명한 방법임을 명심하자. 그리고 푼 다음에는 반드시 다른 사람의 코드를 봐야 한다. 특히 자신만의 가상의 스승을 잡고 그 분의 코드를 보는 것도 좋은 방법이라 생각한다. 너무 갓갓들은 이상한 방식으로도 짜는 경우도 있기 때문에 적당한 사람을 선택해야 한다. 그 사람의 코드를 보면 잘 이해가 되고, BOJ랭킹은 100위 안에 드는 사람이면 적당하다. 근데 처음부터 끝까지 하나하나 세밀하게 볼 필요는 없다. 로직 대충 비슷해보이면 스킵하고, 나랑 완전 다른 방법인데 참신하면 들여다보고 하는거지 뭐... 그 다음 이런 저런 문제들을 풀어보자. 2751, 11650, 11651, 10814, 10825, 10989, 11652, 11004, 10828, 9012, 10799, 10845, 10866, 10808, 10809, 10820, 2743, 11655, 10824, 11656, 1406, 1158, 1168, 10430, 2609, 1934, 1850, 9613, 11005, 2745, 1373, 1212, 2089, 11576, 1978, 1929, 6588, 11653, 10872, 1676, 2004 여기까지 다 풀고 나면 이제 재밌는 그래프 문제(bfs, dfs)를 풀어보자. 그래프 - 1260, 11724, 1707, 10451, 2331, 9466, 2667, 4963, 7576, 2178, 2146, 1991, 11725, 1167, 1967 코포(Codeforces) div2에서도 자주 등장하는 binary search 문제도 풀어보자. (여기엔 ternary search도 있다.) 이분탐색/삼분탐색 - 1654, 2805, 2110, 10815, 10816, 11662 분할정복도 풀어보자~ 분할정복은 DP랑 느낌이 비슷한데, 부분 문제를 dp테이블에 저장할 필요가 없는(cache질을 할 필요가 없음) 부분이 DP랑 다른 것 같다. 분할정복 - 11728, 1780, 11729, 1992, 2447, 2448, 1517, 2261 그리디 알고리즘은 매 순간 최선을 선택한다라는 말 때문에 매우 쉽게 들리지만, 매 순간의 선택이 최선이 되도록 방법을 정하는 것 자체가 매우 어렵다. 그리디 - 11047, 2875, 10610, 1783, 1931, 11399, 2873, 1744 그 다음은 완전탐색(exhaustive search)이다. 완전탐색은 '말하는 대로' 구현하는 문제다. 그냥 무식하게 구현하면 될 것 같지만, 여기서도 고수의 코드를 보면 그들의 멋진 computational thinking 방식을 느낄 수 있다. 처음에는 이런 문제도 어렵지만 나중에는 쉬워진다. 이걸 실수없이 빠른 시간안에 잘 짜야 쉬운 문제들을 척척 풀어나갈 수 있다. 완전탐색 - 1476, 1107, 1451, 9095, 10819, 10971, 1697, 1963, 9019, 1525, 2251, 2186, 3108, 5014, 1759, 2580, 1987, 6603, 1182, 2003, 1806, 1644, 1261, 1208, 7453, 2632, 2143 여기까지 푸는게 딱 4주 분량이다. (BOJ 문제 부분만) 여기까지 푸는데 4주를 안넘기는게 좋다고 생각한다. 왜냐면, PS를 하면서 느낀건데, 단기간에 몰아서 왕창 할 수록 얻는 양은 어마어마하게 달라지는 것 같다. 보통 그리디 문제 전까지 2주를 잡고 그리디랑 완탐부분을 2주 잡으면 될거다. (그리디랑 완탐 양이 꽤 많다. 저 문제 다 풀기 정말 힘들다ㅠ) 출처: [https://plzrun.tistory.com/entry/알고리즘-문제풀이PS-시작하기](https://plzrun.tistory.com/entry/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EB%AC%B8%EC%A0%9C%ED%92%80%EC%9D%B4PS-%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0) [plzrun's algorithm:티스토리]",
      "frontmatter": {
        "date": "2025-02-07T04:19:00+09:00",
        "lastmod": "2025-02-07T04:19:00+09:00"
      }
    },
    "two pointer 알고리즘": {
      "path": "/leetcode75/two-pointer-알고리즘/",
      "filename": "two pointer 알고리즘",
      "content": "two pointer 알고리즘 1차원 배열에서 각자 다른 원소를 가리키는 2개의 포인터를 사용하여 목표값을 구한다 $O(n^2)$ => $O(n)$ 해결하는 기법 이분탐색 vs 투포인터 문제 예시 주어진 정수 배열 height의 길이를 n이라고 할 때, n개의 수직 선이 그려져 있습니다. i번째 선의 두 끝점은 (i, 0)과 (i, height[i])입니다. 두 개의 선을 선택하여 x축과 함께 컨테이너를 형성했을 때, 이 컨테이너가 최대한 많은 물을 저장할 수 있도록 두 선을 찾는 문제입니다. 최대 물의 양을 반환하세요. 컨테이너를 기울일 수 없다는 점에 유의하세요. 예시 1: 입력: height = [1,8,6,2,5,4,8,3,7] 출력: 49 설명: 위의 수직 선들은 배열 [1,8,6,2,5,4,8,3,7]로 표현됩니다. 이 경우, 물이 저장될 수 있는 최대 면적은 49입니다. 예시 2: 입력: height = [1,1] 출력: 1 제약 조건: n == height.length 2 <= n <= 10^5 0 <= height[i] <= 10^4 이중 반복문 $O(n^2)$ => $O(n)$ 반복 1회 int maxArea(vector<int>& height) { int ret = 0; for(int i = 0 ; i< height.size()-1 ; i++){ for(int j = i + 1; j < height.size() ; j++){ int width = j - i; int length = min(height[i], height[j]); int area = width * length; if(ret < area) ret = area; } } return ret; }",
      "frontmatter": {
        "aliases": [
          "투 포인터"
        ],
        "tags": [
          "algorithm"
        ],
        "date": "2024-12-31T13:09:00+09:00",
        "lastmod": "2024-12-31T13:09:00+09:00"
      }
    },
    "슬라이딩 윈도우": {
      "path": "/leetcode75/슬라이딩-윈도우/",
      "filename": "슬라이딩 윈도우",
      "content": "매순간 특정 범위의 계산을 다시 할 필요가 없음(ex 합) database 의 SUM 과 같은 함수 쿼리 최적화 시에 작동하는 것과 비슷하다 More Similar Sliding Window Problems Max Consecutive Ones III Longest Subarray of 1's After Deleting One Element Count Number of Nice Subarrays-atMost(K-1)) Replace the Substring for Balanced String Max Consecutive Ones III Binary Subarrays With Sum Subarrays with K Different Integers-atMost(K-1)) Fruit Into Baskets Shortest Subarray with Sum at Least K-Using-Deque) Minimum Size Subarray Sum",
      "frontmatter": {
        "aliases": [
          "sliding window"
        ],
        "tags": [
          "algorithm"
        ],
        "date": "2025-01-07T17:35:00+09:00",
        "lastmod": "2025-01-07T17:35:00+09:00"
      }
    },
    "05.메모리 관리": {
      "path": "/실습으로-배우는-리눅스-구조/05.메모리-관리/",
      "filename": "05.메모리 관리",
      "content": "free %20image%2020240919163385.png) Pasted image 20240921030745 강제로 프로세스를 종료하여 메모리를 확보하는 OOM-Killer 라는 기능이 있다 서버에는 위의 일이 일어나지 않도록 sysctl 의 vm.paniconoom 파라미터의 기본값을 변경하여 메모리 부족시 시스템을 종료하게 하는 방법이 있다",
      "frontmatter": {
        "series": "실습으로 배우는 리눅스 구조",
        "series_weight": "5",
        "date": "2024-09-19T15:03:00+09:00",
        "lastmod": "2025-09-05T21:00:05+09:00"
      }
    },
    "리눅스 시스템 기본": {
      "path": "/실습으로-배우는-리눅스-구조/리눅스-시스템-기본/",
      "filename": "리눅스 시스템 기본",
      "content": "운영 체제 레벨의 패키지 관리자로는 아래와 같은 것들이 있습니다: dnf : Fedora 리눅스에서 사용되는 패키지 관리자로, yum의 최신 버전입니다. pacman : Arch 리눅스에서 사용되는 패키지 관리자입니다. zypper : openSUSE에서 사용되는 패키지 관리자입니다. apt yum brew 프로그래밍 언어 레벨의 패키지 관리자로는 아래와 같은 것들이 있습니다: gem : Ruby 언어의 패키지 관리자입니다. composer : PHP 언어의 패키지 관리자입니다. nuget : .NET 언어의 패키지 관리자입니다. maven 과 gradle : Java 언어의 패키지 관리자입니다. pip, cargo, npm 시스템 콜의 종류 ﻿﻿프로세스 생성, 삭제 ﻿﻿메모리 확보, 해제 ﻿﻿프로세스 간 통신(IPC) ﻿﻿네트워크 ﻿﻿파일시스템 다루기 ﻿﻿파일 다루기(디바이스 접근) strace : 시스템 호출 목록 확인 os 제공 ﻿﻿시스템 초기화 : init ﻿﻿OS의 동작을 바꿈 : sysctl, nice, Sync ﻿﻿파일 관련 : touch.mkdir ﻿﻿텍스트 데이터 가공 : grep, sort, uniq ﻿﻿성능 측정 : sar, iostat ﻿﻿컴파일러 : gcC ﻿﻿스크립트 언어 실행 환경 : perl, python, ruby ﻿﻿셸 : bash ﻿﻿윈도우 시스템 : X",
      "frontmatter": {
        "tags": [
          "operating-system",
          "system-programing"
        ],
        "series": "실습으로 배우는 리눅스 구조",
        "date": "2024-03-04T17:29:00+09:00",
        "lastmod": "2024-03-04T17:29:00+09:00"
      }
    },
    "프로그램 메모리 구조 파악하기": {
      "path": "/실습으로-배우는-리눅스-구조/프로그램-메모리-구조-파악하기/",
      "filename": "프로그램 메모리 구조 파악하기",
      "content": "프로그램의 실행파일로 프로세스를 생성해 프로그램을 돌리는 과정에서 각 과정에서 메모리 구조를 확인할 수 있다 실행파일 먼저 실행파일이다 실행 파일은 운영체제별 포메멧 별로 다르게 사용되는데 이때 code section 과 data section 은 동일하다 이를 확인하기 위한 방법이 size 명령어이다 하지만 실행 파일의 헤더 및 기타 섹션을 확인하기 위한 방법은 운영 체제와 해당 실행 파일의 포맷(예: ELF, PE, Mach-O 등)에 따라 달라진다 리눅스, 유닉스 계열 시스템에서는 주로 ELF(Executable and Linkable Format) 포맷이 사용되며, Windows에서는 PE(Portable Executable) 포맷, macOS에서는 Mach-O 포맷이 사용된다 ELF 포맷 (리눅스, 유닉스 계열) readelf: ELF 포맷의 실행 파일, 오브젝트 파일, 공유 라이브러리 등의 정보를 표시하는 프로그램입니다. 헤더, 섹션 헤더, 프로그램 헤더, 심볼 테이블 등 다양한 정보를 확인할 수 있습니다. 예시 명령어: readelf -h [파일명] (ELF 파일의 헤더 정보 표시) 예시 명령어: readelf -S [파일명] (섹션 헤더 정보 표시) objdump: 오브젝트 파일, 실행 파일, 공유 라이브러리의 정보를 보여주는 프로그램입니다. readelf 보다 더 다양한 정보를 제공할 수 있으며, 디스어셈블리 결과도 확인할 수 있습니다. 예시 명령어: objdump -h [파일명] (섹션 헤더 정보 표시) 예시 명령어: objdump -D [파일명] (전체 디스어셈블리) PE 포맷 (Windows) dumpbin: Microsoft Visual Studio에 포함된 도구로, PE 포맷의 실행 파일, 오브젝트 파일, DLL 등의 정보를 표시합니다. 헤더, 섹션, 익스포트, 임포트 정보 등을 확인할 수 있습니다. 예시 명령어: dumpbin /headers [파일명] (헤더 정보 표시) 예시 명령어: dumpbin /sections [파일명] (섹션 정보 표시) PEview: 그래픽 사용자 인터페이스(GUI)를 제공하는 도구로, PE 파일의 헤더 및 섹션 정보를 쉽게 검토할 수 있습니다. Mach-O 포맷 (macOS) otool: macOS에 포함된 도구로, Mach-O 파일의 정보를 표시합니다. 헤더, 로드 명령, 섹션 정보 등을 확인할 수 있습니다. 예시 명령어: otool -h [파일명] (Mach-O 파일의 헤더 정보 표시) 예시 명령어: otool -l [파일명] (로드 명령 및 섹션 정보 표시) 프로세스 실행 파일의 code 섹션, data 섹션 크기는 정적으로 미리 정해져 있다 이를 확인하는 것은 size 명령어를 사용하여 정적으로 분석할 수 있습니다. 하지만 프로세스의 코드, 데이터, 힙, 스택 영역을 실시간으로 확인하려면 다른 접근 방법이 필요합니다. 여기서는 리눅스 기반 시스템에서 이를 확인하는 방법을 중심으로 설명하겠습니다. /proc 파일 시스템 사용하기: 리눅스에서는 실행 중인 프로세스의 정보를 /proc 파일 시스템을 통해 접근할 수 있습니다. 각 프로세스는 /proc/[pid] 디렉토리에 해당하며, 여기에서는 메모리 맵, 스택, 환경 변수 등 다양한 정보를 확인할 수 있습니다. /proc/[pid]/maps: 이 파일은 프로세스의 메모리 맵을 보여줍니다. 코드, 데이터, 힙, 스택 영역의 주소 범위와 권한을 확인할 수 있습니다. /proc/[pid]/stat: 프로세스의 상태, 메모리 사용량 등 다양한 통계 정보를 제공합니다. 사용 예: cat /proc/[pid]/maps # 메모리 맵 확인 cat /proc/[pid]/stat # 프로세스 상태 확인 시스템 모니터링 도구 사용하기: top , htop , ps , pmap 등 다양한 시스템 모니터링 도구를 사용하여 프로세스의 메모리 사용량과 상태를 실시간으로 확인할 수 있습니다. top 또는 htop : 시스템에서 실행 중인 프로세스의 목록을 실시간으로 보여주며, CPU와 메모리 사용량을 확인할 수 있습니다. ps : 프로세스의 스냅샷을 보여줍니다. 특정 프로세스의 정보를 확인할 때 사용합니다. pmap [pid] : 특정 프로세스의 메모리 맵을 보여줍니다. 코드, 데이터, 힙, 스택 영역의 메모리 사용량을 확인할 수 있습니다. 디버깅 도구 사용하기: gdb 같은 디버거를 사용하여 프로세스를 분석할 수도 있습니다. 프로세스를 디버깅 세션에 연결하면, 코드, 데이터, 힙, 스택 영역의 상세 정보를 실시간으로 확인하고 분석할 수 있습니다. 사용 예: gdb -p [pid] # 실행 중인 프로세스에 대한 디버깅 세션 시작 디버깅 세션 내에서는 다양한 명령어를 사용하여 메모리의 내용을 확인하고 분석할 수 있습니다. %20image%2020240328041330.png)",
      "frontmatter": {
        "aliases": [
          "size",
          "readelf",
          "objdump",
          "dumpbin",
          "PEview",
          "otool",
          "pmap"
        ],
        "tags": [
          "operating-system"
        ],
        "date": "2024-03-28T04:08:00+09:00",
        "lastmod": "2025-10-01T19:15:19+09:00"
      }
    },
    "프로세스 생성": {
      "path": "/실습으로-배우는-리눅스-구조/프로세스-생성/",
      "filename": "프로세스 생성",
      "content": "프로세스는 두가지 목적으로 생성한다 fork() : 다중 프로세스 프로그램 사용 => 현재 프로세스의 메모리 복사 다른 프로세스로 등록 execve() : 다른 프로그램 실행용( bash 에서 실행할 때 ) => 현재 프로세스의 메모리를 새로운 프로세스의 메모리로 덮어 쓴다 프로세스가 운영체제에게 부여받은 메모리 구조 실행파일 실행파일은 리눅스에서는 ELF (excutable linkable format) 형식이며 \breadelf -h {실행파일 명} readelf -S {실행파일 명} 으로 사용 가능하다",
      "frontmatter": {
        "tags": [
          "operating-system"
        ],
        "series": "실습으로 배우는 리눅스 구조",
        "date": "2024-03-04T14:00:00+09:00",
        "lastmod": "2025-06-03T06:11:53+09:00"
      }
    },
    "university architecture": {
      "path": "/06.university/university-architecture/",
      "filename": "university architecture",
      "content": "PC : 다음에 인출될 명령어의 주소 AC : 임시저장 IR : 최근에 인출된 명령어 MAR : 기억장치 접근 주소 MBR : 기억장치 접근 데이터 SP : 스텍 최상위 주소 2장 사이클 인출 사이클 MAR <- PC MBR <- M[MAR], PC <- PC + 1 IR <- MBR 실행 사이클 load 인터럽트 사이클 MBR <- PC MAR <- SP, PC <- ISR 의 시작 주소 M[MAR] <- MBR , SP <- SP-1 간접 사이클 MAR <- IR(addr) MBR <- M[MAR] IR(addr) <- MBR Sub call 인터럽트 사이클과 비슷 CALL X 명령시 MBR <- PC MAR <- SP, PC <- X M[MAR] <- MBR, SP <- SP - 1 RET 명령시 SP <- SP + 1 MAR <- SP PC <- MBR <- M[MAR] 4장 제어 유니트 명령어 해독기 inscruction decoder CAR 제어 주소 레지스터 : 다음에 실행할 마이크로 명령어의 주소를 저장하는 레지스터 제어 기억장치(control memory) : 마이크로명령어들로 이루어진 마이크로프로그램을 저장하는 내부 기억 장치 CBR 제어 버퍼 레지스터 : 제어 기억장치로 부터 읽혀진 마이크로 명령어를 일시적으로 저장하는 레지스터 SBR 서브 루틴 레지스터 : 마이크로프로그램에서 서브 루틴이 호출되는 경우에 현재의 CAR 을 일시적으로 저장하는 레지스터 순서제어 모듈(sequencing module) : 마이크로명령어의 실행 순서를 결정하는 회로들의 집합 인출 사이클 마이크로 프로그램 ORG 4 INDRT : IRTAR U JMP NEXT ; MAR <- IR(addr), ; 다음 마이크로명령어 실행 READ U JMP NEXT ; MBR <- M[MAR], ; 다음 마이크로 명령어 실행 BRTIR U RET ; IR(addr) <- MBR, ; 실행 사이클 루틴으로 복귀 MESI 수정 (M : Modified) 상태 : 데이터가 수정(변경) 된 상태 베타 (E : Exclusive) 상태 : 유일한 복사본이고, 주기억장치의 내용과 동일한 상태 공유 (S : Shared) 상태 : 데이터가 두 개 이상의 프로세서 캐시에 적재되어 있는 상태 무효 (I : Invalid) 상태 : 데이터가 다른 프로세서에 의해 수정되어 무효가 된 상태 초기 cache miss 후 캐쉬로 등록 ( - -> E ) 수정 (메인 메모리 반영 x) ( E -> M ) 다른 코어가 값을 읽는 경우 ( E -> S , M->S, - -> S , -> S ) .. 캐시간 전송이 가능할 수도 못할 수도 있음 못하면 메인메모리에 update 후 동기화 수정( 메인 메모리 반영 x ) ( S->M ) 수정( 메인 메모리 반영 x ) ( M->M ) 다른 코어가 M 인상태에서 수정 ( M->I, I->M )",
      "frontmatter": {
        "tags": [
          "university"
        ],
        "date": "2025-04-17T03:26:00+09:00",
        "lastmod": "2025-06-08T19:19:34+09:00"
      }
    },
    "university machine leaning": {
      "path": "/06.university/university-machine-leaning/",
      "filename": "university machine leaning",
      "content": "Pasted image 20250314104746 Pasted image 20250314104833 T task : 해야할 작업 E experience : 학습 P performance : 작업 성능 머신러닝 분류 기준1: 사람의 지도/감독 여부 (학습하는 동안 감독의 형태나 주어지는 정보량에 따른 분류) 지도(Supervised) : 레이블(타겟) 이 포함 분류(classification), 회귀(regression) 정답이 포함되어 있는가 비지도(unsupervised) : : 레이블이 없는 훈련 데이터 : 군집(Clustering), 시각화(Visualization), 차원 축소(Dimensionality Reduction), 이상 탐지(Anomaly Detection), 이상 탐지(Anomaly Detection), 연관 규칙 학습(Association Rule Learning) 준지도(semi-supervised) : 지도 + 비지도 강화(reinforcement)학습 : 학습에 대한 피드백 제공 기준2 : 실시간으로 주어지는 데이터에 대한 점진적인 학습 가능 여부 온라인 학습 vs 배치 학습 기준3 : 훈련을 통해 알고 있는 데이터 포인트와 새로운 데이터 포인트를 비교하는 방식인지, 아니면 훈련 데이터셋에서 패턴을 발견하고 그에 기반한 예측 모델을 만드는 것인지 사례 기반 학습 vs 모델 기반 학습 세부 분류 회귀 : 중고자 가격 예측 군집 : 비슷한 특징을 가진 그룹으로 나누는 것 (분류과 다른점은 레이블이 없다) : k-means, DBSCAN, 계층 군집 분석 시각화 : 레이블이 없는 고차원(여러 특성(feature)들로 이루어진) 데이터를 분석하여 도식화가 가능한 2D 또는 3D 표현을 만들어줌 차원 축소 : original information의 손실을 최소화 하면서 데이터의 차원(특성)을 줄이기 ex) 상관관계가 높은 자동차의 주행거리과 연식을 “마모 정도”라는 하나의 특성으로 합침 이상 탐지 : 정상 샘플들을 이용하여 머신러닝 모델 훈련 후 주어진 새로운 샘플의 정상 여부를 판단 특이치 탐지(Novelty Detection) : 전혀 오염되지 않은 clean 훈련 데이터 학습 후 학습된 훈련 데이터와 달라 보이는 데이터 탐지 연관 규칙 학습(Association Rule Learning) : 특성 간 흥미로운 관계 찾기 마트 판매 기록 데이터에 대해 연관 규칙 학습 적용 예) 바비큐 소스와 감자를 구매한 고객이 스테이크도 구매하는 경향을 찾아냄 군집과 vs 시각화의 다른점 이상탐지 vs 특이치 탐지 특이치 탐지 는 \"강아지 vs. 고양이\"처럼 명확한 범주 분류 이상 탐지 는 \"대부분의 강아지와 다른 특이한 사례\" numpy Axis(축): NumPy에서 각 차원을 축이라고 부릅니다. Rank(랭크): 배열의 축 개수입니다. 예를 들어, 3x4 행렬은 랭크가 2입니다. Shape(형태): 배열의 각 축 길이를 나타내는 튜플입니다. 예: (3, 4) Size(크기): 배열의 총 요소 수입니다. 예: 3x4 행렬의 경우 size=12 . boolean indexing m < 25 # equivalent to m < [25, 25, 25, 25] array([ True, True, False, False]) Broadcasting의 세 가지 주요 규칙 규칙 1: 배열의 차원(rank)을 맞추기 위해 작은 차원의 배열에 1을 추가 규칙 2: 크기가 1인 차원은 필요한 크기만큼 확장 규칙 3: 최종적으로 모든 차원의 크기가 일치해야 함 dataframe 여러가지 방법으로 생성이 가능하다 python dict + pandasseries {\"Mango\":series1, \"Apple\":series2, \"Banana\":series3} 리스트를 값으로 갖는 dict dict2 = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada', 'NY', 'NY', 'NY'], 'year': [2000, 2001, 2002, 2001, 2002, 2003, 2002, 2003, 2004], 'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2, 8.3, 8.4, 8.5]} 중첩 dict dict3 = {'Nevada': {2001: 2.4, 2002: 2.9}, 'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}",
      "frontmatter": {
        "tags": [
          "machine_leaning",
          "university"
        ],
        "date": "2025-03-10T15:00:00+09:00",
        "lastmod": "2025-06-03T06:40:37+09:00"
      }
    },
    "고전으로 읽는 인문학 책(기말)": {
      "path": "/06.university/고전으로-읽는-인문학-책기말/",
      "filename": "고전으로 읽는 인문학 책(기말)",
      "content": "Pasted image 20250113210870 Pasted image 20250113210895 Pasted image 20250113211860 시가 보통 1인칭의 고백 형식이라면, 는 2인칭의 부름의 시 형식이다. 네 개의 기둥으로 이루어진 시 마돈나 밤 침실 오라 Pasted image 20250114034102 Pasted image 20250114034132 Pasted image 20250114034139 Pasted image 20250114034114 Pasted image 20250114035150 Pasted image 20250114035107 Pasted image 20250114035533 이 시 속에는 두 개의 서로 다른 시선이 존재한다. 내가 인생의 그 사람을 만난 것은 우연인 가, 운명인가? 이 시의 ‘저녁’은 그 비밀을 담고 있다 Pasted image 20250114035865 Pasted image 20250114035882 Pasted image 20250114035876 Pasted image 20250114035889 Pasted image 20250114035802 천재시인 이상 뛰어난 건축기사이자 화가였다. 그의 여러 시에는 건축 설계도와 같은, 시 의 통념을 뒤엎는 양식의 일탈성이 자주 보읶 다. 숫자‘13’을 단순한 알레고리(allegory)로 해석하려 하면 를 제대로 감상할 수 없다.이상의 시로부터 핚국의 문학은 ‘현대문학’ 의 길로 들어섰다. 어떠한 점에서 그러한가? Pasted image 20250114040463Pasted image 20250114040787 Pasted image 20250114040614 Pasted image 20250114040616 Pasted image 20250114040609 Pasted image 20250114040610 Pasted image 20250114040649 Pasted image 20250114040697 Pasted image 20250114040648 Pasted image 20250114040709 Pasted image 20250114040701 시인 이상이 내면탐구의 새로운 형식을 추구함으로써 한국 현대문학의 새로운 장을 열었듯이, 다른 분야(음악, 미술, 영화, 뮤지컬, 웹툰, 게임, K-Pop, 건축, 바둑, 스포츠, 의학, 공학, 여러분의 전공분야 등)에서 새로운 형식으로 독보적인 자신만의 세계를 구축한 인물을 찾아 그 매력을 이야기해 보십시오. (예: 애플의 창립자이자 IT계 혁신의 아이콘 스티브 잡스) Pasted image 20250114042821 Pasted image 20250114042994Pasted image 20250114043496 ‘죽어도 한은 없다’라는 말은 어떤 의미입니 까? 언제 사용하는 말입니까? 원(怨)과 한(恨)은 어떻게 같고, 다릅니까? Pasted image 20250114045676Pasted image 20250114045680 Pasted image 20250114045752Pasted image 20250114045799Pasted image 20250114045773 Pasted image 20250115075494Pasted image 20250115075475 Pasted image 20250115075496 이 시에서 시인은 ‘님’에 대한 정의를 어떻 게 내리고 있는가 님’이라는 한국말의 원형적 의미는 무엇인 가? Pasted image 20250115075748 Pasted image 20250115075781 Pasted image 20250115075790 Pasted image 20250115075736 Pasted image 20250115075847 Pasted image 20250115075802 Pasted image 20250115075800 Pasted image 20250115080133 Pasted image 20250115080250 Pasted image 20250115080263 여러분이 생각하는 뱀의 이미지는? Pasted image 20250115080208 Pasted image 20250115080300 Pasted image 20250115080350 Pasted image 20250115080340 Pasted image 20250115080384 Pasted image 20250115080384-1 Pasted image 20250115080308 Pasted image 20250115080329 Pasted image 20250115080419 Pasted image 20250115080410 Pasted image 20250115080463 Pasted image 20250115080407 Pasted image 20250115080459 Pasted image 20250115080452 Pasted image 20250115080409 Pasted image 20250115080596 Pasted image 20250115080500 Pasted image 20250115080564 Pasted image 20250115080523",
      "frontmatter": {}
    },
    "고전으로 읽는 인문학 책(중간)": {
      "path": "/06.university/고전으로-읽는-인문학-책중간/",
      "filename": "고전으로 읽는 인문학 책(중간)",
      "content": "Pasted image 20250102152137 Pasted image 20250102152143 Pasted image 20250102152145 Pasted image 20250102152151 Pasted image 20250102152158 Pasted image 20250102152167 엄마야 누나야 젠더 공간 호격 야 부재하는 공간, 연령, 부름 Pasted image 20250102152183 Pasted image 20250102152193 Pasted image 20250102152102 Pasted image 20250102152114 Pasted image 20250102152121 Pasted image 20250102152130 Pasted image 20250102152150 Pasted image 20250102152162 Pasted image 20250102152180 Pasted image 20250102152122 Pasted image 20250102152194 Pasted image 20250102152108 Pasted image 20250102152111 Pasted image 20250102173512 Pasted image 20250102173561 Pasted image 20250102173643 김동환 웃은 죄 행위 코드 => 요구하다 들어주다, 인물 코드 => '마을사람/바깥사람', '정착자/여행자' 대립 해석적 코드 => 규방 처녀가 길가던 나그네와 사랑을 하게 됨 상징적 코드 => 남녀유별의 도덕적 파계, 사회적 관습에서 일탈된 한국적 애정 문화적 코드 샘터의 문화풍속상 젠더(여성)공간 평양성에 해가 뜨지 않는다 => 천변과 관련시킨 일종의 속담",
      "frontmatter": {}
    },
    "소프트웨어 공학 시험문제": {
      "path": "/06.university/소프트웨어-공학-시험문제/",
      "filename": "소프트웨어 공학 시험문제",
      "content": "총 10문제가 나온다 소프트웨어 공학 구성요소 4가지 (상세내용x) 폭포수 모델의 장점과 단점 에자일 모델-스크럼의 장점과 단점 UML 유스케이스 다이어그램 - 구성요소 엑터, 유스케이스, 관계 3가지 (설명x) UML 클래스 다이어그램 - 클래스 다이어그램의 관계 연관관계 합성연관관계 등등 관계들 프로젝트 계획단계 개발비 산정에서 간이 기능점수 산정방법 객체지향 단계에서오버로딩과 오버라이딩에 대한 차이점 설명 응집도과 결합도에 대해 차이점과 설명 ms 아키텍처의 장단점 테스트 단계에서 확인테스트와 검증 테스트 동적 테스트를 각각의 테스트 영세기반 ,화이트, 블랙박스 등등 각각에 대해 알기 cmi 5개 단계 설명 42p?? 소프트웨어 공학 구성요소 4가지 공학적 접근 표준화 자동화 도구 활용 품질 보증 체제 폭포수 모델의 장점과 단점 장점 관리가 용이 체계적으로 문서화할 수 있음 요구사항의 변화가 적은 프로젝트에 적합 단점 각 단계는 앞 단계가 완료되어야 수행할 수 있음(순서가 있음) 각 단계마다 작성된 결과물이 완벽한 수준으로 작성되어야 다음 단계에 오류 를 넘겨주지 않음(앞 순서가 매우 중요함) 사용자가 중간에 가시적인 결과를 볼 수 없어 답답해할 수 있음( 빠른 결과물이 없음 ) 에자일 모델-스크럼의 장점과 단점 장점 반복 주기마다 생산되는 실행 가능한 제품을 통해 사용자와 충분히 의견을 나 눌 수 있음 일일 회의를 함으로써 팀원들 간에 신속한 협조와 조율이 가능 일일 회의 시 직접 자신의 일정을 발표함으로써 업무에 집중할 수 있는 환경이 조성 다른 개발 방법론에 비해 단순하고 실천 지향적 스크럼 마스터는 개발 팀원들이 목표 달성에 집중할 수 있도록 팀의 문제를 해 결 프로젝트의 진행 현황을 볼 수 있어 신속하게 목표와 결과 추정이 가능 프로젝트의 진행 현황을 볼 수 있어 목표에 맞게 변화를 시도할 수 있음 단점 반복 주기가 끝날 때마다 실행 가능하거나 테스트할 수 있는 제품을 만들어야 하는데 이 작업이 많아지면 그만큼의 작업 시간이 더 필요 일일 스크럼 회의 시간(15분)이 넘어가면 작업시간이 늦어지고 작업하는데 방해 받을 수 있음 투입 공수를 측정하지 않기 때문에 작업이 얼마나 효율적으로 수행되었는지 알기 어려움 프로세스 품질을 평가하지 않기 때문에 품질 관련 활동이 미약하고 따라서 품질의 정 도를 알 수 없음 UML 종류 유스케이스, 클래스, 순차, 활동, 컴포던트, 배치 UML 유스케이스 다이어그램 - 구성요소 엑터, 유스케이스, 관계 3가지 (설명x) 엑터, 유스케이스, 관계 UML 클래스 다이어그램 - 클래스 다이어그램의 관계 연관관계 합성연관관계 등등 관계들 연관관계 : 두 객체가 서로 참조할수 있는 관계 직접 연관 관계 : 한쪽 객체만 다른 객체를 참조할 수 있는 관계 일반화 관계 : 상속을 의미 합성 연관 관계 : 범위가 큰 클래스가 사라지면 그 컴포턴트들도 사라지는 관계 프로젝트 계획단계 개발비 산정에서 간이 기능점수 산정방법 사용자 관점의 기능만으로 소프트웨어 개발 비용 산정에 활용 => 모를때(초기에) 는 평균복잡도 가중치를 사용하여 측정한다 데이터 기능(내부 논리 파일, 외부 연계 파일)과 트랜잭션 기능(외부입력, 외부조회, 외부출력) 이 있다 기능점수산정법(정규기능, 간이 기능) 오버로딩 : 중복정의 : 언어별로 지원하는 범위가 다르지만 cpp 의 경우 동일한 메서드 이름임에도 다른 매개변수 개수나 자료형으로 구분할 수 있다 심지어 연산자 또한 오버로딩이 가능하다 오버라이딩 : 재정의 : 상속관계의 클래스 관계에서 상위 클래스에서 정의한 메서드를 무시하고 하위 클래스에서 동일한 이름으로 다시 재정의해서 사용하는 것이다 응집도과 결합도에 대해 차이점과 설명 응집도 : 응집도는 모듈 내부의 구성요소들이 얼마나 밀접하게 관련되어 있는지를 나타내는 지표 결합도 : 결합도는 모듈 사이의 의존성을 나타내는 지표 즉 모듈 내부 지표 = 응집도, 모듈 외부 지표 = 결합도 %20image%2020240611180633.png) Pasted image 20240611180772 ms 아키텍처의 장단점 MSA 아키텍처 • 여러 개의 작은 서비스로 구성되어 각 서비스가 독립적으로 개발되고 배포되는 구조 • MSA로 구성되어 있는 애플리케이션의 경우 전체 시트템이 분산되어 있어 개발, 배포가 독립적으로 가능하며 확장성과 유지관리가 용이함. • 모놀리식 아키텍처 (Monolithic Architecture) : 전통적인 개발 방식으로 하나의 프로젝 트에 모든 기능을 함께 포함 • MSA의 장점 • 서비스 간 독립성으로 인해 확장성과 유연성이 높아진다. • 기능 고립성이라는 특징 때문에 일부 서비스가 실패하더라도 전체 시스템에 큰 영향을 미치지 않는다. • MSA의 단점 • 서비스 간 통신이 필요하며, 서로 간 연결 구축 및 관리의 복잡성이 증가한다. • 초기 개발 및 통신 등에 시간이 소요된다 테스트 단계에서 확인테스트와 검증 테스트 시각에 따른 테스트 확인 테스트 : 개발자 기준에서 의도된 동작이 올바르게 동작하는가(개발 모든 단계) 검증 테스트 : 고객 기준에서 의도된 동작이 올바르게 동작하는가 목적: 확인 테스트: 소프트웨어가 설계 및 명세서 요구 사항을 충족하는지 확인 검증 테스트: 소프트웨어가 사용자와 이해관계자의 요구 사항을 실제로 충족하는지 확인 시점: 확인 테스트: 개발 과정 전반에 걸쳐 지속적으로 수행 검증 테스트: 주로 개발 완료 후, 배포 전에 수행 방법: 확인 테스트: 코드 리뷰, 워크스루, 인스펙션, 정적 분석 등 검증 테스트: 기능 테스트, 시스템 테스트, 사용자 수용 테스트(UAT), 베타 테스트 등 중점: 확인 테스트: 소프트웨어의 내부 품질 검증 테스트: 소프트웨어의 외부 품질 동적 테스트를 각각의 테스트 영세기반 ,화이트, 블랙박스 등등 각각에 대해 알기 명세 기반 테스트 = 블랙박스 테스트 입력 과 출력만의 비료로 신텍스 기법 = 문법을 기준으로 옳은가 분할 기법 = 해당 영역에 들어가는가 경계값 분석 = 경계값에서도 잘 동작하는가 원인 결과 그래프 구현기반 테스트 = 화이트 테스트 코드수준의 검증 문장 검증 기준 분기 검증 기준 = 조건문에서 TF 모두 추적 • 조건 검증 기준 = 큰 조건을 분리하여 작게 각각 추적 • 분기/조건 검증 기준 • 다중 조건 검증 기준 • 기본 경로 테스트 cmi 5개 단계 설명 42p?? 성숙도 수준 초기단계 : 프로세스 없음 예측 불가능 관리단계 : 규칙화된 프로세스 정의단계 : 표준화된 프로세스 정량적 관리 단계 : 예측가능한 프로세스 최적화 단계 : 지속적 개선 프로세스",
      "frontmatter": {}
    },
    "시스템 분석 설계 시험문제": {
      "path": "/06.university/시스템-분석-설계-시험문제/",
      "filename": "시스템 분석 설계 시험문제",
      "content": "총 10문제가 나온다 모두 주관식이다 암기과목 설명하는 문제 5개(주절주절) ,단답형 5개(쓰시오) 2장?? 소프트웨어 라이프사이클 5단계 설명이 가능하야 한다 비기능 항목 5가지 특성 pdf 23p?? 3-6장 dfd 자료 흐름도 4가지 구성요소를 설명하여라 자료사전을 작성법 적는 문제 승차권 도서시스템 수강신청 3가지중 하나 소단위 명세서 안나옴 의사결정표를 사용하는 경우는 무었인가 소프트웨어 설계 소프트웨어 설계는 무었무었이 있어야 하는가 5가지 코드의 기본기능, 3대기능, 부가기능 입출력 단계는 안냄 9장?? 파일의 레코드 형식 4가지 프로세스를 모듈화를 하는 이유가 무었인가 표준 데이터 프로세스 유형 9가지 프로그램 설계원칙 프로그래밍의 표준화 장에 처리의 설계 위의 두가지를 설명할수 있어야 한다 소프트웨어 라이프사이클 5단계 설명이 가능하여야 한다 !!! 요구 분석 : 고객의 요구를 분석하여 sw 로 실현해야 할 내용을 요구 분석서라는 결과물로 정리 설계 : 요구분석에서 정의한 내용에서 구현할 사항을 sw 설계서로 결정하는 작업 구현 : 설계 사양에 따라 코딩하여 프로그램을 만드는 작업 테스트 : 개발한 sw 가 설계대로 제작되었는지를 확인하는 작업 유지보수 : 시스템을 운영하면서 발생하는 문제점을 보완하며 기능 개선하는 작업 비기능 항목 5가지 특성 요구사항 분석서 = 요구사항 명세서 에는 기능요구뿐만 아니라 비기능 요구가 있다 상호 운영성, 보안성 : 필요한 다른 시스템과 연결 가능한가, 적절한 보안책이 마련되어 있는가 신뢰성 : 잠재적인 문제를 해결하는 능력, 고장시 데이터를 복구하는 있는가 사용성 : 사용자가 사용법을 이해하기 쉽게 하거나 표준 사용법에 따르고 있는가 효율성 : 적절한 응답시간으로 반응하거나, 적절한 메모리를 사용하고 있는가 유지보수성 : 결함 진단 및 문제 부분을 식별할 수 있는가, 필요한 수정이 추가되었는가 자료흐름도 자료사전 소단위명세서(의사결정표가 내부에 존재) dfd 자료 흐름도 4가지 구성요소를 설명하여라 !!! 처리 : 입력되는 자료흐름을 출력되는 자료흐름으로 변환하는 것 처리가 수행되는 일(처리), 처리를 수행하는 행위자(행위자) 자료흐름 : 자료흐름도에서 구성 요소들 간의 접속 관계를 나타냄 자료저장소 : 머물고 있는 자료군의 집합 단말 : 자료 흐름도를 이해할 수 있게 사각형 단말 시스템 순수 입력, 순수 출력 자료사전을 작성법 적는 문제 승차권 도서시스템 수강신청 하나씩 그려보자 승차권 승차권 = 승차권번호 + 목적지 + 요금 + 출발일정 + 유효기간 + 고속회사 승차권번호 = 승차권의 번호 기입 목적지 = 출발지에서 목적지까지의 거리표시 + \\[“고속” “우등고속” \\] 요금 = 요금표시 + 부가가치세 포함 출발일정 = 출발일 + 시간 + 좌석 + 고속 유효기간 = 당일 지정차에 한함 고속회사 = 고속회사 이름과 전화번호 기재 도서시스템 도서대출 신청서 = 청구번호 + 저자 + 서명 + 대출자 인적사항 + 대출일 대출자 인적사항 = 성명 + 소속 + 학번 + 보증번호 소속 = \\[\"주\" \"야\"\\] + 학과 + 학년 보증번호 = 대출자 개개인의 보증카드 고유번호 (대출희망, 구입희망)도서 정보 = 청구번호 + 저자 + 출판사 이용자 정보 = 성명 + 소속 + 학번 + 학년 소속 = \\[\"주\" \"야\"\\] + 학과 + 학년 대출 현황 = 청구번호 + 대출자 인적사항 대출자 인적사항 = 성명 + 소속 + 학번 + 보증번호 소속 = \\[\"주\" \"야\"\\] + 학과 + 학년 보증번호 = 대출자 개개인의 보증카드 고유번호 ... 수강신청 수강신청서 = 학년도 + 학기 + 수강신청자 인적사항 + {수강신청 과목}12 + 신청학점계 + (변경주소) 수강신청자 인적사항 = 학번 + 학년 + 학과코드 + 학과명 + 성명 학과코드 = 3자리 숫자로 표현된 학과별 고유번호 수강신청 과목 = 이수구분 + 강좌번호 + 과목명 + 담당교수 + 학점 이수구분 = \\[ “ 공 필 ” “공선” “전필” “전선” “교직” \\] = 별칭 : “공필” = “공통필수” , “공선” = “공통선택” , “전필” = “전공필수” , “전선” = “전공선택” 강좌번호 = 5자리 문자로 구성된 강좌별 고유코드 의사결정표를 사용하는 경우는 무었인가 처리가 산출하는 출력이 복잡한 의사결정에 의해 좌우될 때 (복잡한 알고리즘) 의사결정이 수많은 입력 자료에 의해 좌우될 때 ( 많은 input ) 입력 자료가 광범위한 값을 가질 때 ( input 범위 넓음) 구조적 언어나 선후 조건문으로 기술할 경우 이해하기 어려움 소프트웨어 설계는 무었이 있어야 하는가 5가지 코드 설계 입출력 설계 파일 설계 프로세스 설계 프로그램 설계 코드의 기본기능, 3대기능, 부가기능 기본기능 표준화 기능 간소화 기능 3대 기능 분류기능 식별기능 배열 기능 부가 기능 연상 기능 암호화 기능 오류검출 기능 파일의 레코드 형식 4가지 블록(물리적 레코드) : 입출력 단위 예 4KB .. 레코드(논리적 레코드) : 파일 비블록화 고정길이 레코드 하나의 논리 레코드가 하나의 물리 레코드 구성 블록화 고정길이 레코드 여러개의 물리레코드 하나의 논리 레코드 비블록화 가변길이 레코드 블록화 가변길이 레코드 프로세스를 모듈화를 하는 이유가 무었인가 단일 프로그램으로 요구 사항을 구현할 경우 집중 처리 현상으로 인해 프로그램 작성 이 어렵고 논리가 복잡해짐(복잡도 증가) 처리할 데이터의 발생 시간이 다르므로 하나의 프로그램으로 구현하는 것은 비효율적() 하나의 프로그램으로 요구 사항을 모두 구현하려면 시스템 구성 상 제약이 따름(물리적 제약 ex성능) 요구 사항과 데이터 구성이 일치하지 않을 수 있으므로 모듈화 필요 (수정 용이) 표준 데이터 프로세스 유형 9가지 매체 변환 (Conversion) 정렬 (Sort) 병합 (Merge) 대조 (Matching) 갱신 (Update) 추출 (Extract) 분배 (Distribution) 생성 (Generating) 보고서작성 (Reporting) 프로그램 설계원칙 프로그래밍의 표준화 : 프로그래밍의 표준화는 프로그래밍에 있어 여러 문제의 정리와 통일화, 작업 간소화를 촉진하여 생산성을 높이는 것이 목적 프로그램의 작성이 용이하고 작업 기간을 단축할 수 있음 프로그램의 보수와 변경이 용이함 작업자의 지도와 교육 훈련에 도움을 줌 장애 처리의 설계 프로그램 실행 중 장애가 발생할 경우, 오퍼레이터는 신속히 대처해야 하므로 발생지 와 원인이 정확하게 오퍼레이터에게 통지되고, 복잡한 판단을 하지 않도록 장애의 처 리 순서가 명확히 규정되어 있어야 함 장애 처리 설계에 포함될 핵심 사항 장애의 종류 표시 장애의 처리 절차 장애 메시지 표시 내용 재시동(Restart) 절차",
      "frontmatter": {}
    },
    "아키텍쳐 기말고사 범위": {
      "path": "/06.university/아키텍쳐-기말고사-범위/",
      "filename": "아키텍쳐 기말고사 범위",
      "content": "기말 고사 범위는 다음과 같습니다. 컴퓨터아키텍쳐 (0773) 6/10 (화) 오후 7시 ~ 8시 제5공학관 Y5120 컴퓨터아키텍쳐 (0774) 6/11 (수) 오후 3시 ~ 4시 창조관 Y2446 2장 3 명령어 파이프라이닝 5장 기억장치 7 차세대 비활성 기억장치는 제외 8장 고성능 컴퓨터시스템 구조 교과서 3장과 5장에서 다음을 제외합니다. 4 준안정성 5 동기화기 6 분해능 시간 유도 6 병렬처리 1에서 전치가산기 관련 부분 6 논리배열 HDL 관련부분도 제외합니다.",
      "frontmatter": {
        "date": "2025-06-08T06:34:34+09:00",
        "lastmod": "2025-06-08T06:45:56+09:00"
      }
    },
    "영어2 메모": {
      "path": "/06.university/영어2-메모/",
      "filename": "영어2 메모",
      "content": "12/23 캠프릿지 영영사전%20image%2020241223130011.png) aspire 은 완전자동사 완전 자동사는 주어 동사 만으로도 완전한 문장을 이룬다 불완전 동사 예시 => he went pale 수식이 붙는다면 부사(상당어구)만 가능하다 it 은 완전히 동일한것을 지칭 one 은 비슷한 것을 지칭 The visual village a 문단 스마트폰 시대 이전, 사진작가가 되고자 하는 사람들은 고급 카메라를 사용해야 했고 사진 기술을 배워야 했습니다. 모든 사람이 카메라를 가진 것은 아니었고, 훌륭한 사진을 찍기 위해서는 기술과 안목이 필요했습니다. 하지만 오늘날, 스마트폰에 있는 다양한 카메라 앱 덕분에 우리는 모두 아마추어 사진작가가 되었습니다. 그리고 꽤 잘 찍기도 합니다. 현재 스마트폰 이미지의 품질은 거의 디지털 카메라와 동일해졌습니다. b 문단 사진 촬영의 새로운 용이함은 마법 같은 순간과 일상적인 순간을 포착하고자 하는 엄청난 열망을 우리에게 안겨주었습니다. 우리는 아침 식사, 고양이, 또는 고양이의 아침 식사와 같은 일상의 순간을 기록하는 데 집착하게 되었습니다. 그리고 스크랩북에 사진을 모으는 대신, 우리는 그것들을 전 세계의 친구들과 낯선 이들과 공유하고, 좋아요를 누르고, 댓글을 달고 있습니다. ease =~ take off appetite vs desire Appetite (식욕, 욕구) : 본능 Desire (욕망): 더 넓은 의미로, 감정적이고 심리적인 갈망 whether 가능성 또는 선택: 두 가지 이상의 가능성 중 하나를 나타낼 때 사용합니다. 예: \"I don’t know whether to go or stay.\" (갈지 말지 모르겠다.) 의문 표현: 어떤 일이 실제로 일어나는지에 대한 의문을 나타냅니다 예: \"She asked whether I was available.\" (그녀는 내가 가능한지 물어보았다.) 조건: 특정 조건이나 상황을 언급할 때 사용됩니다. 예: \"I doubt whether it will rain tomorrow.\" (내일 비가 올지 의심스럽다.) \"Whether or not\": 어떤 일이 발생하는지에 관계없이 강조할 때 사용됩니다. 예: \"I will go whether or not it rains.\" (비가 오든 안 오든 나는 갈 것이다.) \"Whether...or\": 두 가지 이상의 선택지나 가능성을 소개할 때 사용됩니다. 예: \"I can't decide whether to have coffee or tea.\" (커피를 마실지 차를 마실지 결정할 수 없다.) c 문단 심지어 사진 기자들조차도 휴대폰을 경험하고 있습니다. 그 이유는 휴대폰의 거의 보이지 않는 특성 덕분에 경계하지 않은 순간들을 포착하기가 더 쉬워지기 때문입니다. 인터넷은 사진 기자들이 전통적인 매체를 피할 수 있게 해줍니다. 그들은 이제 자신의 출판사로 활동할 수 있으며, 인스타그램과 같은 소셜 미디어 사이트를 통해 방대한 관객에게 도달할 수 있습니다. 뉴욕에서 찍은 사진이 업로드된 지 1초 만에 라고스에 있는 누군가로부터 반응을 받을 수 있습니다. their near invisibility : 접근 용이성 via : 필수적인 d 문단 과거에는 잡지들이 중요한 인물과 글로벌 사건의 잊을 수 없는 사진을 게재하여 우리의 상상을 사로잡았습니다. 이러한 사진들은 대중의 의견을 바꾸고 심지어 역사의 흐름을 변화시킬 수 있는 힘이 있었습니다. 하지만 오늘날 기억에 남는 이미지가 적어진 것은 좋은 이미지가 줄어들어서가 아닙니다. 오히려 너무 많은 이미지가 존재하기 때문입니다. 어떤 단일 이미지도 오랫동안 특별하게 여겨지지 않습니다. e 문단 카메라는 어디에나 존재하며, 이는 우리가 극적인 사건을 경험하는 방식을 변화시키고 있습니다. 주요 정치적 사건이나 자연 재해가 발생할 때, 종종 처음으로 뉴스 이미지를 제공하는 것은 사진 기자가 아닌 휴대폰을 가진 일반 시민들입니다. 품질도 여전히 중요하지만, 즉시 공유되는 것만큼 중요하지는 않습니다 f 문단 사람들이 어디에서나 사진 촬영을 받아들이고 미디어가 시민 기자들을 활용함에 따라, 전문적인 기준이 변화하고 있는 듯합니다. 과거에는 대부분의 사람들이 사진 기자들이 현실을 정확하게 표현할 것이라고 믿었습니다. 그러나 오늘날에는 디지털 이미지가 육안으로는 알아차릴 수 없는 방식으로 수정될 수 있습니다. 어떤 이미지든 \"개선된\" 현실의 그림을 만들기 위해 수정될 수 있습니다. 평균적인 시청자는 뉴스 조직이나 사진 기자에 대한 신뢰 외에는 이미지의 정확성을 평가할 방법이 없습니다. Embrace 수용하다 (Accept): 어떤 것을 열정적으로 받아들이거나 수용하는 의미입니다. 예: \"He embraced the opportunity.\" (그는 그 기회를 받아들였다. 포옹하다 (Hold): 사랑, 애정, 동정심 등을 표현하기 위해 두 팔로 누군가를 꼭 껴안는 행위입니다. 예: \"They embraced before saying goodbye.\" (그들은 작별 인사를 하기 전에 포옹했다.) 포함하다 (Include): 여러 가지 것들 중 하나로 포함하는 의미입니다. 예: \"The course embraces various subjects.\" (그 과정은 다양한 주제를 포함하고 있다.) naked eye : 육안 Alter: 어떤 것을 부분적으로 변경하거나 수정하는 것을 의미합니다. 큰 변화보다는 세부적인 수정에 초점을 맞춥니다. Change: 일반적으로 어떤 것을 다른 것으로 바꾸는 넓은 의미를 가집니다. 긍정적이거나 부정적인 변화를 모두 포함할 수 있습니다. Modify: 기존의 것을 조금 수정하거나 조정하는 의미로, 큰 변화보다는 작은 변화를 강조합니다. 특정 요소를 변경할 때 주로 사용됩니다. Adjust: 주로 상황이나 조건에 맞추기 위해 조정하는 것을 의미합니다. 미세한 수정이나 조정이 필요할 때 사용됩니다. Transform: 본질적으로 큰 변화를 나타내며, 어떤 것이 완전히 다른 것으로 변화할 때 사용됩니다. 종종 긍정적인 변화와 관련이 있습니다. Revise: 주로 문서나 계획 등을 다시 검토하고 수정하는 것을 의미합니다. 개선을 위한 변화가 포함됩니다. Amend: 법률 문서나 공식 문서의 내용을 수정하거나 개선하는 의미로, 주로 공식적인 맥락에서 사용됩니다. Adapt: 주어진 상황에 맞추어 조정하거나 적응하는 것을 의미합니다. 환경이나 조건에 따라 변화하는 데 중점을 둡니다. g 문단 이미지의 정확성에 대한 문제는 사진 기자들이 Flickr나 Instagram과 같은 카메라 앱을 실험하기 시작할 때 더욱 복잡해집니다. 이러한 앱은 필터 사용을 장려합니다. 이미지는 색상을 조정하거나 밝기를 조절하고, 흐리게 하거나 스크래치를 추가하여 사진을 더 예술적으로 만들거나, 고풍스러운 느낌을 줄 수 있습니다. 전쟁과 갈등을 다루기 위해 카메라 앱을 사용하는 사진 기자들은 강력한 이미지를 만들어냈지만, 동시에 논란도 일으켰습니다. 비평가들은 고풍스러운 사진이 전쟁을 미화하고, 전투에 참여하는 사람들로부터 우리를 멀어지게 한다고 우려합니다. trickier : 어렵다 h 문단 그러나 사진은 우리가 생각하는 것보다 항상 더 주관적입니다. 각 사진은 일련의 결정의 결과입니다—어디에 서 있을지, 어떤 렌즈를 사용할지, 어떤 것을 프레임에 포함할지 또는 제외할지를 선택하는 것입니다. 카메라 앱 필터로 사진을 수정하는 것이 그것을 덜 진실하게 만들까요? i 문단 디지털 시대가 우리에게 강요한 실험에는 강력하고 흥미로운 무언가가 있습니다. 이러한 새로운 도구들은 우리가 자신의 이야기를 더 쉽게 전달할 수 있게 해주며, 다른 사람들도 같은 힘을 가질 수 있게 합니다. 많은 언론인들은 선거, 정부, 전쟁, 재난과 같은 동일한 이야기들에 갇혀 있습니다. 그 과정에서 그들은 일상 생활의 덜 극적인 이미지들을 놓치게 되며, 이러한 이미지들도 그만큼 드러내고 관련성이 있을 수 있습니다. Revealing 노출이 많은 (Showing more than is usual): 의복이나 상황에서 일반적으로 보이는 것보다 더 많은 신체를 드러내는 것을 의미합니다. 예를 들어, \"a revealing dress\"는 몸을 많이 드러내는 드레스를 뜻합니다. 드러내는 (Showing something previously unknown or unseen): 어떤 것을 이전에 알려지지 않았거나 보이지 않았던 것을 드러내는 의미입니다. 예를 들어, \"A joke can be very revealing about what someone's really thinking.\"는 농담이 누군가의 진짜 생각을 드러낼 수 있다는 뜻입니다. relevant 관련된 (Connected with what is happening or being discussed): 어떤 사건이나 논의와 관련이 있는 것을 의미합니다. 예를 들어, \"Education should be relevant to the child's needs.\"는 교육이 아이의 필요와 관련이 있어야 한다는 뜻입니다. 특정 목적에 적합한 (Correct or suitable for a particular purpose): 특정한 목적이나 상황에 적합한 것을 나타냅니다. 예를 들어, \"plans to make schooling more relevant to life beyond school\"는 학교 교육이 학교 밖의 삶과 더 관련이 있도록 만들려는 계획을 의미합니다. j 문단 사진과 사진 기자의 수가 증가하는 것은 민주주의 자체에 긍정적인 영향을 미칠 수 있습니다. 수억 명의 잠재적인 시민 기자들이 세상을 더 작게 만들고, 지도자들을 정직하게 지킬 수 있도록 도와줍니다. 사람들은 이제 자신들이 직면한 상황을 보여줄 수 있어, 정부가 자신의 행동을 숨기는 것이 점점 더 어려워지고 있습니다. 모든 사람이 카메라를 가진다면, '빅 브라더'만이 감시하는 것이 아닙니다. Be up against it : 심각한 문제나 어려움에 직면해 있다 (to be having or likely to have serious problems or difficulties): 이 표현은 누군가가 큰 어려움이나 도전에 직면해 있음을 나타냅니다. 예를 들어, \"With seven members of the team missing, Hull are going to be up against it.\"는 팀의 7명이 결석하여 큰 어려움에 처할 것이라는 뜻입니다. k 문단 누가 알겠습니까? 우리의 문서화에 대한 집착과 끊임없이 연결된 상태는 우리의 존재 방식에 급진적인 변화를 가져올 수 있습니다. 아마도 우리는 보편적인 시각 언어의 발전을 목격하고 있는 것일지도 모릅니다. 이는 우리가 서로 및 세상과 관계를 맺는 방식을 변화시킬 수 있는 언어입니다. 물론, 모든 언어와 마찬가지로 시를 창작하는 사람도 있고, 쇼핑 리스트를 만드는 사람도 있을 것입니다. 여기서의 one 의 의미 : universal visual language l 문단 이러한 이미지 제작의 만개가 대중이 이미지를 더 잘 이해하고 감상하게 만들 것인지, 아니면 잘 만들어진 이미지가 미치는 깊은 영향을 무감각하게 만들 것인지 확실하지 않습니다. 어쨌든, 변화는 되돌릴 수 없습니다. 오늘 만들어진 수백만 개의 새로운 사진들이 우리를 구별하는 것이 아니라, 우리가 공유하는 것들을 볼 수 있도록 도와주기를 바랍니다. appreciates 가치를 인정하다 (To recognize how good someone or something is and to value them or it): 어떤 사람이나 사물이 얼마나 좋은지를 인식하고 가치를 두는 것을 의미합니다. 예를 들어, \"He doesn't appreciate expensive wines\"는 그가 비싼 와인의 가치를 이해하지 못한다는 뜻입니다. 중요성을 이해하다 (To understand a situation and realize that it is important): 어떤 상황의 중요성을 이해하는 것을 나타냅니다. 예를 들어, \"I appreciate the need for immediate action\"는 즉각적인 행동의 필요성을 이해한다는 의미입니다. 감사하다 (To be grateful for something): 누군가에게 감사함을 표현하는 의미로 사용됩니다. 예를 들어, \"I really appreciate your help\"는 당신의 도움에 진심으로 감사한다는 뜻입니다. 가치가 상승하다 (To increase in value): 재무적 맥락에서, 자산의 가치가 상승하는 것을 의미합니다. 예를 들어, \"Our house has appreciated in value\"는 우리의 집의 가치가 상승했다는 뜻입니다. numb : 무감각 irreversible : 되돌릴 수 없는 Reversible 되돌릴 수 있는: 어떤 상태나 변화가 일어난 후에 원래의 상태로 돌아갈 수 있는 것을 의미합니다. 예를 들어, \"The damage is reversible\"는 그 피해가 되돌릴 수 있음을 뜻합니다. 양면이 있는 (특히 의복에서): 의복이나 물체가 양면으로 사용할 수 있음을 나타내기도 합니다. 예를 들어, \"a reversible jacket\"는 양면으로 입을 수 있는 자켓을 의미합니다. 문제 Reding comprehension A-1 : because most images are not interesting to a global audience A-2 : images that are important to people and can be shared quickly A-3 : The writer feels it is questionable whether the picture is truly improved. A-4 : government leaders A-5 : Some people will use it for everyday things, and others for more creative things B A - e C - a E - c G - d L - b vocabulary practice A dramatic embrace instantly obsessed controversial B agreement shows have still a lot strongly C athletic , genetic democratic dramatic FEATHERS OF LOVE a 문단 부드럽고 검은 깃털로 덮인 이 고귀한 공연자는 관객에게 깊이 허리를 숙입니다. 그의 머리 꼭대기에서 여러 개의 긴 깃털이 자라나며, 그가 춤을 시작할 때 땅에 닿습니다. 이 춤추는 새는 카롤라의 파로티아로, 뉴기니 섬에 사는 많은 극락조들 중 하나입니다. 이 수컷 새는 위의 나뭇가지에서 그를 지켜보는 여러 암컷들에게 감명을 주기 위해 노력하고 있습니다. bow : 허리를 숙이다 noble : 고귀한 live in vs live on : 특정한 장소에 거주 vs 생계를 유지하는 방법이나 자원을 언급 예: \"I live in Seoul.\" (나는 서울에 살고 있다.) 예: \"I live on rice and vegetables.\" (나는 쌀과 채소로 살고 있다.) b 문단 암컷들의 관심을 끌기란 쉽지 않습니다. 그는 드라마틱한 효과를 위해 잠시 멈춘 후 다시 춤을 시작합니다. 그의 목이 가라앉고 머리가 위아래로 움직이며, 머리 깃털이 튕깁니다. 그는 뛰어오르고 깃털을 흔들며 결국 그의 공연이 암컷 중 한 마리의 관심을 끌게 됩니다. Begin: 일반적인 의미로 \"무언가를 시작하다\"는 뜻입니다. 일상적인 상황에서 가장 널리 사용됩니다. 예: \"Let’s begin the meeting.\" (회의를 시작합시다.) Start: \"Begin\"과 비슷하지만, 좀 더 비공식적이고 동적인 느낌이 있습니다. 특히 활동이나 행동을 시작할 때 자주 사용됩니다. 예: \"I will start my workout now.\" (지금 운동을 시작할 거예요.) Initiate: 공식적이고 계획된 상황에서 \"시작하다\"라는 의미로, 어떤 과정이나 절차를 개시할 때 사용됩니다. 일반적으로 주도적인 행동과 관련이 있습니다. 예: \"They initiated a new project.\" (그들은 새로운 프로젝트를 시작했다.) Launch: 주로 제품이나 서비스를 \"시작하다\"는 의미로 사용되며, 대규모 행사나 캠페인에서 자주 사용됩니다. 또한, 로켓을 발사할 때도 사용됩니다. 예: \"The company will launch a new smartphone.\" (회사가 새로운 스마트폰을 출시할 것이다.) Inaugurate: 공식적이고 특별한 의미를 가지며, 주로 정부나 기관의 새 출범을 알릴 때 사용됩니다. 취임식과 같은 중요한 사건과 관련이 있습니다. 예: \"The president will be inaugurated next week.\" (대통령이 다음 주에 취임할 예정이다.) Commence: 공식적이고 격식 있는 상황에서 \"시작하다\"는 의미로 사용됩니다. 특히 어떤 행사나 절차가 시작될 때 자주 사용됩니다. 예: \"The ceremony will commence at noon.\" (행사는 정오에 시작될 것입니다.) c 문단 뉴기니의 울창한 정글에는 자연의 가장 기이한 극장이 있습니다. 그것은 극락조들의 특별한 구애 게임입니다. 암컷을 끌어들이기 위해 수컷의 깃털은 무대에 어울리는 의상처럼 보입니다. 밝은 빨강, 노랑, 파랑이 숲의 초록색과 선명하게 대비됩니다. 수컷의 의상과 색상이 극단적일수록 짝을 끌어들일 확률이 더 높아 보입니다. Absurd: 뜻: 비현실적이고 논리적으로 말이 안 되는 상황이나 주장을 나타냅니다. 용례: \"It’s absurd to think that we can solve all our problems overnight.\" (우리의 모든 문제를 하룻밤에 해결할 수 있다고 생각하는 것은 터무니없다.) Ridiculous: 뜻: 웃음거리로 만들거나 우스꽝스러운 상황을 나타냅니다. 보통 비웃음의 대상이 됩니다. 용례: \"It’s ridiculous to think that we can finish this project in one day.\" (이 프로젝트를 하루 만에 끝낼 수 있다고 생각하는 것은 우스꽝스럽다.) Preposterous: 뜻: 매우 비합리적이고 논리적으로 불가능한 것을 강조할 때 사용됩니다. 용례: \"His idea to solve the problem was preposterous and not practical at all.\" (그의 문제 해결 아이디어는 터무니없고 전혀 실용적이지 않았다.) Ludicrous: 뜻: 웃음을 자아내는 정도로 어처구니없는 상황을 나타내며, 때때로 경멸의 뉘앙스를 포함합니다. 용례: \"The ludicrous costume he wore to the party made everyone laugh.\" (그가 파티에 입고 간 어처구니없는 의상은 모두를 웃게 만들었다.) Nonsensical: 뜻: 말이 안 되거나 무의미한 것을 나타내며, 주로 논리가 결여된 주장에 사용됩니다. 용례: \"The argument he presented was nonsensical and lacked any logical basis.\" (그가 제시한 주장은 말도 안 되고 논리적 근거가 전혀 없었다.) Irrational: 뜻: 이성적이지 않거나 비논리적인 감정이나 행동을 강조할 때 사용됩니다. 용례: \"Her irrational fear of spiders kept her from enjoying the outdoors.\" (그녀의 비이성적인 거미에 대한 두려움은 그녀가 야외 활동을 즐기는 것을 방해했다.) Worthy: 뜻: 가치가 있는, 존경받을 만한, 또는 어떤 것을 받을 자격이 있는 상태를 나타냅니다. 주로 긍정적인 평가와 관련됩니다 용례: \"She is a worthy candidate for the award.\" (그녀는 그 상을 받을 만한 가치가 있는 후보입니다. Suitable: 뜻: 어떤 특정한 목적이나 상황에 적합한 상태를 나타냅니다. 일반적으로 적절함을 강조합니다. 용례: \"This dress is suitable for the occasion.\" (이 드레스는 그 행사에 적합합니다.) d 문단 아주 아름다운 깃털 외에도, 각 종은 고유한 형태의 행동을 보여줍니다. 일부는 자신이 치우고 준비한 땅에서 춤을 추며, 마치 자신만의 댄스 플로어처럼 보입니다. 다른 종은 나무 위에서 높은 곳에서 공연을 합니다. e 문단 수컷 빨간 극락조는 \"나비 춤\"이라고 불리는 공연에서 그의 빨간색과 노란색 깃털을 뽐냅니다. 그는 거대한 나비처럼 날개를 활짝 펴고 강렬하게 움직입니다. 그러나 수컷 카롤라의 파로티는 극락조들 중에서 춤의 왕으로, 뛰어난 춤 솜씨를 가지고 있습니다! 그 춤 중 하나는 그의 깃털을 드레스처럼 펼치는 \"발레리나 춤\"이라고 불립니다. 일부 극락조들은 혼자 공연하는 반면, 다른 새들은 그룹으로 공연하여 암컷 새들이 거부할 수 없는 시선을 사로잡는 공연을 만들어냅니다. 근처의 가지에 매달린 수컷 골디의 새들은 날개를 퍼덕이며 등에서 솟아오르는 부드러운 빨간 깃털을 뚜렷하게 드러냅니다. 흥분한 암컷들은 곧 가장 마음에 드는 수컷을 선택합니다. prominent 중요한 눈에 잘 띄는 툭 튀어나온 f 문단 이 화려한 색상의 극락조들은 수백만 년에 걸쳐 고대의 새들로부터 발전해 왔습니다. 그 고대의 새들은 깃털이 어둡고 지루해 보였습니다. 오늘날 존재하는 45종의 화려한 색상의 극락조들 중 대부분은 뉴기니에만 살고 있습니다. 이 극락조들은 우리에게 자연의 신비를 풀어보도록 초대합니다. 이렇게 극단적인 깃털과 색상이 진화 과정에서 선호되었다는 것은 모순처럼 보입니다. 결국, 짝을 유인하는 이 밝은 색의 깃털은 또한 포식자에게 새들이 더 눈에 띄게 만듭니다. 그 해답은 새들이 살고 있는 안전한 환경과 성 선택으로 알려진 진화 과정에 있습니다. have been pp : 과거에 시작되어 현재까지 영향을 미치는 상태: 예: \"The results have been analyzed.\" (결과가 분석되어 왔다 해석: 분석이 과거에 시작되어 현재까지 계속되고 있음을 나타냅니다. After all : 결국 g 문단 \"여기의 삶은 극락조들에게 꽤 편안합니다. 이 섬의 독특한 환경은 그들이 다른 곳에서는 들어본 적 없는 극단까지 갈 수 있게 했습니다.\"라고 생물학자 에드 스콜스가 말합니다. 그는 더 가혹한 조건에서는 \"진화가 이러한 새들을 만들어내지 않았을 것\"이라고 말합니다. 과일과 곤충이 연중 풍부하고, 포식자는 거의 없습니다. 그 결과는 새들에게 완벽한 환경입니다. harsher : 가혹한 abundant : 풍부한 all year round : 1년 내내 h 문단 성적 선택은 극락조의 진화에서 주요한 원동력이 되어왔습니다. 다른 압박에서 벗어난 극락조들은 짝을 유인하는 데 전문화하기 시작했습니다. 수백만 년에 걸쳐 그들은 색상, 깃털 및 다른 재능에서 서서히 변화를 겪었습니다. 한 새를 다른 새보다 더 매력적으로 만드는 특성은 시간이 지남에 따라 전해지고 강화되었습니다. “여기서는 생존의 일반적인 규칙보다 성공적인 짝짓기의 규칙이 더 중요합니다!”라고 쇼울스는 덧붙입니다. pass on: 의미: 전달하다, 전하다. 예시: \"Please pass on this message to him.\" (이 메시지를 그에게 전달해 주세요.) pass down 의미: 대대로 전해주다, 물려주다. 예시: \"These traditions are passed down from generation to generation.\" (이 전통은 대대로 전해진다.) pass in: 의미: 제출하다, 들어가다. 예시: \"You need to pass in your assignment by Friday.\" (금요일까지 과제를 제출해야 합니다.) pass out: 의미: 의식을 잃다; 나누어 주다. 예시: \"He passed out from exhaustion.\" (그는 탈진하여 의식을 잃었다.) / \"She passed out the invitations.\" (그녀가 초대장을 나누어 주었다.) pass by: 의미: 지나가다, 무시하다. 예시: \"I saw him pass by the window.\" (나는 그가 창문 옆을 지나가는 것을 보았다.) pass up: 의미: 포기하다, 거절하다. 예시: \"I can't pass up this opportunity.\" (이 기회를 포기할 수는 없다.) pass through: 의미: 통과하다. 예시: \"We passed through the tunnel.\" (우리는 터널을 통과했다.) i 문단 뉴기니의 새들의 다양성은 다양한 환경에서 비롯됩니다. 해안 평야부터 구름 숲, 습기 있는 지역부터 5,000미터에 이르는 산에 이르기까지 다양한 지형이 존재합니다. 이 풍경에는 동물 집단을 고립시키는 많은 물리적 장벽이 있어, 이들이 독특한 종으로 발전할 수 있게 합니다. spring 명사 (계절) 예시: \"Many bulbs bloom in (the) spring.\" (많은 구근 식물이 봄에 꽃을 핀다.) 명사 (금속) 의미: 통통 뛰는 금속 스프링. 예시: \"The children have jumped on the couch so much that they've ruined the springs.\" (아이들이 소파에서 너무 많이 뛰어서 스프링이 망가졌다.) 명사 (물) 의미: 물이 자연적으로 지면에서 나오는 장소 예시: \"bubbling/hot springs\" (끓는/온천) 동사 (빠르게 움직이다) 의미: 특정 장소로 빠르게 움직이다. 예시: \"I sprang out of bed to answer the door.\" (나는 문을 열기 위해 침대에서 벌떡 일어났다.) 동사 (갑자기 나타나다) 의미: 갑자기 나타나다. 예시: \"Where did you spring from? - I didn't see you come in!\" (너는 어디서 갑자기 나타났니? - 나는 네가 들어오는 걸 보지 못했어!) j 문단 뉴기니의 사람들은 수세기 동안 극락조들의 과시적인 행동을 지켜보아 왔습니다. \"현지인들은 자신들이 숲에 들어가서 새들의 의식을 따라 했다고 말할 것입니다.\"라고 인류학자 질리언 길리슨이 말합니다. 지역의 춤 공연에서, 채색된 무용수들은 여전히 그들의 움직임과 아름다운 의상으로 새들을 연상시킵니다. \"깃털을 착용함으로써,\" 길리슨은 말합니다, \"...당신은 동물의 생명력을 포착하게 됩니다.\" Local : 지역 , 원주민 \"evoke,\" \"invoke,\" \"provoke\"는 모두 비슷한 형태를 가지고 있지만, 접두사에 따라 의미가 다릅니다. 아래에서 각 단어의 의미와 접두사에 따른 차이를 설명하겠습니다. evoke 접두사: \"e-\" (밖으로) 의미: 감정, 기억, 이미지 등을 불러일으키다; 어떤 것을 떠올리게 하다. 예시: \"The painting evokes feelings of nostalgia.\" (그 그림은 향수를 불러일으킨다.) invoke 접두사: \"in-\" (안으로) 의미: 법적 조항, 규칙 등을 호출하다; 신이나 영혼을 부르다. 예시: \"The lawyer invoked a specific law during the trial.\" (변호사는 재판 중 특정 법을 호출했다.) provoke 접두사: \"pro-\" (앞으로) 의미: 어떤 반응이나 감정을 유발하다; 자극하여 특정 행동을 하게 하다. 예시: \"His comments provoked a heated debate.\" (그의 발언은 치열한 논쟁을 유발했다.) k 문단 과거에는 새의 깃털에 대한 수요로 인해 대규모 사냥이 이루어졌습니다. 1900년대 초 무역의 절정기에는 매년 80,000개의 가죽이 뉴기니에서 유럽 여성의 모자를 위해 수출되었습니다. 현재는 패션이나 전통 의상을 위해 몇몇 새가 죽습니다. 의식용 깃털은 대대로 전해져 내려옵니다. 지역 주민들은 여전히 전통적인 용도로 새를 사냥할 수 있지만, 보통 나이가 많은 수컷 새를 목표로 하여 젊은 수컷이 계속 번식할 수 있도록 남겨둡니다. l 문단 그러나 새들에게는 더 심각한 위협이 있습니다. 불법적인 깃털 시장이 여전히 존재합니다. 대규모 농장은 한때 천국의 새들이 살았던 수천 헥타르의 숲을 사용합니다. 벌목, 석유 탐사, 광업 또한 뉴기니의 숲에 위험을 초래합니다. 한편, 인간 인구는 계속해서 증가하고 있습니다 Logging : 벌목 물론입니다! 각 항목을 한 줄로 간결하게 정리했습니다. preset 의미 명사 (선물): 특별한 경우에 주어지는 것. 예: \"They gave me theatre tickets as a present.\" 명사 (현재): 현재의 시간. 예: \"That's all for the present.\" 형용사 (현재의): 특정 장소에 있는. 예: \"The whole family was present.\" 형용사 (현재 시제의): 현재 시제를 나타내는 동사 형태. 예: \"Her book is written entirely in the present tense.\" 동사 (제공하다): 무엇인가를 주거나 알리다. 예: \"The winners were presented with medals.\" 동사 (소개하다): 사람을 소개하다. 예: \"She presents the late-night news.\" 동사 (문제를 제기하다): 문제를 발생시키다. 예: \"Falling tax revenues present a problem for the city.\" 동사 (자기 자신을 소개하다): 자신을 알리다. 예: \"He presented himself at the doctor's at 9:30 a.m.\" j 문단 환경 보호론자인 데이비드 미첼은 새들이 어떤 곳에서 나타나는지와 무엇을 먹는지를 기록하기 위해 지역 주민들의 도움을 받고 있습니다. 그는 데이터를 수집할 뿐만 아니라 새들의 서식지를 보호하도록 독려하는 것을 목표로 하고 있습니다. 이 전략은 효과를 보고 있는 것 같습니다. 미첼의 농부 중 한 명인 앰브로즈 조셉은 말합니다. “나는 나무를 베고 얌 덩굴을 심으러 왔습니다. 그런데 새들이 그곳에 앉는 것을 보았고, 그래서 나무를 그대로 두었습니다.” 수백만 년 동안 이 인상적인 새들은 짝을 찾기 위해 춤을 춰왔습니다. 숲이 그들에게 무대를 제공하는 한 그들은 계속해서 춤을 출 것입니다. habitat : 서식지 vine : 덩굴 문제 reding comprehension A-1 : b A-2 : c A-3 : a A-4 : c A-5 : d B-1 : b B-2 : a B-3 : a B-4 : a B-5 : b THE BATTLE FORBATTLE BIOTECH a 문단 작물과 동물의 유전자 조작(DNA 조작을 통한 유전자 공학)은 식량 생산에 혁신을 일으키고 있습니다. 우리가 먹는 음식의 품질과 영양 가치를 개선할 수 있는 가능성은 무한해 보입니다. 그러나 이러한 잠재적인 이점에도 불구하고 비판자들은 유전자 조작 제품—소위 생명공학 식품—이 그 영향이 완전히 이해되기 전에 서둘러 시장에 출시되고 있다는 우려를 표하고 있습니다 notwithstanding : 그럼에도 불구하고 critics : 비평가 b 문단 생명공학 식품은 유전적으로 변형된 동물과 식물에서 생산됩니다. 유전자 변형은 새로운 일이 아닙니다. 인간은 수천 년 동안 가장 좋은 작물에서 씨앗을 보관하고 다음 해에 심으며, 품종을 교배하여 더 달콤하게, 더 크게, 또는 더 오래 지속되도록 만들면서 식물의 유전적 특성을 변경해 왔습니다. 이 과정을 통해 우리는 야생 토마토를 작은 돌 크기의 과일에서 오늘날의 거대한 토마토로 변형시켰습니다. c 문단 반면, 유전 공학 기술은 새롭고 다릅니다. 전통적인 육종가들은 항상 관련이 있거나 유전적으로 유사한 식물이나 동물을 사용했습니다. 이 과정에서 수만 개의 유전자가 전이되었습니다. 반면, 오늘날의 유전자 엔지니어들은 유전자적으로 멀리 관련된 종이나 전혀 관련이 없는 종 간에 단 몇 개의 유전자만을 전이할 수 있습니다. 놀라운 예로는, 쥐 유전자가 상추 식물에 삽입되어 비타민 C를 생산하는 식물이 만들어졌고, 나방 유전자가 사과 나무에 삽입되어 질병 저항력이 추가되었습니다. 전통적인 기술과 현대 기술의 목적은 동일합니다. 원하는 특성을 가진 유전자를 가지지 않은 유기체에 삽입하는 것입니다. 현재 시장에는 여러 종류의 생명공학 식품 작물이 있으며, 그중에는 옥수수, 대두, 면화 품종이 포함됩니다. 이들 대부분의 작물은 농부들이 잡초, 해충 및 질병과 같은 일반적인 농업 문제를 해결하는 데 도움을 주기 위해 설계되었습니다. d 문단 우리가 아는 한, 지금까지 문제는 거의 없었습니다. 사실, 미국 국립 과학 아카데미의 2016년 보고서에 따르면, “이 유전자 조작 식품이 비유전자 조작 식품보다 인간 건강과 안전에 더 높은 위험을 나타내는 차이가 발견되지 않았다”고 합니다. 일부 유전자 조작 식품은 비유전자 조작 식품보다 더 안전할 수도 있습니다. 해충에 의해 손상된 옥수수는 종종 높은 수준의 푸모니신(fumonisins)을 포함하고 있는데, 이는 손상된 옥수수의 상처에서 자라는 독소입니다. 연구실 테스트에서는 푸모니신이 동물의 암과 연관되어 있다고 나타났습니다. 연구에 따르면, 해충 저항성을 위해 수정된 대부분의 옥수수는 해충에 의해 손상된 전통적인 옥수수보다 푸모니신 수치가 낮습니다. e 문단 그러나 생명공학 식품은 과거에 문제를 겪은 적이 있습니다. 그 중 하나는 1990년대 중반에 발생했으며, 이때 콩이 견과류의 유전자를 사용하여 수정되었습니다. 수정된 콩에는 견과류에 알레르기가 있는 사람들에게 반응을 일으키는 단백질이 포함되어 있었습니다. 이 단백질은 피해가 발생하기 전에 발견되었지만, 비판자들은 유전자 조작을 통해 생성된 다른 유해한 단백질이 눈에 띄지 않게 지나칠 수 있다고 우려합니다. 쥐와 상추와 같이 매우 다른 종 간에 유전자를 이동하는 것도 비판자들을 불안하게 만듭니다. 그들은 삽입된 유전자의 기능이나 숙주 DNA의 기능에서 문제가 발생할 수 있으며, 예기치 않은 건강 영향을 초래할 가능성을 걱정합니다. f 문단 대부분의 과학자들은 유전자 조작 작물의 주요 안전 문제가 사람보다는 환경과 관련되어 있다고 동의합니다. 오하이오 주립대학교의 식물 생태학자인 앨리슨 스노우는 유전자 조작 작물이 너무 빠르게 개발되고 있으며, 충분히 테스트되기 전에 시장에 출시되고 있다는 우려를 표하고 있습니다. g 문단 반면, 유전자 조작 작물의 지지자들은 일부 유전자 변형 식물이 실제로 토지에 유익할 수 있다고 주장합니다. 이러한 식물은 물을 오염시키고 동물에게 해를 끼칠 수 있는 농약의 환경 친화적인 대안을 제공합니다. 유전자 조작을 통해 자연 농약을 생산하도록 수정된 면화 식물에는 훨씬 적은 양의 농약이 필요합니다. 화학 농약이 밭의 거의 모든 해충을 죽이는 반면, 자연 농약을 가진 생명공학 작물은 실제로 그 작물을 먹으려는 해충만 해를 입힙니다. h 문단 “이 지구상에 8억 명의 사람들이 영양실조 상태입니다,”라고 인도 출신이자 미국 터스키지 대학교 식물 생명공학 연구 센터의 과학자인 채나파트나 프라카시가 말합니다. “그리고 그 숫자는 계속 증가하고 있습니다.” 프라카시와 많은 다른 과학자들은 유전자 조작이 식량 부족과 기아라는 긴급한 문제를 해결하는 데 도움을 줄 수 있다고 주장하며, 작물의 생산량을 증가시킬 수 있다고 말합니다. 작물은 혹독하고 건조한 기후에서 자라거나 일반적으로 농사에 적합하지 않은 토양에서도 자랄 수 있도록 설계될 수 있습니다. i 문단 세계 보건 기구에 따르면, 전 세계에서 약 2억 5천만 명의 어린이가 비타민 A 결핍으로 고통받고 있습니다. 그 결과 매년 25만에서 50만 명의 어린이가 실명하며, 그 중 절반은 시력을 잃은 지 1년 이내에 사망합니다. “황금 쌀”은 그 노란색 때문에 붙여진 생명공학 품종으로, 비타민 A 결핍으로 인한 고통과 질병의 잠재적 해결책으로 여겨지고 있습니다. j 문단 그러나 다른 전문가들은 생명공학 산업이 황금 쌀의 이점을 과장했다고 주장합니다. 뉴욕 대학교의 마리온 네슬 교수는 “황금 쌀만으로는 비타민 A 결핍을 크게 줄일 수 없다”고 말합니다. “베타 카로틴은 과일과 채소에서 이미 널리 이용 가능하지만, 사람들이 영양실조일 때는 비타민 A로 전환되지 않습니다. 황금 쌀은 베타 카로틴이 많이 포함되어 있지 않으며, 그것이 비타민 A 수치를 개선할 수 있을지는 두고 봐야 합니다.” k 문단 생명공학 식품이 세계의 기아를 없애고 모든 사람의 삶을 향상시킬 수 있을지는 두고 봐야 합니다. 그들의 잠재력은 엄청나지만, 위험도 따릅니다. 만약 과학이 신중하게 진행되어 새로운 제품을 철저히 테스트하고 합리적인 판단을 사용한다면, 세계는 유전자 조작의 위험을 피하면서 그 혜택을 누릴 수 있을 것입니다. DESIGN BY NATURE: BIOMIMETICS THE DNA TRAIL give rise to : provoke cause produce bring about engender create occasion generate induce originate a문단 모두가 좋은 이야기를 좋아하고, 이야기가 끝났을 때 이것이 지금까지 가장 위대한 이야기일 수 있습니다. 이야기는 아프리카의 한 무리의 사람들로 시작됩니다. 아마도 몇 백 명 정도로, 그들은 동물을 사냥하고 과일, 채소, 견과류를 모으며 생존하고 있습니다. 이야기는 약 20만 년 후, 그들의 70억 후손이 지구 곳곳에 퍼져 있는 모습으로 끝납니다. 최상급 i have ever pp : 내가 지금까지 한 것 중에 최고 these are the best oatmeal cookies i ve ever had artifact : an object such as a tool that was made in past b 문단 기록된 역사 이전 대부분이 일어나는 생존, 이동, 고립, 정복의 흥미진진한 이야기이다. 아프리카의 그 최초의 현대인들은 누구였을까요? 그들이 고향 대륙을 떠나 유럽과 아시아로 확장할 때 어떤 경로를 택했을까요? 인류는 언제 어떻게 아메리카에 도달했을까요? 수십 년 동안, 우리 조상들이 남긴 소수의 흩어진 뼈와 유물만이 유일한 증거였습니다 하지만 지난 20년 동안, DNA 기술 덕분에 과학자들은 현대인의 DNA에서 고대 인류의 이주 기록을 찾을 수 있게 되었습니다. \"had left\"는 과거완료형으로, 과거의 특정 시점 이전에 발생한 일을 나타냄. 이 문장에서 \"our ancestors had left behind\"는 \"우리 조상들이 남긴\"이라는 의미로, 조상들이 과거의 특정 시점 이전에 유물과 뼈를 남겼음을 강조 \"have allowed\"는 현재완료형으로, 과거부터 현재까지의 지속적인 상황이나 결과를 나타냄. 이 문장에서 \"DNA technologies have allowed\"는 \"DNA 기술이 허락해왔다\"는 의미로, 지난 20년 동안 DNA 기술이 발전하여 과거의 인간 이주 기록을 찾는 데 기여했음을 나타냅니다. 즉, 과거의 성과가 현재에도 영향을 미치고 있다는 점을 강조 c 문단 \"인간의 혈액 한 방울마다 우리의 유전자 언어로 쓰인 역사책이 담겨 있다\"고 인구 유전학자 스펜서 웰스는 말합니다. 인간의유전코드는전세계적으로99.9% 동일합니다. 우리의DNA 대부분은같습니다. 하지만 나머지 부분은 우리의 개인적인 차이 예를 들어 눈 색깔이나 질병 위험과 같은 것을 결정짓는 원인입니다. 매우 드문 경우에, “돌연변이”라고 불리는 작은 변화가 발생할 수 있습니다. 이는 그 사람의 모든 후손에게 전달될 수 있습니다. 세대가 지난 후, 두 사람의 DNA에서 같은 돌연변이를 찾는 것은 그들이 같은 조상을 공유한다는 것을 나타냅니다. 다양한 인구 집단에서 돌연변이를 비교함으로써, 과학자들은 그들의 조상 간의 연결고리를 추적할 수 있습니다. bulk: 대량이나 대부분을 의미합니다. identical: 두 개 이상의 사물이 완전히 같음을 나타냅니다. remainder: 어떤 것에서 남은 부분이나 양을 뜻합니다. be responsible for: 여기서는 책임보다 원인이라고 해석 d 문단 이 고대 돌연변이는두곳에서 가장 쉽게 추적할 수 있습니다. 하나는 어머니에서 자식에게 전달되는 DNA(미토콘드리아 DNA, 또는 mtDNA라 불립니다)입니다. 다른 하나는 아버지에서 아들로 이동하는 DNA(남성의 Y 염색체라고 알려져 있으며, 이는 아기가 남자라는 것을 결정하는 DNA의 일부입니다)입니다. 다양한 인구 집단의 mtDNA와 Y 염색체를 비교함으로써, 유전학자들은 그 집단들이 지구를 가로지르는 대이동 동안 어디에서 언제 분리되었는지에 대한 대략적인 아이디어를 얻을 수 있습니다. must have pp : ~했음에 틀림이 없다 e 문단 1980년대 중반에, 한 연구는 전 세계 사람들로부터 mtDNA(미토콘드리아 DNA)를 비교했다. 아프리카 혈통의 사람들이 다른 사람들보다 두 배 많은 유전적 차이점이 있다는 것을 알게 되었다. 돌연변이는 시간 지남에 따라 일정한 비율로 발생하는 것처럼 보이기 때문에, 과학자들은 현대인들이 다른 곳에서보다 적어도 두 배는 아프리카에 살았음이 틀림없다고 결론지었다. 오늘날 그들은(과학자들은) 모든 살아있는 인간들은 대략 15만 년 전 아프리카에 살았던 한 여성의 “미토콘드리아 이브” 라는 모계 유전자에서 내려왔음을 추정한다. 만약 유전학자들이 맞다면, 모든 인간은 끊을 수 없는 모계의 사슬을 통해 이브에 연결되어 있다. 이 (미토콘드리아) 이브는 또한 아프리카에서 온 우리 모두의 유전적 아버지일 가능성 있는 “Y 염색체 아담”에 의해 합류되었다. DNA 연구들은 지구상 모든 사람이 그들의 조상을 고대 아프리카 사람들로 추적할 수 있다는 것을 확인해줬었다. maternally : 모계와 관련된 주절 (제3형식 - SVO): \"They now calculate \\[that...\\]\" 주어: They (그들은) 동사: calculate (계산하다) 목적어: \\[that절\\] (명사절 전체) 명사절: \\[that all living humans maternally descend from a single woman {who lived roughly 150,000 years ago in Africa}, a \"mitochondrial Eve.\"\\] 이 전체 절이 \"calculate\"의 목적어 역할을 합니다 해석: \"모든 현생 인류가 모계로 아프리카에 살았던 한 여성으로부터 내려온다는 것\" 형용사절: {who lived roughly 150,000 years ago in Africa} \"a single woman\"을 수식하는 관계대명사절입니다 해석: \"약 15만년 전 아프리카에서 살았던\" f 문단 확실해 보이는 것은 놀라울 정도로 최근의 날짜에 – 5만~7만 전 사이 – 아프리카 밖 현대인의 조상인 작은 그룹의 사람들이 서아시아를 향해 아프리카를 떠났다. 그들은 더 넓은 홍해 북쪽의 끝 주변에 이주하였거나, 홍해의 남쪽 좁은 틈을 가로질렀었다. either-or : 2가지중 1개 g 문단 유전적 증거에 따르면 과거 아시아에서 인구는 분열했다. 한 그룹은 일시적으로 중동에 머물렀고, 반면 다른 그룹은 수만 년 지속할 여행을 시작했다. 세대가 거듭될수록 조금씩 더 멀리 이동하면서, 그들은 아라비아 반도, 인도, 그리고 동남아시아 해변을 따라, 온 힘을 다해 호주까지 갔다. “이 움직임은 아마 감지할 수 없다” 라고 Spencer Wells가 말했다. “이는 여행이라기보다는 아마 군중에게서 벗어나기 위해 해변을 조금 더 걷는 것 같다.” imperceptible : 감지할 수 없는 all the way to : 프로세스나 구조의 높은 수준에 있는 누군가 또는 무언가에 관한 한 h 문단 아프리카에서 호주로 이 13000km 이주의 고고학적인 증거는 거의 완벽하게 사라졌다. 하지만, 이주를 한 이 그룹의 유전적 흔적들은 ‘존재했다’. 그들은 말레이시아와 파푸아뉴기니의 원주민들, 그리고 거의 모든 호주 원주민의 DNA에서 발견되었다. 호주의 Lake Mungo 라고 불리는 장소에 묻힌 45,000년 된 유골의 현대적 발견들은, 이론들에 대한 물리적 증거를 제공했다. indigenous : originaly or naturally 하게 살던 원주민 또는 동식물 aborigines : 호주 원주민 i 문단 나머지 아시아와 유럽의 사람들은 서로 다르지만 mtDNA와 Y염색체 돌연변이를 똑같이 공유한다. 이 돌연변이는 대부분이 이동 전 몇 천 년 동안 중동에서 살았던 그룹의 후손들이라는 것을 보여준다. 아마도 약 40,000년 전에, 현대 인류는 유럽으로 처음 진출했다. advanced into : 발전하다 x => 진출하다 people : 동사일때 살다 j 문단 현대인류가 유럽으로 진출한 것과 거의 동시에, 중동에서 잠시 멈춰 있던 동일한 집단 중 일부가 동쪽으로 중앙아시아로 퍼져 나갔습니다. 그들은 마침내 시베리아, 한반도, 일본까지 도달했습니다. 여기에서 인류 이야기의 마지막 장 중 하나인 아메리카 대륙의 역사가 시작됩니다. 대부분의 과학자들은 오늘날의 아메리카 원주민이 마지막 빙하기 때 시베리아에서 알래스카로 건너간 고대 아시아인으로부터 내려온 사람이라고 믿습니다. 그 당시에, 낮은 해수면이 노출시켰을 것 입니다 대륙 사이의 연결 다리를. 아마도 그들은 단지 수백 명에 불과한 사람들이 해안을 따라 여행하고 있었을 것입니다, 한가지 땅의 조각으로부터 옆으로움직이며, 사이에 얼어붙은 바다와 얼음벽을 둔채로. \"해안길이 아마 가장 쉬운 길이었을 겁니다.” 라고 웰스는 말했습니다. \"그러나 이것은 여전히 힘든 힘든 여행이었습니다.\" 일단 건너고, 그들은 방대한 양의 동물 떼를 따라 본토로 들어갔습니다. 그들은 천 년 만에 남아메리카 끝까지 퍼졌습니다. most 를 주어로 사용 가능 as 를 when 으로 교체 가능 People (명사): 일반적으로 \"사람들\" 또는 \"인류\"를 의미합니다. 예: \"The people in the room are friendly.\" (방에 있는 사람들이 친절하다.) Peopling (동사 형태): \"사람들을 살게 하다\" 또는 \"사람이 거주하게 하다\"라는 의미로 사용됩니다. 예: \"The peopling of the Americas occurred thousands of years ago.\" (아메리카 대륙의 인구 형성은 수천 년 전에 일어났다.) k 문단 유전 연구자들은 지금까지 쓰여진 어떤 것보다 더 복잡한 인류 이주 이야기의 기본 개요만 알려줄 수 있습니다. 우리 조상의 이동과 셀 수 없이 많은 개인의 삶에 대한 많은 세부사항은 상상으로만 가능합니다. 하지만 유전학 연구자들, 즉mtDNA 이브와Y 염색체 아담의 후손들 덕분에 우리는 고대 조상들의 기원과 움직임에 대한 중요한 비밀을 밝혀내기 시작했습니다 ALTERNATIVE ROUTES 필요 없음 문제 A-1 : c A-2 : a A-3 : b A-4 : a A-5 : d , a의 경우 \bto find 가 to track 대응 B-1 : c B-2 : a B-3 : e B-4 : b B-5 : d 돌연변이는 ~ 찾기 위한 가장 쉬운 방법이다 Mutations are easiest to find in mtDNA and in the Y chromosome 돌연변이를 찾기 가장 쉬운 방법은 ~ The easiest way to find mutations is in mtDNA and the Y chromosome. A CROUD IN HARMONY cloth bath bath (뻔데기) clothe 옷 bathe 동사(드) clothes clothing 의류 bathing a 문단 새벽이 밝기 전, 축제의 두 번째 주요 목욕일이 시작되며 안개가 강을 감싸고 있습니다. 하루 동안 수천만 명의 사람들이 인도 알라하바드에서 갠지스 강에서 목욕을 할 것입니다. 달빛 아래, 강둑에 군중이 모이기 시작합니다. 이미 여기에는 수천 명이 있지만, 군중은 차분하고 단결된 모습입니다. 밀치거나 당황하는 일은 없고, 순례자들이 차가운 물에 들어가 목욕을 하고 다시 나오는 동안 오히려 목적 의식이 느껴집니다. 사람들은 서로 협력하고 도와줍니다. 이후에는 모두가 기쁩니다. swell : to become lager an rounder riverbank : 강둑 pilgrims : 순례자 one onother = each other b 문단 날이 지나면서 강에 들어가는 사람의 수가 늘어납니다. 어떤 사람은 물에 첨벙 거리며, 어떤 사람은 꽃을 물에 띄우고, 또 다른 사람은 기름 램프를 켜서 강 위에 떠내보냅니다. 칼을 든 채로 극적으로 물에 뛰어드는 남자들도 있습니다. 부모에게 끌려 완전히 옷을 입은 채로 들어가는 내키지 않는 아이들도 있습니다. 밝은 주황색 로브를 입고 신성한 흰 재로 덮인 피부를 가진 성스러운 남자들도 있습니다. 자신의 종교에 따라 재를 바르지만 거의 옷을 입지 않은 다른 신실한 남자들도 있습니다. 사람들은 어디에나 있지만, 놀랍게도 아무도 밟히지 않고, 아무도 익사하지 않으며, 도움을 요청하는 소리도 들리지 않습니다. 모든 것이 조화롭습니다. progeress : 발전 또는 진보의 의미도 있지만 일이 진행된다는 의미도 가지고 있음 theatrically : 극적으로 unwilling : 내키지 않은 devout men 신실한 남자들 stepped on : 밟히다 \"somehow\"는 다음과 같은 의미로 사용됩니다: 어떻게든: 어떤 방법이나 수단으로, 특정 방법이 알려지지 않았거나 명시되지 않은 경우. 예: \"We'll get across the river somehow.\" (어떻게든 강을 건널 거예요.) 이유가 불분명하게: 명확하지 않은 이유로 어떤 일이 발생했음을 나타냄. 예: \"Somehow, the news leaked out.\" (어떻게든 소식이 새어나갔어요.) 어딘가에서: 어떤 방식으로든 연결되어 있다고 느끼는 경우. 예: \"Everything felt somehow connected.\" (모든 것이 어딘가에서 연결된 듯한 느낌이었어요.) 예: \"I'm somewhat tired.\" (저는 약간 피곤해요.) 예: \"The movie was somewhat interesting.\" (그 영화는 어느 정도 흥미로웠어요.) \"somehow\"는 방법이나 수단에 초점을 맞추며, 불확실성을 강조합니다. : 어떻게든 \"somewhat\"는 정도나 양에 초점을 맞추며, 상대적인 수치를 표현합니다. : 어느 정도 c 문단 쿰브 멜라(Kumbh Mela)는 모든 힌두교 순례 중 가장 크고 신성한 모임으로 여겨지며, 세계에서 가장 큰 평화로운 집회로 간주됩니다. 매년 수백만 명의 힌두교 신자들이 이곳에서 신성한 갠지스 강에 목욕을 합니다. 12년마다 이 모임은 훨씬 더 커지며, 참가자들을 수용하기 위해 거대한 텐트 도시가 세워집니다. pilgrimages : 순례 house : 명사뜻의 집이 아니라 거주하다의 의미의 동사로 사용됨 d 문단 2013년에는 쿰브가 55일 동안 지속되었고, 약 1억 2천만 명의 순례자들이 의식 목욕, 기도, 노래, 빈민 급식, 종교 토론 등의 활동에 참여한 것으로 추정됩니다. 쿰브 텐트 도시는 25제곱킬로미터 이상의 면적을 차지했으며, 14개 구역으로 나뉘어 각 구역마다 병원, 경찰서, 도로, 식료품점, 전기 및 음용수 공급이 마련되었습니다. 이는 놀라운 성과로 평가됩니다. 기본 군중 통제 전략은 다리와 기차역과 같은 “핫스팟”에서의 위험한 과밀을 피하는 것이었습니다. 하버드 대학교의 도시 디자인 및 계획 교수인 라훌 메흐로트라(Rahul Mehrotra)는 이 축제를 관찰한 후 “믿을 수 없을 만큼 잘 조직되어 있고, 믿을 수 없을 만큼 깨끗하며, 매우 효율적으로 운영된다”고 말했습니다. lasted 마지막으로 쓰인것이 아닌 지속되다 로 사용됨 즉 지속되어졌다 ritual : 의식적 행위 grocery store : 식료품점 e 문단 영국 세인트 앤드류스 대학교의 심리학자 스티븐 리처(Stephen Reicher)는 군중이 그 안에 있는 개인의 건강에 긍정적인 영향을 미친다고 의심하고 있습니다. 그는 “우리 연구는 군중이 사회에 필수적이라는 것을 보여줍니다. 군중은 우리가 누구인지에 대한 감각을 형성하고, 타인과의 관계를 형성하며, 심지어 우리의 신체적 웰빙을 결정하는 데도 도움을 줍니다.”라고 말합니다. suspect 의심하다 critical : 비난의 의미가 아닌 필수적인(중요한) 으로 사용됨 f 문단 리처와 그의 동료들은 군중이 유익하다는 아이디어를 테스트하고, 쿰브가 참가자들에게 미치는 건강한 효과를 확인하기 위해 이 대규모 힌두교 축제에 참석했습니다. 2011년 축제가 시작되기 전, 그의 연구자들은 인도 시골로 나가 예비 순례자 그룹에게 정신적 및 신체적 건강에 대해 질문했습니다. 또한 참석할 계획이 없는 사람들에게도 질문했습니다. 연구자들은 쿰브가 끝난 한 달 후 두 그룹 모두를 다시 질문했습니다. 마을에 남아 있던 사람들은 연구 기간 동안 실제로 변화가 없다고 보고했습니다. 반면, 순례자들은 통증 감소, 불안 감소, 에너지 수준 증가를 포함해 10%의 건강 개선을 보고했습니다. 더욱이, 이러한 긍정적인 효과는 그 후에도 오랫동안 지속되었습니다. g 문단 왜 군중에 속하는 것이 건강을 개선할까요? 심리학자들은 공유된 정체성이 그 원인이라고 생각합니다. 영국 던디 대학교의 닉 홉킨스는 “당신은 ‘나’가 아닌 ‘우리’라는 관점에서 생각하게 됩니다”라고 설명합니다. 이러한 사고 방식은 인간 관계를 변화시킵니다. 군중의 구성원들은 서로를 지지하고, 경쟁은 협력으로 바뀌며, 사람들은 혼자서는 이룰 수 없는 방식으로 목표를 달성할 수 있게 됩니다. a shared identity is the cause 문장이 we 를 가리킴 in a way they wouldn’t be able to alone. 혼자서는 이룰 수 없는 방식 in term of : 어떠한 측면에서. h 문단 안타깝게도, 쿰브에서 분명히 드러나는 상호 지원에도 불구하고 2013년 2월 10일 알라하바드 기차역에서 36명이 압사 사고로 목숨을 잃었습니다. 군중이 그 조화를 잃어버린 것입니다. 레이처는 그 원인 중 하나로 순례자들이 더 이상 심리적 군중을 형성하지 않았다는 점을 지적했습니다. 그들은 주변의 사람들을 동료 순례자로 보지 않고, 기차 좌석을 차지하기 위한 경쟁자로 인식하게 된 것입니다. stampede : 괴멸 one possible cause : 1가지 원인 i 문단 이 안타까운 사건 이전에, 레이처는 기차역에서 군중의 느낌을 묘사해 달라고 요청받은 한 순례자와 인터뷰를 했습니다. 그녀는 “사람들이 자신이 당신보다 더 강하다고 생각합니다. 그들은 당신을 밀어낼 수 있습니다”라고 말했습니다. 이후 쿰브에서의 느낌을 묘사해 달라고 요청받았을 때, 그녀는 “사람들이 당신을 걱정해 줍니다. 그들은 당신을 정중하게 대합니다”라고 답했습니다. 압사 사고는 군중의 심리적 협력이 무너질 때 어떤 일이 발생할 수 있는지를 보여주는 사례였습니다. j 문단 압사 사고와 같은 사건은 쿰브에서 드물며, 이 사건이 순례자들이 향후 이 행사에 참석하는 것을 막을 가능성은 낮습니다. 경찰은 분명히 이번 경험에서 교훈을 얻고 역을 더 안전하게 만들 것입니다. 하지만 쿰브와 같은 대규모 군중에서는 개인들이 스티븐 레이처가 말하는 “심리적 협력”의 힘에 믿음을 두어야 합니다. 다시 말해, “이웃을 사랑하라”는 것입니다. undoubtedly : 의심할 여지없이 문제 A-1 : c A-2 : d A-3 : c A-4 : a A-5 : d ㅠ WHO KILLED EMPEROR 135p Ben Weider : 왕당파들에게 독살 당했다고 믿는다 킨즈의 머리카락 분석을 근거로 David Jones : 독극물(비소) 중독의 이유로 벽의 비소 방출 some toxicologists : 비소중독의 증상이 내부출혈 없음 Steven Karch : 당시에 사용하는 화학물질을 고용량사용으로 인한 사망 (의사 실수) 삼장 멈춤 단 비소 노출 이 부분적인 원인임을 인정 Jean Tulard : 암과 위궤양이 나폴레옹의 사망 원인 (질병) One cancer specialist : 체중 감소와 같은 암의 증상이 없음 오리려 체중이 늘어남 François de Candé-Montholon : 백작의 복수수 a 문단 그것은 어떤 살인 미스터리보다도 매력적인 이야기입니다. 이야기는 1821년 남대서양의 외딴 영국 섬인 세인트 헬레나 에서 시작됩니다. 이곳은 한때 프랑스의 황제였던 나폴레옹 보나파르트가 1815년 워털루 전투에서 패한 후 수감된 곳입 니다. 1821년 2월, 나폴레옹의 건강이 악화되기 시작하고, 그는 3개월 후 51세의 나이로 사망합니다. 다음날 실시된 부 검에서는 위궤양이 발견되었고, 이는 암일 가능성도 있다고 합니다. compelling 매력적인 compel =~ force + 설득 present one time : former 문장에서 \"is held\"는 수동태 reportedly autopsy : 부검 stomach ulcer : 위궤양 b 문단 그러나 그의 사망 원인은 그 이후로도 논란이 되어 왔습니다. 역사학자, 독성학자, 의사 및 기타 전문가들, 그리고 아마추 어 조사자들까지 나폴레옹의 죽음이 어떻게 그리고 왜 발생 했는지에 대한 질문을 고려해 왔습니다. 많은 이들이 그가 실 제로 살해당했을 것이라고 확신하지만, 전문가들은 여전히 합의에 이르지 못하고 있습니다 여기서의 many 는 명사로 많은 사건들로 해석 convinced 는 확신으로 해석 dispute : 논란 c 문단 벤 와이더(Ben Weider)는 국제 나폴레옹 학회의 창립자로, 나폴레옹이 독성 화학물질인 비소로 중독되었다고 믿고 있습 니다. 와이더는 40년 이상 나폴레옹의 사망 원인을 추적해 왔 으며, 이 미스터리를 해결하기 위해 상당한 자원을 투자해 왔 습니다. 그의 관점에서 나폴레옹은 그를 영원히 제거하고자 했 던 영국과 프랑스 왕당파에 의해 독살되었다고 주장합니다. 와 이더의 가설의 핵심은 스트라스부르 법의학 연구소의 프랑스 독성학자인 파스칼 킨츠(Pascal Kintz)가 실시한 머리카락 분석입니다. 킨츠는 나폴레옹의 머리카락을 분석하여 비소가 포함되어 있음을 확인했습니다. 킨츠는 비소가 어떻게 또는 왜 존재했는지 정확히 설명할 수는 없지만, 와이더는 \"나폴레옹 의 독살은 계획적이고 의도적이었다\"고 확신하고 있습니다. arsenic 비소 relentlessly : 끈질기게 sought : seek 의 과거 분사 once and for all : 영원히 considerable : consider + able : 상당한 hypothesis : 가설 deliberate : 의도적인 d 문단 영국 뉴캐슬 대학교의 면역학자인 데이비드 존스(David Jones)는 나폴레옹이 마지막 몇 년을 보낸 세인트 헬레나의 롱우드 하우스 (Longwood House) 벽을 연구했습니다. 그는 벽지에 비소가 포 함된 물질로 칠해져 있다는 것을 발견했습니다. 존스에 따르면, 더 운 습한 섬의 조건 때문에 비소가 공기 중으로 방출되었을 가능성이 있다고 합니다. immunologist : 면역학자 e 문단 하지만 롱우드 하우스의 페인트만이 세인트 헬레나에서 비소의 유 일한 원인은 아닐 수 있습니다. 일부 독성학자들은 해산물을 많이 섭취하는 사람들에게서 비소 수치가 비정상적으로 높게 나타나는 경우가 흔하다고 말합니다. 세인트 헬레나는 본토에서 2,000킬로 미터 떨어진 작은 섬이기 때문에, 나폴레옹의 식단에서 해산물이 큰 비중을 차지했을 가능성이 높습니다. 또한, 나폴레옹의 사망 후 그 의 몸을 검사한 의사들은 심장 내부 출혈과 같은 비소 중독과 관련 된 일반적인 증상을 발견하지 못했습니다. then again : it might be f 문단 미국 심장병 전문가인 스티븐 카치(Steven Karch)는 나폴레옹이 자신의 의사들에 의해 살해되었다고 믿고 있습니다. 그들은 당시 일반적으로 약으로 사용되던 위험한 화학물질을 고용량으로 그에 게 투여했습니다. 카치의 이론에 따르면, 나폴레옹의 사망 하루 전, 그는 수은염화물(mercurous chloride)이라는 화학물질의 대량 을 투여받았으며, 이는 심장병 환자에게 종종 사용되던 약물입니 다. 카치는 이 약물과 다른 약물이 나폴레옹의 심장 박동을 방해했 고, 결국 그의 심장이 멈추게 했다고 주장합니다. 카치는 비소 노출 이 부분적인 원인임을 인정하지만, 실제로는 의사들의 실수가 심장 마비를 초래했다고 믿습니다. mercurous chloride : 수은 염화물 cease : 멈추다 g 문단 역사학자 장 튈라르(Jean Tulard)는 암과 위궤양이 나폴레옹의 사망 원인이라고 믿고 있습니다. 튈라르는 킨츠의 머리카락 분석에 납득하지 못하고 있으며, 테스트된 머리카락이 나폴레옹의 것이 아 닐 수도 있다고 평가합니다. 그는 또한 독살 이론을 반박하며, 영국 이나 프랑스 왕당파 또는 다른 어떤 사람도 나폴레옹의 생명을 겨 냥한 음모와 관련된 증거를 찾지 못했다고 주장합니다. 그럼에도 불구하고 암이 주요 원인 중 하나였다는 데에는 의문이 남아 있습 니다. 한 암 전문의는 나폴레옹이 진행된 위암에 걸리지 않았을 가 능성이 높다고 믿고 있는데, 왜냐하면 그 병에 걸린 사람들은 항상 많은 체중을 잃기 때문입니다. 보고에 따르면, 나폴레옹은 세인트 헬레나에 머무는 동안 체중을 잃지 않았고, 오히려 상당량 증가했 습니다. plot 음모 h 문단 “내 조상이 그렇게 했다”고 프랑수아 드 칸데-몽톨롱(François de Candé-Montholon)이 말합니다. “나는 귀족입니다. 귀족들 은 혁명을 좋아하지 않으며, 나폴레옹은 혁명을 일으켰습니다.” 칸 데-몽톨롱의 증조증조할아버지인 몽톨롱 백작은 나폴레옹과 함께 세인트 헬레나에 주둔해 있었고, 나폴레옹은 백작의 아내와 사랑 에 빠졌으며, 나폴레옹이 그녀의 막내 아이의 아버지라는 소문이 돌기도 했습니다. 백작은 나폴레옹의 와인 셀러와 음식을 관리하 고 있었으며, 복수심에 차서 와인에 독을 타 넣었을 가능성은 없었 을까요? a love affair 사랑에 빠지다 The count : 백작 i 문단 “모두가 옳고, 누구도 옳지 않다”고 파리의 조르주 퐁피두 병원 (Paul Fornes)의 폴 포르네스가 말합니다. 포르네스는 1821년 부검 보고서와 기타 역사적 기록을 검토한 결과, “나폴레옹은 암으 로 죽었을 수도 있지만, 암으로 죽지는 않았다”고 결론지었습니다. 그는 또한 머리카락 분석이 비소의 존재를 나타내고 있지만, 누가 의도적으로 비소를 주었는지, 혹은 그것이 궁극적으로 그를 죽게 했는지는 아무도 확신할 수 없다고 말합니다. 포르네스의 의견에 따르면, 독살에 의한 살해 증거는 불확실하며 법정에서 통용되기 어렵다고 합니다. inconclusive : 결론내릴수 없는 : 불확실한 j 문단 나폴레옹 보나파르트의 시신은 1840년에 프랑스로 돌아왔고, 그 이후로 파리의 웅장한 무덤에 안치되어 있습니다. 일부 사람들은 이 제 무덤을 열고 현대적인 방법으로 유해를 조사할 때가 되었다고 생 각합니다. 그러나 프랑스 역사학자이자 의사인 장-프랑수아 르메르 (Jean-François Lemaire)는 진지한 과학과 역사에는 더 이상 관 련이 없다고 믿고 있습니다. 그는 “우리는 이제 오락의 세계에 있 다”고 말합니다. 새로운 사실들이 이 문제를 해결할 가능성은 낮아 보이며, 사람들은 이 미스터리를 너무 즐기기 때문입니다. 문제 A-1 : d A-2 : a A-3 : c A-4 : b A-5 : d B-1 : fact B-2 : s B-3 : fact B-4 : fact B-5 : s B-6 : s B-7 : fact TECHNOLOGY AS TRASH a 문단 태양이 가나의 수도 아크라의 습한 공기를 데우면서, 악취가 나는 검은 연기가 아그보그블로시 시장 위로 피어오르기 시작합니다. 채소 상인들을 지나면, 오래되고 부서진 전자기기 폐기물로 가득 찬 고철 시장이 있습니다. 이 폐기물은 부서진 TV, 컴퓨터, 모니터로 구성되어 있으며, \"e-waste\"로 알려져 있습니다. 고철 시장 너머에는 여러 개의 작은 불이 피어오르고 있습니다. 오래된 자동차 타이어로 연료를 공급받아, 이들은 e-waste의 귀중한 전선에서 플라스틱 외피를 태우고 있습니다. 사람들은 독성 연기 속을 걸으며 화려한 색깔의 컴퓨터 전선을 가득 안고 있습니다. 그들 중 많은 이들이 어린이들입니다. scrap : 고철물 piles : 쌓인 past : 시간일 경우 과거, 거리인 경우 지나다 arm : 팔 또는 안다 b 문단 이스라엘 멘사(20세)는 여기서 어떻게 생계를 유지하는지 설명합니다. 매일 고철 판매자들이 오래된 전자기기를 가져옵니다. 멘사와 그의 친구, 가족은 몇 대의 컴퓨터나 TV를 사서 분해하여 귀중한 금속과 전선, 재판매 가능한 부품을 제거합니다. 그런 다음 전선의 플라스틱 외피를 태워서 판매합니다. 돈을 버는 핵심은 안전이 아닌 속도입니다. “가스가 코로 들어오면 머리에서 무언가 느껴진다”고 멘사는 주먹으로 머리를 두드리며 말합니다. “그럼 머리와 가슴이 아프기 시작한다.” 부서진 컴퓨터와 모니터 케이스는 원치 않는 것이어서 근처의 늪에 버려집니다. 다음 날, 비가 내리면 그것들이 바다로 씻겨 내려갈 것입니다. his fist against his head : 그의 머리를 치다 lagoon : 바다 근처의 호수 또는 강 c 문단 e-waste는 이전에 없었던 규모로 생산되고 있습니다. 컴퓨터, 휴대폰 및 기타 전자 장비는 불과 몇 년 만에 구식이 되어 소비자들은 최신 제품을 구매할 수밖에 없는 상황에 놓입니다. 전 세계의 각 개인은 매년 평균 6킬로그램 이상의 e-waste를 폐기합니다. 이는 뉴욕에서 방콕까지, 그리고 다시 돌아오는 데 줄지어 서 있는 120만 대의 트럭을 채울 수 있는 양입니다. obsolete : 구식 out of date keep up : 유행을 따라가다 back again : 왕복 d 문단 안타깝게도 전 세계 대부분에서 이 많은 폐기물의 대부분은 매립지에 처분됩니다. 그곳에서 환경을 오염시키며, e-waste는 납, 수은, 비소와 같은 다양한 독성 물질을 포함하고 있습니다. 재활용은 여러 면에서 이 문제에 대한 이상적인 해결책입니다. e-waste에는 은, 금, 구리와 같은 귀중한 금속이 상당량 포함되어 있습니다. 이론적으로, 오래된 컴퓨터에서 금을 재활용하는 것은 땅에서 캐내는 것보다 훨씬 더 효율적이며 환경 파괴도 덜합니다. 문제는 부유한 국가에서 재활용을 위해 버려진 e-waste의 상당 부분이 개발도상국, 즉 가나와 같은 국가로 전환된다는 것입니다. 전 세계적으로 e-waste의 양이 증가함에 따라, 개발도상국에 살고 있는 사람들의 건강에 대한 위협도 점점 커지고 있습니다. ends up : (어찌하다보니) 끝나다 diverted : 방향이 전환되다 poses : (문제를) 야기시키다 e 문단 1989년, 170개국은 e-waste의 국제 거래 문제를 해결하기 위해 바젤 협약에 서명했습니다. 이 협약은 선진국이 개발도상국에 유해 폐기물 수송에 대한 통지를 하도록 요구했습니다. 6년 후, 환경 단체와 개발도상국의 압력에 따라 바젤 협약은 유해 폐기물 수송을 빈곤 국가에 완전히 금지하도록 수정되었습니다. 재활용 인프라가 잘 갖춰진 유럽 연합에서는 한 법률이 제조업체에게 그들이 생산한 전자기기의 안전한 처분에 대한 책임을 부여하고 있습니다. adress : 주소가 아닌 다루다로 해석 shipments : 수출, 수송 hold : (책임을) 묻다 the safe disposal of the electronics they produce. : 그들이 생산한 전제제품의 폐기 f 문단 e-waste가 계속해서 해외로 수출된다면, 결국 선진국에도 해를 끼칠 수 있습니다. 오하이오주 애슐랜드 대학교의 화학자인 제프리 와이덴하머는 그의 수업을 위해 개발도상국에서 만든 일부 보석을 구매하여 분석했습니다. 그 보석에 높은 양의 납이 포함되어 있다는 사실은 우려스러웠지만, 미국 상점에서 납이 포함된 보석이 발견된 적이 있었기 때문에 그다지 놀랍지는 않았습니다. 더 주목할 만한 것은 납과 혼합된 구리와 주석과 같은 금속의 양이었습니다. 와이덴하머는 과학 논문에서 이 금속들의 비율이 이 보석이 재활용된 컴퓨터 부품으로 만들어졌음을 시사한다고 주장했습니다. distressing 우려스러운 lead : 납 turned up : 발견되다 revealing : (흥미러운) 드러남 g 문단 선진국이 납을 포함한 물질을 대량으로 개발도상국에 보내고 있기 때문에, 이러한 국가들이 이를 제조 과정에서 활용할 것이라는 것은 예상할 수 있습니다. \"이제 우리는 오염된 제품을 다시 받게 되는 것이 전혀 놀랍지 않습니다\"라고 와이덴하머는 말합니다. 글로벌 경제에서는 어떤 것을 다른 나라로 보내서 없애는 것이 더 이상 불가능합니다. 옛 속담이 있듯이, \"돌고 도는 것은 다시 돌아온다\"는 말이 있습니다. rid : 제거 “What goes around comes around.” : 돌고 도는 것은 다시 돌아온다 h 문단 하지만 더 많은 국가들이 \"순환 경제\"로 전환할 것이라는 희망이 있습니다. 순환 경제는 자재 재사용과 폐기물 최소화에 초점을 맞춘 경제 모델입니다. 그 예로 호주가 있습니다. 호주는 최근 세계 최초의 e-waste 마이크로 공장을 열었습니다. 이 마이크로 공장은 면적이 겨우 50제곱미터로, 여러 대의 작은 기계가 e-waste를 재활용합니다. 먼저 기계가 버려진 e-waste를 분해하고, 그 후 로봇이 부품을 식별하여 분리합니다. 분리된 부품은 가열되어 재사용 및 재목적화할 수 있는 귀중한 자재로 변환됩니다. 이 과정은 깨끗하고, 상대적으로 저렴하며, 반복될 경우 현재 호주의 매립지에 쌓이는 막대한 양의 e-waste를 줄이는 데 도움이 될 수 있습니다. breaks down : 분해 identifies 실별 i 문단 마이크로 공장은 그 작은 크기 덕분에 e-waste를 처리하고 관리하는 방식을 크게 변화시킬 수 있습니다. 이는 특히 e-waste를 운반하고 재활용하는 데 매우 비싼 비용이 드는 외진 지역에서 더욱 그렇습니다. 뉴사우스웨일스 대학교의 비나 사하주왈라 교수는 e-waste 마이크로 공장이 지역에서 e-waste 문제를 해결하고 비즈니스 기회를 제공할 잠재력을 가지고 있다고 말합니다. 이는 환경과 비즈니스 모두에게 윈-윈(win-win) 상황입니다. 또한 현재 e-waste를 해외로 보내는 다른 국가에서도 적용할 수 있는 모델을 제공합니다. 사하주왈라 교수는 e-waste 마이크로 공장과 같은 혁신이 “우리 시대의 가장 큰 환경 문제 중 하나에 대한 비용 효율적인 해결책을 제공한다”고 말합니다. 문제 A-1 : c A-2 : b A-3 : d A-4 : a A-5 : b B-1 : d B-2 : c B-3 : f B-4 : b B-5 : g B-6 : a",
      "frontmatter": {}
    },
    "캐시 매핑 방식의 이해": {
      "path": "/06.university/캐시-매핑-방식의-이해/",
      "filename": "캐시 매핑 방식의 이해",
      "content": "공통 시스템 환경 설정 주기억장치 용량: 4KB (4096 바이트). 따라서 주소는 12비트가 필요합니다 (212=4096). 워드(Word)의 크기: 4 바이트. (32비트 시스템에서 흔히 볼 수 있는 구조) 블록(Block) 및 캐시 라인(Line) 크기: 4 워드. 즉, 4워드×4바이트/워드=16바이트 입니다. 캐시 메모리 크기: 256 바이트. 캐시 라인의 수: 256바이트/16바이트/라인=16개의 라인을 가집니다. 메모리 주소는 12비트이며, 이 주소를 어떻게 Tag , Line(or Set) , Word(or Offset) 필드로 나누는지가 각 사상 방식의 핵심입니다. Word(Offset) 필드: 블록 크기가 16바이트이므로, 블록 내 특정 바이트를 가리키기 위해 log2​(16)=4비트가 필요합니다. 직접 사상 방식 (Direct Mapping) 직접 사상 방식에서는 주기억장치의 각 블록이 캐시의 정해진 단 하나의 라인에만 저장될 수 있습니다. 주소 구조 캐시 라인이 16개이므로, 라인 번호를 지정하기 위해 log2​(16)=4비트가 필요합니다 ( Line 필드). 나머지 비트는 Tag 가 됩니다: 12−4(Line)−4(Word)=4비트. Tag (4비트) Line (4비트) Word (4비트) 동작 예시 CPU가 메모리 주소 150번지( 0000 1001 0110 ₂)의 데이터를 요청합니다. 주소를 분해합니다: Tag: 0000 Line: 1001 (9번 라인) Word: 0110 캐시는 9번 라인으로 가서 Tag를 확인합니다. 만약 9번 라인의 Tag가 0000 과 일치하면 캐시 히트(Hit)가 발생하고, 해당 블록의 6번째( 0110 ₂) 바이트를 CPU에 전달합니다. Tag가 다르거나 라인이 비어있으면 캐시 미스(Miss)가 발생합니다. 주기억장치의 150번지가 포함된 블록(9번 블록, 144~159번지)을 통째로 가져와 캐시의 9번 라인에 저장하고, Tag 필드를 0000 으로 갱신합니다. 완전 연관 사상 방식 (Fully Associative Mapping) 완전 연관 사상 방식에서는 주기억장치의 블록이 캐시의 어떤 라인으로든 저장될 수 있습니다. 주소 구조 정해진 라인이 없으므로 Line 필드가 없습니다. Tag가 주소의 대부분을 차지합니다: 12−4(Word)=8비트. Tag (8비트) Word (4비트) 동작 예시 CPU가 메모리 주소 150번지( 0000 1001 0110 ₂)의 데이터를 요청합니다. 주소를 분해합니다: Tag: 0000 1001 Word: 0110 캐시는 모든 16개 라인의 Tag를 동시에 비교하여 0000 1001 과 일치하는 Tag가 있는지 찾습니다. 일치하는 Tag를 찾으면 캐시 히트(Hit)가 발생하고, 해당 블록의 6번째( 0110 ₂) 바이트를 CPU에 전달합니다. 일치하는 Tag가 없으면 캐시 미스(Miss)가 발생합니다. 주기억장치에서 해당 블록(144~159번지)을 가져와 캐시의 비어있는 아무 라인에 저장합니다. 만약 빈 라인이 없다면, 특정 교체 정책(예: LRU)에 따라 기존 라인 중 하나를 교체합니다. 세트 연관 사상 방식 (Set-Associative Mapping) 세트 연관 사상은 직접 사상과 완전 연관 사상을 절충한 방식입니다. 캐시 라인들을 여러 개의 세트(Set)로 묶고, 블록은 정해진 세트 안의 어떤 라인으로든 저장될 수 있습니다. 여기서는 2-Way 세트 연관 사상을 예시로 들겠습니다. (하나의 세트가 2개의 라인을 가짐) 세트 계산 세트의 수 = 라인 수 / Way 수 = 16/2=8개의 세트. 주소 구조 세트 번호를 지정하기 위해 log2​(8)=3비트가 필요합니다 ( Set 필드). 나머지 비트는 Tag 가 됩니다: 12−3(Set)−4(Word)=5비트. Tag (5비트) Set (3비트) Word (4비트) 동작 예시 CPU가 메모리 주소 150번지( 0000 1001 0110 ₂)의 데이터를 요청합니다. 주소를 분해합니다: Tag: 0000 1 Set: 001 (1번 세트) Word: 0110 캐시는 1번 세트로 갑니다. 1번 세트에는 2개의 라인이 있습니다. 이 두 라인의 Tag만을 비교하여 0000 1 과 일치하는지 확인합니다. 일치하는 Tag를 찾으면 캐시 히트(Hit)가 발생합니다. 일치하는 Tag가 없으면 캐시 미스(Miss)가 발생합니다. 주기억장치에서 해당 블록을 가져와 1번 세트 내의 비어있는 라인에 저장합니다. 만약 1번 세트의 두 라인이 모두 사용 중이라면, 교체 정책에 따라 둘 중 하나를 교체합니다. 요약 비교 구분 주소 구조 (Tag-Index-Offset) 특징 장점 단점 직접 사상 4 - 4 - 4 블록 → 특정 라인 (1:1) 구현이 간단하고 빠름 특정 라인에 충돌이 잦음 (Conflict Miss) 완전 연관 8 - 없음 - 4 블록 → 모든 라인 (1:N) 매우 유연하여 히트율이 높음 모든 태그를 비교해야 하므로 회로가 복잡하고 비쌈 세트 연관 5 - 3 - 4 블록 → 특정 세트 (1:K) 직접 사상과 완전 연관의 장점을 절충함 직접 사상보다 복잡하고 완전 연관보다 히트율이 낮을 수 있음",
      "frontmatter": {
        "tags": [
          "architecture",
          "university"
        ],
        "date": "2025-06-10T17:50:28+09:00",
        "lastmod": "2025-06-11T15:04:42+09:00"
      }
    },
    "01.철학의 개념": {
      "path": "/06.university/철학과-인간/01.철학의-개념/",
      "filename": "01.철학의 개념",
      "content": "철학이란 무엇인가 철학은 다른 학문들과는 달리 그것의 대상이 무엇이며 무엇을 하는 학문인지가 우선 불분명하다. 원론적인 정의로서 우리는 “어떤 특수한 분야의 지식, 경험, 활동에 적용되는(될 수 있는) 일반원리들에 관한 탐구(Oxford 영어 사전)”라는 규정을 발견할 수 있겠는데, 사실 다른 거의 모든 학문들도 자기 분야의 원리들을 연구하는 것에 다름 아니기에, 과연 철학은 어떻게 다른 학문들과 구분될 수 있겠는가 하는 의구심이 든다. 이처럼 철학의 정체성이 쉽게 파악되지 않는 이유는 다음의 두 가지에서 찾아볼 수 있겠다. 1) ‘철학’의 등장과 학문의 기원 널리 알려진 바와 같이 ‘철학’이라는 단어는 플라톤의 대화편에 등장하는 소크라테스의 정의에서 비롯되는데, 그는 파이드로스라는 청년이 당시 풍부한 학식으로 유명했던 몇 몇 인물들(호머, 소크라테스 등)을 무어라고 칭해야 하는가라는 질문에 다음과 같이 대답한다. “파이드로스여, 누군가를 지혜 있다고 일컫는 것은, 내가 보기엔 너무 높이 올라간 것 같고 그런 말은 신에게나 적용하면 적절한 것 같네. 그러나 지혜를 사랑하는 자(philosophos) 혹은 그 비슷한 말로 일컫는다면, 그 자신도 차라리 동의할 것이고, 더 합당할 것 같네(Platon, Phaidros, 278d).” 신이나 간직할 법한 완전한 지식·완벽한 지혜는 아직 소유하지 못한, 그러나 이에 도달하려고 노력하는 이들을 일컬어 필로소포스라고 칭했고 이로부터 그들이 하는 활동들을 지칭하는 ‘철학(philosophia)’이라는 명칭도 생겨났다. 그래서 어떤 주제와 관점을 갖든 간에 (참된) 지식을 추구하는 활동들은 모두 철학이었고 그런 사람들 또한 철학자라고 불렸다. 따라서 고대 그리스 시대의 철학이란 다름 아닌 ‘학문 일반’을 뜻하는 것이었고 초기의 철학자들도 오늘날의 ‘학자’들에 해당할 뿐이었다. 이처럼 학문은 최초에 ‘철학’이라는 이름으로 등장했지만 철학 고유의 의미가 따로 있지는 않았다. 2) 제 학문들의 독립 – 분과 학문 당시의 소위 ‘철학자’들은 비단 오늘날 우리에게 철학이라 불리는 특수한 분야에만 매진했던 것이 아니라 다른 분야들, 예컨대, 정치학, 생물학, 의학 등에도 많은 관심을 쏟았다. 이런 상황은 근대까지 이어져 왔는데, 1687년에 출간된 뉴턴의 대표적인 저서가 『자연철학의 수학적 원리』라는 제목을 갖고 있는 것만 봐도 그러한 사정을 짐작할 수 있다. 근대 초기에 활동했던 데카르트는 모든 학문들이 서로 유기적으로 연결되어 있다고 전제한 뒤 학문 전체, 그러니까 ‘철학’을 하나의 나무로 비유해서 “그것의 뿌리는 형이상학이요, 줄기는 물리학(자연학)이며, 가지들은 의학, 역학, 윤리학과 같은 여타 학들이다”라고 얘기하고 있다. 이렇게 모든 학문들이 ‘철학’이라는 명칭 아래 하나로 묶여 있던 상태는 근대 중기에 들어서 무엇보다 자연과학의 성장과 함께 변하게 된다. 뉴턴을 위시하여 갈릴레이, 케플러 등의 자연과학자들이 내놓은 눈부신 성과들은 당시의 세계관 뿐 아니라 학문 전체에 본질적인 변화를 가져왔다. 자연과학의 발전을 통해 그들은 전문적인 언어(수학), 전문적인 방법(실험, 관찰, 통계) 또한 전문적인 도구(과학기기들)들을 갖게 되었고 이를 통해서 지금까지 일반 ‘철학자’들이 다루었던 것과는 전혀 다른 방식으로 자연을 연구하게 되었다. 자연과학의 특수성과 전문성은 이제 그들을 학문 일반, 즉 ‘철학’으로부터 독립시켰고 이를 통해 ‘학문의 부분 영역’, ‘분과 학문’ 즉 ‘과학’들이 생겨났다. 이 분과 학문들은 자신들의 문제 영역을 자신들만의 특수한 수단과 방법들을 통해서 연구하였고 이제까지 ‘철학’의 주제였던 대상들은 하나 둘씩 제 과학들의 고유한 소유물이 되기에 이르렀다. 이를 통해 철학은 정체성의 위기를 맞게 된다. 고대 그리스인들이 관심 갖던 자연의 근본재료, 천체의 운동법칙, 식물과 동물의 생태, 정치형태와 정치적 활동, 심지어 인간의 심리현상마저도 그것들을 더 전문적이고 객관적으로 연구할 수 있는 분과 학문들이 주제삼음으로써 도대체 철학이 연구할 수 있는 대상은 무엇이며 또 어떤 방법으로써 이에 접근할 수 있겠는가라는 물음이 떠오르게 되었다. 역으로 말해 철학은 다른 분과학문들이 더 이상 다루지 않는, 혹은 다룰 수 없는 주제들만을 그 대상으로 삼게 되는 숙명에 놓이게 됐다고도 할 수 있겠다. 그래서 철학은 마치 다른 학문들이 남겨놓은 부스러기와 잡동사니, 혹은 연구할 가치가 없는 사소한 것들만을 붙잡고 늘어지는 짓거리처럼 보일 수도 있겠고, 또한 어느 정도의 지성과 일반적인 일상 체험만 가지면 누구나 할 수 있는 수필작업 정도로 여겨질 수도 있게 됐다. 이런 사정 탓에 ‘철학은 도대체 무엇인가’라는 물음은 항상 있어 왔고 아직도 있을 수밖에 없다. 근본학으로서의 철학 이런 위기에 봉착해서 이제 철학자들도 전문적인 철학적 연구가 필요하다고 자각하게 되었고 이후 모든 철학적 연구도 철학의 정체성 문제와 분리해서 생각할 수 없게 되었다. 근대 이후의 철학적 이론들은 자신들만의 고유하고 전문적인 방법과 관점, 그리고 주제들을 들고 나온다. 하지만 다른 학문들과의 차별을 위한 이런 철학의 전문화 시도에도 불구하고 철학은 여전히 다른 학문들과 분리해서 생각될 수는 없을 것이다. 다른 학문들과의 관계를 여전히 유지하면서도 철학의 정체성을 새롭게 규정하고자 했던 근대 이후의 본격적인 시도로서 우리는 칸트(1724-1804)의 경우를 들 수 있겠다. 그는 무엇보다도 이성 그 자체에 관심을 가졌고 철학은 다름 아닌 “개념들에 의한 이성인식의 체계”라고 보았다. 그런데 사실 다른 모든 학문들도 이성을 ‘가지고서’ 연구에 임하며 그 결과물들은 어떻게 보면 오히려 더 합리적이다. 그러니 이성이 철학만의 고유한 주제라는 주장은 일견 근거가 없는 것처럼 보인다. 하지만 우리는 다른 그 어떤 학문에서도 이성이 하나의 독립된 주제로서 다루어지는 것을 보지 못한다. 그들은 이성을 하나의 도구로서 사용할 뿐이지 이성 자체에 대해서 본격적인 연구를 하지는 않는다. 다른 학문들의 방법, 추론, 기본 개념 등은 모두 이성적 원리와 합리적 바탕 위에서 형성되었고 제 학문들의 실질적인 연구 활동에서 적극 활용되지만, 그것들이 도대체 무엇인지, 어떻게 형성된 것인지, 그 자체로 과연 정당성이 있는지 등을 되묻는 분과학문은 있지 않다. 철학이 이성이라는 주제로 눈을 돌린 이유는, 이성을 인간의 가장 고유한 속성으로 여기기 때문이다. 이렇게 이성에 대한 탐구를 통해서 인간의 본성을 파헤치려는 시도가 다름 아닌 철학인데 앞서 말한 바와 같이 제 분과 학문들의 기초에는 어김없이 이성에 의해 정립된 개념과 원리가 놓여 있기에 그런 이성을 탐구하는 철학은 이제 자연스럽게 여러 학문들의 ‘근본학’으로 규정될 수 있겠다. 근본학으로서의 철학은 다른 모든 분과 학문들의 출발점이자 기초가 되는 원리들을 반성해서 그것들의 의미를 밝혀내는 작업인 셈이다. 그래서 오늘날의 철학은 비록 어떤 특정한 분야와 대상은 하나도 갖고 있지 않지만 다른 모든 학문들과 관계하여 그들의 기초를 파악하는 활동을 함으로써 여전히 다른 학문들과 그 영역을 나눠 갖고 있다. 또한 다른 분과 학문들에서도 그 자신의 원리와 기초를 반성하는 작업이 이루어지는 부분이 있다면(이론 물리학에서 시간과 공간의 개념을 묻는 경우가 해당된다) 그 부분은 다름 아닌 철학적인 부분으로 불리어도 무방할 것이다. 철학과 제학문과의 이러한 밀접한 관계를 그 배경으로 할 때 우리는 철학에 대한 원론적인 규정인 ‘자연과 인간 사회 문화 제 영역의 최고 원리와 제 영역의 통일 원리를 반성적으로 탐구하는 지적 활동 또는 그 결실’이라는 정의를 진정으로 이해할 수 있겠다.",
      "frontmatter": {}
    },
    "02.철학의 유용성과 가치": {
      "path": "/06.university/철학과-인간/02.철학의-유용성과-가치/",
      "filename": "02.철학의 유용성과 가치",
      "content": "철학의 유용성과 가치 철학의 필요성에 대한 의문 우리는 철학과 관련해서 일반적으로 ‘철학은 도대체 어디에 쓸모가 있는가?’, ‘왜 우리는 철학을 하는가?’ 와 같은 질문들을 떠올린다. 철학은 그것의 근본적인 ‘목적’과 ‘가치’가 물어지는 유일한 학문이다. 다른 학문들(구체적이고 실질적인 학문들 뿐 아니라 추상적이고 이론적인 학문들조차도)은 그 쓰임새가 직관적이고도 쉽게 드러나지만 철학의 필요성은 쉽게 파악되지 않기 때문이다. 오히려 ‘쓸모없는 학문’이라는 통속적인 생각이 잘못된 선입견이 아닌 것처럼 여겨지기도 하고, 또한 실제로도 철학이 인간의 삶에 구체적인 도움을 주는 것 같지는 않다. 이처럼 그 필요성을 쉽게 느끼기 힘든 철학이라는 학문을 우리는 도대체 왜 하는가? 필요성의 개념: 상대적 유용성과 절대적 가치 이 문제에 본격적으로 대답하기 위해서 우리는 먼저 무언가가 ‘필요하다’는 말의 의미가 과연 무엇인지를 생각해 보아야 한다. 우선적으로 떠오르는 일반적인 생각은 어떤 것이 또 다른 어떤 것을 위해 쓰이는 경우일 것이다. 망치는 벽에 못을 박을 때 사용되고, 책상은 책과 필기도구 등을 올려놓기 위해 있는 것처럼 어떤 것이 다른 목적을 위한 하나의 수단으로 필요한 경우 우리는 이를 유용하다고 부른다. 이 유용성에 의해서 하나의 사물이나 도구는 그 가치가 매겨지기 마련이다. 어떤 것이 유용한지 아닌지는 그것이 쓰이는 목적이 밝혀졌을 때 비로소 드러나는 것이다. 철학도 마찬가지로 그것이 사용되는 목적이 분명히 밝혀졌을 때에야 그 유용성이 나타날 것이다. 그런데 철학을 통해서 우리가 실제로 얻을 수 있는 것이 무엇인지 생각해 본다면 그 대답은 부정적일 수밖에 없다. 따라서 결론적으로 우리는 철학이라는 것은 도무지 아무 데에도 소용이 없으며 애써 추구할 가치는 없는 것이라고 말해야 할지 모르겠다. 그런데 여기서 우리는 이렇게 다른 어떤 것을 위해서 필요한 ‘상대적인 유용성’과는 다른 의미의 필요성을 또한 생각해 볼 수 있다. 만약 어떤 것이 다른 목적을 위해 존재하지 않고 자기 자신만을 위해서 있다면 그것은 그 자체로 가치 있는 것, 즉 (상대적인 것이 아닌) 무조건적으로 필요한 것이라 여겨지기 때문이다. 예를 들어 클래식 음악을 듣거나 연주하는 것, 미술작품들을 감상하거나 만들어 내는 활동 등은 단순히 돈을 벌기 위한 목적으로 하는 것이 아닌데 철학적 사고와 같은 것도 이런 심미적 활동과 비슷하다고 생각해 볼 수 있겠다. 이 경우 우리는 그런 활동들을 어떤 특정한 (재화를 얻기 위함과 같은) 목적을 위해서 하는 것이 아니라 그것을 그저 하고 싶어서, 혹은 그것이 주는 기쁨과 행복을 위해서 하므로 이는 그 자체로 의미 있고 가치 있는 활동이라 간주할 수 있다. 철학도 이와 같이 어떤 원리를 찾고자 하는 열망 그 자체를 따른다거나 합리적인 사고를 일관성 있게 견지하려는 태도와 성향에 의해서 자연스럽게 추구된다면 그 자체로 의미 있는, 자족적인 활동이라 볼 수 있을 것이다. 인간이 사실 ‘철학적 기질’과도 같은 것을 가지고 있다는 것을 부정하기는 힘들다. 인간을 흔히 ‘이성적 동물’이요, ‘생각하는 존재’라고 부르는데 이는 그만큼 인간의 사고 능력이 인간의 특징을 잘 표현하고 있기 때문이다. 그래서 이 사고하는 능력과 이에 근거한 철학적 활동을 인간이 행사하는 것은 자연스러운 일처럼 보이고, 마치 육체를 이용한 직립보행이나 코와 호흡기 등을 이용한 호흡과 같이 인간에게는 당연한 일로 받아들일 수도 있을 것이다. 하지만 이를 일반적으로가 아니라 ‘철학’적으로, 즉 최고의 원리를 발견하겠다는 정도로까지 추구해야 하는지는 별개의 문제처럼 보인다. 왜냐하면 일반적인 분과 학문들을 통해서 우리는 이미 우리의 삶을 윤택하게 해 주는 탁월한 ‘실용적인’ 원리들을 얻을 수 있기 때문이다. 따라서 ‘학문 일반’으로서의 처음의 철학의 개념은 무리 없이 수용될 수 있겠지만, 근대 이후의 철학의 개념인 ‘근본학’의 당위성과 필요성은 아직 확보되지 않은 것처럼 보인다. 게다가 우리가 어떤 특정한 속성을 가지고 있다고 해서 그것의 당위성이 반드시 보장되는 것도 아니다. 인간의 여러 속성들 중 어떤 것은 일정한 규칙과 조건에 따르는 한에서만 허용되는 것들도(쾌락에 대한 욕구 등) 있고 경우에 따라서는 엄격히 제한되는 것들도(폭력성과 같은 경우) 있기 때문이다. 그러므로 인간의 사고하는 능력도 어쩌면, 그것이 우리에게 언제 어떻게 그리고 어느 정도로 필요한지를 따져봐야 할지도 모른다. 더 나아가 이런 관점으로 보아 철학적 사고를 인간의 여러 속성들 중 하나로 보게 된다면 그 의미는 제한적일 수밖에 없다. 음악이나 미술 등을 어떤 이는 좋아하고 어떤 이는 좋아하지 않듯이 철학도 그것을 하고 싶어 하는 사람도 있고 전혀 매력을 느끼지 못하는 사람도 있을 것이다. 따라서 철학에 전혀 관심이 없는 이에게 철학의 필요성을 설득력 있게 설명하기란 매우 어려운 일이고, 그에게 있어서 철학이란 그것을 선호하는 일부 사람들의 ‘취향’에 의거한 활동일 뿐이지 모든 사람들에게 반드시 필요한 것은 아닐 것이다. 그런데 이제 우리가 진정으로 찾고자 하는 것은 우리 인간에게 철학이 ‘반드시’ 필요한지에 대한 진지한 대답이지 누구에게는 필요할 수도, 또 누구에게는 필요하지 않을 수도 있다는 소극적인 대답이 아니다. 그래서 이제는 혹시 철학이 인간에게 필연적이고 불가결하다는 것을 밝힐 수는 없는지 한 번 생각해 보기로 하자. 우리는 어느 속성이나 활동이 어떤 것에게 반드시 필요한 경우 이를 그것에게 없어서는 안 되는 성질, 즉 필연적인 본성이라고 부른다. 즉, ‘어떤 것이 그것이기 위해서 꼭 필요한 무엇’이라는 개념으로 이해될 수 있겠다. 이는 새에게 있어서 ‘날다’라는 특징, 물고기에게 있어서 ‘헤엄치다’라는 특징과도 같은 것으로서 ‘본질’이라는 개념으로 대치될 수도 있다. 따라서 철학이 우리에게 반드시 필요한 것이라고 말할 수 있으려면 그것이 사람에게 본질적인, 혹은 없어서는 안 되는 것임을 밝힐 수 있어야 한다. 철학은 인간 자신을 포함한 세계와 우주의 근본원리에 대한 탐구라고 했으니, 이제 우리는 인간이라는 존재가 그러한 것을 필연적으로 추구하는 존재임을 밝혀야 할 것이다. 지식의 한계와 철학적 반성 인간은 과연 자신에게 주어진 세계와 우주를 반드시 근원적으로 밝혀야만 하는가? 인간은 왜 눈앞에 보이는 것, 벌써 드러난 것, 이미 아는 것에 만족하지 않고 그것의 ‘진상’을 계속해서 파악하려 하는가? 그 이유는 인간에게 주어지는 세계의 모습이 완전하지 않고 우리가 그것에 대해 알고 있는 것이 우리가 (처음에) 믿는 것처럼 그렇게 확실하지 않다는 데에 있다. 우리는 이 세계를 경험할 때 (처음의 인상과는 달리) 그것이 그저 부분이며 파편에 불과할 뿐임을 알게 된다. 우리 앞에 있는 하나의 책상과 같은 경우, 처음에는 그것이 무엇인지를 누구나 쉽게 파악할 수 있다; 우리는 그것이 무엇을 쓰거나 읽을 때 사용하는 가구의 일종이라고 즉각적으로 알 뿐더러 그것의 모양, 색, 질감 등을 어렵지 않게 파악할 수 있다. 하지만 그것을 자세히 들여다보면 처음의 인상과는 다른 것들을 무수히 발견할 수 있을 것이다; 그것이 약간 흔들거린다거나 표면이 상해서 온전한 색감을 갖고 있지 않다거나 아니면 모서리에 긁힌 자국들이 있다거나 하는 점들을 곧 발견할 수 있다. 또한 다른 사람은 나와는 다소 다른 인상과 내가 발견하지 못했던 점들을 내게 전해줄 수도 있다. 그리고 책상을 ‘개념적으로’ 온전히 파악하기 위해서 노력해도 사정은 마찬가지이다. 책상이라는 것을 더 잘 알기 위해서 우리는 그것을 포괄하는 개념인 ‘가구’를 떠올릴 수 있겠고 그것보다 더 큰 개념인 ‘사물’이나 ‘물체’, 심지어 ‘존재’ 등에 이르게 된다. 그래서 책상이라는 구체적인 하나의 사물은 그보다 훨씬 추상적이고 보편적인 개념들을 포함하고 있는데 이런 추상적인 개념들에 대해서 우리가 그렇게 잘 알고 있는지 의심스럽다. 자연과학적 방법을 써도 마찬가지로 책상을 이루는 근원적인 구성요소인 분자, 원자, 심지어는 미립자 등으로 우리의 탐구는 끝없이 거슬러 올라갈 텐데 그런데도 우리는 그것에 대한 파악이 완전히 끝났다고 안심하지는 않을 것이다. 그래서 결국 이렇게 책상이라는 단순한 개별 사물에 대한 우리의 지식조차도 끝없이 새롭게 나타나고 변하는 감각적 정보들, 지극히 보편적이고 추상적인 개념들, 그리고 아직까지 모두 밝혀내지 못한 물리적인 속성들 등에 기초하고 있으므로, 그것에 대한 우리의 지식이 처음의 확신과는 달리 정말 확실한 것인지 의심스러워진다. 이렇게 우리는 우리의 경험과 지식이 제한적임을 알게 되는데 이는 우리가 이 세계와 그것에 속한 것들을 그저 제한적으로만 경험하고 알 수 있기 때문이다. 처음에는 이를 깨닫지 못하겠지만 계속되는 경험과 실질적인 필요에 의해서 그리고 우리 자신의 반성적인 사고에 의해서 우리는 우리가 알고 있다고 생각하는 것이 보다 광범위한 실재의 단편에 불과하다는 것을 깨닫게 될 것이다. 이를 통해 우리가 우리의 이런 한계를 직면했을 때 또한 이를 넘어서서 전체적이고 완전한 지식을 갖고자 하는 열망이 생기게 되는 것이다. 이성의 본질적인 구조와 진리의 개념 이러한 사정을 염두에 두고서 고대 그리스에서는 ‘진리’라는 말을 일찍이 ‘aletheia’라고 표현했다. 이 단어에 포함된 ‘lethe’는 본래 그리스 신화에 나오는 망각의 강을 의미한다. 영혼이 새로 태어난 인간의 육체에 깃들기 전에 마셔야 하는 강물인데 이를 마시면 영혼은 이전에 자신이 알고 있던 모든 것들을 잊게 된다고 한다. 그래서 부정의 의미를 가진 접두어 ‘a’가 그 앞에 붙으면 망각으로부터의 탈출, 즉 ‘상기’를 의미한다. 진리, 즉 참된 실재는 이때까지 잊었던 것, 숨겨졌던 것, 은폐되었던 것으로부터의 벗어남을 의미하는 것이다. 우리가 처음 맞닥뜨린 불완전한 현상, 전체가 은폐된 상태, 그저 부분적인 진상으로부터 완전하고 온전한 진리를 향해 나아가는 것이 바로 인간 이성의 본질적인 구조인 셈이다. 따라서 인간은 주어진 세계로부터 얻는 자료에 만족하지 않고 그것에 숨겨진, 아직 드러나지 않은 본래의 모습을 밝혀내려는 본성을 갖고 있고 이는 곧 철학의 모습 그 자체이다. 완전한 지식을 얻기 위해서 철학은 우리가 일차적으로 얻는 감각적 자료를 넘어서서 그 배후를 묻게 되고, 누구나 갖고 있는 상식과 기본 신념을 의심하여 그것이 과연 정당한지를 묻는다. 그래서 보다 완전한 지식, 보다 확고한 기초를 찾으려고 노력하는데 이는 타 학문에 대한 근본학으로서의 철학의 위치와 역할에도 부합하는 것이다. 철학적 태도란 무엇인가 철학의 이러한 경향이 반드시 확실한 결과를 가져오는 것은 아니다. 성과의 측면에서 보았을 때 철학은 오히려 미진한 면을 보이기에, 앞서 말한 대로 그것은 ‘유용한’ 도움을 인류에게 전혀 주지는 못한다. 철학의 진정한 가치는 진리를 향한 태도 그 자체에 있는 것이고 끊임없는 문제제기에 그 의의가 있을 것이다. 소크라테스가 말했듯이 그것은 ‘망각’의 늪에 빠져 있는 안일한 정신을 등에와도 같이 톡 쏘는 역할을 할 것이다. 주어진 현실에 안주하고 그것이 ‘전부’라고 생각하는 이에게는 철학이 성가시고 귀찮은 훼방꾼에 불과할 것이다. 또한 기존의 선입견과 상식을 버리기 위해서는 많은 용기가 필요하고 자연히 갈등도 뒤따르기 마련이다. 그럼에도 불구하고 우리가 이를 극복하고 미지의 세계로 나아가려 한다면 그에 걸맞은 용기를 또한 필요로 하겠고 기꺼이 모험을 받아들이겠다는 개방적인 태도도 역시 요구될 것이다. 따라서 철학에 관심을 갖고 그에 맞는 사고와 태도를 갖겠다는 것은 오직 이러한 정신을 갖고 철학이 가져다 줄 어려움마저 수용하겠다는 결심까지도 의미할 것이다. 참조: 철학이란 무엇인가, 호세 이 가세트, 5강 철학의 필요성. 민음사 2006. 철학이란 무엇인가, 러셀, 1장 현상과 실재, 15장 철학의 가치. 문예출판사 1977.",
      "frontmatter": {}
    },
    "03.플라톤과 좋은 삶": {
      "path": "/06.university/철학과-인간/03.플라톤과-좋은-삶/",
      "filename": "03.플라톤과 좋은 삶",
      "content": "1\\. 플라톤의 대화편에서 소크라테스는 정의, 용기 혹은 경건과 같은 주제들을 다루면서 우리가 이와 같은 것들이 무엇인지 알아야 그러한 덕목을 갖출 수 있다고 얘기한다. 그래서 이들의 의미를 정확히 밝히고자 하는데 이 과정에서 이데아라는 것을 가정하게 된다. 먼저 그는 각 대화편들에서 문제가 되는 주제의 다양한 예들을 열거한다. 그리고는 그 모든 예들에서 문제의 덕, 경건, 정의, 용기 등이 모두 발견되는 것을 지적한다. 하지만 그 구체적인 예들이 곧 그 덕목 자체는 아니라는 것도 언급한다. 따라서 그런 구체적인 경우들 외에 덕 그 자체가 따로 있지는 않을까하는 발상을 갖게 된다. 대화편 『에우튀프론』의 경우 토론되는 주제는 경건이라는 덕목인데, 경건에 대한 여러 사례들을 에우튀프론으로부터 들은 소크라테스는 다음과 같이 얘기한다. “자네도 기억하다시피 내가 자네에게 수많은 경건한 것들로부터 하나 혹은 두 개의 경건한 것들을 가르치려는 것이 아니라 모든 경건한 것들이 이를 통해서 경건하게 되는 형태(eidos) 그 자체를 가르치려 하지 않았나? 왜냐하면 자네도 인정했다시피 모든 경건한 것들이 경건한 것은 그것들이 갖고 있는 어떤 하나의 모습(idea) 때문이지 않은가?\" (6d9이하) “내가 이 형태 자체를 보면서 (모든 경건한 것들의) 원형으로 사용할 때에, 이 형태가 내게, 무엇을 자네(또는 다른 이들)의 행동들에서 경건한 것으로, 또 어떤 것은 그렇지 않은 것으로 설명할 수 있을지를 가르쳐 준다네.” (6e4이하) 여기서 소크라테스는 여러 다양한 경건한 행위들과는 구분되는 경건함 그 자체, 경건함의 원형이 있고, 이를 통해서 비로소 여러 행위들이 경건해 질 수 있는 것이 아니냐고 묻는다. 경건한 행위들은 다양하고 천차만별이지만 경건함 자체는 하나이고 동일한 것이다. 어떤 행위가 이 경건함의 모습, 원형을 그 안에 가지고 있어야 경건한 행위가 될 수 있으므로 우리는 경건함 자체의 모습을 이미 알고 있어야 이를 기준으로 어떤 행위가 경건한지 아닌지를 알 수 있다. 2\\. 그의 이 생각을 다음의 예를 들어 구체적으로 살펴보기로 하자. “이 포도주는 훌륭하다” “그 운동선수는 훌륭하다” 이 두 문장에서 우리는 ‘훌륭하다’라는 술어가 ‘포도주’와 ‘운동선수’라는 상이한 개별적 대상에 대해서 아무런 문제없이 사용되는 것을 보았다. 그런데 그 이유가 이 포도주와 운동선수가 서로 어떤 밀접한 관계에 있거나 특별한 연관이 있어서는 아닐 것이다. 포도주와 운동선수는 각기 다른, 어떻게 보면 전혀 상관없는 개개의 대상들이다. 그럼에도 불구하고 왜 우리는 ‘훌륭하다’라는 동일한 서술어를 자연스럽게 이 둘에게 적용하는지를 플라톤은 묻고 있다. 또 다른 예를 들어 보자. “마이클 조단은 크다” “샤킴 오닐은 크다“ 각각의 문장에서 두 선수들은 모두 ‘크다’고 서술되고 있다. 2미터가 조금 안 되는 신장을 가진 농구 선수 조단을 보고 ‘크다’라고 한다면 아마도 대부분의 사람들이 동의할 것이다. 그런데 이제 2미터를 훌쩍 넘는 또 다른 선수 오닐이 그의 옆으로 다가 온다면 누구나 조단의 상대적으로 ‘작은’ 신장을 확인할 수 있을 것이다. 플라톤은 이처럼 여러 가지 속성들이 어떨 때는 그렇게 보이기도 하고 어떨 때는 그렇게 보이지 않기도 하다는 것에 주목하여 다음과 같이 묻는다. \"이 많은 아름다운 사물 가운데에는 추해 보이는 어떤 것도 있지 않을까? 그리고 정의로운 것들 중에는 정의롭지 않게 보이는 것 또한 있지 않을까?\" \"그러면 많은 두 배의 것들은 어떠한가? 그것들도 어떻게 보면 두 배 못지않게 반으로도 보이겠지?\" \"그리고 우리가 크다거나 작다거나 가볍다거나 혹은 무겁다고 부르는 것들의 경우 그것들이 (그 반대로 불리지 않고) 꼭 그렇게 불리어져야만 할 정당성이 있는 것일까?\" 『국가』 (479a이하) 플라톤은 우리가 이처럼 상이한 대상들과 다양한 맥락들 속에서도 자연스럽게 여러 가지 개념들과 의미들을 문제없이 언급할 수 있는 이유를 그것들의 원형에서 찾고자 했다. 우리가 생각할 수 있는 모든 속성들에는 그것의 원형들이 있기에 우리는 그에 힘입어 여러 사물들을 그러하다고(훌륭하다거나 크다거나) 여기는 것이다. 이런 공통된 기반이 있기에 서로 전혀 다르고 연관성도 없는 대상들에 대해서 그 서술어가 공통으로 사용될 수 있다는 것이다. ‘맛있음’, ‘큼’, ‘붉음’, 심지어는 ‘인플루엔자’와 같은 개념들은 모두 어떤 특정한 내용의 의미이자 원형으로서 여러 관계없는 개별적 대상들이 때에 따라 그렇게 되는 것을 가능하게 한다. 그리고 개별적인 대상들, 가시적인 사물들은 그 자체로 어떤 성질을 갖는 것이 아니라고 볼 수 있다. 마이클 조단이 홀로 일반인들 사이를 거닌다면 우리에게 그는 ‘크다’고 보이겠지만, 그가 농구코트에서 다른 장신 선수들 사이에서 공을 리바운드 하려 한다면 우리는 그 즉시 그의 ‘작음’을 알아차릴 것이다. 따라서 ‘크다’는 속성은 조단 자신에게 필연적으로 속한 속성이 아니다. 그것은 제한적으로만 그에게 귀속되었을 뿐이고 오닐과의 비교에서는 더 이상 그에게 속하지 않는 것이다. 그러므로 조단은 그 자체로, 항상, 혹은 본질적으로 큰 것이 아니다. 이 ‘큼’의 원형은 조단 안에 있지 않고 따로 있어서 때때로 그에게 ‘크다’는 속성을 가능하게 한다. 또한 조단, 포도주 등의 개별적 대상에서 ‘크다’, 혹은 ‘훌륭하다’와 같은 속성이 나타나는 것은 사실이지만 ‘큼’, 혹은 ‘훌륭함’ 그 자체가 완벽하게 구현되지는 않는다. 어떤 개념과 완전히 동일한 실제 대상은 자연 세계에 없으며 따라서 이들의 원형은 감각으로 경험할 수 있는 현실에서는 찾을 수 없다. 여러 가지 개별대상들에서 이데아는 완전히 나타나지 않고 단지 부분적으로만 나타날 뿐이다. 이처럼 가시적 사물들이 본질적으로 크거나 작은 것도 아니고, 또한 ‘큼’, ‘작음’ 자체가 가시적 사물들과 별개로 존재한다면, 가시적 사물들은 도대체 어떻게 경우에 따라 ‘크다’거나 혹은 ‘작다’고 서술되며 또한 그렇게 나타나는 것인가? 플라톤은 가시적 사물들이 이데아에 참여(Methexis)하기 때문에 그렇다고 대답한다. 조단이 컸던 이유는 그가 ‘큼’ 자체에 참여하고 있었기 때문인데 오닐이 이제 그보다 ‘크다’고 나타나는 이유는 그가 ‘큼’의 이데아에 (조단보다 더 많이) 참여하고 있기 때문이다. 따라서 상식적으로는 다소 이상하게 들릴지 모르겠지만, 조단은 그 자신이 본래(본질적으로) 크기 때문에 큰 것이 아니라 어느 정도로만 ‘큼’의 이데아에 참여하기 때문에 크다고 말할 수 있을 것이다. 이데아와 가시적 사물들의 관계는 또한 사물과 그림자의 비유로도 묘사된다. 이것은 마치 화가가 어떤 대상을 본으로 해서 그것을 똑같이 그리는 것과도 같아서, 이데아가 원형인 반면 가시적 사물들은 그것의 그림자, 혹은 본뜬 모습에 불과하다는 것이다. 그런데 그림이란 실재하는 사물의 오직 한 측면만을, 혹은 제한적으로만 보여줄 수 있듯이 이데아는 그것을 모사한 사물들보다 더 온전한 존재이고 더 충만한 내용을 갖고 있다. 3\\. 4가지 관점을 통해 이데아를 구체적으로 살펴 본 내용은 교재 136-149p.를 참조하시기 바랍니다. 4\\. 이데아의 인식에 관하여 우리가 어떻게 이 이데아가 있다는 것을 알게 되며 또 어떻게 그것에 대한 인식에 이를 수 있는가를 위한 방법으로서 플라톤이 생각한 것은 대화하는 기술로서의 변증법이다. 여기서 변증법은 논의의 주제와 대상에 대해 적절하게 대화하는 기술, 또한 논의 대상에 대해서 그것이 무엇인지 해명하고 변론하는 기술을 의미한다. (헤겔이나 마르크스 등이 거론하는 변증법은 다소 다른 의미이다. 플라톤에서도 두 가지 의미의 변증법이 있는데, 논리학의 초기 형성단계에 해당하는 변증법은 대상들을 분류하여 하나의 특정한 개념에 이르려는 시도를 뜻하고 주로 플라톤의 후기 사상에서 많이 발견되는 반면, 전기 사상에서 변증법이라 지칭되는 것은 이 논쟁술에 해당한다) 플라톤의 대화편에서 항상 대화를 이끌어 나가는 소크라테스는 상대방과 의견을 주고받으며 어떻게 하면 주제에 대한 참된 지식에 이를 수 있는가를 모색한다. 그 첫 번째 단계에서는 우선 대화 주제에 대한 정의를 내리기 위해 그에 대한 물음을 던지며 이에 대한 상대방의 대답들을 듣는다. 그리고는 그 대답들이 서로 모순 없이 합치하는지를 검토하는데 늘 부정적인 결론을 도출한다. 대화 상대방의 지식들은 대부분 근거 없거나 상호 모순적인 의견들이기 때문이다. 그래서 소크라테스는 이 의견들의 잘못된 점들을 일일이 지적하고 논박해서 결국 해결책이 보이지 않는 것처럼 보이는 난해한 상황(아포리아)으로까지 이끈다. 변증법의 우선적인 본질은 바로 이런 논증 방식에 있다. ‘elenchos’라고도 부르는 이 논쟁을 통해서 기존의 의견과 주장들은 철저히 검증돼서 (한갓 추정이나 믿음들뿐이라는) 그 정체가 밝혀진다. 이렇게 대화 상대방은 자신의 헛된 지식이 논증에 의해서 파괴된 후 자신의 무지를 인식하게 된다. 잘못된 속견들이 이렇게 제거된 후에야 그는 참 지식에 도달하기 위한 준비가 된 것이다. 이 논증을 통해서 그의 영혼은 정화된다고 플라톤은 생각했고 이제 지금까지 그를 얽어맸던 잘못된 의견들로부터 해방된 대화 상대방은 참된 지식이 무언지를 배우려는 자세를 갖게 된다. 이후 ‘자유롭게 된’ 영혼은 계속되는 비교와 검증을 통해서 보다 일반적인 개념으로 거듭해서 올라가게 되고 종국에는 이데아의 세계에 대한 지식에 이를 것이라고 플라톤은 서술한다. 그 구체적인 방법보다 여기서 더 중요한 것은, 참된 지식을 위한 여정은 감각세계를 떠나 이데아의 세계로 향해야 한다는 것이다. 플라톤은 가시적인 세계에서는 참된 지식과 실재적인 대상들을 찾을 수 없다고 보았는데 이를 설명하기 위해서 그가 끌어들인 것은 거의 신화적인 색채를 띤 이야기였다. 그는 우리가 죽었을 때에도 우리의 영혼은 남아서 이데아의 세계로 돌아가 있다가 가시적인 세계의 육체라는 감옥에 다시 돌아오는데 이 때 이데아에 관한 지식을 모두 잊는다고 했다. 그래서 가시적인 세계에서는 이데아에 대한 뚜렷한 인식은커녕 마치 어두컴컴한 동굴 속에서 동굴 입구에 위치한 모닥불의 불빛에 비친 그림자들만을 보는 것과 같이 혼미한 인식만을 갖는다고 하였다. 그러다가 앞서 말한 변증법적 논쟁 등에 의해서 영혼이 정화되는 계기를 갖고서 감각적인 지식으로부터 해방되면 이전의 지식을 상기하여 이데아를 다시 알아보게 된다고 생각했다. 그래서 동굴과도 같이 불분명한 지식의 상태에서 벗어나 점차 태양이 내리쬐는 밝은 곳과도 같은 진리의 세계로 나와서 이데아를 바라보고 마침내 참된 지식에 도달할 것이라는 생각을 하였다. 『플라톤과 좋은 삶』 관련 참조: 고대 그리스 철학. 프리도 릭켄. 서광사 2000. 111 – 134쪽, 207 – 236쪽. 희랍철학입문. 서광사 2000. 112 – 133쪽, 159 – 182쪽.",
      "frontmatter": {}
    },
    "04.인간과 로고스": {
      "path": "/06.university/철학과-인간/04.인간과-로고스/",
      "filename": "04.인간과 로고스",
      "content": "1) 지식의 발전 단계 인간이 자연을 있는 그대로 보는 데에 그치지 않고 그것의 근본 원리를 묻게 된 이유를 아리스토텔레스는 다음과 같이 설명하고 있다. 원시 상태에 있는 무지한 인간은 모든 자연현상을 놀랍고 경이로운 눈으로 바라보았을 것이다. 그렇기에 이를 이해하고 설명하는 데에 있어서 어려워하고 당혹스러워 했을 것이다. 하지만 동시에 그것이 왜 그런지 알고 싶어 하는 호기심도 생겼을 것이다. 그래서 그것의 원인과 원리를 알고자 하는 마음 역시 생기게 되었을 것이고 이것이 바로 본격적인 앎의 시작이라고 아리스토텔레스는 추측하고 있다. 그리고 이 앎이 심화되고 그 단계가 높아지면 우리 인간이 처음에 가졌던 경이로운 감정과 호기심은 점점 소멸될 것이다. 이제 이 지식이 발전하는 모습을 그는 다음의 네 단계로 그리고 있다. ① 감각의 단계. 이 단계에서 인간은 감각 기관을 통한 지각이라는 가장 단순한 형태의 앎을 갖고 있다. 이 단계에서 인간은 아직 동물과 구분이 되지 않는다. ② 기억의 단계 일부 고등동물들은 기억의 능력을 갖고 이를 통해서 학습할 수 있다. ③ 경험과 기술의 단계 지속적이고 반복된 감각적 지식을 갖고 이를 기억하게 되면 그에 관한 경험의 능력에 도달할 수 있다. 과거에 이미 지각했던 적이 있는 어떤 사물이나 사태를 접하면 인간은 과거의 기억에 힘입어 그것이 무엇인지를 즉각적으로 알고 그것을 어떻게 ‘처리’해야 하는지 알 수 있다. 이런 경험들이 하나하나 쌓이면 유사한 사물들과 사태들에 대한 하나의 보편적인 판단이 세워지는데 이를 ‘기술’이라 한다. 경험이 그저 개별적인 대상들에 대한 앎이라면 기술은 보편적인 지식에 해당한다. 기술을 가진 사람은 사물이 왜 그런지 그 원인을 아는 사람이다. ④ 지혜의 단계 기술은 삶에 필요해서 발명되었거나 쾌감의 충족을 위하여 활용되지만, 그런 실천적인 목적 없이 그저 이론적이기만 학문이 이제 등장하게 된다. 그런데 손만 사용하는 기술보다 통치하는 기술이 더 상위에 있고 더 지배적인 위치에 있다고 말하듯이, 다른 유용성에 얽매여 있지 않으며 그 결과 때문에 선택되지 않는, 앎 그 자체만을 위한 학문이 더 자유롭고 더 지혜로운 학문이라고 불릴 수 있다. 지혜를 통하여 파악된 가장 보편적인 것, 가장 제일 가는 원리와 원인은 그 밑의 다른 모든 구체적인 원인과 원리들에 선행하고 그것들을 포섭한다. 그러므로 그런 학문이 인간의 앎의 과정에서 최상위에 위치할 것은 쉽게 추측 가능하다. 아리스토텔레스는 형이상학이 바로 이 지혜의 단계에 있는 학문이라고 생각했기 때문에 형이상학을 통하여 존재하는 것들의 가장 보편적이며 원리적인 지식을 갖게 될 것이라고 생각했다. 그런데 그에게 있어서 무엇인가를 안다는 것은 그것의 원리와 원인을 안다는 것을 뜻했기 때문에 존재의 가장 근본이 되는 원인은 무엇인지를 우선 살펴보고자 한다. 2) 자연의 네 가지 원인 아리스토텔레스는 자연에는 다음의 네 가지 원인이 있다고 한다. ① 질료인: 한 사물이 그로부터 생겨나는 것과 산출물 안에 구성요소로서 들어 있는 것. 예) 청동은 청동상의 질료(인)이다. ② 형상인: 형상이나 유형, 즉 문제가 되는 사물이 무엇인지에 대한 규정. 형태, 본질. ③ 작용인: 운동이나 정지의 근접적인 근원인 것. ‘운동인’이라고도 한다. 예) 아버지는 아이의 작용인이다. ④ 목적인: 목적이나 목표. 예) 건강은 내가 운동을 하는 목적이다. 나는 건강을 위해서 운동을 한다. 이렇게 네 가지 원인을 설명하고 난 뒤 그는 혹시 다른 이들은 이 원인들 말고 다른 원인들을 찾아서 이를 통해 자연을 설명했는지를 알고자 했다. (결론적으로 그는 그가 제시한 네 가지 원인들 이외의 다른 원인을 찾은 사람은 아무도 없었다고 한다) 그래서 이전 철학자들의 이론들을 하나하나 살펴보는 절차를 밟는다. i) 자연철학자들에 대한 비판 먼저 탈레스를 위시한 최초의 ‘자연철학자들’의 경우를 살펴보았는데, 그들은 물질의 성질을 띠고 있는 원리들만을 모든 사물의 원리라고 생각했음을 그는 지적한다. “최초의 철학자들 대부분은 오직 질료적인 것만을 만물의 원리로 여겼다.” 따라서 이들은 아리스토텔레스가 말한 원인들 중 오직 질료인만을 자연의 원인이라고 생각한 것이다. 그러므로 그들이 사물의 본성, 혹은 본질에 대해서 언급하지 않은 것은 어쩌면 당연한 일일 것이다. 이런 방식의 설명은 ‘어떻게?’라는 물음에 대한 답은 될 수 있겠지만 ‘왜?’라는 물음에 대해서는 아무런 답도 줄 수 없다. ii) 플라톤의 이데아론에 대한 비판 아리스토텔레스는 플라톤의 이데아론에 의한 설명은 ‘왜 어떤 것이 바로 그것인가’에 대한 답으로는 충분하다고 말한다. 플라톤의 이론은 감각적인 사물, 개별적인 것들에 대한 나열한 기술에 그치는 것이 아니라 보편적인 정의나 본성과 같은 것을 언급함으로써 자연의 형상인에 관한 설명으로서는 충분하다고 아리스토텔레스는 평가한 것이다. 하지만 아리스토텔레스에 의하면 플라톤은 그 자체로 실재성을 갖는, 즉 정말로 존재하는 이데아가 감각적인 사물과 어떻게 관계하는가에 관해서는 자세히 설명하지 않았다고 비판한다. 플라톤은 물론 참여, 분유, 모사 등의 개념들로써 이 둘의 관계를 설명했지만 아리스토텔레스는 이것을 그저 비유에 불과한 것이라고 치부한다. \"이데아는 원형이고 다른 사물은 이데아에 참여하고 있다는 말은, 알맹이가 없는 말이고 시적 비유를 쓰고 있는 것이다.“ 아리스토텔레스는 이데아라는 것이 감각적으로 지각할 수 있는 사물들을 위해서, 그리고 생성과 소멸에 예속돼 있는 사물들을 위해서 도대체 무슨 일을 할 수 있느냐고 묻고 있다. 왜냐하면 이데아는 이 사물들의 운동이나 변화의 원인이 아니기 때문이다. 그리고 이데아는 사물들을 인식하는 데에도 도움이 되지 않고, 이데아로부터 다른 사물들이 생겨나는 것도 아니기에 이 둘의 관계는 그저 모호할 뿐이라고 했다. 그래서 플라톤은 가시적 사물의 진정한 원인을 찾겠다고는 했지만 실상 그것과는 전혀 관계가 없는 것을 그것의 원인이자 근거인 양 주장했을 뿐이라고 아리스토텔레스는 결론내리고 있다. 이렇게 이전 철학자들의 주장들이 가진 문제점들을 지적하고 난 뒤 그는 이제 존재에 관한 올바른 연구는 무엇인지에 대해서 설명하고 이에 맞춰 자신의 이론을 제시하고자 한다. 3) 제 1 철학으로서의 형이상학 i) 존재론의 가능성 아리스토텔레스는 “존재, 특히 그것이 존재인 한에서의 존재의 원리들과 원인들”이 형이상학의 탐구의 대상이라고 했다. 이 때 “존재(자)로서의 존재(자);on hei on”란 어떤 의미인가? 그는 이를 다른 학문들과의 차이를 통해서 설명하고 있다. 형이상학은 존재의 원리와 원인을 찾는 학문이라고 했는데 사실 다른 모든 학문들도 제각각 원리와 원인을 구하고 있다. 하지만 다른 학문들은 특정한 조건과 전제를 가지고서 특정한 대상만을 다룬다. 자연학은 자연에 관한 탐구일 뿐이고 수학은 양적 존재만을 탐구한다. 반면 형이상학은 개별적인 분야들을 연구하는 것이 아니므로 어떤 영역에 제한되지 않는다. 존재 개념은 특정한 존재 양식을 지칭하는 범주들을 넘어서며 또 이들을 포괄한다. 이런 의미에서 형이상학은 존재 그 자체와 존재 전체를 탐구한다. 그런데 이렇게 존재를 존재로서 탐구하고 존재 전체를 탐구하는 학문이 도대체 가능할 수 있을까? 이 물음도 다른 학문들과의 비교를 통해서 대답할 수 있을 것이다. 모든 학문들은 무릇 존재에 관해 말하고 존재와 연관된 개념들을(있다, 없다, 같다, 다르다 등등) 사용하고 있으므로 존재라는 개념을 이미 전제하고 있는 것이다. 그런데 그 어느 학문도 자신이 직접 대하는 주제 이상을 묻지 않고 또 물을 수도 없다. 그러므로 모든 대상들의 배후에 있는 존재라는 가장 보편적인 개념을 직접 연구하는 학문은 하나도 없는 셈이다. 그런데 무언가를 안다는 것은 그것의 원리와 원인을 안다는 것인데, 모든 대상들의 원리가 되고 원인인 것을 모르고서는 진정한 지혜를 갖지 못하는 것이다. 이런 이유로 해서 모든 것들의 제일가는 원리, 첫 번째 원인으로서의 존재에 관한 학문인 형이상학이 반드시 필요한 것이다. ii) 존재를 설명하는 원리로서의 질료와 형상 이제 이 존재가 무엇인지를 밝히기 위해 아리스토텔레스는 그 원리를 물었고, 이전 철학자들의 사상을 종합하여 ‘질료’와 ‘형상’이 바로 존재하는 모든 것을 가능하게 하는 원리라고 결론 내린다. 그 중 아리스토텔레스는 플라톤의 영향을 받아 여전히 형상이 질료보다 더 중요한 요소라고 보고 있기는 하지만, 형상 뿐 아니라 질료도 함께 있어야 어떤 것이 실제로 존재하는 것이 가능하다는 입장을 취하기에 플라톤과 결정적으로 다른 생각을 갖고 있는 셈이다. 한편 형상이 질료에 대해서 왜 우선하는지, 그리고 존재와 관련해서 어떤 중요한 역할을 하는지는 아리스토텔레스의 생성에 대한 설명을 살펴보면 분명해진다. 앞서 우리는 아리스토텔레스가 이전 철학자들을 비판하며 그들이 존재와 그것의 원인을 근본적으로 밝히지 못했다고 지적했음을 것을 보았다. 이제 그는 자신이 발견한 네 가지 원인들을 총동원해서 자연의 존재원리를 밝히면 이제껏 설명하지 못했던 현상들, 특히 플라톤이 밝히지 못했던 생성의 문제도 해결할 수 있다고 주장한다. 생성이란 아리스토텔레스에 의하면 “어떤 것에 의해서 어떤 것으로부터 어떤 것이 된다.”는 것을 의미한다. 이 때 우리는 그것이 무엇으로부터 생성되었는지를 물을 수 있다. 이에 대한 대답은 ‘질료’로서, 이는 어떤 것이 다른 어떤 것이 될 수 있는 가능성을 제공해 주는 하나의 존재원리이다. 의자는 나무로부터 생겨나고 나무가 없었다면 의자는 의자가 될 수 없었을 것이다. 그런데 이 ‘생성’의 원인을 묻는다면, 즉 어떤 것에 의해 그것이 생겨났는가를 묻는다면 이번에는 ‘형상’이라고 대답해야 할 것이다. 따라서 형상은 생성에 있어서 일종의 작용원인이 된다. 의자는 나무에게 의자의 형상이 부여됐을 때에야 의자가 될 수 있다. 비너스 여신의 형상이 없었다면 밀로 섬의 대리석 덩어리는 오늘날의 비너스 조각상이 되지 못했을 것이다. 이처럼 질료와 형상의 개념을 결합하면 생성이 어떻게 가능한지를 보여줄 수 있다고 아리스토텔레스는 생각했다. 형상은 또한 개별적 대상의 ‘존재’를 위한 원인이다. 어떤 개별적 대상이 바로 그것으로서 존재하려면 그 안에 형상이 일종의 질서와도 같은 역할을 해서 그 구성요소들이 통일체를 이루어야 한다. 따라서 형상은 개별적 대상에게 있어서 일종의 본질의 역할을 하면서 그것의 제 1원인이 된다. iii) 가능태(dynamis)와 현실태(energia) 질료와 형상이 어떻게 실체, 즉 개별적 대상을 가능하게 하는지를 더 구체적으로 설명해 주는 것은 존재의 가능태와 현실태 개념이다. 가능태와 현실태는 각각 존재의 어떤 상태들을 지칭한다. 가능태에 있는 어떤 존재는 그것의 본성과 본질, 그리고 능력을 아직 그대로 다 구현하지 못한 채 가능성으로만 있게 된다. 예를 들어 어떤 사람이 눈을 감고 있으면 그 사람은 비록 시각 능력은 갖고 있어도 지금은 그 능력을 사용하고 있지 않은 일종의 가능태의 상태에 있는 것이다. 어떤 나무토막은 완성된 조각상에 대해서는 일종의 가능태의 상태에 있는 것이다. 그런데 설령 가능태의 상태에 있다고 해도 그 존재는 이미 그 능력이 주어져 있는 것이고 단지 그것을 실천에 옮기지 않았을 뿐이다. 즉 아지 활동하고 있지 않은 것이다. 종종 형상과 동일시되는 현실태는 어떤 존재가 자신이 가진 가능성과 능력을 모두 발휘한, 자신의 본성을 모두 실현시킨 상태일 것이다. 그런데 아리스토텔레스에게 있어서 현실태는 가능태에 대해서 여러 모로 우선적이다. 어떤 씨앗을 심어야 그로부터 꽃이나 풀이 생겨나겠지만, 이 씨앗은 그것의 성장된 상태인 풀이나 꽃으로부터 유래한 것이다. 따라서 어떤 것을 낳는 것이 먼저 있어야 하듯이 현실태도 가능태보다 먼저 있어야 한다. 보다 중요한 존재론적인 의미로도 현실태는 가능태보다 우선해야 한다. 형상으로서의 현실태는 질료로부터 개별적 대상이 생성되게 하는 ‘원인’이므로 우선적으로 이 원인이 주어져야 존재도 가능할 것이다. 또한 자연의 모든 존재자들은 그들의 완성된 형태로 성장하고자 하는데 그것의 완성된, 완전한 원형인 형상, 즉 현실태는 가능태들에게 추구해야 할 목적으로서 기능하게 된다. 어떤 것의 목적은 그것보다 우선해야 하므로 이때에도 현실태의 가능태에 대한 우선성이 보장된다. 올챙이는 개구리가 되기 위해서 성장하는데 이 때 개구리라는 완전한 형태는 이미 올챙이 안에 가능성으로 주어져서 올챙이를 성장시키는 원인이자 그것이 되어야 할 목적도 되는 것이다. 그래서 결국 아리스토텔레스가 생각하는 자연의 모습은, 모든 가시적 사물의 구조적 원리로서 질료와 형상이 있는데 우선은 가능태로서만 있는 이 존재자들이 형상이라는 목적으로 나아가기 위하여 운동하고 변하는 상태라는 것이다. 이 설명 안에 자연 철학자들이 말한 질료적 원인도 포함되어 있고 플라톤의 형상도 존재와 생성의 원인이자 목적으로서 기능하게 된다. 따라서 아리스토텔레스의 형이상학은 존재의 원리와 원인 뿐 아니라 생성과 운동이 어떻게 자연 안에서 가능한지를 보여주는 폭 넓은 이론이 되었다. 참고서적: 고대 그리스 철학. 프리도 릭켄. 서광사 2000. 111 – 134쪽, 207 – 236쪽. 희랍철학입문. 서광사 2000. 112 – 133쪽, 159 – 182쪽.",
      "frontmatter": {}
    },
    "06.논리와 사고": {
      "path": "/06.university/철학과-인간/06.논리와-사고/",
      "filename": "06.논리와 사고",
      "content": "논리와 사고 우리는 평소 바쁜 일상생활 속에서 상대방의 주장이 합당한지를 따지기도 전에 관습이나 주변 상황 등에 의해서 판단하고 이에 따라 그의 주장에 반응하곤 한다. 엄밀하고 논리적인 사고와 판단에 따라 행동하는 경우보다 그렇지 않은 경우가 더 많은 것 또한 사실이다. 그래도 어떤 결정적인 순간들에는 반드시 다른 사람들의 주장이 옳은지 그 정당성을 따져야 할 때가 있고, 또한 자기 자신의 주장도 신중하고 합당하게 내세워야 할 때가 있다. 이를 위해서 우리는 평소 논리적이고 합리적으로 사고하는 훈련을 행해야 한다. 또한 평소에 비논리적이고 비합리적으로 살고 있기 때문에 더더욱 이런 훈련이 필요할지도 모른다. 사실 우리의 일상은 이런 비논리나 (논리적 사고를 압도하는) 다른 억지에 둘러싸여 있다. 그러므로 그런 흐름에 휩쓸리지 않도록 논리적 훈련과 자기성찰이 더욱 요구되는 것이다. 올바른 사고가 이루어져야 그 합리적인 기초 위에서 진정한 사회적 활동이 가능하고 다른 이들과의 건설적인 토론도 가능해 지기에 논리적 사고는 인간사회의 소통과 발전을 위한 가장 중요한 기초가 되는 것이다. 논리학의 개념 이렇게 중요한 논리에 관한 연구인 논리학의 개념을 살펴보자면, 그것은 우선 로고스라는 단어와 밀접히 연결되어 있음을 알 수 있다. 논리학(logica)은 말 그대로 로고스에 관한 학문이다. 그런데 로고스의 의미가 다양하듯이 논리학도 역시 광범위한 영역을 갖고 있다. 만약 자연에 중점을 두고 본다면 그것은 우주의 질서와 원리에 관한 학문이 될 테고 인간에 더 관심을 갖는다면 그것은 인간 이성에 관한 학문이 될 것이다. 근대 이전에는 주로 전자의 의미, 즉 세계의 시원 내지 원리의 학문으로 이해되었고 근대 이후부터 오늘날에는 주로 후자, 즉 사고 기술 내지 사고 법칙의 학문으로 주로 논의되었다. 논리와 사고 사고의 구성 사고의 기본원리와 법칙 사고의 기본방식과 단위 ※ 2-5장까지의 내용은 강의시간에 자세히 설명될 것입니다. ※ 아래 내용과 강의시간에 설명한 PPT자료를 또한 참조하시기 바랍니다. 판단들이 모여서 근거 있는 주장의 형태를 띨 때 우리는 이를 추론이라 부른다. 모든 주장은 동시에 그 근거도 제시해야 하는데 이 근거가 제시되는 방식에 따라 다음의 두 가지 대표적인 추론들을 구분할 수 있다. 1) 연역적 추론 추론은 전제와 결론의 형식으로 이루어진다. 하나 혹은 여럿의 전제에 의거해서 결론인 주장을 이끌어 낼 때 그 결론이 이미 전제에 포함되어 있다면 우리는 이를 연역적 추론이라 부른다. i) 모든 사람은 죽는다. ii) 소크라테스는 사람이다. iii) 소크라테스는 죽는다. 위의 세 문항에서 위의 두 문항이 전제인 셈인데 사실 첫 번째 문장인 ‘모든 사람은 죽는다’라는 전제에 결론인 ‘소크라테스는 죽는다’라는 주장이 이미 포함되어 있는 것이다. 만약 모든 사람이 죽는다는 것이 참이라면 한 사람인 소크라테스가 죽는 것은 필연적이기 때문이다. 2) 귀납적 추론 반면 귀납적이라 불리는 형태의 추론에서는 경험적 사실들을 근거로 해서 하나의 결론을 이끌어 내는 형태를 갖고 있는데, 이 때 등장하는 결론은 언제나 전제들을 넘어서는 주장이며 연역논증에서와 같이 전제들에 포함되어 있지 않다. i) 철수는 근면성실하다. ii) 영희는 근면성실하다. .... 한국 사람은 근면성실하다. 상기한 추론은 많은 한국 사람들에게서 근면성실함을 발견한 후 그로부터 한국 사람들은 근면성실하다는 결론을 이끌어 내고 있는데, 비록 아무리 많은 한국 사람들의 경우를 근거로 끌어 들인다 하더라도 그로부터 ‘모든 한국 사람은 근면성실하다’라는 결론이 도출되지는 않기 때문에 연역적 논증과 같은 필연성을 가질 수는 없다. 따라서 하나의 보편 명제를 결론으로 주장하려는 귀납적 추론은 오직 확률적인, 즉 개연적인 정당성만을 가질 수 있을 뿐이다. 이처럼 연역적 추론과 귀납적 추론은 전제가 결론을 지지하는 정도에 따라 구분된다. 연역적 추론은 전제가 참이라는 것이 보장되기만 한다면 그 결론도 확실히 참이 되는 장점이 있기는 하지만 참임이 확실한 전제를 찾는 것이 항상 쉬운 일은 아니다. 예를 들어 ‘신’, ‘존재’, ‘정의’ 등과 같이 추상적이거나 보편적인 개념들에 대한 (항상) 참된 명제란 사실상 없다고 해도 좋은데 고대와 중세의 숱한 이론들은 그런 종류의 전제에 근거한 결론들을 마치 정당한 것인 것 마냥 주장하는 일이 비일비재 했다. 한편 귀납적 추론은 그런 종류의 참된 보편 명제를 미리부터 전제할 필요가 없으며 개별적이고 단편적인 명제들이나 사실들로부터 출발해도 되는 절차상의 수월함도 있고 또한 그로부터 도출되는 결론도 경험에 대한 어느 정도 의미 있는 명제가 되는 장점이 있지만 그 주장이 갖고 있는 제한적인 개연성으로 인한 한계 또한 갖고 있다. 근대에 들어서 자연과학의 발달과 함께 귀납적 추론이 하나의 학문적 방법론으로 정립되기도 했는데 우리는 앞으로 이에 대해서 자세히 배우며 또한 그것이 갖는 문제점을 지적할 기회를 가질 것이다.",
      "frontmatter": {}
    },
    "07.1.고대에서 근대로": {
      "path": "/06.university/철학과-인간/07.1.고대에서-근대로/",
      "filename": "07.1.고대에서 근대로",
      "content": "고대에서 근대로 / 존재론에서 인식론으로 우리는 이제 철학의 다음 시대인 근대로 넘어가 인간과 철학에 대한 새로운 사상들을 알아볼 텐데, 지금까지 살펴본 고대 그리스 시대의 이론은 형이상학과 존재론이라는 명칭으로 주로 불렸다면, 근대의 이론들은 인식론이라고 불리게 된다. 따라서 철학과 인간에 대한 논의도 이제 인식이라는 주제와 관점 아래에서 진행된다. 인간의 인식에 대한 본격적인 논의는 근대에서나 비로소 시작되기에 지금까지와는 다른 지평에서 철학이 전개되며, 이를 사전지식 없이 바로 접하면 그 이해가 어려울 수 있다. 이처럼 근대의 철학은 이전의 철학과 그 성격이 많이 다르기 때문에 고대와 근세시대 사이에 철학사와 문화(명)사가 어떻게 전개되었는지를 간단히 살펴보는 것이 어느 정도 도움이 될 수 있을 것이다. 아리스토텔레스 사후 고대 그리스의 사정 - 헬레니즘과 그리스 사상의 전파 아리스토텔레스 사후와 거의 동시에 그리스의 도시국가들도 전성기를 뒤로 한 채 붕괴한다. 그런데 고대 그리스 문명은 역설되게도 도시국가의 붕괴와 함께 전 유럽에 전파되었다. 알렉산더는 거의 모든 그리스 도시국가들을 정복하여 하나의 거대한 제국을 만들었다. 도시국가(polis)는 거대국가(kosmopolis)가 되었다. 하지만 그에 의해 세워졌던 이 마케도니아 제국도 그의 죽음과 함께 같이 붕괴했다. 알렉산더의 시대는 이렇게 일찍 지나갔지만 그의 정복에 의해 같이 전파된 그리스 문명의 영향은 오히려 오래 남게 된다. 소위 헬레니즘으로(헬라스는 그리스인을 가리킨다) 발전된 이 문화는 민족의 특성이나 특수한 생활 형태보다는 세계인·전 인류적 사상을 강조해서 민족적 차별이 없는 그리고 풍부한 교양을 가진 인류를 이상으로 삼았다. 플라톤과 아리스토텔레스는 죽었지만 그들이 세운 학원들은(아카데미아, 뤼케이온) 존속했고, 이 두 학원들 외에 스토아학파와 에피쿠로스학파라는 새로운 사조들도 생겨나 약 6세기 중반까지도(529년에 당시 로마 황제 유스티아누스 1세는 모든 학원들을 이교도적이라는 이유로 폐쇄하였다) 헬레니즘의 기반을 이루며 많은 영향을 끼쳤다. 스토아학파와 에피쿠로스학파는 윤리적 삶과 영혼의 평정을 목표로 하는 사상들이었고 플라톤과 아리스토텔레스라는 고대 그리스 철학자들의 이론은 이때도 여전히 시대의 중심적인 학문 틀을 제공하였다. 로마제국은 마케도니아 제국의 멸망 후 유럽을 지배하게 되었는데 헬레니즘은 로마식으로 번역, 수용되어 여전히 지배적인 사상으로 남았다. 이 때 중요한 사건은 기독교가 313년, 당시 로마 제국 황제였던 콘스탄틴 대제에 의해서 정식종교로 공인된 것이다. 로마 제국 초기에 심하게 박해받았던 기독교는 이제 무서운 속도로 전 로마 제국에 전파 돼 유럽의 중심종교이자 사상으로 자리 잡게 된다. 중세 사상의 중심 – 신학 우리가 중세철학이라고 하면 이는 4~5세기 로마 제국의 몰락부터 15세기 르네상스 시대까지 서양에서 일어난 철학적 사변을 가리킨다. 중세시대를 가리키는 말로 ‘암흑의 시대’라는 말이 있고, 중세 철학에는 흔히 ‘신학의 시녀’라는 수식어가 따라 다닌다. 하지만 이 시대가 반드시 전적인 침체기는 아니었다. 유명론과 실재론의 논쟁이라는 유명한 논쟁도 있었고, 다양한 신 존재 증명 등도 등장했다. 그 밖에도 논리학이 발달했으며 숱한 철학적 개념들이 정립되었다. 그럼에도 불구하고 많은 철학자들이 중세를 중요하게 생각하지 않은 가장 큰 이유는 아마도 이 시대의 사상이 ‘기독교 신앙’이라는 대전제를 가졌기 때문일 것이다. 철학이란 본래 모든 생각과 전제를 의문시해서 그 정당성과 근거를 따져야 하는데 중세 사상의 출발점인 신앙은 의심되지 않는 전제이기 때문에 이러한 철학의 정신과 부합하지 않는다. 중세 철학은 이렇게 기독교 신학과 밀접한 관계를 맺고 있고 또한 중세 철학자 대부분은 기독교 신학자였다. 그런데 예수 사후 직후 교회들이 초기에 자리를 잡아 갈 무렵 기독교의 전도자들이 직면했던 큰 문제들 가운데 하나가 바로 당시 지식인들의 도전이었다. 기독교 신앙은 다신교가 아니라 일신교, 즉 여호와라는 유일신을 섬기는 종교이다. 따라서 타 종교나 이교도에 대해서 본질적으로 배타적일 수밖에 없다. 그래서 당시에 교회 밖의 여러 사상적 도전이 있었을 때 이에 맞서서 신앙을 지키기 위한 변증(辨證)의 도구로서 신학이 발전되었다. 또 이때 신앙의 반대세력은 대부분 지적인 논변을 수단으로 했기 때문에 종교를 지키려는 노력도 자연히 이성적인 형태를 띠게 됐다. 이렇게 자신들의 신앙을 인간 이성이 이해할 수 있게 제시하는 일이 초대 교회 지도자들이 해야 할 가장 중요한 일이었다. 후에 교부(敎父)라고 불리게 된 이 교회 지도자들이 위와 같은 이른바 호교론(護敎論)적 목적에서 저술한 글들, 사상을 우리는 교부 철학이라고 하며 중세 철학의 시작으로 본다. (교부시대는 2~7세기 또는 8세기까지에 이른다. 처음엔 헬레니즘과 겹친다.) 이런 과정을 거쳐서 자연스럽게 고대 그리스의 이성적 사유와 기독교 교리가 종합되는 결과가 생기게 되었다. 이렇게 신앙과 이성을 결합하는 경향은 전 중세에 걸쳐 나타난다. 이런 이성과 신앙의 조화를 제일 먼저 체계화한 교부 아우구스티누스의 제일 중요했던 모토 중 하나는 ‘이해를 요구하는 신앙’이었다. 그는 플라톤적 사상의 영향을 받아서 감각 세계 너머에 진리의 영원한 정신적 영역이 있으며, 이 영역은 인간정신의 대상이고 인간의 모든 노력의 목표라고 믿었다. 인간은 감각을 통해서가 아니라 자기 정신과 지성을 통하여 진리의 세계로 나아갈 수 있다고 하면서 이 진리를 기독교의 신과 동일시했다. 또한 인간은 두 개의 실체, 즉 육체와 영혼의 복합체이며 그중 영혼이 훨씬 더 우월하다고 생각했는데 이는 플라톤의 영혼에 대한 생각과 기본적으로 그 궤를 같이 하는 것이다. 아우구스티누스가 가졌던 이 플라톤적 사상은 오랜 기간 동안 중세신학에 영향을 끼쳤다. 그 이후에 등장하는 교부들과 신학자들은 대부분 그와 비슷한 방향으로 기독교 신앙을 설명하였다. 한편 아리스토텔레스의 저작은 이미 우리가 살펴보았듯이 그의 사후 대부분 소실되었고 원고들은 뿔뿔이 흩어져 그 소재조차 제대로 알려지지 않았다. 안드로니코스가 비록 원고들을 편집하여 출간하게 되었지만 초기에는 널리 전파되지 않았고 설상가상으로 이슬람제국의 동로마제국 침공으로 인하여 결국 아리스토텔레스의 책들은 서방세계로 전해지지 못한 채 이슬람세계에 남게 된다. 여기서 아리스토텔레스의 철학은 오히려 활발히 연구되어 아랍어로 번역되기도 했다. 이 때 많은 학자들도 이슬람 세계로 옮겨갔으며 때마침 유럽 지역은 암흑기를 맞이한다. 근근이 이어지던 기독교철학의 명맥은 8-9세기 무렵에나 이르러 카롤링거 왕조의 출현에 힘입어 정립된 교육 제도의 등장과 함께 학원 철학의 형태로 다시 모습을 드러내기 시작했다. 이 때 유럽 각지에 설립된 신학원들을 중심으로 기독교의 교의는 학문적으로 체계화 되었다. 이 신학원 교수(doctores scholastici)의 명칭에서 스콜라 철학(Scholasticism)이라는 용어가 유래했고 그 후 중세의 신학원과 대학에서 널리 연구되는 학문을 스콜라학이라고 부르게 되었다. 11·12세기 들어 십자군 원정으로 인해 부분적으로나마 서유럽과 이슬람 제국의 문화교류가 시작될 때 아리스토텔레스의 책들도 유럽으로 역수입되게 된다. 그의 책들이 라틴어로 번역되면서 그의 사상은 이후 중세에 큰 반응을 불러 일으켰다. 주지하는 대로 플라톤의 사상은 초월적인 이데아들과 비감각적인 진리의 세계에 천착하는 데에 비해 아리스토텔레스는 개별적 대상의 존재를 인정하고 여기에 형상이 실현된다는 현실적인 생각을 갖고 있었다. 이에 영향 받은 중세의 중·후기 스콜라 신학자들은 감각적 세계에도 신의 진리가 내재함을 밝히려고 애썼고 아리스토텔레스의 철학을 기독교 신앙을 설명하는 도구로 사용하는 데에 주저함이 없었다. 이렇게 중세의 신학은 플라톤주의와 아리스토텔레스주의의 각축장이 되었고 많은 논의들이 이 두 관점들 아래에서 기독교 신앙을 설명하는 것을 목적으로 하였다. 중세 초기의 교부 철학이 플라톤 철학과 가까웠다면, 후기의 스콜라 철학은 아리스토텔레스 철학과 가까웠다고 할 수 있다. 이렇게 중세 전반에 걸쳐 신앙과 이성사이의 긴장, 플라톤적 초기 교부 전통과 후기 아리스토텔레스적 스콜라 사상 사이의 긴장 등이 유지되었다. 이런 긴장들을 어떻게 해결하는가에 따라 시기별로 등장한 각 사상가들의 주장하는 바가 (비록 같은 신앙을 수호하고 있지만) 조금씩 다르게 된다. 이제 우리는 이런 중세의 사상을 잘 나타내는 두 가지 예를 살펴보겠는데 이 논의들에서 우리는 플라톤과 아리스토텔레스의 흔적들을 쉽게 찾아볼 수 있다. \\1) 첫째는 신 존재 증명에 관한 논의이다\\. 존재자의 근원인 신의 존재를 증명하는 것은 중세 시대에 빈번히 시도되었다. 안셀무스(1033-1109)는 삼단논법의 형식을 빌려 다음과 같은 ‘존재론적 증명’을 시도했다. 그는 먼저 신은 개념상 최고로 완전한 것이라고 전제한다. 그리고 완전성에는 존재도 포함한다고 했다. 따라서 완전한 신의 개념에는 존재도 포함된다는 결론이 도출되기에 신은 필연적으로 존재한다고 하였다. 또 다른 형식의 신 존재증명은 토마스 아퀴나스가 시도했던 ‘우주론적 증명’이다. 그에 따르면 이 세계 안에 있는 모든 것들은 있을 수도 있고 없을 수도 있는 우연적인 존재들이다. 우연적인 존재들의 원인 또한 우연적인 것이므로 우리는 그 원인을 계속해서 무한히 소급해서 찾을 수 있다. 따라서 그 자신은 원인을 필요로 하지 않고 다른 모든 우연적인 것들을 가능하게 해 주는 최초의 원인이 이 세계 밖에 있어야 한다. 이 필연연적인 존재자는 신이다. 마지막으로 ‘목적론적 증명’은 자연의 모든 사물들은 어떤 목적을 향해 움직인다는 전제로부터 출발한다. 따라서 누군가가 이들이 어떻게, 어디로 움직이는가를 계획해 놓았을 것이다. 그 운동의 기획자는 신이다. 중세의 이런 신 존재 증명들은 근대 초까지 활발히 시도됐지만 그 이후에는 대부분 ‘공허’한 것으로 비판받고 관심 밖으로 밀려났다. \\2) 두 번째로는 유명론과 실재론 사이의 이른바 ‘보편논쟁’이다\\. 플라톤은 오직 이데아들만이 실재하는 것이라고 하였고 반대로 아리스토텔레스는 개별적 대상이 실체라고 하였다. 여기서 이데아는 보편적인 것이고 개별적 대상들은 독립적인 개별자이다. 이럴 때 그 보편자라는 것이 개별적인 것들의 공통된 특징에 의한 한낱 개념 내지는 이념, 즉 이상인지 아니면 오히려 개별적인 것들이 보편자를 어느 정도 닮았거나 본 따서 있는 것인지에 관한 물음을 물을 수 있다. 이러한 개별적인 것들과 보편적인 것의 관계는 철학사에 많은 의문을 남겨 주었다. 개별적인 것들은 끊임없이 발생하고 소멸하지만 이들에게 공통된 보편적인 성질들은 이 변화들에도 불구하고 존속한다. 하지만 이 보편적인 속성들은 그들 자체로 우리에게 지각되지 않는다. 따라서 이 둘 중 무엇이 먼저이고 나중인지, 무엇이 실재하는지 결정하기가 쉽지 않다. \\- 실재론(realism)과 유명론(nominalism) 이 문제를 보는 두 가지 관점을 생각할 수 있는데, 먼저 보편자가 실재하는 것이라고 생각하는 사고는 실재론(realism: realis 그리스인 --> 사람. 이 경우 사람과 같은 보편적인 개념은 덜 보편적인 하위 개념들의 바탕에 놓여 있는 기체이고 개별자들은 이 기체의 여러 가지 상태가 된다. ➁ 반면 유명론은 아리스토텔레스적 사상에 근거하여 실체는 개별자이고 이 개별자가 모든 것의 바탕에 놓이는 것, 즉 기체라고 보았다. 이 때 보편자는 판단에서 술어의 역할을 할 뿐이다. 따라서 보편자는 실재하는 것이 아니고 여러 실재하는 개체들에 공통으로 붙여진 명칭, 이들을 통합해서 부르는 이름, 혹은 공통의 부호이며 낱말일 뿐이다. 이 논쟁은 플라톤과 아리스토텔레스 사이의 대화나 마찬가지이다. 이를 통해서 우리는 중세 시대에 이 두 사상이 얼마나 첨예하게 대립했으며 또한 얼마나 그 시대에 깊게 영향을 주었는지를 알 수 있다. 근대의 시작 – 종교개혁, 자연과학의 발전 그리고 르네상스와 인본주의. 이제 근대라는 시기에 접어들면서 철학의 모습도 바뀌게 되는데 그 시대적 배경을 다음의 세 가지 측면에서 정리해 보자. 1) 중세의 교회권력과 종교개혁 중세에 교세가 크게 확장된 교회는 이후 점차 권위적이고 형식적인 제도가 됐다. 마녀사냥이나 종교재판 등을 빈번히 집행했을 뿐 아니라 사회 전반에 걸쳐 많은 영향력도 행사했다. 심지어는 왕권도 위협하는 지경에 이르렀으니 학문 활동에 끼치는 막대한 영향은 굳이 언급할 필요조차 없다. 교회의 승인을 얻지 못하면 출판할 수 없었고, 종교적 제재라도 받으면 사회적으로 많은 제약이 따랐다. 이런 중세의 상황이 진행되면서 지속적으로 불만이 누적되었다. 그래서 근세철학은 중세에 대한 반발이라는 중요한 특징을 갖고 있다. 지금까지 있었던 속박과 억압으로부터의 자유와 해방이라는 동기와 정서가 근대의 사상 깊숙이 깃들어 있는 것이다. 종교적·역사적으로는, 16세기 초 독일의 신부 마틴 루터로부터 시작된 종교개혁에 의해서 교회의 권력과 권위가 서서히 무너지며 새로운 시대가 열리게 된다. 그 바탕에는 이론과 형식에 치우친 교회에 대해 불만을 갖는 민중들의 종교적 욕구가 있었다. 신학 교수였던 루터는 1517년 로마 교황청의 면죄부 판매로 첨예화 된 교회의 부패와 독선에 반대해서 95개조의 반박문을 발표했고 이것이 후에 전 유럽을 휩쓸게 된 종교개혁의 불씨가 되었다. 그는 자신이 발표한 반박문에서 인간이 각자 자신의 이성을 신뢰하고 자신의 이성에 기초하여 신을 믿는다면 자유를 얻을 수 있을 것이며, 교회는 여기에 간섭해서는 안 된다고 주장했다. 또한 오직 믿음만을 통하여 인간은 구제 될 수 있다는 그의 유명한 주장에 따라 전 유럽의 각지에서 일어난 이 운동을 통해서 후에 오늘날 프로테스탄트라 부르는 교파가 생겼다. 종교개혁은 로마가톨릭의 독재와 부패에 항거하여 그리스도교의 본연인 성경으로 돌아가자는 기치 또한 갖고 있었다. 루터는 1522년에 신약을, 1534년에 구약을 각각 그리스어와 히브리어로부터 독일어로 번역하였다. 그때까지만 해도 라틴어로 된 성경만 있었기 때문에 일반 신도들은 성경을 제대로 읽어 볼 수도 없었다. 루터의 번역 덕에 일반인들도 누구나 성경을 읽을 수 있게 되었고 이것이 교회 권위와 종교생활에 어떤 영향을 끼쳤는지는 세삼 말할 필요가 없겠다. 이렇게 중세에 가장 중심이 되었던 기독교 신앙에 대한 개방적 접근이 허용됨에 따라 근대에 와서 다양한 사고들이 가능하게 되었다. 2) 자연과학의 발달 자연과학이 발달하게 된 계기들 중 하나는 바로 십자군 전쟁이었다. 십자군 원정대가 당시 자연과학이 발달되었던 이슬람문명과 접촉하면서 그들의 뛰어난 자연과학의 성과들이 유럽으로 흘러들어왔다. ➀ 천문학의 발달 이로 인해 자연에 관한 새로운 지식들이 나타났는데 가장 대표적인 것이 코페르니쿠스의 지동설이다.(1530년 경) 그때까지 기독교인들은 우주의 중심은 지구이며 천체는 오직 지구를 중심으로 선회한다는 천동설을 믿고 있었다. 그런데 코페르니쿠스는 ‘지구는 태양주위를 돌고 있는 많은 위성중의 하나에 불과하다.’라고 하였다. 그의 이런 천문학적 이론은 중세적 우주관에서 근대적 우주관으로의 이행을 촉발했으며, 이러한 우주관·세계관의 대변혁은 이후 흔히 「코페르니쿠스적 혁명」이라 일컬어진다. 코페르니쿠스의 뒤를 이어 케플러(천체의 운동 법칙)와 갈릴레이가 등장하는데, 갈릴레이는 양적, 수학적 기초 위에서 자연과학의 원리를 세우려고 하였다. 이후 실험과 관찰을 통한 탐구를 중심으로 자연과학이 형성되었는데 이제까지 사물의 본질을 중심으로 질적 측면에서 자연을 관찰했던 고대인들과 달리 이제 근대의 과학자들은 양적 관계에 중점을 두고 자연을 탐구하게 된 것이다. ➁ 고전 역학의 완성 자연에 관한 양적, 수학적 관찰이 낳은 가장 큰 결실은 1787년 뉴턴의 『자연철학의 수학적 기초』라는 책과 함께 완성된 고전 물리학이었다. 여기서 뉴턴은 자연현상을 수학적 법칙으로 환원하고자 했는데 이 보편적 수학 법칙은 신의 개입과 무근거한 전제를 상정하는 일을 배제해 주었다. 또한 철저히 관찰과 추론으로만 자연에 내재한 힘들의 원리를 밝힘으로써 중세와는 전혀 다른 새로운 세계관을 제시했다. 그는 자연은 일정한 법칙에 따라 운동하는 복잡하고 거대한 기계라고 했는데 이 법칙성은 이후 자연과학이 추구하는 목표가 되었다. 3) 르네상스 시대와 인본주의 중세 철학자들의 관심은 현실을 넘어선 초자연적인 대상에 집중되어 있었다. 그러나 근대로 넘어오면서부터 관심의 초점은 신에서 자연으로 바뀌었으며, 신학 대신에 자연과학이 모든 학문의 앞자리에 나서기 시작하였고, 종교에 있어서도 교회의 울타리를 벗어나 개인적으로 신앙의 대상과 접촉하길 원했다(종교개혁 후에 등장한 개신교의 특징). 이러한 상황 속에서 새로운 인간성의 회복 및 고전의 연구와 고대문화의 부흥운동이 발생하는데 이것이 곧 르네상스(Renaissance)운동이다. 르네상스는 14세기 중엽부터 16세기까지 약 250년에 걸친 시기로서 중세와 근대를 잇는 과도기였다. 르네상스는 부활, 재생이라는 뜻으로서, 중세에는 죽었던 인간의 부활, 그동안 잊혔던 고대 그리스 문명의 재생 등을 원했다. 르네상스의 중심적 내용은 인간 회복을 위한 인본주의 정신이었다. 중세기에는 인간이 신을 위해 존재했으며, 사회는 교회를 중심으로 생활하고 있었다. 그러므로 인간의 본질파악과 자율성은 종교적 전통과 관습 때문에 소외당했다. 이제 르네상스시대에는 중세의 신 중심 사상이 아닌 인간이 중심이 되는 새로운 인간관이 전면에 등장했는데 그 이상적인 인간상을 그들은 고대 그리스의 문화에서 찾으려고 하였다. 왜냐하면 고대의 정신과 예술 속에는 인간의 본성과 꾸밈이 없는 삶의 모습이 잘 나타난다고 생각했기 때문이다. 그런 배경에서 이 시대의 여러 예술가들은 고전과 고대로 돌아가자는 운동을 활발히 전개시켰다. 새로운 주제의 등장 - 인식론 15-17 세기에는 이렇게 신앙과 이성, 신학과 철학의 분리가 시작되었고 인간 중심의 경향, 근대 자연과학의 부흥 등이 발생했다. 또한 동시에 수많은 지식들, 즉 중세 때부터 전해지던 아리스토텔레스주의와 플라톤주의, 새롭게 대두한 수학과 여러 자연과학 등이 등장했다. 그래서 사람들은 이 숱한 흐름들 속에 과연 어떤 것을 믿고 받아 들여야 하는가 하고 고민하게 되었다. 왜냐하면 새로운 지식들과 세계관들은 자꾸 혼란스럽게 등장하고 있는데 이전의 교회가 가졌던 절대적인 권위는 더 이상 존재하지 않았고 이들을 평가해서 판단할 만한 중심적인 기관과 체계가 전혀 존재하지 않았기 때문이다. 그래서 많은 이들은 이른바 \"지식의 위기: intellectual crisis\"라는 것을 느끼게 된다. 근대의 철학은 이런 위기를 극복하려 했고, 그 시대에 등장한 여러 사상적 단초들을 정리할 수 있는 새로운 체계를 세우는 것이 근대철학자들이 가졌던 가장 큰 학문적 동기였다. 이 시대는 앞서 말했다시피 지식의 위기가 있었기 때문에 그 어느 때보다도 더 지식에 대한 관심이 높았다. 지식에 관한 이론, 그리고 인식 일반의 근본 문제를 다루는 철학의 한 부문을 우리는 인식론(認識論)이라고 부른다. 인식론(Epistemology)은 고대 그리스어의 episteme(지식 또는 인식)와 logos(이론)을 합친 데에서 비롯한다. 진리나 지식의 성질과 원리, 기원 그리고 범위(사람이 이해할 수 있는 한계 등)에 대하여 고찰하는 것이 인식론이다. 근세에는 이 인식론이 철학에 있어서 가장 중요한 관심분야가 된다. 그래서 근세에는 인간 인식의 기원에 관한 물음이 가장 빈번했는데 그에 대한 가장 대표적인 대답을 우리는 이성론과 경험론이라고 부른다. 경험론은 주로 영국에서, 이성론은 주로 영국을 제외한 나머지 유럽, 흔히 말하는 대륙에서 발달했다. 그래서 각각 영국의 경험론, 대륙의 경험론이라고도 불린다.",
      "frontmatter": {}
    },
    "07.2.인간과 지식 1": {
      "path": "/06.university/철학과-인간/07.2.인간과-지식-1/",
      "filename": "07.2.인간과 지식 1",
      "content": "근대에 와서 중심 주제로 부각된 인식론은 인식, 혹은 지식 일반의 근본 문제들을 다루는 철학의 한 부문이다. 그 중 인식의 기원에 관한 이론들은 다음과 같고 이를 중심으로 근대 철학자들의 사상을 정리해 볼 수 있다. 인식의 기원에 관한 이론들 \\1) 경험론 (empiricism) 경험론자들은 오관에 의해 제공되는 경험을 그들의 인식론의 기초로 삼았고 인간의 지식은 감각경험을 통해서 얻어진다고 주장한다. 존 로크는 우리 인간의 이성은 태어났을 때는 백지(tabula rasa)와 같다고 했다. 아무런 감각 자료가 새겨져 있지 않은 인간의 의식은 이제 생활하면서 많은 감각적 인상을 획득하게 된다는 것이다. \\2) 합리론 (rationalism) 합리론자들은 인간에게는 경험에 앞서는 선험적인 인식원리가 있다고 주장한다. 우리 인간에게는 자명한 명제들(예: 모순율과 같은 논리적 규칙, 수학적 공리 등)이 있는데, 이 관념들은 학습할 수도 없고 우리가 임의대로 만들어 낼 수도 없다. 이 원리들은 경험에 앞서는, 즉 선험적인 것이다. 경험은 오히려 이 원리들에 의해서 이해되는 것이다. 따라서 지식은 지성의 영역에서 산출되며 참된 지식은 이미 관념의 형식으로 우리에게 존재한다. 합리론(이성론) 경험론자들이건 합리론자들이건 근대 철학자들이 가장 큰 관심을 갖고 있었던 것은 확실한 지식으로서의 참된 진리였다. 그들은 분명해서 흔들리지 않는 지식의 가능성을 확보하는 것을 최고의 목표로 삼았다. 그것을 획득하는 조건과 방법에 대한 생각에 있어서 이제 경험론자들과 합리론자들의 길이 갈리는 것이다. 먼저 경험론자들은 그 지식이 실재하는 대상과 합치하는지의 여부에 따라 그 지식의 확실성이 보장된다고 보았다. 그래서 그들은 인식의 내용, 즉 우리의 관념을 우선 감각 경험과 비교해 보는데 만약 그 관념이 경험에 근거해서 생성되었다고 입증되면 그것은 실재적이고 정당한 지식으로 인정된다. 반면 합리론자들은 진리의 근거를 이성, 혹은 합리성에서 찾는다. 그들은 우리의 이성이 명증적이라고 여기는 지식들만을 확실하고 정당한 것으로 여긴다. 어떤 지식이 명증적인지 여부는 오직 이성만이 판단하고 경험에 의한 입증이나 실재와의 대응은 중요한 근거가 아니다. 형식적인 측면에서 보자면 동일률, 모순율, 배중률과 같은 논리적 법칙들은 명증적이다. 그것의 정당성을 이성적으로 의심할 수 없기 때문이다. 그런데 이것들은 사고의 형식적인 규칙들에 불과하므로 세계와 관련된 지식들 중 명증적인 지식에 대한 규정은 더 구체적이어야 한다. 따라서 합리론자들의 노력은 명증적인 것이라고 확보된 개념과 원리들을 외부 세계의 현실과 조화시키는 데에 집중된다. 최초의 합리론자라고 불리는 데카르트는 명석하면서도 동시에 판명한 지각의 상태를 명증적이라고 규정했다. 명석하다는 것은 우리 정신이 인식 대상의 구성요소를 하나하나 세심하게 알아서 분명하게 파악하고 있을 때를 지칭하며, 판명하다는 것은 정신이 그 대상을 다른 대상과 확연하게 구분할 수 있을 때를 가리킨다. 이때 명석함의 반대말은 애매하다는 것이고 판명함의 반대말은 혼잡한 것이다. 이에 의하면 어떤 인식의 내용이 불분명하거나 애매하지 않고 내 의식에 그것이 무엇인지 뚜렷이 나타나면서 다른 인식들과 뒤엉켜 있지 않게 분명히 나타나는 그런 상태에 있다면 그 지식은 명증적인 것이다. 합리론자들은 우리의 이성적 사고와 반성을 통해서, 문제되는 어떤 지식이 위와 같은 상태라고 여겨질 때 주저 없이 그것을 명증적이라고 판단하고, 오직 그런 것들만을 정당하고 확실하고 참된 지식으로 인정한다. 이것이 합리론자들의 기본 입장인데 그렇다면 우리는 어떻게 이런 지식을 가질 수 있는지, 그리고 이를 통해서 어떤 성과를 얻을 수 있는지 우선 데카르트의 경우를 예로 들어 살펴보기로 하자. 데카르트 (René Descartes, 1596 – 1650) 『방법서설』 (Discours de la méthode, 1637) 『제1 철학에 관한 성찰』 (Meditationes de Prima philosophia, 1641) 데카르트는 우리가 절대적으로 확실한 지식을 갖는 것은 모든 형태의 회의주의를 논파해야만 가능하다고 생각했다. 그가 활동했던 근대 초기는 -이미 아는 바와 같이- 기독교의 유일한 절대 권위가 지배했던 중세시대가 막을 내리면서 르네상스라는 과도기를 거쳤던 회의주의와 혼란이 극대화된 상태였다. 하지만 이런 시대적 상황에 대한 반동으로 확고한 지식에 대한 열망이 그 어느 때 보다도 강했던 것 또한 사실이다. 로크 역시 비슷한 동기를 가졌는데 그는 경험론적 방법론으로써 이를 시도했다면, 데카르트는 이제 합리적 절차를 통해서 이를 얻으려 한다. 흄과 같은 경우도 비록 그 결말은 회의적인 방향이었지만 애초에는 이런 확실한 지식의 가능성을 확인하려 한 시도였다고 볼 수 있다. 이성 사용의 규칙 데카르트는 그런데 회의주의자들과는 달리 처음부터 모든 인간은 자연적으로, 그리고 그 본성상 이성을 소유하고 있다고 생각했다. 진리에 이르는 길은 누구에게나 열려 있다는 생각과 그런 보편적인 인식능력에 대한 믿음을 그는 처음부터 갖고 있었다. 그래서 당시 팽배했던 회의주의와 관련해서도 이런 혼란이 인간정신의 본성, 즉 이성에게 결함이 있기 때문에 야기됐다고 생각하지 않았고 단지 인간이 그 이성을 잘못 사용했기 때문이라고 생각했다. 그래서 진리에 도달하기 위해서는 이성을 단지 소유하기만 해서는 안 되고 이를 올바로 사용할 수 있어야 한다고 생각했다. 그래서 어떻게 하면 이 이성을 잘 사용할 수 있을까 하는 방법론에 우선 관심을 가졌다. 이런 배경에서 그가 생각해낸 이성의 올바른 사용을 위해 지켜야 할 규칙들은 다음과 같다. 첫째, 명증성의 규칙. 명증적으로 참이라 인식한 것 외에는 무엇도 참된 것으로 받아들이지 말 것. 속단과 편견을 신중히 피하고 의심의 여지가 없을 정도로 명석 판명하게 내 정신에 나타난 것 외에는 그 어떤 것에 대해서도 판단하지 말 것. 둘째, 분해의 규칙. 검토해야 할 어려움들을 각각 잘 풀 수 있도록 가능한 한 작은 부분들로 나눌 것. 셋째, 종합의 규칙. 가장 단순하고 알기 쉬운 대상에서 출발하여 계단을 오르듯 조금씩 올라가 가장 복잡한 것을 인식하는 데에까지 이를 것. 순서가 없는 것들에 대해서도 순서를 설정하여 나아갈 것. 넷째, 열거의 규칙. 아무 것도 빠뜨리지 않았다는 확신이 들 때까지 완벽한 열거와 전반적인 검사를 어디에서나 행할 것. 데카르트는 참인 것으로 완전하게 신뢰할 수 있는 지식을 얻으려면 위와 같은 방법으로 탐구해야 한다고 주장했다. 이러한 데카르트의 방법 가운데 명증성을 강조하는 첫 번째 규칙이 가장 중요했는데, 그것이 모든 지식들의 시작이고 출발점이고 전제이기 때문이다. 이제 이런 명증적인 지식을 얻기 위한 방법으로서 그가 채택한 것은 모순되게도 ‘회의’라는 것이었다. 회의가 모든 철학적 성찰의 출발이자 필수 불가결한 구성요소인 이유는 우선 상식과 이론 사이의 차이에 있을 것이다. 이미 여러 번 언급했던 현상과 실재에 관련된 내용을 상기해 보면, 철학적 성찰이 있기 전에는 누구나 상식적으로 우리에게 나타나는 것이 실제로도 그러할 것이라고 여긴다는 것을 떠올릴 수 있을 것이다. 하지만 철학적으로 그것을 반성해 보면 현상과 실재를 구분할 수밖에 없다. 또한 근대의 자연과학의 발전에 힘입어 자연사물의 ‘진정한’ 모습에 대한 관심도 증대되었다. 그래서 상식적이고 감각적인 지식을 넘어서는 진정한 지식을 획득하려고 근대 철학자들은 노력했던 것이다. 그런데 이전까지의 상식적 관점에서 쉽게 벗어나기가 쉽지 않고, 또 인간은 기본적으로 육체에 얽매여 있고 감각에 구속되어 있는 존재이므로 이성의 온전한 가능성과 능력대로 지식을 갖는 일이 결코 일순간에 이루어질 수 있는 일이 아니다. 따라서 근대 철학자들은 하나 같이 인간을 이런 미성숙으로부터 해방시키고자 했고, 이렇게 “정신을 감각으로부터 벗어나게 하는 것”에 가장 적절한 방법론으로서 데카르트는 회의를 선택했다. 우리가 자연적으로 얻게 되는 모든 지식들에는 이렇게 상식과 단편적이고 감각적인 지식이 전제되어 있으므로 이 지식들을 하나하나 회의해 보는 것이 그 지식들의 부당성을 가장 잘 보여줄 수 있기 때문이다. 방법론적 회의 이런 데카르트의 회의를 우리는 방법론적 회의라고 부른다. 그것은 단순히 회의 그 자체를 위한 회의라는 뜻이 아니라 진리에 도달하기 위한 방법으로서 행해지는 회의라는 뜻이다. 우리가 회의에 회의를 거듭하고 가능한 모든 회의를 다 했는데도 도저히 회의하고 의심할 수 없는 것이 남아 있다면, 그리고 이것이 모든 회의를 다 이겨낸 지식이라면 그것이 곧 우리가 찾는 진리인 것이다. 그래서 이제 데카르트는 우리의 모든 지식들을 하나하나 다 의심해 보고자 한다. 그럴 때에 가장 먼저 간단하게 의심할 수 있는 것은 우리 주위의 모든 것들일 것이다. 여기 보이는 책상, 의자, 나무 등 여러 가지 것들인데 그런 모든 개별적 대상들에 대한 개별적 지식들을 하나하나 철저하고 완벽하게 의심하는 것은 끝내기가 매우 어려울 것이라고 쉽게 생각할 수 있다. 왜냐하면 개별적인 지식들은 그 수가 무척 많고 또 다양하기 때문이다. 그래서 그런 다양한 개별적 지식을 낳는 방법을 의심하면 철저하고 완벽하게 의심할 수 있을 것이다. 우리가 갖는 수많은 지식들을 낳는 방법은 당연히 감각 경험을 통하는 것이다. 우리는 경험을 통해서 내 앞에 있는 책상, 의자 등 모든 것에 대한 지식을 얻는다. 그러나 이런 감각 경험이라는 방법은 우리가 이미 숱하게 말했듯이 확실한 방법이 아니다. 왜냐하면 우리는 지각을 할 때 흔히 착각하고 잘못 판단하기 때문이다. 우리의 감각기관은 종종 우리를 속이고 기만한다는 것을 우리는 알 수 있다. 그런데 데카르트는 단 한 번이라도 우리를 속인 일이 있는 것은 결코 전폭적으로 신뢰하지 않겠다는 규칙을 세운다. 그래서 우리가 종종 갖는 감각에 의한 오류는 곧 감각 경험 일반에 대한 불신으로 이어진다. 우리가 감각경험을 믿을 수 없는 두 번째 이유는 우리가 경험하는 모든 것이 꿈속에서 일어난다고 의심할 수 있기 때문이다. 이것이 이른바 ‘꿈의 가설’인데, 우리는 감각 경험을 통해 얻은 상식적인 세계를 실재한다고, 현실이라고 여기지만 그것은 어쩌면 꿈일지도 모른다는 것이다. 이 생각이 얼핏 허무맹랑하게 보일지도 모르겠지만 이 가설을 부정할 수 있는 확실한 근거도 없다는 것 또한 사실이다. 우리가 알고 있는 몇 몇 문학 작품들, 예를 들어 ‘구운몽’이나 ‘이상한 나라의 엘리스’ 등에서 주인공들은 자신이 꿈속에 있는 줄도 모르고 그 꿈속에서 일어난 일들을 현실이라 착각하고 있다. 그들이 그 꿈에서 깨어나지 않았다면 그들은 아마도 영원히 그것이 진짜 세계라고 착각할 것이다. 또한 공상과학영화 ‘Matrix’의 예를 들어 보자면, 영화 속에서 사람들은 멋있는 모습으로 거리를 활보하고 다니는데 사실 이 환경은 사이버공간이며 가상의 틀인 Matrix일 뿐이고 실제로 인간들은 기계들이 마련해 놓은 일종의 인큐베이터 안에서 생체 에너지를 빼앗기고 있는 것이다. 그리고 주인공들을 제외한 대다수의 사람들은 이 가상의 세계 매트릭스가 현실이라고 굳게 믿고 있다. 이렇게 꿈이나 가상 세계 등에서 현실과 똑같은 느낌이 들도록 모든 것이 그럴듯하게 짜여 있으면 그 안에 있는 당사자는 그 사실을 전혀 모르고 있을 가능성이 충분하다. 이런 가설까지 총동원하면 결국 우리는 감각경험과 그를 통해 얻은 모든 지식들과 이 현실 세계가 확실하지 않은 것으로 의심할 수밖에 없다. 그런데 곰곰이 생각해보면 꿈속에서건 현실 안에서건 항상 성립하는 것도 있다. 가령, 2+3=5라는 것은 꿈이 아닌 세계에서도, 꿈에서도 참이다. 우리가 꿈꾸고 있는 동안에도 여전히 2+3=5이고, 삼각형은 세 개의 변을 가지며, 둥근 사각형은 불가능하다. 꿈에서도 이런 지식이 성립한다는 사실은 우리가 이러한 수학적 지식들을 획득하는 데 있어, 감각 경험이 불필요하다는 것을 보여준다. 그런 지식들은 감각 경험이 아니라 순수하게 정신적인 방법을 통해서 획득될 수 있는 것이다. 물체의 연장, 형태, 크기, 수와 같은 것들은 꿈속에서도 현실과 똑같이 역시 그러하고 (기껏해야 그 배열과 배치를 우리의 무의식이 임의대로 뒤바꿀 수 있을 뿐이다) 따라서 꿈의 가설을 이용해도 흔들리지 않는 확실한 지식인 셈이다. 그러므로 이런 종류의 지식들을 명증적인 지식으로 삼고 회의를 멈추어도 될 것 같은데 여기서 데카르트는 한 걸음 더 나아가 다소 엉뚱한 상상을 덧붙인다. 그것은 혹시 우리를 창조한 전지전능한 창조주가 악한 심성을 갖고 있는 존재일 수 있지 않을까 하는 가정이다. 아니면 어떤 악마와도 같은 존재가 있어서 우리를 순수한 악의로 계속해서 기만하는 것은 아닐까 하는 생각이다. 이것이 바로 ‘악마의 가설’인데, 어떤 전능한 악마가 있어서 수학적 지식을 획득하는 방법, 즉 순수하게 정신적인 방법도 그 악마가 만들었다고 생각해보는 것이다. 그리고 이 악마는 물론 이를 통해 악의적으로 잘못된 지식을 산출한다. 하지만 이 악마는 우리 인간이 이 정신적인 방법을 사용해서 무언가를 알 때마다 이 지식이 자명하고 참된 것이라고 생각할 수 있도록 우리 정신을 조작하는 것이다. 그래서 2+3은 본래 6인데 악마는 우리로 하여금 5라고 잘못 계산하게끔 만들었고 더 나아가 우리는 그것이 확실하게 참이라고 잘못 의식하게끔 만들기도 했다. 만약 사정이 정말로 이렇다면 우리가 이 전 단계의 회의에도 굳건히 살아남은 수학적 지식, 이성을 통한 지식도 참되고 확실한 것이라고 자신할 수 없다. 데카르트는 이제 우리가 과연 이런 악한 존재가 없다고 확신할 수 있는 근거를 갖고 있는지 물음으로써 그의 방법론적 회의의 절정에 다다른다. 이런 극단적인 회의는 물론 현실성이 부족해 보이지만 과연 우리의 지식이 얼마나 확실하지를 따지는 데에 있어서 중요한 역할을 한다. 그래서 이제 수학적 진리를 포함해서 가능한 모든 것이 의심스럽게 되는데 이렇게 의심에 의심을 계속하던 데카르트는 의심의 끄트머리에서 다음과 같은 생각을 한다. 내가 지금 꿈을 꾸고 있다고 하더라도, 이 꿈을 꾸고 있는 나는 존재해야 하지 않을까 하는 생각이다. 그리고 내가 지금 전능한 악마에서 철저히 속고 있다고 하더라도, 그 속임을 당하는 나는 반드시 존재해야 하지 않을까 하는 것이다. 왜냐하면 이런 의심을 하고 있는 바로 이 순간에도, 의심을 하는 내가 존재해야 의심도 가능하고 회의도 가능할 것이기 때문이다. 그래서 그는 이렇게 의심하는 나, 회의하는 나, 이 모든 것을 생각하는 나는 반드시 존재해야 한다는 결론에 다다른다. 그의 말을 인용하자면, “그러나 이런 식으로 모든 것이 거짓이라고 생각하고 있는 동안에도 이렇게 생각하는 나는 반드시 어떤 것이어야 한다는 것을 알게 되었다. 그리고 ‘나는 생각한다, 그러므로 나는 존재한다(ego cogito, ergo sum)’라는 진리는 아주 확고하고 확실한 것이고, 회의론자들이 제기하는 가당치 않은 억측으로도 흔들리지 않는 것임을 주목하고서, 이것을 내가 찾고 있는 제일원리로 거리낌 없이 받아들일 수 있다고 판단했다.” - 『방법서설』 1권 7장 이런 체계적 회의 끝에 데카르트는 회의할 수 없는, ‘나 자신은 존재한다.’는 진리에 도달한다. “악령이 온 힘을 다해 나를 속인다고 치자. 그러나 내가 나 자신이 어떤 것이라고 생각하는 동안, 그는 결코 내가 아무것도 아니게 끔은 할 수 없을 것이다. 이렇게 이 모든 것을 세심하게 고찰해본 결과 ‘나는 있다. 나는 존재한다.’라는 명제는 내가 이것을 발언할 때마다 혹은 마음속에 품을 때마다 필연적으로 참이라는 결론에 이르게 된다.” - 『제1 철학에 관한 성찰』 중 제 2성찰. 이 회의 불가능한 진리, ‘나는 생각한다, 그러므로 나는 존재한다(ego cogito, ergo sum).’라는 명제는 철학적 탐구가 획득한 첫 번째 결론이자 데카르트 철학에서 제 1원리의 위치를 차지한다. 그는 이 원리로부터 진리의 기준을 이끌어 내는데, 그는 이제 모든 철학적 탐구의 결과는 이 명제처럼 회의 불가능해야 한다고 생각했다. 신 존재 그런데 이 명증적인 지식은 그저 첫 걸음일 뿐이지 다른 문제들이 어떻게 해결되어야 하는지에 대한 실마리는 아직 보이지 않는다. 왜냐하면 그가 지금까지 얻은 것은 오직 자아의 존재일 뿐 외부 세계의 존재나 다른 지식들의 확실성 등은 아직 밝혀지지 않은 채로 남아있기 때문이다. 예를 들어 우리는 먼저 그가 확보했던 이 명증성이 언제까지나 유지되는지 물을 수 있다. 즉, 어제 명증적이었으면 오늘 또 명증적인지를 물을 수 있다. 이렇게 얼핏 사소해 보이는 이런 질문이 제기되는 이유는, 데카르트가 오랜 회의 끝에 겨우 얻어낸 그 명증성은 전적으로 우리의 의식에 의존하는 것이기 때문이다. 즉, 의심하는 가운데 부정할 수 없는 자아를 떠올리는 나의 의식 그 자체가 명증적인 것이기 때문에 만약 내 의식이 이 자아의 존재를 더 이상 의식하지 않거나 다른 사태에 관심을 가져서 다른 대상을 의식하려 하면 최초의 명증성이 소멸할 것이기 때문이다. 이때는 자아에 관한 의식이 다시 애매해지고 혼잡스러워질 수 있다. 그래서 마치 버클리에게서 발견한 것과 같이 나의 의식이 집중하고 있는 순간을 떠나서도 이 명증성이 타당한 것이지를 묻게 된다. 즉, 한 번 명증적인 것은 이후에도 명증적인 것으로 밝혀져야 여기서부터 출발하는 다음 탐구의 정당성도 확보되는 것이다. 이 문제와 다른 모든 문제들의 해결을 위해서 데카르트가 끌어들인 것은 바로 신의 존재이다. 정신과 자아의 존재라는 주관적인 영역에서만 입증된 명증성만으로는 그것의 항구적 타당성 뿐 아니라 외부 세계의 존재 또한 보장될 수 없다. 데카르트는 이제 신이 존재해야만 비로소 이 모든 의문들이 해결된다고 생각했다. 그는 신의 존재를 한 번은 이성론적으로, 그리고 또 한 번은 존재론적으로 증명했는데, 그 중 이성론적 증명이 밟는 단계는 다음과 같다. 1\\. 명증적인 인식은 참이다. 2\\. 내 의식에 명증적인 인식으로서 신의 관념이 있다. 3\\. 원인 없이는 결과도 없으므로 신 관념의 내용을 있게끔 하는 원인이 있어야 한다. 4\\. 이 원인의 내용은 그 결과인 신의 관념의 내용보다 크거나 적어도 같아야 한다. 5\\. 그런데 신의 관념은 무한하고 완전하며 전지전능하다는 것이다. 6\\. 그러므로 이 완전한 신의 관념의 원인은 나 자신이거나 내 의식 내에 있는 또 다른 관념일 수가 없다. 왜냐하면 나나 내 안의 다른 관념들은 무한하지도 완전하지도 않기 때문이다. 7\\. 그러므로 내 의식 내에 있는 신의 관념을 일으킨 원인은 내 의식 밖에 있는 어떤 것이어야 한다. 8\\. 따라서 신 관념을 내 의식에 일으킨 원인으로서의 신은 내 밖에 실재한다. 두 번째 존재론적 증명은 이미 중세의 안셀무스가 다음과 같이 시도한 바가 있다. 신은 개념상 최고로 완전한 것이다. 완전성에는 존재도 포함된다. 따라서 신은 존재한다. 그의 이러한 신 존재 증명은 후대에 와서 타당하지 않은 것으로 비판 받아 왔지만 여기서 우리에게 중요한 것은 신 존재가 그의 주관적인 명증성의 한계를 어떻게 극복하는지를 밝히는 것이다. 그러기 위해서 우리는 신의 존재가 그의 철학에서 어떤 역할을 하는지를 다양한 관점에서 살펴보기로 하자. 먼저 인식론적인 측면에서 볼 때 신은 이성을 창조w한 존재자라는 점이 강조된다. 완전한 존재자인 신은 선한 심성을 가질 수밖에 없고(악한 심성이 완전성의 개념에 포함되는 것을 상상하기는 어렵기 때문에) 그런 존재인 신이 이 이성을 인간에게 구비시킨 의도 역시 선할 수밖에 없기 때문에 이 이성은 본성적으로 결함이 없는 것이고, 그것이 우리에게 가르쳐 주는 것들 역시 신뢰할 만하다고 생각해야 한다. 그렇다면 우리가 종종 빠지는 오류들은 이성 자신의 구조적인 결함 때문이라기보다는 이것을 우리가 (과한 욕구나 강한 의지 등으로) 잘못 사용했기 때문에 발생하는 것으로 보아야 한다. 그러므로 우리는 우리 이성이 명증적이라고 판단한 것들을 의심할 아무런 이유가 없다. 악마의 가설에 의해서 의심되었던 수학적 지식 등의 타당성도 이렇게 다시 회복된다. 또한 신이 우리에게 선사한 이 이성의 능력에 비추어 볼 때 한 번 명증적인 것으로 확인된 인식을 다시금 의심할 이유도 없다. 왜냐하면 “한 번 행해진 것은 결코 행하지 않은 것일 수가” 없기 때문이다. 한 번이라도 (옳은 방식으로) 명증적임이 밝혀졌다면 그것은 앞으로도 그럴 것이라고 우리는 조물주인 신의 선한 본성에 의거해서 생각할 수 있다. 또한 존재론적으로 볼 때 신은 바로 우리가 대하는 세계를 창조한 존재자이고 역시 이를 선한 의도와 함께 창조했기 때문에 우리가 이 세계에서 발견하는 합리적 원리의 타당성도 의심할 이유가 없고, 또한 우리가 명석 판명하게 지각하기만 한다면 그 지각의 내용을 의심할 이유도 없다. 왜냐하면 “신은 우리를 기만하지 않기 때문이다.” 물론 감각적 지식에서는 오류가 발생할 가능성이 더 크므로 전적으로 신뢰할 수 없겠지만, 그런 상식적인 지식조차도 이제 데카르트의 이론에서는 실천적인 관점에서 볼 때는 유용한 것으로 여겨진다. 우리가 자연으로부터 얻는 “가르침”은 신이 우리에게 자연을 통해 주는 교훈으로서 이것이 우리에게 전해주거나 경고해 주는 것을 잘 따르면 우리의 실제 삶에 도움이 될 것이라고 보았다. 마지막으로 신은 이 세계 전체와 우주의 운동을 발생시킨 궁극적 원인이다. 신은 이 모든 것을 창조했을 뿐 아니라 그것이 진행하는 법칙까지 만들었는데, 한 번 창조된 우주는 이제 이 법칙에 따라서 운동하게 된다. 이렇게 데카르트는 최초에는 극단적인 회의를 가지고서 철학적 사고를 시작했는데 자아의 존재에 대한 확신을 얻은 후 신의 존재 증명을 거쳐 결국 우리 이성의 능력에 대한 신뢰, 외부 세계의 존재, 그 안에 깃들어 있는 합리성, 심지어 일반적인 감각적 지식과 상식조차도 각각의 영역에서 그 정당성을 보장받는다는 결론에 이르게 되었다. 데카르트의 실체개념: 정신과 물체 데카르트는 실체를 “존재하기 위해 다른 어떤 것도 필요로 하지 않고 독립적으로 존재하는 것”이라고 정의했다. 그래서 실체는 자존적이고 독립적인 존재인데 이 규정에 온전히 합당한 것은 오직 ‘신’뿐이다. 신은 ‘무한실체’로서 다른 모든 것들은 이 신에 의존하는 존재이다. 그런데 데카르트는 이 세상에도 비록 신처럼 완전한 의미는 아니지만 역시 실체로 불릴 수 있는, 이른바 ‘유한실체’가 있다고 보았다. “비록 존재하기 위해서 신의 협력을 필요로 하지만, 다른 피조물에는 의존하지 않는” 것들은 ‘정신’과 ‘물체’라는 두 개의 실체이다. 데카르트는 신이 비록 이 세계를 창조했지만 그 자신은 이 세계와 분리되어 있는 존재라고 생각했기 때문에 이 세계의 실재를 설명하기 위해서 이 ‘약화된’ 실체들의 존재도 인정한 것이다. 이 두 실체들은 각각 ‘사유’와 ‘연장’이라는 서로 배타적인 속성을 가지고 있다. 물체와 정신을 분리한 그의 이원론은 자연과학적 탐구와 정신적인 탐구 모두를 정당화했으며 이후 과학과 철학의 분리를 촉진시키는 계기가 되었다. 그의 이런 생각은 새로운 인간관과 새로운 세계관을 역시 가져왔다. 그는 우선 진정한 자아를 순수한 사유만을 가지고 있는 정신적 존재라고 함으로써 이전의 전통적인 철학에서 주장된 유기적인 통일체로서의 인간개념을 더 이상 받아들이지 않는다. 다른 한편 이 세계는 철저히 기계론적으로 운행하는 체계로 규정된다. 이 세계의 모든 것들은 이제 여타의 구분 없이, 즉 동물과 식물, 생물과 무생물 등으로 나뉘지 않고 모두 물체로만 여겨지고, 그것들이 수량으로 측정되는 한에서 모두 동일하게 취급된다. 따라서 정신이나 영혼과 같은 것은 물질로만 존재하는 이 기계적 세계에서 추방되었다. 그런데 인간의 경우 특수한 문제가 발생한다. 하나의 인간에는 오직 사유속성만을 가진 정신실체와 오직 연장속성만을 가진 물체실체 둘이 모두 깃들어 있기 때문이다. 이들은 서로 독립적인 만큼 각각 정신세계의 법칙과 기계적 세계의 법칙만을 따를 텐데, 이 둘이 하나로 ‘결합’된 존재인 인간은 이제 이 두 개의 법칙 중 어느 것을 따라야 하는가라는 딜레마가 발생하는 것이다. 신체와 정신이 만약 서로 아무런 관계도 없이 병존만 한다면 이 난점을 벗어나기 어려울 것이다. 그래서 데카르트는 인간의 신체가 기계적 세계의 법칙에 따르지 않고 마음의 의지와 결정에 따라서 행동할 수 있다는 해결책을 내놓는다. 정신이라는 원인에 의해서 신체적 행동이 결과한다는 이 관계를 설명하기 위해서 그는 뇌의 뒷부분에 ‘송과선’이라는 인체의 기관이 있어서 이것이 정신과 육체를 연결해 준다는 가설을 끌어들인다. 그리 환영받지 못한 이 가설을 통해서 전개된 데카르트의 이론은 ‘심신상호작용설’이라고 일컬어진다. 인간에서는 정신과 신체가 상호 관계하지만 사실상 그는 신의 영역과 물체의 기계적 세계, 그리고 정신의 세계라는 세 가지 차원의 세계를 서로 독립적으로 분리한 셈이다. 그래서 그의 인간관은 “기계속의 유령”과 같은 말로 조롱되기도 하였다. 세계를 하나의 전체로 보는 합리론적 관점에서는 그의 이런 설명은 더더욱 받아들여지기 어려웠기에 이후의 합리론자들인 스피노자와 라이프니츠는 그의 이론에서 대두된 이 어려움을 피할 수 있는 새로운 이론을 내세우려고 노력하였다. 참조: 서양근대철학. 서양근대철학회, 창작과 비평사. 2001. 106-116 서양근대철학의 열 가지 쟁점. 서양근대철학회. 창비 2004. 164-202",
      "frontmatter": {}
    },
    "08.인간과 지식 2": {
      "path": "/06.university/철학과-인간/08.인간과-지식-2/",
      "filename": "08.인간과 지식 2",
      "content": "John Locke(1632–1704)와 경험론의 시작 (우리의 모든 지식은 외부 대상의 경험으로부터 유래하는가?) 인간지성론. 1690. (An Essay concerning human understanding) 지식의 원천 ＝ 경험 로크는 “우리의 모든 지식은 경험에 근거를 두고 있으며, 궁극적으로는 경험에서 지식이 도출된다.”고 주장했다. 그는 인간이 태어났을 때부터 갖고 있다는 본유 관념의 존재를 부정했다. 만약 그런 것들이 있다면 어린아이나 백치들도 손쉽게 수학이나 논리학 등을 알 텐데 그렇지 않다는 점을 들어서 결국 우리의 지식 중 본유적인 것은 아무 것도 없다고 말한다. 단순관념과 복합관념 그는 우리 의식이 갖는 모든 대상, 즉 감각, 지각, 기억, 상상 등을 모두 관념이라고 부른다. 이 관념들은 다시 단 하나의 내용만을 갖고 있는 단순관념과 이 단순관념들이 결합된 형태로 있는 복합관념으로 구분된다. 그는 이제 이 관념들에 대하여 그것들이 과연 우리 의식에서만 그런 내용을 갖고서 존재하는지, 아니면 경험·자연으로부터 그 기반을 갖고 있는지를 가늠하고자 한다. 먼저 단순 관념의 경우 그것은 우리 외부 대상들로부터 산출된 것이기에 ‘실재적’이라고 했다. 로크에 의하면 우리는 단순관념들을 스스로 만들 수 없기 때문에 단순관념들은 모두 실재적인 것에 합치한다. 반면 복합 관념들은 어떤 것은 실재적이고 어떤 것은 공상적이다. 복합된 관념들일지라도 우리 외부에 존재하는 사물들에 준거해서 만들어졌으면(＝연합됐으면) 그것은 실재적이다. 하지만 우리가 단순관념들을 결합시킬 때 상상을 덧붙여 우리 임의대로 복합관념을 산출했다면 그것은 거짓이고 공상적인 관념일 뿐이다. 이렇게 로크는 우리 인식의 참·거짓 여부를 그것과 외부 대상과의 합치에 준거하여 판단한다. 따라서 그는 경험론자이면서 또한 실재론자인 셈이다. 대신 우리가 의식 속에 갖고 있는 것들은 관념들이어서 이들은 실재하는 대상들과 구분된다는 생각을 했기 때문에 상식적인 실재론이 아니라 표상적 실재론을 쫓는 것이다. 제 1성질과 제 2성질의 구분 그의 이런 생각은 그가 물체의 제 1성질과 제 2성질을 구분한 것에서 잘 나타난다. 로크는 어떤 상태에서건 물체와 분리할 수 없고 어떤 변화가 있더라도 지속적으로 유지되는 성질들이 있다고 했고 이를 제 1성질이라고 부른다. 반면 어떤 성질들은 대상 그 자체에는 없지만 우리 안에는 특정하게 나타나는 그런 성질들이다. 제 1성질들로서 로크는 고정성, 연장성, 모양, 운동성, 수, 용적, 조직을 거론했고, 제 2성질들로는 색깔, 소리, 맛, 냄새, 촉각 등을 꼽았다. 제 2성질 같은 경우 물체 자체는 그저 어떤 힘만을 갖고 있는데 이것이 우리 감관을 자극하면 우리에게는 어떤 성질처럼 나타날 뿐이다. 그래서 이런 종류의 지각을 일으키는 원인은 물체 자체에 있지만, 그 산출물, 결과물인 지각내용들은 물체에는 있지 않은 그런 것들이다. 우리의 관념들이 외부 대상으로부터 온 것은 맞지만 관념의 내용은 대상과 차이가 나므로 그의 이론은 관념과 실재의 차이를 인정하고 따라서 지각 표상설의 범주에 속한다고 할 수 있다. 데이비드 흄 (David Hume, 1711-1776) 『인성론』 (1739) (A Treatise of Human Nature) 『인간 지성에 관한 탐구』 (1748) (An Enquiry Concerning Human Understanding) 인상과 관념의 구분 흄은 인식에 있어서 먼저 인상과 관념을 구분한다. 인상(impression)이란 생생하고 직접적인 경험으로서 지금 내가 체험하고 있는 모든 것들이다. 흄은 이 인상만이 이 순간 실재하는 확실한 것이고, 단 한 순간이라도 지나가면 바로 이전 순간에 지각됐던 인상들은 기억이라는 절차를 통해서만 소환될 수 있는 간접적인 관념이 된다고 했다. 관념들은 따라서 이미 없어진 것을 일으켜 불러내는 덜 생생한 것이고 그저 기억이나 상상을 통한 인상의 복사물에 불과하다. “모든 관념은 그에 앞선 인상에서 나온다.” 이제 우리는 이 (단순)관념들이 서로 근접해서 있거나 서로 유사하면 이들을 연합해서 복합 관념들을 만들기 마련이다. 이렇게 만들어진 복합 관념들은 우리의 지식을 형성하는데 그 중 대표적인 복합 관념인 인과성의 관념에 흄은 특히 주목한다. 인간의 기초신념들과 인과성의 관념 인과성의 관념은 흄에 의하면 여러 일반적인 관념들 중 하나가 아니라 우리의 모든 지식들이 기초와 토대로 삼는 기초신념들 중 하나이다. 흄은 인간이 갖고 있는 모든 지식들이 몇 개의 기본 전제, 가정을 토대 삼아 이를 기초로 해서 하나의 전체를 이루고 있다고 생각했다. 그는 전부 세 개의 기초 신념들을 얘기했는데 바로 외부세계의 존재, 자아의 존재 그리고 인과성의 관념이었다. 단순하고 쉽게 말하자면, 일단 나 자신이 있어야 하고, 또 내 앞의 이 세계가 있어야 하고, 그리고 이 세계가 인과성의 원칙에 의해서 운동한다고 믿을 수 있어야만 이제 내가 이를 기초로 해서 무언가 의미 있는 인식이나 지식을 쌓아 올릴 수 있다는 것이다. 흄은 이 세 가지 근본신념들이 정당한지를 검토하는데, 결론부터 말하자면 그는 이 셋 모두 경험적 근거가 없고, 정당성이 없는 믿음들에 불과하다고 선언하게 된다. 그래서 결국엔 우리 인간은 우리의 모든 지식들을 정당성 없는 믿음들 위에 세워서 갖고 있다는 회의주의적 진단을 내리고 있다. 인과추론의 본성 그러면 이제 우리는 흄이 왜 인과성에 대한 신념을 정당성이 없다고 비판했는지 그의 논증에 따라 밝혀 보기로 하자. 먼저 우리가 인과의 관념을 떠올리게 되는 상황을 살펴보면, 우리는 어떤 사태 A가 발생하면 항상 다른 사태 B 역시 뒤따른다는 것을 경험하게 된다. 이런 경험이 계속해서 반복하면 어느 때 우리는 사태 A가 발생했을 때, 바로 그 순간에 사태 B가 이제 발생할 것이라고 예측하게 된다. 이때 예측하는 시점은, 사태 A는 발생했지만 사태 B는 아직 일어나지 않았을 때이다. 흄은 우리가 이렇게 예측하는 이유는, 우리가 두 사태 A 와 B 사이에 원인과 결과라는 관계가 있다고 생각했기 때문이라고 설명한다. 그런데 또 우리가 이렇게 원인과 결과로 이 두 사태를 연합하려는 이유는, 이 두 사태가 단지 비슷한 성질의 사태이고 시간적, 공간적으로도 근접해 있기 때문만은 아니다. 우리가 ‘이 둘은 서로 필연적으로 연결된다.’고 믿을 때에만 우리는 굳이 이 둘을 인과관계로서 바라보게 되는 것이다. 문제는 이 필연적 결합이 과연 경험에서 발견되느냐는 것이다. 흄은 우리가 경험에서 발견하는 것은 그저 이 두 사태가 잇따라 발생한다는 것뿐이고 또 이것이 계속해서 반복된다는 것뿐이라고 지적한다. 따라서 필연적 결합이라는 계기는 경험에서 전혀 발견되지 않는다. 그럼에도 불구하고 우리는 사실 누구나 이 두 사태가 인과적이라고 여긴다. 그런데 흄은 그 정당한 이유를 경험에서 발견하지 못했고 따라서 이를 이제 우리의 마음에서 찾고자 했다. 그에 의하면 우리는 자연을 바라보며 그것이 이전에도 그랬던 것처럼 앞으로도 똑같은 방향으로 진행될 것이라고 믿는 경향이 있다고 한다. 이런 자연의 일양성(uniformity)을 굳게 믿는 우리는 이제 사태 A가 발생했을 때 과거의 무수한 경험을 바탕으로 해서 또 다시 사태 B가 발생할 것이라고 ‘추론’하게 된다. 즉, 인과성의 관념은 우리 마음의 일종의 추론에 불과한 것이다. 그래서 이미 경험한 ‘기억’으로부터 아직 발생하지도 않은 것을 ‘기대’하게 되며 이것이 바로 인과성 관념의 본성인 것이다. 하지만 이 추론의 근거는 이미 살펴본 바와 같이 우리의 기대, 믿음, 신념일 뿐이다. 인과 추론의 밑바탕에서 움직이는 것은, 경험한 것에서 경험하지 않은 것으로, 발생한 사태에서 발생하지 않은 사태로 전이하는, 즉 옮겨가는 우리 마음의 활동일 뿐이다. 이런 신념은 그 자체가 경험적 관찰을 넘어선 어떤 것에 대한 신념이기 때문에 경험적인 정당성이 없다. 또한 논리적으로도 정당하지 못한데, 우리는 경험적인 사태 A의 결과로서 굳이 사태 B 뿐 아니라 다른 사태 C, D 등을 얼마든지 상상해 볼 수 있기 때문이다. 따라서 그것은 우연적인 결과이지 필연적인 추론에 의한 도출은 아니므로 논리적으로도 정당화되지 못한다. 그래서 이제 인과성이라는 관념은 아무런 논리적 필연성도 없고 경험적 정당성도 없는 한갓 믿음으로 전락하고 만다. 그럼에도 불구하고 우리 인간은 평소에 이 신념을 확실히 믿는다. 누구나 내일 태양이 떠오른다고 믿고, 누구나 높은 곳에서 물건을 놓으면 아래로 떨어져서 깨질 것이라고 믿고, 누구나 과속으로 달리다가 사고가 나면 크게 다칠 것이라고 믿기에 매일 조심스럽게 운전을 한다. 따라서 우리가 이렇게 믿는 원인은 우리의 자연적 경향에 있다고 흄은 보고 있다. 우리는 본성상, 본능적으로, 습관적으로 그렇게 믿고 그것에 따라 행동한다는 것이다. “이제 과거의 반복으로부터 진행하는 모든 것을 습관이라 부름으로써 우리는 다음을 확실한 하나의 진리로 확립할 수 있다. 즉 현재의 인상에 뒤따르는 그 어떤 신념도 오로지 그러한 근원(즉, 습관)으로부터 온다. 우리가 두 개의 인상이 서로 연관되는 것을 보는데 익숙해질 때 관념은 즉시 우리를 다른 하나의 관념으로 움직이게 만든다.” “대상들 사이에서는 아무런 상호 연결도 발견할 수 없다. 또한 우리가 하나의 출현으로부터 다른 하나의 존재를 추론할 수 있는 것은 상상력에 작용하는 습관 이외의 그 어떤 다른 원리에 의한 것이 아니다.” 흄의 경험론적 이론은 단순히 습관에 의해서 인과성을 믿는 우리 인간을 합리적·이성적 존재로 그리지 않고, 오히려 본능, 습관과 같은 감성과 느낌에 의해 이끌리는 자연적 존재로 보았다. 그리고 우리의 지식에 숨겨져 있는 전제와 기초들이 이렇게 자연적 본성에 의해 생겨난, 엄밀하게 보자면 그 정당성이 결여된 한갓 습관이나 믿음에 불과하다고 폭로하고 있다. 인식론의 쟁점들과 경험론 관련 참조: 철학에의 초대. 헌트. 서광사 1992. 79-130쪽. 서양근대철학의 열 가지 쟁점. 서양근대철학회. 창비 2004. 119-127쪽. 흄. 이룸. 2004. 39-53쪽.",
      "frontmatter": {}
    },
    "09.인간과 지식 3": {
      "path": "/06.university/철학과-인간/09.인간과-지식-3/",
      "filename": "09.인간과 지식 3",
      "content": "칸트와 당시의 학문적 배경 우리는 오늘날 정보의 시대에 살고 있다. 학문의 세계도 그 덕을 많이 보기 때문에 이제 세계 어느 곳에서 어떤 유명한 학자가 어떤 중요한 학적 결실을 발표하면 멀리 있는 다른 사람들도 이를 빠르게 접할 수 있다. 대신 오늘날의 학문은 전문화와 세분화가 심해서 한 특정 분야에서 일어난 일들은 바로 그 분야의 전공자들 외에는 큰 관심을 받지 못하기 마련이다. 그래서 대부분의 중요한 학문적 성과들도 화젯거리조차 되지 못하는 일이 비일비재하다. 약 300년 전의 유럽은 이런 면에서는 오히려 더 활발한 학적교류가 있었던 것처럼 보인다. 물론 그 당시에는 오늘날처럼 기술문명이 발달하지 못했으니까 정보교환이 그렇게 빠르지는 않았지만 대신 학문적 관심은 더 집약적이어서 어느 학자의 새로운 이론에 많은 학자들의 관심이 몰리는 일이 빈번했다. 당시 철학자들의 관심과 추구하는 바는 이미 살펴본 바와 같이 인식론에 우선적으로 집중되어 있었기 때문에 그들의 생각과 토론은 어느 정도 공통된 기반 위에서 이루어지고 있었기 때문이다. 당시 학자들은 서신을 통한 학적 교류를 즐겼고, 자신의 저작들에 대한 다른 학자들의 비평 등에 민감하게 반응했었다. 오늘날의 기술 수준에 비교해서 초라한 당시의 사정을 감안하면 근대의 학문적인 의사소통은 놀랄 만큼 활발했던 것이다. 당시의 이론들은 구조적으로나 역사적으로 서로 연관성이 뚜렷했기에 근대 초기부터 약 150여년 정도의 기간 동안 그들은 인식론과 그에 기초한 이론들을 연속성을 가지고 발전시켜 나갈 수 있었다. 독일의 철학자 임마누엘 칸트가 쾨니히스베르크 대학에서 학업을 시작했을 때 유럽의 철학은 근대와 함께 시작된 여러 가지 새로운 주제들, 즉 인식, 인간, 철학의 정체성 등에 대한 논의가 무르익을 대로 익었다. 이런 상황에서 칸트는 그때까지의 서양철학의 진행을 완결시킬 수 있을 만한 혁신적인 계기를 찾으려 했고, 그 모든 발전을 하나의 거대한 체계로 통합하려는 시도를 하게 된다. 그는 그때까지 전개된 이론들의 결정적인 요소들을 모두 통합하여 새로운 체계 안에 넣었고, 그 안에서 우리는 이전 이론들에서 논의됐던 요소들을 모두 다시 발견할 수 있을 것이다. 철학의 위기: 경험론과 이성론의 한계 이미 학기 초에 언급했듯이 칸트는 새로운 근대적 이성 개념에 근거하여 철학의 새로운 가능성을 찾으려 했다. 칸트가 이렇게 새로운 철학을 꿈꿨던 이유는 이전의 철학이 위기에 처해 있었다는 나름의 진단이 있었기 때문이다. 그는 근대에 들어서 생겨났던 많은 사조들이 결국에는 만족할 만한 결과를 가져다주지 못하고 우리를 오히려 혼란에 빠뜨렸다고 생각한 것이다. 그는 먼저 우리가 경험론의 견해를 쫓으면 전적인 회의주의에 빠질 수밖에 없어서 확실한 인식의 가능성을 포기해야 한다고 했다. 흄과 같이 인과성 관념을 비롯한 지식의 기초신념들은 모두 한갓 습관이나 상상력 등에 기인한 근거 없는 믿음이라는 결론을 내리면 우리 지식 전체도 마찬가지로 불확실하고 근거 없는 것이 되고 만다. 반면 이성론의 생각에 동의하면, 우리가 어떤 것을 명증적으로 인식할 때 그것은 곧바로 실재적이고 존재하는 것이 된다.(플라톤의 이데아와 같이) 자아 뿐 아니라 신과 같은 존재도 우리 이성이 필연적이라고 밝히기만 하면 그것의 실재성이 동시에 주장되는 것이다. 그리고 더 나아가 그것의 경험적 근거를 찾지도 않고 그 개념으로부터 출발해서 하나의 체계를 세우려 한다. (데카르트, 스피노자, 라이프니츠 모두 신과 이성적 원리에 의한 세계의 전체적인 모습을 자신들의 이론들에서 표현했다) 이런 태도를 칸트는 독단적이고 교조주의적인 태도라고 부른다. 중세 교회에서 교리라고 공표한 것들은 의심을 허용하지 않고 무조건적으로 따르고 믿어야 하는 것들이었다. 이성론자들은 자신들의 생각, 혹은 주장이 경험이나 실재와 부합하는지 입증하려 하지도 않았고 그저 이성적, 논리적 원칙에 의해서만 진행, 발전시켰기에 그런 의미에서 이를 독단적이고 교조주의적이라고 칭했다. 그래서 칸트는 이 두 방면으로부터의 위기를 모두 극복하고 새로운 정당한 철학을 세우는 것을 목표로 삼았다. 이제 우리는 그가 이것을 어떤 전략으로 추구했고 또 그것이 얼마나 설득력 있는지 살펴보기로 할 것이다. 본격적인 철학체계를 세우기 전에 반드시 필요한 작업이라고 그가 생각한 것은 바로 순수 사변 이성에 대한 비판이었다. 칸트와 순수이성비판 \\1) 재판관으로서의 이성 \\- 먼저 그는 이런 상황에 직면한 인간 이성은 일종의 재판관과도 같은 역할을 수행해야 한다고 생각했다. \\- 왜냐하면 지식의 위기와 혼란으로 인해 지금까지의 학문들이 쓸모없다고 판단된 이상, 더 이상 그릇된 지식에 빠지지 않도록 모든 주장들을 그것들의 정당성에 따라 수용하거나 거절할 수 있는 법정과도 같은 것이 필요하다고 생각했기 때문이다. --> 순수이성비판 \\2) 순수이성비판의 의미 \\- 순수이성비판은 특정한 대상에 대한 비판이 아니라 자기 자신, 즉 이성 (능력) 일반을 비판함을 의미한다. = 자기 인식. \\- 비판, 혹은 판단하는 주체도 이성이고 법정에 서는 그 판단 대상, 비판 대상도 역시 이성이다. (Kritik der reinen Vernunft) \\- 그래서 정당한 주장을 펴는 이성은 보호하고, 반면 근거 없는 모든 부당한 주장들은 거절하려 한다. \\3) 순수이성비판의 과제 \\- 이성 능력 일반을 비판적으로 파악하면 이제 그것에 의한 학문인 형이상학에 관해서도 더 확실한 판단을 할 수 있으리라는 것이 칸트의 생각이었다. 그래서 순수이성비판이 궁극적으로 추구하는 것은 형이상학이라는 것이 도대체 가능한 것인지, 그리고 만약 가능하다면 어떤 원천으로부터 생겨날 수 있는지, 어디까지 가능할지(범위), 그리고 그 한계는 어디까지인지를 규정하는 것이다. \\4) 새로운 형이상학 \\- 칸트가 우선 밝히고자 했던 것은 우리가 대상들을 인식하고 자연을 파악하는 데에 사용하는 원리들 혹은 규칙들은 무엇인가이다. (그 결과 우리는 이 원리들을 경험으로부터 얻지 않고 선험적으로 갖고 있음을 밝히게 된다.) 또한 이 원리들을 어떤 조건에서, 그리고 어느 범위 내에서 사용해야 할 것인지 역시 밝히고자 했는데 이것이 칸트 형이상학의 새로운 점이다. \\- 왜냐하면 이전 형이상학에서는 이성적 개념과 원칙들을 그것들이 사용될 수 있는 조건들을 따지지 않고 그저 무조건적으로, 즉 모든 종류의 대상들에 다 적용해서 지식을 얻었다. 그런데 이제 새로운 형이상학에서는 이성의 규칙들이 사용될 대상들이 경험적인 성격의 대상들인지, 아니면 감각으로 경험할 수 있는 한계를 넘어서 있는 대상들인지를 따짐으로써 이성적 인식의 한계를 명확히 규정해 주고자 한다. (이렇게 확정된 것만이 진정한 학문으로서의 형이상학이라고 칸트는 보았다. 그렇지 않으면 그저 뜬구름 잡는 소리와도 같은 것. 근거도 없고 그저 사변적인 사고, 그러니까 공상이나 상상력에만 의지하는 사고의 유희, 놀이에 불과하다고 보았다) \\- 이렇게 ‘비판’에 의해서 이성이 할 수 있는 일과 할 수 없는 일이 명확히 밝혀지면, 그때까지의 형이상학의 주장들 대부분이 월권적인, 즉 부당한 주장이었다고 밝혀낼 수 있기 때문에, 결국 형이상학의 전반적인 혁명이 가능할 것으로 그는 또한 보았다. \\5) 형이상학의 제한과 확장 (- 이성의 자기 자신의 능력에 대한 선행적 비판이 없다면 이성은 순전히 교조주의적인 방식으로 이성을 사용할 것이다.) \\- 이 비판을 통해서 이성이 사용될 수 있는 영역이 제한되면 이제 그 영역을 넘어서는 세계에서 다른 방식으로 비경험적인 대상들에 대해 생각해 볼 수 있다; 자연기계성에 대한 자유, 세계의 시초로서의 신, 영혼의 단순한 본성. \\- 이런 대상들을 생각할 때 이들을 경험적 대상들과는 다른 종류의 대상들로 다루며 자연세계로부터 구분할 때 (비로소, 오히려) 새로운 영역의 확장이 이루어진다. 인간 이성의 구조 칸트는 우리의 이성이 갖고 있는 능력, 혹은 본성을 크게 두 가지로 보았는데 감성과 지성이 그것이다. 1) 감성 (Sinnlichkeit, sensibility) 그 중 외부로부터의 모든 감각적 자극을 받아들이는 능력을 감성이라고 한다. 칸트는 감성에게 시간적·공간적으로 정리하는 기능까지 부여한다. 즉, 감성은 지각된 것을 그저 받아들이기만 하는 것이 아니라 그것들에게 ‘지금’, ‘이전’, ‘나중’이나 ‘여기’ 혹은 ‘저기’와 같은 시·공간적 질서를 부여해 준다. 감성이 이렇게 정리한 것(직관)은 생각하는 힘인 지성에게 소재로서 제공된다. 2) 지성 (Verstnad, intellect) \\- 감각으로 얻은 것을 재료로 하여 사고 작용을 하는 능력. \\- 개념을 통하여 이 재료들을 종합(연결)하여 지식으로 정돈하는 의식의 작용. 인식의 구조: 질료와 형식 칸트는 우리의 인식을 질료, 즉 재료와 형식으로 구분할 수 있다고 생각했다. 그 중 질료는 우리 외부로부터 우리의 감각기관을 통해 주어지고, 형식은 앞서 말한 바와 같이 그 이전에 미리, 즉 선험적으로 우리에게 구비돼 있는 것이다. 이 형식은 우리 이성의 두 부분인 감성과 지성에 각각 있는데, 칸트 철학의 핵심적인 부분은 이 인식의 형식이 무엇인가를 밝히는 데에 있다. “현상에서 감각에 대응하는 것을 나는 그것의 질료라고 부르며, 그러한 현상의 잡다한 것이 일정한 관계 속에서 질서지어질 수 있도록 만드는 것을 나는 형식이라고 부른다.” (순수이성비판 A20 B35) 칸트철학의 주요개념인 ‘선험적 (a priori a posteriori)’에 대하여 경험에 의존하지 않고 경험에 앞서는 인식을 선험적 인식이라고 말하고, 경험에 기초하여 생겨나는 인식을 후험적 인식이라고 부른다. 여기서 말하는 선후란 발생적인 의미에서가 아닌 논리적인 선후이다. 따라서 선험적이라고 하는 것은 경험적 인식보다 원리적으로 앞서서 이것의 전제가 되는 조건을 의미할 때 사용된다. “우리의 모든 인식이 경험과 함께 시작된다는 것은 전혀 의심할 여지가 없다. … 그러므로 시간상으로는 우리에게 어떠한 인식도 경험에 선행하는 것은 없고 오직 경험과 함께 모든 인식은 시작된다. 그러나 우리의 모든 인식이 경험과 함께 시작된다 할지라도, 그렇다고 해서 우리의 인식 모두가 바로 경험으로부터 생겨나는 것은 아니다. 왜냐하면 우리의 경험인식조차도 우리가 감각인상들을 통해 수용한 것과 우리 자신의 인식능력이 자기 자신으로부터 산출해 낸 것의 합성이겠으니 말이다.” (순수이성비판 재판 서론. KrV B 1) 칸트는 우리 인간이 우리 바깥에 있는 무엇을 경험하기도 전에 이미 어떤 것을 자신 안에 갖고 있고, 이것이 인식을 하는 데에 있어서 일종의 조건, 혹은 형식이 된다고 생각했다. 인식의 질료는 감성에 의해서 수용되므로 후험적, 경험적이지만, 인식의 형식은 어떠한 경험에도 의거하지 않은 채 이미 인식 주관에 구비되어 있기 때문에 선험적인 것이다. 이렇게 선험적인 조건들이 있다고 주장하고 이것이 인식을 형성하는 데에 영향을 끼친다고 하는 것은 곧 인간의 인식이 단순히 수동적인 행위, 즉 경험으로부터 얻어지는 것을 그저 받아들이기만 하는 행위가 아니라 적극적이고 능동적인 인식 주체의 활동이라고 주장하는 것을 의미한다. 인간의식이 갖는 이러한 선험적 요소와 기능은 한낱 주관적인 성격에 머무르지 않고 한 걸음 더 나아가 우리 외부에 있는 대상을 경험하고 규정해서 결국 인식을 가능하게 해 준다는 것까지도 의미할 수 있다. 인간의 지식(인식)은 이 선험적 형식(조건)이 기능했을 때에만 성립되고, 이럴 때에만 비로소 우리에게 어떤 대상으로 나타날 수 있다는 것이다. 물론 일상적으로는 우리의 인식과 관련해서 감각 혹은 경험적인 내용들만을 생각하지 우리가 갖고 있다고 하는 이런 형식적인 것들은 전혀 생각하지 않지만, 칸트는 이제 역으로 선험적인 조건들이 형식으로 작용할 때에만 그 결과로서 우리가 어떤 것을 하나의 대상으로서 인식할 수 있다고 주장한다. 극단적인 표현일 수도 있지만, 이 형식이 없으면 인식 대상도 없는 것이라고 말할 수도 있겠다. 그래서 칸트는 “경험 일반을 가능하게 하는 조건들은 동시에 그 경험의 대상들을 가능하게 하는 조건들”이라고도 말한다. 경험 자체도 선험적 형식들이 없었다면 불가능하고, 또 그것의 내용들인 대상들도 우리의 선험적 형식 탓에(혹은 덕분에) 우리에게 나타나는 바로 그대로의 대상들이 되는 것이다. 만약 주관의 형식들이 없었다면, 혹은 그것들이 지금 갖고 있는 것들과 전혀 달랐다면 우리의 인식도 다르게 성립되었을 테고 그 인식의 내용인 대상들도 지금과는 다르게 우리에게 나타났을 것이다. 이처럼 처음엔 의식되지도 않았던 주관적 형식들이 이제는 모든 것을 가능하게 해주는 출발점이 되는 셈이다. 이렇게 칸트는 문제의 중심을 인간 외부에서 우리 인간과 그것의 내면적인 형식(조건)들에게로 전환했고, 이를 천동설에서 지동설로 전환된 천문학적 사건에 빗대어 ‘철학의 코페르니쿠스적 전환’이라고 부른다. (참고: 선험적 형식들은 본래는 그저 인간의식의(즉, 내적인) 요소들일 뿐이지만, 이제 이 주관적인 것들이 이렇게 그 주관성을 넘어서 객관으로 초월한다는(넘어간다는) 의미에서 이 형식들은 ‘초월적(transzendental)’이라고 불리기도 한다. 선험적인 형식들은 단지 우리 의식에 내재하는 것에 그치는 것이 아니라 그 영향력이 주관을 넘어서 우리 외부의 대상들에까지 미치기 때문이다) 감성의 형식: 공간과 시간 칸트는 우선 감성에서 감각에 의해 갖게 되는 경험적인 것, 즉 질료적인 것은 후험적으로 주어지지만, 그것에게 질서를 주는 어떤 것은 다시 질료가 될 수 없기 때문에 어떤 형식적인 것이어야 한다고 생각했다. 물론 이 형식적인 것은 질료적인 것이 주어지기 이전에 이미 내게 구비되어 있는 것이다. 따라서 칸트는 이것을 질료와 구분해서 따로 고찰해야 한다고 생각했는데, 결론적으로 공간과 시간을 감성의 두 가지 선험적인 형식이라고 간주했다. 칸트는 공간과 시간에 대한 다각도의 설명을 통해, 이들이 경험적이지 않은 선험적인 형식이라는 점, 그리고 지성의 개념처럼 추상이나 추론을 통한 것이 아니기에 어디까지나 감성에 속한다는 점을 보여주려 했다. 1) 공간과 시간은 경험들로부터 추출된 경험적 개념이 아니다. \\- 내 밖의 어떤 것이 있다는 감각을 얻으려면 이미 공간이라는 표상이 그 기초에 놓여 있어야 한다. 오히려 이 외적 경험이라는 것 자체가 공간이라는 표상을 통해서 비로소 가능하다. \\- 우선 시간을 전제해야만 동시에, 혹은 잇따라 등을 표상할 수 있다. 2) 공간과 시간은 모든 (외적) 직관의 기초에 놓여 있는 선험적이고 필연적인 표상이다. \\- 공간에서 아무 대상을 마주치지 않는다는 것은 상상할 수 있어도 공간이 없다는 것은 표상할 수 없다. \\- 시간들로부터 현상들을 완전히 제거할 수 있을지는 몰라도, 시간 자체를 제거할 수는 없다. 3) 공간과 시간은 사물들 일반의 관계에 대한 보편적인 개념이 아니라 순수한 직관이다. \\- 우리는 단 하나의 공간만을 표상할 수 있을 뿐이다. 많은 공간들은 하나의 공간의 부분들일 뿐이지 이 많은 공간들이 합쳐져서 하나의 공간을 이루는 것이 아니다. 우선 공간이라는 표상이 있어야 그 후에 공간에 관한 모든 다른 표상들이 가능하다. \\- 서로 다른 시간들은 동일한 시간의 부분들일 뿐이다. 단 하나의 대상에 의해서만 주어질 수 있는 표상은 직관이다. 종합적인 성질을 가진 개념으로부터는 이 표상이 생겨날 수 없다. 4) 공간은 무한한 양으로 표상되는데 우리는 무한히 많은 표상들을 자기 아래에 포함하고 있는 어떤 개념도 생각할 수 없다. 그러므로 공간은 선험적 직관이지 개념이 아니다. 근원적 시간은 무제한으로 표상된다. 각각의 시간들은 이런 무제한적인 시간을 제한함으로써 생겨난다. 이런 전체 표상은 개념에 의해서 주어지지 않고 그 기초에 직관이 놓여 있다. 5) 결론적 주장 \\- 공간은 어떤 사물들 자체의 속성이나 그것들의 관계를 나타내지 않는다. 그저 우리의 외감, 즉 외적 대상들과 관계하는 감성의 형식일 뿐이다. 우리는 오직 이 공간이라는 주관적 조건에 의해서만 외적 직관을 가질 수 있다. 우리가 외부의 대상들을 받아들일 때에 항상 ‘서로 곁하여’라는 공간적 형식을 통해서 수용하게 된다. \\- 시간은 우리 의식의 내감의 형식으로서 우리가 내 의식 속의 표상들을 수용할 때 항상 ‘서로 잇따라’라는 시간적 형식을 갖기 마련이다. 시·공간의 (초월적) 관념성과 (경험적) 실재성. 1) 주관적 형식 공간과 시간은 어디까지나 인식 주관인 인간의 형식이다. 우리는 오직 이를 통해서만 대상을 인식한다, 하지만 이런 우리 인간의 인식조건을 떠나 사물 자체의 모습이 어떠한지 우리는 알지 못한다. 이 사물이 그 자체의 모습만을 놓고 보면 공간과 시간에 구속되는지 어떤지는 우리가 알 길이 없다. 이런 면에서 공간, 시간은 우리의 경험을 떠나서 생각해 보면 한낱 우리의 관념, 생각, 형식에 불과한 것이다. 따라서 초월적, 혹은 반성적, 철학적으로 생각해 보면 이들은 그저 관념에 불과하다는 것을 알 수 있고, 따라서 이들은 관념성을 지닌다고 볼 수 있다. 이런 면으로 공간, 시간은 어디까지나 (인간에게만 국한돼 있다는 의미로) 제한적, 상대적, 조건적이고, 이런 형식을 사물 자체도 갖고 있는지는 회의적이다. 2) 경험에서의 타당성 그런데 이 공간과 시간은 우리 인간에게는 선험적이다. 즉 보편적이고 필연적이다. 인간이라면 누구나 공간과 시간이라는 형식과 틀을 갖고서 사물을 바라볼 수밖에 없다. 즉 인간인 한에서는 누구나 이 조건을 사용해야 하고 모든 대상들을 이 형식을 갖고 받아들이게 된다. 이렇게 우리 인간의 경험적인 관점에서 보면 이 형식은 객관적이고 보편적으로 타당하므로 실재성을 갖는다. 우리에게 어떤 것이 주어지면 이것은 항상 공간과 시간의 형식을 띄기 때문이다. (절대적 실재성, 경험적 관념성과 반대) 1 감성의 형식인 공간과 시간: 부연설명 우리는 일상적으로 물체라는 것이 마치 우리 주변의 텅 빈 공간과 일정하게 흐르는 텅 빈 시간을 가로질러 가는 것이라고 생각한다. 이른바 ‘절대 공간’과 ‘절대 시간’이 고정적으로, 그리고 우리 외부에 독립적으로 존재해서 그 틀 위에 사물이 움직이는 것이라고 생각하는 것이다. 이는 우리의 상식적인 믿음일 뿐 아니라 뉴턴의 고전역학과 근대의 자연과학이 갖고 있던 일반적인 생각이다. 물론 칸트도 경험적으로는 그렇게 보이는 것을 부정하지 않았다, 그렇기에 공간과 시간이 (경험적 관점으로 보았을 때) 실재성이 있는 것으로 보아도 된다고, 즉 실제로 우리 주위에 텅 빈 공간과 텅 빈 시간이 따로 존재하는 것처럼 생각해도 된다고 했다. 칸트는 단지 그것이 우리 인간의 개입 없이도, 즉 우리 인간이 없을 때에도 정말 그러한지에 대한 보장이 없다는 점을 지적하는 것이다. 우리 인간이 신체적 조건을 가지고서 사물로부터 감각자료를 얻을 때만 그러한지, 아니면 그런 물리적인 접촉 없이 사물들 스스로 그 자체로도 공간적·시간적인지는 모른다는 것이다. 그래서 그것에 대해서는 모르는 상태 그대로 판단을 유보해야지, 마치 사물들 그 자체도 우리 인간이 그것들을 경험하는 것처럼 3차원적인 모양을 공간에서 갖고 있다고 여기거나, 과거에서 현재를 거쳐 미래로 나아가는 시간성을 지녔다고 생각하면, 우리가 알지도 못하는 것에게 우리의 추측을 뒤집어씌우는 것에 불과하다고 말하려는 것이다. 그래서 칸트는, 경험적으로는 공간·시간이 실재적이라고 여겨도 되고, 철학적으로는 그것들이 그저 우리 인간에게 속한 것, 우리의 형식, 우리가 갖고 있는 관념에 불과하다고 생각해도 된다고 보았으며 이런 이중적인 입장이 전혀 모순적이지 않다는 생각을 갖고 있다. 우리는 공간과 시간이라는 것이 어떻게 우리의 관념에 불과하기도 하고, 또 다른 한편으로 어떻게 우리 외부에 실제로 존재하는 것으로 생각될 수 있는가하고 반문할 수 있겠지만, 칸트는 이 두 생각들은 서로 보는 관점이 다르기 때문에 양립 가능하고, 둘 다 모순 없이 생각될 수 있다고 보았다. 이렇게 공간과 시간을 우리에게 속하는 것으로 규정하게 되면 경험이나 인식이라는 것의 의미가 보다 확실해 진다. 우리는 우리 외부의 모든 것을 공간적이고 시간적으로 받아들일 수 있지 다르게는 받아들이지 못하니까 그것들은 이제 일종의 틀, 형식, 비유적으로 말하자면 그물과도 같은데, 이제 이 그물에 걸리지 않는 것들, 다시 말해 공간적·시간적이지 않은 모든 것들은 진정한 의미의 인식의 대상이 아닌 것이다. 우리에게 비록 공간적·시간적으로 나타나지는 않았지만, ‘그 너머에’ 있는 것으로 ‘추측’되는 것은 실상 우리에게 아무 의미도 없는, 즉 우리에게 아무 것도 아닌 것이다. 일상적으로 우리가 무엇을 안다, 인식하다는 말을 대부분 무분별하게 사용하고는 있지만, 그것의 본래 의미는 실재하는 것, 정말로 있는 것에 대해서 우리가 안다는 것이다. 반면 있지도 않은 것, 있는지도 모르는 것을 내가 인식한다고 말하는 것은 아무 의미도 없을 것이다. 칸트적 관점으로 이제 의미 있고 실재적인 것은 공간적·시간적 속성을 가진 것뿐이니까 그렇지 않은 것, 그러니까 공간적·시간적으로 지각하지 못하는 것들, 예를 들어 영혼, 신 등에 대한 생각들은 그저 단순한 ‘생각’에 그칠 뿐이지 어떤 실재하는 것에 대한 ‘지식’이나 진정한 의미의 ‘인식’이 될 수 없다. 그래서 칸트가 공간과 시간을 우리 인식의 선험적 조건으로 본 것은 인식에 대한 기존의 의미를 대폭 제한하고 그 영역을 축소하는 결과를 가져오게 된다. 감성과 지성: 초월적 감성론과(transzenetale Ästhetik) 초월적 논리학(transzenetale Logik) 감성에서 선험적 형식으로서 공간과 시간을 찾았다면 이제 지성에서 선험적인 형식들이 무엇이지를 찾는 일이 남았다. 칸트는 감성 뿐 아니라 지성도 인간 인식의 형성에 필수적이라고 생각했다. “우리가 (...) 표상들을 받아들이는 우리 마음의 수용성을 감성이라고 부르고자 한다면, 이에 반해 표상들을 스스로 산출하는 능력, 바꿔 말해 인식의 자발성은 지성이다. (...) 감성적 직관의 대상을 사고하는 능력은 지성이다. 이 성질들 중 어느 것도 다른 것에 우선할 수 없다. 감성이 없다면 우리에겐 아무런 대상도 주어지지 않을 터이고, 지성이 없다면 아무런 대상도 사고되지 않을 터이다. 내용 없는 사상들은 공허하고, 개념들 없는 직관들은 맹목적이다.” (순수이성비판 A51 B75) 칸트는 감성과 지성은 그 기능을 서로 바꿀 수 없고, 오직 이 두 능력이 통일됨으로써 인식이 생길 수 있다고 보았다. 이 둘이 모두 있어야 의미 있는 인식이 성립하니까 이 둘은 인식에 있어서 서로 떼려야 뗄 수 없는 관계인 셈이다. 칸트는 이 둘의 관계를 이렇게 설정함으로써 경험주의적 전통과 합리주의적 전통을 종합했다고 평가받고 있다. 경험주의는 인식의 기원이 우리 외부에 있다는 점만 강조했지, 그것이 어떻게 우리가 인식으로 불리는 것으로 형성되었는지 설명하지 않았고, 이성론적 전통에서는 우리 이성이 어떻게 작용해서 명증적인 상태에 도달했는지 만을 연구했을 뿐 우리 외부의 대상들과의 관계에 대해서는 설명이 부족했다. 이제 칸트는 감성과 지성이 모두 필요하고 오직 이 둘이 연합해야만 인식이 형성된다고 보았으므로, 이 둘이 어떻게 인식이라는 결과물을 산출하는지를 밝히는 일은 자연스럽게 그의 다음 과제가 되었다. 이 지성에 대해서 다루는 이론을 칸트는 논리학이라고 부른다. 반면 감성 일반에 관한 이론은 ‘감성론(Ästhetik)’이라고 부르는데, 이 때 어원이 되는 ‘aistheia’라는 말은 ‘Logos’에 반대되는 의미로서 ‘감각에 의한 지식’을 의미한다. 그런데 칸트는 전통적으로 연구돼 왔던 논리학은 ‘일반 논리학’이라고 부르고 자신의 것은 ‘초월적 논리학’이라고 구분한다. 일반 논리학에서는 ‘순수한 사고의 작용들’과 그것들이 지니고 있는 규칙들만을 그 자체로, 즉 그것이 사용될 조건이나 그것과 연관된 대상들에 대한 고려 없이 다루는 데에 그치는 반면, 초월적 논리학에서는 이 규칙들이 어떻게 대상들과 관계 맺는가를 다루게 된다. 우리가 참된 인식, 즉 진리를 떠올리면 우선 생각나는 것은, 우리가 인식한 것과 실재하는 대상이 과연 부합하는지, 서로 대응하며 잘 맞아 떨어지는지의 여부인데, 형식적인 논리학은 여기서 충분한 답을 주지 못한다고 칸트는 생각했다. 왜냐하면, 우리가 익히 아는 동일률, 모순율 등의 형식적 사고의 규칙들은 올바른 사고가 갖추어야 할 필수적인 규칙들만을 설명하는 것이지 그 이상의 의미는 없기 때문이다. 그런 점에서 일반 논리학은 모든 진리의 소극적인 조건이다. 그래서 이제 칸트는 자신의 초월적 논리학이 지성의 규칙들과 대상들의 관계를 해명해줌으로써 진리와 참된 인식을 적극적으로 밝혀준다고 주장한다. 지성의 활동: 판단 초월적 논리학에서 칸트는 먼저, 감성이 받아들인 직관은 아직 무질서한 상태라는 점에 주목한다. 직관들은 그 자체로 아무 연관성이 없이 그저 (시간적 형식에 의해) 잇따라, 그리고 (공간적 형식에 의해) 서로 서로 곁에 있는 그런 형태로만 주어진다고 생각했다. 이 외에는 아무 연관이 없이 그저 잡다하게 주어진 이 자료들을 이제 지성이 연결해서 통일하고 질서를 부여해 준다고 그는 생각했다. 이것이 바로 지성의 역할인 것이다. 그러기 위해서는 일정한 규칙들이 필요한데 이것은 감성에서 찾을 수 없고 지성에게서만 발견될 수 있으며, 이것도 역시 선험적으로, 그러니까 경험적으로부터 얻어서 습득한 것이 아니라 우리에게 이미 “예비되어 놓여” 있는 그런 성질의 것이라고 한다. 이제 이 규칙들이 무엇인지 찾기 위해서 칸트는 우선 지성 일반을 탐구한다. 그 때 그는 “우리는 지성의 모든 활동을 판단으로 환원할 수 있고, 지성 일반은 판단하는 능력이라고 표상될 수 있다”고 말한다. 여기서 판단이란 “서로 다른 표상들을 하나의 공통적인 표상 아래에서 정돈하는 통일 활동”을 말한다. 그래서 지성적 파악, 개념적 파악이란 다름 아닌 판단이고, 따라서 사고하는 능력인 지성은 곧 판단하는 능력이다. 그래서 칸트는 판단이라는 행위에서 드러나는 지성의 모든 논리적인 기능들과 통일의 기능들을 완벽하게 밝혀낼 수만 있다면 그에 상응해서 사고능력으로서의 지성의 기능들도 모두 발견될 수 있을 것이라고 생각했다. 이를 목표로 삼아 그가 정리한 지성의 모든 종류의 판단들은 다음과 같다. 판단들의 양(Quantität) 전칭(Allgemeine) 모든 S는 P이다 특칭(Besondere) 어떤 S는 P이다 단칭(Einzelne) 이 S는 P이다 판단들의 질(Qualität) 긍정(Bejahende) S는 P이다 부정(Verneinende) S는 P가 아니다 무한(Unendliche) S는 비(非)P이다 판단들의 관계(Relation) 정언(Kategorische) P는 Q이다/S는 P이다 가언(Hypothetische) 만약 P이면 Q이다 선언(Disjunktive) P이거나 Q이다 판단들의 양태(Modalität) 미정(problematische) S는 P일 수 있다 확정(Assertorische) S는 P이다 명증(Apodiktische) S는 P이어야 한다. 지성의 형식: 범주 지성은 이렇게 감성을 통해 자료로서 주어진 잡다한(=질서 없는) 직관들을 결합한다. 이러한 활동을 칸트는 ‘종합(Synthesis)’이라고 부르기도 하는데 우리가 앞서 살핀 방식과 같이 지성이 자료들을 종합하는 것을 보면 그것의 기준이 되는 것, 혹은 규칙이 되는 것이 그 판단에 앞서 있는 것이 아닐까하고 칸트는 생각했다. 그래서 그 기준이 되는 개념들도 판단들처럼 정확히 12가지가 있음에 틀림없다고 생각했는데, 이들이 곧 지성의 사고 형식, 즉 ‘범주(Kategorien)’인 것이다. 이렇게 발견된 결과물은 “지성이 선험적으로 자기 안에 함유하고 있는, 종합의 근원으로 순수한 모든 개념들의 목록”이고, 우리는 이 개념들, 즉 범주에 의해서만 무질서한 직관들의 혼란에서 무엇인가를 이해할 수 있으며, 그로 인해 대상을 개념적으로 파악하고 인식한다. 이 개념들이 없었다면 우리는 애초부터 아무 것도 이해하지 못했을 것이라고 칸트는 얘기한다. 그 개념들도 역시 양, 질, 관계, 양태의 네 가지 측면에서 각각 3 가지로 가능하다. 양의 범주들 하나(Einheit) : 단일성 여럿(Vielheit) : 다수성 모두(Allheit) : 총체성 질의 범주들 실재성(Realität) : ~이다 부정성(Negation) : ~이 아니다 제한성(Limitation) : ~은 아니다 관계의 범주들 내속성과 자존성(Inhärenz und Subsistenz): 우유(遇有)와 실체(實體)의 관계 원인성과 의존성(Kausalität und Dependenz) : 원인과 결과의 관계 상호성(Gemeinschaft) : 능동과 수동의 관계 양태의 범주들 가능성-불가능성(Möglichkeit-Unmöglichkeit) : ~일 수 있다 - ~일 수 없다 현존-부재(Dasein-Nichtsein) : 있다 - 없다/ 이다 - 아니다 필연성-우연성(Notwendikeit-Zufälligkeit) : ~이어야 한다 - ~일 수도 있고 아닐 수도 있다 판단과 범주 판단 범주 양 전칭특칭단칭 하나(단위)여럿(다수)모두(전체) 질 긍정부정무한 실재성(-임, -함)부정성(-아님, -아니 함)제한성(-이지는 않음) 관계 정언가언선언 내속성과 자존성(실체와 속성)의 관계원인성과 의존성(원인과 결과)의 관계상호성의 관계 양태 미정확정명증 가능성-불가능성현존-부재필연성-우연성 결국 이렇게 감성이 자료들을 외부로부터 공간적·시간적으로 받아들이고 그것을 지성이 자신에게 이미 선험적으로 구비되어 있는 12개의 개념들을 사용해서 종합 통일함으로써 하나의 대상, 나에 대한 어떤 것이 된다. 따라서 인식되는 존재는 인식하는 의식에 의해서 바로 그것으로 규정되는 것이다. 그러므로 사고의 형식인 범주는 인식의 성립 조건일 뿐 아니라 또한 그 인식에서 인식되는 대상의 성립조건이기도 한다. 인식을 가능하게 하는 조건이 또한 인식된 대상의 가능조건인 것이다. 전통적으로 진리는 “사물과 지성의 일치”라고 여겨져 왔고 이 때 인식하는 자가 인식되는 것에 동화됨으로써 이에 이르는 것으로 이해되어 왔는데, 이제 칸트는 진리의 기준을 더 이상 대상에 두지 않고 인식하는 자, 곧 의식에 둠으로써 자신이 바라던 철학의 혁신을 가져왔다고 믿는다. 그에게 있어서 인식을 가능하게 하는 형식원리는 인식되는 것의 존재원리가 되므로 그의 인식론은 곧 존재론이고, 존재론이란 그에게 인식론에 불과하다. 현상과 사물자체 그러나 칸트가 존재론을 인식론으로 ‘축소’시켰을 때의 존재가 의미하는 것은 사물 그 자체의 모습, 우리와 전혀 관계없는 미지의 것이 아니라 우리에게 의미 있는 대상, 즉 인식 속에서 우리에게 나타난 현상일 뿐이다. 현상은 그저 객관적으로 우리와 독립적으로 있는 어떤 것이 아니라 우리 주관적 형식으로 인해 구성된 대상이다. 따라서 현상은 그것이 개입 안 된, 우리의 형식과 독립적이라는 의미로서의 대상, 즉 사물 그 자체와 항상 대비되는 개념이다. “나는 어떤 사물도 그 자체로는 인식할 수 없고, 오직 나에 대해서 인식될 수 있는 사태만을 인식할 수 있는 것이다. 나의 직관의 작용 방식이 나로 하여금 사물 그 자체는 인식할 수 없고, 현상만을 인식하도록 규정한다. 나는 사태가 나에게 현상하는 방식 그대로 인식한다.” 우리는 이러한 인식의 형식, 즉 범주를 경험될 수 있는 대상들에게 사용해서 인식을 얻어내야 하는데, 종종 그렇게 하지 않는 경우가 있다. 그래서 일종의 상상, 환상과도 같은 것을 진정한 인식인 양 착각할 때도 있다. 그래서 생기는 것인 소위 이성적(예지적) 형이상학인데, 칸트는 영혼, 전체로서의 우주, 신의 존재 등 초월적 대상들에 대해서는 이론철학적으로 무엇을 논증하거나 주장할 수 없다고 생각했다. 그래서 이런 이성의 유혹에 흔들리지 말고 이론적 인식의 유한성과 한계를 숙지해야 한다고 경고했다.",
      "frontmatter": {}
    },
    "10.인간과 도덕": {
      "path": "/06.university/철학과-인간/10.인간과-도덕/",
      "filename": "10.인간과 도덕",
      "content": "인간과 도덕 도덕이란 무엇인가 관습과 도덕 우리는 일상생활에서 흔히 “부모를 공경하라”, “선한 일을 행하라”, “거짓말을 하지 말라” 등과 같은 말을 대한다. 우리는 이런 교훈들을 우리가 살아가면서 마땅히 지켜야 할 도리들이라고 생각한다. 이와 같이 일반적으로 우리 모두가 수긍할 만한, 그리고 마땅히, 혹은 당연히 그렇게 해야 한다고 생각하는 것들을 우리는 ‘규범’, ‘관습’ 혹은 ‘도덕’이라고 부른다. 그 어원인 그리스 단어 ‘ethos’와 라틴어 ‘mores’ 등은 본래 습관, 혹은 관행이라는 뜻을 가졌는데, 도덕이라 불리는 것도 본래는 사소하고 우연적인 것 같은 습관에서 비롯되었다는 발상을 내포하고 있다. 한 사람에게 있어서 이런 습관이 오랜 시간을 거치면서 몸에 익어 익숙해진 후, 결국 그로부터 떨어질 수 없는 그의 성격과 성품이 되기도 하는데, 이 경우 우리는 이를 ‘덕(virtue, Tugend)’이라고 부르기도 한다. 이를 사회적인 차원에서 접근하면 일종의 ‘관습’으로 볼 수 있는데, 우리 사회에서 오랫동안 지켜 내려와서 우리 사회 구성원들이 널리 인정하는 ‘질서’·‘규범’·‘풍습’ 과 같은 것이 이에 해당한다고 볼 수 있다. ‘도덕’은 이렇게 두 가지 차원, 즉 개인적인 차원과 사회적인 차원 모두에서 올바르고 중요하다고 여겨지며 마땅히 지켜지고 행해져야 한다고 생각되는 규범·원칙·도리 등을 통칭하는 말이다. 도덕이 개인과 사회의 구체적인 삶의 현장에서 실현되는 원칙을 주로 가리킨다면, 윤리학은 이런 사실적인 도덕에 관한 이론적·반성적인 성찰과 연구를 하는 철학의 한 분과이다. 도덕이 겉으로 드러나는 현상적 차원을 통치하는 것이라면 윤리학은 그것의 내면에 담겨 있는 본성과 근거에 관한 이론적 학문이다. 윤리학의 근본문제 도덕을 규명하기 위해 윤리학이 고민하는 근본적인 문제 중 다음의 두 가지를 논하고자 한다. 1) 존재(자연)와 가치. 사실과 당위 윤리학은 이른바 당위에 관한 학문인데, 그것의 원칙은 자연으로부터 도출되지 않는다. 우리가 사실관계를 아무리 명확히 밝혀도, 그리고 자연의 원리를 아무리 잘 파악해도 그로부터 가치나 의무와 같은 도덕과 연결된 개념을 찾을 수 없다. 자연은 단어 뜻 그대로 ‘스스로 그러한 것’일 뿐 우리 인간의 행위에 대한 지침이나 방향을 직접적으로 제시해 주지 않으며 심지어 어떤 경우에는 도덕적인 가치와 어긋나는 것을 지향하는 것처럼 보이기도 한다. 인간의 본성(=유전자)에는 자신의 생존과 종족보존을 우선시하는 경향이 각인돼 있다. 따라서 이타적 행위와 같은 것은 우리의 자연적 본성을 거스르는 것일 텐데 그럼에도 불구하고 이런 일이 일상에서는 자주 발생하고 우리는 이를 도덕적이라고 칭하며 긍정적으로 평가한다. 왜 이런 행위가 발생하고 또 우리는 왜 이것을 도덕적인 행위라고 부르고, 또 마땅히 추구해야 하는 것이라고 여기는지를 설명하는 것이 또한 윤리학의 과제이다. 물론 우리가 항상 도덕적인 원칙에 따라서만 행위를 하는 것은 아니다. 우리 사회에 만연한 여러 이기적인 행위들, 탈세, 편법, 불법 행위 등은 명백히 도덕적이지 않은데 많은 이들이 그런 부정행위를 저지르곤 한다. 그리고 그들이 경제적으로나 사회적으로 오히려 더 성공하는 경우가 비일비재하다. 그러니 자연과 당위, 욕구와 도덕사이의 관계와 갈등에서 우리는 어떻게 행동해야 하는지, 또 어떤 근거에서 그렇게 해야 하는지를 윤리학은 밝히려는 것이다. 이에 대한 실마리로서 우리는 당위라는 개념의 본성을 떠올리게 된다. 우리는 물리적인 신체를 갖고 있다는 점에서 분명히 자연의 일부인데 만일 우리의 행위 전부가 이 자연법칙에 의해서 지배 받는다면 당위라는 개념 자체가 가능하지 않을 것이다. 그래서 만약 우리가 도덕적 당위라고 여기는 행위들이 자연적 본성에 의해서 저절로 이루어졌다면, 즉 우리의 의지에 의해서 행해진 것이 아니라면, 당위나 도덕, 그리고 윤리학과 같은 것들은 모두 필요하지 않을 것이다. 따라서 우리의 어떤 행위들은 다른 종류의 원인에 의해서 가능하다거나 또는 자연법칙과는 다른 종류의 법칙에 따라서 이루어진다고 말할 수 있어야만 당위적 행위나 도덕이 가능해지는 것이다. 그래서 이러한 것이 어떻게 가능할 수 있는지를 밝히는 것이 윤리학의 최우선 과제인 셈이다. 2) 자연과 자유 윤리학에서 가장 근본이 되는 문제로서 우리는 자연과 자유를 들 수 있다. 인간은 살면서 실로 많은 행위를 하는데 그것의 근본적인 원인은 무엇인지에 대해서 생각해 볼 수 있겠다. 물론 외면적으로는 우리의 근육, 신경 등이 작용하여 우리의 몸을 움직여서 가시적인 행위가 발생하는 것이지만 그것을 내적으로 가능하게 하는 동기라는 측면을 생각해 보지 않을 수 없다. 이 때 우리가 과연 자발적인 의지에 의해서 자유롭게 결정해서 어떤 행위를 하는지, 아니면 의식적이건 무의식적이건 내게 영향을 준 주변 환경과 조건들에 의해서 이런 행위를 하는지(그러면서도 내가 자유롭게 결정하고 행했다고 착각하는지)는 윤리학이 던지는 가장 우선적인 질문이다. 만약 여기서 모든 것은 인과율에 의해서 자연적 원인과 근거 때문에 발생한다고 대답한다면 인간의 자유는 없는 것일 테고, 도덕이나 윤리도 없어질 것이다. 이와 함께 우리가 한 사람의 행위에 대한 도덕적·사회적·법적인 책임을 물을 수 있는 근거도 없어진다. 누구나 자신의 행위에 전혀 책임이 없게 되는 것이다. 이렇게 자유의지가 없으면 책임도 없으므로 인간이 자유롭게, 즉 스스로 자신의 의지로써 무언가를 행할 수 있다는 것을 밝혀야만 윤리와 도덕도 가능해지는 것이다. 그런데 인간은 자연의 법칙에 지배를 받는 신체를 가진 존재이고 자연 전체는 자연법칙에 의해서 진행된다는 것을 상기할 때 자유라는 것이 존립할 수 있는 여지가 있을까하는 의문이 생긴다. 그러므로 어떻게 자유가 인과성의 사슬을 끊을 수 있을지를 설명해야 하는 의무가 윤리학의 앞에 놓여 있다. 결정론과 자유의지의 양립 가능성 자연과 자유사이의 관계에 대한 이론을 우리는 크게 두 가지로 구분할 수 있겠다. 우선 인과성의 원칙과 충분근거율을 바탕으로 해서 자연의 모든 것의 진행이 법칙적으로 이미 결정되었다는 주장을 우리는 결정론(Determinism)이라 하는데, 이 결정론적 입장에 대해서 자유가 가능하지 않다는 입장과 결정론을 어느 정도 수용해도 자유는 모순 없이 가능하다는 입장이 있다. 엄격한 결정론의 입장을 따르자면 인간과 세계는 모두 자유롭지 않고 엄격한 인과성의 계열과 체계에 의해서 서로서로 필연적으로 연계되어 있다고 생각해야 할 것이다. 복잡한 자연현상의 원인을 우리는 비록 지금 현재 다 찾지 못했을 뿐, 그것의 원인은 항상 있기 때문에 우리가 자유롭게 결정했다고 생각하는 것들도 사실은 이런 자연법칙에 의해서 생리학적으로나 신경학적으로 이미 결정된 것일 뿐이라는 주장이다. 반면 소위 양립론자들은 자연은 비록 결정론적으로 진행되지만, 그럼에도 불구하고 인간은 자유롭게 행동할 수 있다고 주장한다. 이 주장의 근거는 이 둘, 즉 인과성의 세계와 자유의 세계는 그 존재론적·경험적 영역이 서로 다르다는 생각에 있다. 그래서 하나의 세계는 인과적 필연성이 지배하고, 또 다른 하나의 세계는 자유가 가능한 세계라고 설명하고 있다. 그런데 앞서 말한 대로 우리는 적어도 이 둘의 양립 가능성을 확보해야만 도덕과 윤리학을 논할 수 있기 때문에 모든 윤리학적 이론은 적어도 이 둘의 양립 가능성을 어떤 방식으로건 인정함을 전제한다고 볼 수 있다. 칸트의 양립론: 초월적 이상으로서의 선과 도덕적 행위의 원인으로서의 자유 기계적 인과성에 의한 결정론과 그로부터 자유로운 인간의 의지가 양립가능하다고 생각한 대표적인 철학자로 우리는 칸트를 떠올린다. 그는 특유의 환원적 방법을 쫓아 우선 도덕적 행위와 자유를 (우리가 이를 확실히 경험한다고 함을 근거로 해서) 하나의 사실로 받아들인다. 그리고 이제 이것이 도대체 어떻게 가능하게 되었는가에 관심을 기울인다. 그래서 마치 공간·시간, 범주가 우리 인간의 선험적 형식이어서 우리의 인식을 가능하게 해 준 것처럼, 자유로운 의지도 우리에게 선험적으로 주어져 있고 이로 인해 도덕적 행위가 가능해진 것이 아닌가하고 주장한다. 따라서 그에게 있어서 자유의지는 처음부터, 즉 선험적으로 인간에게 구비되어 있는 것이고 이것이 원인이 되어 도덕적 행위가 발생하게 된다. 이 때 자유(의지)가 도덕적 행위를 가능하게 해주는 것은 어떤 자연적 원인에 의해서 또 다른 어떤 자연적 결과가 발생하는 것과는 다르게 이루어져야 한다. 내가 어떤 음료를 마시는 것의 원인이 단순한 수분고갈과 그에 수반한 갈증이라면 이 둘의 관계는 전형적인 자연 내의 인과적 관계일 것이다. 내가 무언가를 마시는 행위도 자연 안에 있고, 그것의 원인도 하나의 생리현상으로서 자연 안에 있다. 그런 한에서 이 둘은 철저히 인과적 사슬에 묶여 발생한 일련의 상황일 뿐인데 이는 결정론적 관점에 의하면 이미 정해져 있고 예측 가능한 것이다. 하지만 도덕적 행위를 야기하는 나의 자유의지는 (아리스토텔레스의 말을 빌리자면) 일종의 목적인으로서 무언가를 추구하고 지향하는 종류의 원인인 셈이다. 내가 무언가를 행위하는 데에는 항상 목적과 가치가 있다. 아무런 목적 없이 그저 행하는 것은 단순한 ‘행동’일 뿐이지 인간의 의도적인 ‘행위’라 불릴 수 없다. 그런데 이제 도덕적 행위의 목적과 그것이 추구하는 바는 당연히 선, 즉 도덕적 가치일 텐데, 이러한 순수한 가치, 그 자체로 값있는 어떤 것은 공간과 시간에서 발견될 수 있는 경험적인 것이 아니다. 이는 자연과 현상 밖에 놓여 있는, 즉 ‘초월적인’ 어떤 것이다. 그래서 그 목적이 현상계를 초월해 있는 어떤 것은 원인과 결과 모두 현상계 내에 머물러 있는 경험적 사태와 그 차원을 달리하는 것이 된다. 두 종류의 윤리체계 선의 개념: 윤리적 가치로서의 선 우리는 도덕을 ‘당연히 행하고 지켜야 할 규범이자 원칙’이라고 정의했다. 그런데 우리가 어떤 규범을 왜 당연한 것이라고 여기는지에 대해서는 아직 밝히지 않았다. 그런 판단을 하는데 있어서 기준과 근거가 있어야 하는데 사람들은 전통적으로, 그리고 일반적으로 ‘선’이라는 것을 그 답으로 삼는다. 즉 어떤 원칙이나 규범, 그리고 어떤 행위가 그 자체로 선하거나 선한 것을 목표로 삼으면 그것은 도덕적인 것이 된다. 그래서 흔히들 선을 도덕의 본질이라고 정의하기도 한다. 철학자들은 역사적으로 선에 대해서만 다루었지 악에 대해서는 크게 관심을 두지 않았는데, 그것은 선의 개념이 벌써 ‘그 자체로 추구되어야 할 가치’라고 정의되었기 때문이다. 따라서 악이란 자동적으로 ‘추구되어서는 안 될 것’이 되는데 어떤 이가 추구해서는 안 될 것, 즉 악을 의도적으로 추구한다는 것은 모순적이라고 생각한 것이다. 따라서 악은 선을 추구하고자 하는 가운데 피치 못할 사정으로 이에 다다르지 못한 일종의 결핍이나 결여의 상태에 불과하다는 것이 일반적인 견해이다. 그만큼 선은 ‘가치 그 자체’, 행위가 지향해야 할 ‘참된 가치’와 동일시됐다. 선은 이처럼 도덕의 본질이자 근거가 되는 중요한 개념이기 때문에, 도대체 이것이 무엇인지에 대한 이론들이 철학사에서 다양하게 등장했다. 1) 법칙주의 칸트의 도덕이론을 일컬어 법칙주의라고 하는데, 이는 도덕적 요구가 우리에게 마치 법칙과도 같은 강제성을 띄고 있다고 그가 설명했기 때문이다. 그는 우리의 실천이성이 선의지에 의해서 도덕적인 행위를 의욕할 수 있다고 보았는데, 이 때 이성은 우리의 자연적 본성, 즉 감각적 충동에 이끌리는 경향에 반하기 위하여 명령의 형태로서 행위의 규범을 정립하려고 한다. 그리고 이 명령은 여타의 조건에 관계없이 무조건적으로 행해야 하는 필연적인 성격을 지니고 있으므로 일종의 법칙과도 같다. 따라서 선한 행위는 (자연적 본성이 아니라) 이 실천법칙을 따르는 행위이다. 또한 그것이 선한 이유는 내가 그 행위의 결과가 가져오는 것이 내게 유리할지 불리할지를 따져서 행했기 때문이 아니라, 그 자체로 옳은 것을 내가 선택했고 이에 따랐기 때문이다. 이 실천법칙을 따르기로 결심한 그 동기가 벌써 선한 것이지 그 행위의 결과에 의해서 그 행위가 선해지는 것이 아니다. 그의 이론을 또한 의무론적 윤리학이라고도 하고, 또한 (구체적인 내용이 아닌) 오직 도덕적 명령의 형식만을 정립했다고 해서 윤리적 형식주의라고 부리기도 한다. 존재의 세계와 당위의 세계 칸트의 도덕철학은 존재와 당위, 자연과 자유에 대한 본격적인 구분으로부터 시작한다. 이 두 세계는 각각 그들의 원칙에 따라 양립 가능함을 밝혀야 도덕의 가능성도 확보되기 때문이다. 이를 위해 그는 먼저 이성을 순수이성과 실천이성으로 분류하여 순수이성은 존재 세계를 탐구하고, 실천이성은 당위의 세계를 탐구한다고 보았다. “이성의 사변적 사용의 관심은 최고의 선험적 원리에 이르는 객관의 인식에 있고, 실천적 사용의 관심은 궁극적인 완전한 목적과 관련하여 의지를 규정하는 데 있다” (칸트는 이렇게 이성을 둘로 구분해서 설명하는데, 그렇다고 해서 우리가 두 개의 별도의 이성을 갖고 있다고 주장하는 것은 아니다. 이성이란 궁극적으로 하나이고 동일한 것인데, 그것의 적용에서만 구별된다. 그저 하나의 이성이 두 가지 다른 대상과 관계하고, 그 대상이 갖는 특성으로 인해 이를 대하는 이성의 모습도 그때마다 달라지는 것으로 보아야 한다) 그 대상들 중 첫 번째인 존재의 세계는 자연법칙이 지배하는 세계이고 두 번째의 세계는 당위와 도덕의 세계인 것이다. 칸트는 이 둘이 서로 구분되는 것이기에 각각 개별적으로 탐구되어야 한다고 여겼다. 이미 인식론에서 다루었던 것처럼 이론적 이성에 관한 질문이 “나는 무엇을 알 수 있을까?”였다면, 이제 실천적 이성에 관한 물음은 “나는 무엇을 행해야 하는가?”일 것이다. 그리고 사실 이 물음에 대한 답도 칸트는 이미 가지고 있는데, 그것은 우리가 ‘선’을 마땅히 행해야 한다는 것이다. (그에 의하면 선의 개념이 벌써 ‘우리가 마땅히 행해야 하는 것’이다) 따라서 이제 그의 윤리학의 물음은 구체적으로 도대체 무엇이 선인지, 선의 본질에 관한 물음이 되고, 또한 이를 수행하는 인간 이성의 능력은 무엇인지에 대한 탐구가 된다. ‘순수이성비판’에서 칸트는 경험적 세계와 초월적 세계의 경계를 분명히 하여 이들에 대한 적절한 관점과 방법을 사용할 것을 권고한 셈인데, 그가 특히 강조한 것은 ‘신’이나 ‘세계 전체’와 같은 초월적 대상에게나 사용함직한 사변적 사고를 경험적 대상, 즉 시공간에서 발견되는 대상에게 적용하는 월권을 방지하는 것이었다. 세계의 기원으로서 신이 있겠거니, 이 세계는 무한하겠거니 하는, 경험적으로 입증가능하지 않은 사고를 마치 자연에 대한 의미 있는 발언인 것처럼 포장하는 데에 대해 그는 경고를 아끼지 않았다. 이에 따라 의미 있는 엄밀한 의미의 인식은 오직 시공간에서 만날 수 있는 대상들에 대한 선험적 형식에 의한 인식이라고 이미 제한된 바 있다. 반면 ‘실천이성비판’에서는 그 반대의 경우, 즉 경험적 대상에게 적용되는 객관적 사고는 (경험적 자연 세계와 다른) 당위의 세계에는 사용될 수 없음이 강조된다. 즉 인간의 행위의 근간이 되는 의지는 경험적인 조건에 구애받지 않는 ‘자유로운’ 성격의 것이기에 자연과학과 경험의 세계에서나 통용되는 객관적 사고의 잣대와 관점에 의해서 이를 규정해서는 안 된다는 것이다. 윤리 ‘형이상학’의 필요 불가결성 사실 칸트는 처음부터 사실의 세계 뿐 아니라 당위의 세계도 가능함을 상정했고, 또한 인간은 자연적 속성에만 따르지 않고 이와는 다르게, 혹은 독립적으로, 심지어 반대로 행동할 수 있는 의지와 능력을 가졌다는 것을 당연한 듯이 전제했다. 그의 과제는 이것이 어떻게 가능한지를 밝히는 것이었는데 이를 위해 그는 다음의 일련의 저작들을 저술하게 된다. 실천이성비판 (1788) - 윤리 형이상학 정초 (1785) - 윤리 형이상학 (1797) 제목에서 알 수 있듯이 그는 먼저 실천이성비판을 일종의 예비학으로서 내세웠고, 그 후에 이를 바탕으로 윤리형이상학을 세우는 것을 목표로 삼았다. 그가 윤리학을 그저 단순한 이론이라고 하지 않고 굳이 형이상학이라고 칭한 이유는, 윤리학을 경험적인 이론이 아니라 경험 이전의 원칙들에 의해서, 즉 선험적으로 탐구되는 학문으로서 세우겠다는 의도에 있었다. 윤리학이 대상으로 하는 것은 인간의 여러 가지 행동들 중 도덕적 의지를 가진 행위와 그 원리들인데, 이 원리들을 칸트는 이제 선험적으로 밝혀낼 수 있다고 본 것이다. 칸트는 이런 윤리 형이상학이 반드시 필요하다고 생각했는데, 만약 모든 행위들의 올바른 규범이 정해지지 않으면 우리 인간은 갖가지 유혹과 부패에 굴복할 것이라고 생각했기 때문이다. 어떤 행위들이 선한지는 오직 이 규범들에 의해서 밝혀지는 것이다. 물론 이 규범들에게 그저 표면적으로만 부응한다고 해서 어떤 행위가 선해지는 것은 아니다. 오직 정당한 규범과 원칙들에 따라서 욕구된 것들만이 비로소 선한 행위들인 것이다. 그렇지 않으면 그저 결과적으로 선한 행위들도 모두 선한 것으로 여겨질 수 있기 때문이다. 그리고 이 규범들은 여러 가지 도덕적 행위들의 구체적인 사례들에 대한 그저 단순한 경험적인 기술을 통해서는 밝혀질 수 없다. 이성적인 성찰과 반성을 통해 그 근거가 선험적으로 밝혀져야 비로소 누구에게나 어떤 경우에나 보편적으로 타당한 도덕적 원칙이 확보되는 것이다. 그는 이렇게 확실한 법칙과 규범들의 체계가 있어야 어떤 행위가 진정으로 도덕적인지 알 수 있고 또한 이에 따라 도덕에 관한 진정한 이론을 정립할 수 있다고 믿었다. 선의지 윤리 형이상학을 정초하기 위하여 칸트가 주의를 기울인 첫 번째 작업은 일반적인 도덕관념에 대한 분석이었다. 일반적으로 도덕적 행위가 어떻게 발생하는가를 먼저 살펴본 칸트는, 우리 인간은 자연적으로 욕망이라는 본성을 갖고 있음을 인정한다. 그런데 이런 본성이 있다고 해서 늘 이에 따라서만 행위하는 것이 아니고 이른바 ‘양심’이라 불리는 또 다른 본성, 혹은 마음의 소리를 쫓아 행하기도 한다. 그래서 무언가를 자연적인 욕망에 따라 행하면 이기적인 행위가 되는 것이고, 양심에 따라 행하면 도덕적 행위가 된다고 보았다. 이제 우리가 각각의 경우에서 욕망을 따를 것인가, 아니면 양심을 따를 것인가를 결정하는 것이 바로 우리의 의지라고 볼 수 있다. 우리의 심성, 더 구체적으로 실천적 힘을 가진 이성은 자신만의 의지를 가지고서 이 둘 중 하나를 택하여 행위할 수 있는데 , 당연히 선한 의지를 가지고서 선택할 때 도덕적 행위를 야기할 것이다. 따라서 이 모든 과정에서 도덕적 행위를 가능하게 하고 선한 것의 기반이 되는 것은 다름 아닌 우리 실천 이성의 선한 의지인 것이다. 오직 ‘선의지(guter Wille)’만이 그 자체로, 무조건적인 가치를 갖는 것이다. “이 세계에서 ... 아무런 제한 없이 선하다고 생각될 수 있을 것은 오직 선의지뿐이다.” “선의지는 그것이 생기게 하는 것이나 성취한 것으로 말미암아, 또 어떤 세워진 목적 달성에 쓸모 있음으로 말미암아 선한 것이 아니라, 오로지 그 의욕함으로 말미암아, 다시 말해 그 자체로 선한 것이다.” 선의지란 옳은 행위를 오로지 그것이 옳다는 이유에서 택하는 의지를 말한다. 그것은 행위의 결과를 고려하는 마음도 아니고, 그저 자연적인 성향에 따라 행하는 것도 아닌, 단적으로 어떤 행위가 옳다는 바로 그 이유만으로 그 행위를 택하는 의지이다. 따라서 여기에는 어떤 것이 ‘옳다’, ‘선하다’는 판단이 선행해야 하는데, ‘옳음’과 ‘선함’은 경험적인 개념이 아니고 순수 이성의 이념이므로, 선의지는 오직 이성적 존재자만이 가질 수 있다는 생각과 연결된다. 선의 개념은 이렇게 “이미 자연적인 건전한 지성에 내재해 있으며, 가르쳐질 필요는 없고, 오히려 단지 계발될 필요만 있는 것이다.” 하지만 선의지는 자연 발생적으로 생겨나거나 모든 행위에 자동적으로 발동하는 것이 아니다. 앞서 자연과 당위의 세계의 관계에 대한 설명에서 보았듯이 이 둘이 반드시 일치하지는 않기 때문이다. 그래서 선의지는 도덕적 이념을 반드시 실천하겠다는 의무개념을 수반해야 한다. 당위로서 ‘...을 하라’는 ‘명령’의 형태를 띠어야만 윤리적 규정일 테고 그렇지 않은 단순한 제안이나 가정 등은 도덕적 실천규정이 될 수 없다. 이성은 자신에게 윤리적이라고 여겨지는 규정에 대해서 무조건적으로 복종해야 한다는 의무감을 가질 것이고 그 규정을 실천명령으로, 즉 필연적으로 그에 따라 행위를 해야 하는 실천‘법칙’으로(마치 자연적 대상들이 ‘자연법칙’에 따라 운동하듯이) 여길 것이다. “의무여! 너 숭고하고 위대한 이름이여, 너는 사람들이 좋아할 아무것도 가지고 있지 않았으되 복종을 요구하는구나.” 결국 어떤 행위가 도덕적적으로 가치 있다고 불리려면 그 행위가 의무로부터 행해졌음이 밝혀져야 한다. 의무로부터의 행위는 그것의 결과에 기대해서 행위를 하는 것이 아니므로 선은 오로지 그 행위의 동기 가운데 있으며 그 행위를 하는 인격 자체 안에 이미 있는 것이다. 어떤 행위가 도덕적으로 선한 것인가, 아닌가의 기준도 칸트에 따르면 그 행위의 동기가 선의지에 따른 것인가 아닌가에 달려있을 뿐, 그 행위의 결과와는 아무런 상관이 없다. 실천이성의 자율성 이렇게 칸트는 선의지의 본성을 드러냄으로써 인간이 비단 자연법칙에만 종속되는 것이 아니라 윤리법칙, 실천법칙에도 종속해 있음을 밝힌다. 인간은 도덕법칙을 존경하고 의무로부터 무언가를 행할 수 있기 때문이다. 이렇게 인간은 자연법칙에 의해 지배되는 속성도 갖고 도덕적 명령에도 따라야 한다는 의무감도 갖는 이중적 속성을 지녔기에, 도덕적 결정을 해야 하는 상황에서 그저 외적인 동기들(신체적 요구나 심리적 상태 등)에 의해 좌지우지되지 않고 도덕적으로 행동하기 위해 자기 스스로 이 법칙에 따라 행위를 할 것을 강요해야 한다. 따라서 이 자기 강제는 이성이 스스로에게 내라는 일종의 명령(Imperativ)이다. 이제 어떤 명령이 일회성에 그치지 않고 지속적으로 영향을 끼치는 ‘법칙’이 되려면 보편성과 필연성을 가져야 하므로 도덕법칙으로서의 명령은 단순한 명령이 아니라 ‘누구에게나 타당하고, 무조건적으로 타당’해야 한다. 그래서 이것은 이제 경험적이거나 결과론적이지 않은, 선험적이고 단정적인(예외를 허용하지 않는) ‘정언적 명령(kategorischer Imperatov)’이 된다. 정언적 명령은 선택적이고 조건적인 명령과는 다른데 예를 들어, ‘노후에 행복하려면 젊었을 때 부지런해라’와 같은 명령은 미래의 행복이라는 조건을 고려한 명령으로서, 그 결과와 자신이 처한 주관적인 상황과 상관없이 무조건 복종해야 하는 칸트의 정언적 명령과는 다른 것이다. 그리고 이 명령은 이성이 자기 자신에게 선험적으로 무조건적으로 부과하는 규범이므로 이성의 ‘자율(Autonomie)’이라고도 한다. “정언명령에서의 행위는 어떤 목적을 지시하지 않고서도, 즉 어떤 다른 목적이 없이도, 그 자체가 객관적으로, 필연적으로 수행된다.” 이성이 자기 스스로에게 내리는 이 의무의 보편적 명령의 형식을 칸트는 이제 다음과 같이 표현한다. “그 준칙이 보편적 법칙이 될 것을, 그 준칙을 통해 네가 동시에 의욕할 수 있는, 오직 그런 준칙에 따라서만 행위하라.” “마치 너의 행위의 준칙이 너의 의지에 의해 보편적 자연법칙이 되어야 하는 것처럼, 그렇게 행위하라.” 반드시 도덕적으로 행위를 해야 할 의무가 ‘자연법칙적으로는’ 전혀 없음에도 불구하고 자기 스스로 자신의 행위의 준칙을 세우고 이를 마치 보편적 자연법칙인 것 마냥 따르려는 이러한 인간의 의지가 곧 그 자체로 신성하다고 칸트는 규정하고, 또한 이런 자율성이야말로 “인간과 모든 이성적 자연존재자의 존엄성의 근거”라고 주장한다. 인간은 이처럼 그 자체로 존엄하기 때문에 결코 다른 그 어떤 것을 위한 수단이 될 수 없고 오직 그 자체로서 가치를 갖는 인격, 즉 목적으로 생각되어야 한다. 그래서 다음과 같은 실천명령도 인간에게는 자명한 실천명령이 된다. “네가 너 자신의 인격에서나 다른 모든 사람의 인격에서 인간(성)을 항상 동시에 목적으로 대하고, 결코 한낱 수단으로 대하지 않도록, 그렇게 행위하라.” 도덕적 행위의 본성을 법칙에 대한 종속으로 보면서도 칸트가 이를 타율, 즉 타의에 의해서 자신의 행위가 결정되는 것이 아닌 자율로 여기는 이유는, 인간이 자신이 세운 법칙을 따르려는 것이 자신의 ‘자유의지’에 의해서라고 보기 때문이다. 통속적인 이해에 따르면 ‘자유롭게’, 즉 마음이 내키는 대로 행동해야 자유라고 여기기 쉬운데, 칸트는 그런 행위를 그저 타율에 의한 것이거나 기껏해야 자의적이고 방임적인 것에 불과하다고 본다. 칸트에게 진정한 자유란 모든 외적인 원인들로부터 벗어나서 스스로 옳다고 여기는 규칙을 따르겠다는 의지에 있는 것이다. 자연적 존재로서의 인간은 자연법칙의 종속 아래에 놓여있고 거기에 따라 행동하는 타율적인 존재지만, 또한 도덕적 관점으로 보면 행위의 법칙을 스스로 세워서 이에 스스로 따르는 자유로운 존재이다. 우리가 자연법칙과 무관한 도덕법칙을 세울 수 있다는 사실 자체가 우리에게 자유의지가 있다는 것을 보여주는 것이고, 또 우리에게 자유의지가 있으니까 그런 도덕법칙을 수립하고 그에 따르게 되는 것이다. 따라서 얼핏 모순처럼 보이는 법칙과 자유라는 두 개념은 칸트의 도덕이론에서는 상호 모순 없이 양립 가능한 것이다. 2) 공리주의(Utilitarianism) 흄의 도덕이론 공리주의는 영국의 경험론적 전통에서 유래한다. 먼저 흄은 도덕이라는 것이 이성에서 도출되는지 감정에서 도출되는지를 묻고는 감정에 도덕의 바탕이 있다고 주장한다. 경험론자인 흄은 이성이 사실의 내용과 관련하여 어떤 확실한 지식도 제공하지 못하는 것처럼 인간 행위의 문제에 있어서도 어떠한 결정적인 역할도 하지 못한다고 보았다. 우리의 이성은 기껏해야 어떤 목적에 대한 수단만을 우리에게 가르쳐 줄 수 있고, 우리에게 무엇이 유용한지를 발견하는 데에 도움을 줄 뿐이지, 무엇이 그 자체로 선한지, 선이 도대체 무엇인지에 대해서는 아무것도 가르쳐 주지 못한다고 생각했다. 그는 이성은 감정·정념의 노예일 뿐이라고 했고, 덕이란 보는 이에게 기쁘게 하는 감정을 주는 심리적 행위 내지는 성질이라고 했다(악은 그 반대라고 했다). 그래서 사람들에게 유쾌한 감정을 일으키는 것들과 그렇지 않은 것들을 살펴보면, 비록 도덕의 본질과 근거와 같은 추상적인 개념에 대해서는 명료하게 알 수 없지만, 무엇이 덕인지에 대해서는 확실하고 쉽게 알 수 있다고 생각했다. 칸트가 도덕의 본질과 근거를 밝혀내려고 노력한 반면, 흄은 그것을 아예 불가능한 일이라 여기고 시도조차 하지 않았는데, 이는 도덕에 관한 사람들의 일반적인 견해가 그것의 모습을 보여주기에 충분하다고 그가 여겼기 때문이다. 선의 관념과 척도를 사람의 정서에서 찾는 이런 경험주의적 윤리학의 전통은 이제 벤담에게서 공리주의라는 이름으로 다시 나타난다. 공리주의의 기본개념 공리주의는 영국의 철학자 벤담(1748-1832)과 밀(1806-1873)에 의해서 주장된 윤리체계로서 행위의 도덕적 가치기준을 그 행위의 유용성(utility)에 두는 이론이다. 벤담도 흄처럼 감각경험의 범위를 넘어서는 모든 종교적·형이상학적 주장들에서 벗어나야 한다고 주장한다. 그래서 인간에 대한 파악은 감각이 우리에게 전해주는 것들에 기초해야 한다고 말한다. \"자연은 인류를 고통과 쾌락이라는 두 주권자의 지배 아래에 두어 왔다. 우리가 무엇을 해야만 하는지를 지시하고, 우리가 무엇을 할 것인가를 결정하는 것은 오로지 고통과 쾌락일 뿐이다. 한편으로는 옳고 그름의 기준이, 다른 한편으로는 원인과 결과의 연쇄가 이 왕좌에 결부되어 있다. 고통과 쾌락은 우리가 하는 모든 일, 우리가 말하는 모든 말, 우리가 생각하는 모든 일에 있어서 우리를 지배하고 있다.\" 벤담은 쾌락과 고통이라는 이 두 요소에 의해서 심리이론을 세울 수 있을 뿐 아니라 더 나아가 이를 기초로 윤리이론까지도 만들 수 있다고 생각했다. 쾌락의 추구와 고통의 회피라는 두 요소를 기준으로 해서 과학적이고도 전반적인 이론을 정립하는 것이다. 도덕에 관한 기준으로 그가 내세우는 기준은 이제 유용성의 원리인데, \"유용성의 원리가 의미하는 바는 곧 그것이 모든 행위를, 어떤 행위든지 간에, 그 행위가 그것과 자신의 이익이 관련되는 사람들의 행복을 증가시키는 경향성을 지니는 듯이 보이는가 아니면 감소시키는 경향성을 보이는가에 따라서 그 행위를 시인하거나 부인한다는 점이다. 바꾸어 말하면 어떤 행위가 행복을 증진시키는가 그렇지 않은가에 따라 그 행위를 판단한다는 점이다.\" 이 유용성의 원리에 의하면, 옳은 행위란 바로 유용한 행위이다. 유용성이 곧 옳고 그름의 척도인 것이다. 공리주의는 따라서 일종의 목적론적 윤리체계이고 오직 행동의 결과만을 그 정당성의 기준으로 삼는다. 어떤 행위가 결과적으로 유용한지 아닌지에 따라서 그 행위가 가치 있는지 아닌지를 결정하는 것이다. (반면 의무론적 윤리체계는 어느 행위가 도덕법칙과 의무라는 형식을 따르는지를 기준으로 해서 그 행위의 정당성을 따진다. 이렇게 이 두 입장은 동기와 결과라는 상반된 기준을 갖고서 도덕적 행동을 판단하려는 서로에게 대립적인 견해들이다.) 공리주의적 입장에서는 어떤 행위가 유용하다면 그 행위는 옳다(그 자체적으로 옳은 행위란 없다). 그러면 곧 어떤 목적에 유용한가라는 질문이 자동적으로 나올 것이다. 이런 행위의 목적을 공리주의자들은 본래적 가치라고 부른다. 우리의 행위들은 항상 그 어떤 본래적 가치를 가진 다른 목적을 갖고 있고 우리의 행위들 그 자체는 그에 대해서 도구와 수단이 될 수 있다. 그리고 또 다른 행위들은 이 행위들에 대해서 도구와 수단이 되고 이 행위들이 그것들의 목적이 될 수 있다. 따라서 수단과 목적의 연결고리가 끝없이 진행될 수 있다. 따라서 문제되는 어떤 행위가 옳은가 아닌가를 판가름하는 근거를 공리주의자들은 그 행위를 하는 목적이 결과적으로 그 행위로 말미암아 좋게 되는지 아닌지에 있다고 본다. 이 때 그 행위는 순전히 도구적으로 좋고 도구적으로 옳은 것이지 그 자체로 옳은 것은 아니다. 그 결과가 좋기 때문에 그 행위도 덩달아 그것을 위한 도구로서 옳게 되는 것이다. 쾌락 공리주의와 밀의 비판 벤담은 쾌락이 본래적 가치라 했고, 어떤 행위가 야기하는 이 유쾌한 경험(＝쾌락)을 그 강도와 지속성을 기준으로 해서 계산함으로써 어떤 행위가 얼마만큼 유용한지를 따질 수 있다고 생각했다. 그래서 더 많은 쾌락을 가져다주는 행위들을 함으로써 쾌락이 증진된다고 보았다. 그런데 밀은 같은 양의 쾌락들 중에서도 더 큰 본래적 가치를 주는 쾌락이 있을 수 있다고 생각한다. 바로 어떤 쾌락이 더 높은 질을 가지고 있는 경우인데, 밀은 모든 쾌락이 다 같은 질을 갖는 것이 아니라 어떤 쾌락은 더 높은 질을, 어떤 쾌락은 더 낮은 질을 갖는다고 주장한다. 그리고 이 쾌락의 질이 양보다 더 본래적 가치에 가까우므로 비록 어떤 쾌락이 적은 양의 쾌락을 줄지라도 높은 질을 가졌다면 이 쾌락은 많은 양의 쾌락을 주지만 낮은 질을 가진 쾌락보다 더 큰 본래적 가치를 갖는다고 여기게 된다. “만족한 돼지보다는 불만족한 인간이 되는 것이 더 낫다. 만족한 바보보다는 불만족한 소크라테스가 되는 것이 더 낫다.” 벤담처럼 그저 쾌락의 외적인 양에만 관심을 갖는 쾌락주의적 공리주의와 비교해서 밀의 공리주의를 행복주의적 공리주의라고 부르고 있다. 최대다수 최대행복 공리주의에서 행위의 기준이 되는 본래적 가치(쾌락, 혹은 행복)는 그럼 누구를 위한 것인가라는 질문을 할 수 있다. 여기서 그 기준이 행위를 하는 본인이라고 대답하면 윤리적 이기주의가 되며, 가족이거나 몇몇 친한 사람들이라고 해도 역시 그와 비슷한 입장이 될 것이다. 그렇지 않고 모든 사람을 위한 쾌락과 행복이라고 대답하는 것이 바로 공리주의자의 입장이다. 공리주의는 기본적으로 각 개인의 쾌락에 대한 공평하고 보편적인 판단을 전제로 한다. 한 사람의 쾌락은 다른 사람의 쾌락과 똑같이 계산되어야 한다. 한 행위자의 이익도 다른 모든 사람의 이익과 똑같이 고려되어야 한다. 모든 인간들은 그들의 이익을 충족하는 데에 있어서 똑같은 권리를 가진다. 그래서 쾌락이나 불쾌함을 각각 계산 가능한(긍정적이고 부정적인) 단위로 측정할 수 있다고 보고 어느 개인이나 집단의 쾌락(과 행복)의 결과적인 전체 양을 계산하고자 한다. 이 때 기준이 되는 것은 어떤 행위가 가져다주는 쾌락의 강도, 지속성, 확실성, 신속성 등인데 이를 통해서 쾌락의 총량을 정밀하게 계산해 낼 수 있다고 공리주의자들은 믿고 있다. 그래서 한 개인의 경우, 앞서의 기준에 따라 그 사람이 느끼는 쾌락(과 행복)의 긍정적, 부정적 양의 총계를 내서 그 결과에 따라 그 사람의 쾌락(과 행복)의 정도를 가늠할 수 있다. 또한 한 집단의 경우에는, 그 집단의 구성원들 각각의 쾌락(과 행복)의 양들을 합하여 얼마만큼의 결과가 나왔는지를 계산해서 그 집단의 쾌락(과 행복)의 정도를 따질 수 있다. 이 때 한 집단의 구성원들의 쾌락(과 행복)의 정도를 증가시키거나 감소시킬 수 있는 조건들이 있을 수 있는데, 결과적으로 구성원들의 쾌락(과 행복)의 총량이 최대치가 될 수 있는 조건들이 ‘정당한’ 조건들이 될 것이다. 반대로 구성원들의 쾌락(과 행복)의 총량을 증가시키지 못하는 조건들은 선택돼서는 안 될 것이다. 이 경우 그로 인하여 한 집단의 어느 구성원들은 극단적으로 피해를 보지만 다른 구성원들은 그 쾌락(과 행복)의 양이 (더 많이) 증가하게 되는 수단이 있다면 공리주의의 입장에서는 주저하지 않고 이것을 선택하게 된다. 반대로 모든 구성원들의 쾌락(과 행복)의 양이 평등하게 분배되지만 그 총량이 앞서의 경우보다 낮다면 이 정책은 마땅히 채택될 수 없을 것이다. 바로 이와 같은 경우가 공리주의의 맹점이 지적되는 상황이 된다. 공리주의의 ‘최대다수 최대행복’의 원칙을 따르면 불가분 사회적 불평등과 불공평한 부의 재분배와 같은 문제에 직면하기 때문이다. 이처럼 공리주의의 근본개념인 유용성과 사회적 정의라는 개념은 근본적으로 충돌하기 마련이다.",
      "frontmatter": {}
    },
    "11.인간과 사회": {
      "path": "/06.university/철학과-인간/11.인간과-사회/",
      "filename": "11.인간과 사회",
      "content": "사회의 개념 1) 아리스토텔레스의 정치적 인간관 사회라는 말은 본래 아리스토텔레스의 인간에 대한 정의인 \"정치적 동물(zoon politikon)\"이라는 표현에서 나왔다. politikon은 polis, 즉 도시 국가라는 원래의 뜻에서 파생된 말이니 처음부터 국가와 밀접한 연관을 맺고 있었다. 이 말이 후에 라틴어로 번역되면서 '사회적 동물(animal sociale)'이라는 말이 됐으므로, 결국 '국가', '정치(공동체)', '사회'라는 용어들은 공통의 기반으로부터 유래한 개념들이라고 볼 수 있다. 아리스토텔레스가 인간을 정치적 동물이라고 한 것은 두 가지 이유에서이다. 첫째, 그는 인간이 그 자연적 본성상 무리를 떠나서 살 수 없다고 보았다. 쉽게 말해 인간은 혼자 있으면 불안하고 외롭다고 느껴서 의례 다른 인간들을 찾기 마련이라는 의미이다. 하지만 두 번째로 이 말 속에는 인간이 다른 그 어떤 동물들보다도 고도의 군집생활을 할 수 있고, 하고 있는 존재라는 의미가 또한 들어 있다. 물론 사자나 원숭이들도 무리를 이루어 살아가고 있다. 하지만 인간의 모임이 더 높은 수준의 공동체를 이룬다는 데에는 이견이 없을 것이다. 흥미로운 점은, 아리스토텔레스가 그 근본이유를 인간의 언어구사 능력에서 찾는다는 데에 있다. 다시 말해 겉으로 보이는 것들, 예를 들어 웅장한 파르테논 신전이나 거대한 피라미드를 갖고 있기 때문에 인간의 공동체가 수준 높은 것이 아니라, 고작(?) 언어를 통한 복잡한 의사교환이 가능하다는 점 때문에 인간사회가 다른 동물들의 사회보다 위대하다는 것이다. 크게 두드러져 보이지도 않는 언어활동을 아리스토텔레스는 왜 중요하게 여겼을까? 그것은 바로 이를 통해서 인간 삶과 인간사회의 근본적인 가치들, 즉 정당한 것과 부당한 것, 선한 것과 악한 것 등에 대한 구성원 각자의 의견과 생각을 교환하거나 공유하는 것, 심지어 합의하는 것까지 가능하기 때문이다. 이런 중요한 가치에 대한 의사교환과 잇따르는 (어느 정도의) 합의에 의해 그 공동체는 정체성과 나아갈 방향성을 얻는 것이다. 따라서 아리스토텔레스는 언어를 통해 형성된 정신적 공감대가 사회와 국가라는 공동체를 형성하는 가장 중요한 토대가 된다고 본 것이다. 아리스토텔레스는 이렇게 정치적 공동체의 기반을 단순한 재화도 아니고, 많은 인구수도 아닌, 언어와 의견 교환을 통한 공동의 선에 대한 합의 능력과, 또한 이를 실현하기 위한 의지라고 보았기 때문에, 국가 구성원의 자격 또한 정의와 불의, 선과 악을 분별하는 능력이라고 생각했다. 이를 식별하고 또 실천할 수 있는 사람, 그의 말을 빌리자면 \"자유로운 자\"만이 국가라는 공동체의 구성원, 즉 시민이 될 수 있는 것이다. 잘 알려져 있다시피 당시 노예나 여성 등은 정치에 참여할 수 없었고 이를 신분제 사회의 한계라고 비판해야 마땅하지만, 아리스토텔레스는 이들이 교육을 받지 못했고, 따라서 이성적 사고능력이 부족하다는 이유에서 이들을 자유롭지 않은 자로 분류했다는 점에 주목하면 국가의 진정한 구성원이 무엇을 의미하는지를 다시 한 번 생각해 볼 수 있을 것이다. 오늘날 우리 사회도 미성년자나 다른 이유에서 지적 능력이 부족한 사람들, 즉 -아리스토텔레스의 말을 빌리자면- '자신의 미욱함으로부터 완전히 자유롭지 못한 자'들에게 정치적 참여권을 제한하는데 이 역시 같은 맥락이라고 볼 수 있을 것이고, 표면적으론 그렇지 않은 '보통' 선거권자들도 위에서 말한 의미로 '자유롭게' 판단하여 자신의 의사를 정치적으로 반영하지 못한다면, 예를 들어 혈연, 지연 등 갖가지 비이성적 조건으로 인해 분별없이 투표를 한다면 국가의 구성원이라고 불릴 자격이 사실은 없을 것이라는 점을 생각해 볼 수 있겠다. 2) 근대의 시민사회 근대의 사상가들이 수립하고자 한 새로운 사회 개념을 우리는 '시민사회(societas civilis)'라고 부른다. 홉스는 일종의 '시민 연합체'인 국가, 혹은 시민사회가 이전의 자연 상태를 일신했다고 보았다. 본래의 자연 상태에서 인간은 서로가 서로와 경쟁하며 배척하는, 이른바 '만인의 만인에 대한 투쟁'을 하기 마련인데, 시민들이 연합하여 국가를 형성함으로써 이 상태가 종식되었다는 것이다. 반면 루소와 로크는 자연 상태를 만인이 자유롭고 평등한 상태로 보았는데, 그들 역시 이것이 전쟁 상태로 추락하는 것을 방지하기 위해 시민들의 자유로운 계약에 의해 시민정부가 수립되고 그로부터 국가가 성립했다고 서술한다. 루소는 국가가 성립하게 된 배경으로 시민들의 '사회 계약(contrat social)'을 들었으며, 칸트 또한 시민들의 자유로운 계약이 시민사회 구성의 원동력이 되었다고 보았다. 물론 이런 계약이 역사상 실제로 있었다고 주장하는 이는 아무도 없다 그것은 일종의 암묵적인 무언의 사상적 동의와 공감에 가깝고 사회와 국가의 성립에 대한 정신적인 기반이 무엇이었을지 묻는 철학적인 성찰일 뿐이다. 이렇게 형성된 근대의 시민사회 개념은 오늘날의 국가에서도 여전히 정치적 공동체로서의 사회가 갖는 기본 성격을 나타내는 중요한 개념으로 기능하고 있다. 일례로 1776년의 미국 독립선언문이나 1789년의 프랑스 혁명 때 발표됐던 '인간 및 시민의 권리선언'과 같은 중요한 정치적 선언에는 이러한 시민사회의 이념이 잘 나타나 있다. 서구와 세계 곳곳의 '자유민주주의' 국가이론은 바로 이러한 이념 위에 형성된 것이다. 미국의 독립선언문(1776) \"우리들은 다음과 같은 것을 자명한 진리라고 생각한다. *[즉, 모든 사람은 평등하게 태어났으며, 조물주는 몇 개의 양도할 수 없는 권리를 부여했으며, 그 권리 중에는 생명과 자유와 행복의 추구가 있다. 이 권리를 확보하기 위하여 인류는 정부를 조직했으며, 이 정부의 정당한 권력은 인민의 동의로부터 유래하고 있는 것이다.]{.underline}* 또 어떠한 형태의 정부이든 이러한 목적을 파괴할 때에는 언제든지 정부를 변혁 내지 폐지하여 *[인민의 안전과 행복을 가장 효과적으로 가져올 수 있는]{.underline}*, 그러한 원칙에 기초를 두고 그러한 형태로 기구를 갖춘 새로운 정부를 조직하는 것은 인민의 권리인 것이다. \" '인간 및 시민의 권리 선언' (1789.8.26) 제1조 \"인간은 나면서부터 자유로우며 평등한 권리를 가진다.\" 제2조 \"모든 정치적 결합의 목적은 인간의 소멸시킬 수 없는 자연권의 보존이다. 그 권리는 자유, 재산, 안전 및 압제에 대한 저항이다.\" 제3조 \"모든 주권의 원리는 본질적으로 국민에게 있다.\" 자유주의적 사회이념 1) 사회 계약설 앞서 말한 바와 같이 오늘날 민주주의 사회에서 널리 받아들여지고 있는 '시민사회 국가' 이론은 '사회계약설'이라 불리는 사상을 배경으로 삼고 있다. 홉스를 시작으로 루소, 로크 등 근대의 이론가들이 하나같이 집중해서 다루었던 이 사회계약설은 시민사회 탄생의 이론적 토대가 되었다. 루소의 경우 인간의 자연적 본성을 '자연 상태', 혹은 '자연인'이라 하여 본래 인간은 자신에게 꼭 필요한 만큼 최소한의 욕구만을 갖고서 타인에게 피해를 주지 않는 자족과 평등의 상태에 있었을 것이라 상정한다. 그런데 이것이 여럿이 모여 사는 사회적 생활로 접어들며 이 속에서 사회적 경험, 예를 들어 타인과의 비교 등에 의한 자기반성을 통해 인위적인 모습으로 변질된다고 보았다. 이 상태에서는 타인의 소유와 구분되는 나의 재산, 즉 사유 재산이 형성되고, 더 나아가 남보다 더 나고자 하는 경쟁의식에 의해 부를 창출하려는 노력이 생기게 된다. 이 현상이 공동체에 만연하게 되면 결국 부와 가난, 지주와 노예 등 지배와 종속의 관계와 이에 의한 사회적 불평등이 생겨난다. 이 빈부격차와 각 집단들의 욕망이 점점 상호 충돌하면 결국 전쟁상태에 접어들게 될 것이다. 전쟁이라는 극단적 갈등에 의한 위기와 손실을 최소화하기 위해 인간은 이제 최선을 다해 방책을 마련하는데, 이것이 곧 사회와 법률인 것이다. 하지만 이렇게 형성된 사회에서는 부와 권력을 소유하고 있는 계층이 이 불평등을 영구화하기 위해 전체주의나 절대왕권 등을 확립하고자 한다. 이를 통해 강자는 약자를 지배하려고 하는데 이는 인간의 자연권을 침해하는 것이므로 이런 상태를 방지·지양하기 위한 장치로서 사회적 계약이 필요하다고 루소는 본 것이다. 홉스의 경우 계약은 왕과 시민사이에 성립되는 반면 루소는 다수의 개인이 한 국가의 국민이 되고자 스스로 맺는, 즉 시민들 사이의 계약, 혹은 자기 자신과의 계약을 의미하고 있다. 이 경우 주권은 어디까지나 국민에게 있고(국민주권) 주권자를 선택하여 그에게 주권을 위임하는 것도 국민이다. 따라서 국민이 스스로 결정하여 실행한 이 사회계약은 자유의지에 의한 정당한 것이지 자유를 억압하는 것이 아니다. 그렇기에 그 결과로 따라오는 특정한 사회적 구속이나 의무도 역시 정당성을 얻게 된다. 계약을 맺는 당사자인 개인은 하나의 개체이기도 하지만 또한 동시에 자신을 포함한 공동체의 일부분이기도 하다. 따라서 인간은 자신의 안위를 염려하는 개인으로서 '개별의지'도 갖지만 또한 공동체의 선을 생각하며 '일반의지(voloute generale)'를 갖기도 한다. 이 일반의지는 개별의지를 합쳐 놓은 '만인의 의지'(다수결)와는 다르며 공동체를 위해 최선의 것이 무엇일지를 먼저 고려하는 보편적인 목적을 갖고 있다. 법률과 같은 사회적 제도는 이러한 일반의지의 발현이어야 하고 개인은 (자신을 포함한) 공동체의 선을 위해 사회적 구속도 감수하는 것이다. '자연 상태'에서의 개인이 [자연적 자유]{.underline}를 누리며 자연인으로 산다면 '사회 상태'에서의 개인은 일반의지에 의해서 통제받는 [시민으로서의 자유]{.underline}를 가지는 것으로 구분될 수 있다. 로크도 비슷한 맥락에서 사회를 가능하게 하는 하나의 계약 개념을 생각해 보았다. 그는 먼저 국가가 성립되기 이전에 사람들이 어떤 상태였을까를 상상해 보는데, 그 때 사람들은 일종의 자연 상태에 있었을 것이라고 상정한다. 자연 상태의 사람들은 모두 자연적 권리로서 생명, 자유, 재산의 권리를 가지고 있었는데, 이 권리들은 천부적으로 가지고 있는 인권이며 다른 이에게 양도할 수 없는 성격이다. 그리고 이 상태의 사람들은 본래 완전한 [자유의 상태]{.underline}이자 [평등의 상태]{.underline}*이다. 먼저 이 상태에서 사람들은, \"타인의 허락을 구하거나 타인의 의지에 구애받지 않고, 자연법칙의 테두리 안에서 스스로 적당하다고 생각하는 바에 따라서 자신의 행동을 규제하고 자신의 소유물과 일신을 처분할 수 있는 완전한 자유의 상태\"에 있고, \"또한 그 안에서 모든 권력과 권한이 호혜적이며, 어느 누구도 다른 사람보다 더 많이 갖지 않는 평등의 상태\"에 있다고 로크는 얘기한다. 이 자연 상태에서 사람들은 어떤 종류의 지배-종속 관계도 없고, 자신의 소유와 권력에 있어서 다른 사람들과 동등하다. 그런데 이 자연에는 일종의 법이 있어서 이 자유가 무질서나 방종의 상태로 치닫는 것을 막는다고도 생각했다(=자연법 사상). \"바로 법인 이성은 의논을 바라는 모든 인류에게, 인간은 평등하고 독립적인 존재자이므로 어느 누구도 다른 사람의 생명, 건강, 자유 또는 소유물에 해를 끼쳐서는 안 된다고 가르친다.\" 이러한 자연 상태가 변하게 되는 계기는 사유재산의 발생이라 할 수 있다. 최초의 자연 상태에서 자연은 만인의 공동 소유물이자 공동자산이었다. 이에 대해 그 어느 누구도 사적인 지배권을 갖지 못하는 것이다. 그러나 모든 사람들은 자신의 신체와 그 신체에 의한 노동의 결과물에 대해서는 (배타적인) 소유권을 가지고 있다. 그래서 자연을 가공하고 그로부터 얻은 결실에 대해 자신의 소유, 즉 재산이라고 주장하게 된다. 자신의 노동에 의해 변형된 자연의 일부분은 더 이상 공유의 상태에 놓여 있지 않고 타인의 공동권리 또한 배제된다. 이 사유재산의 정당성과 그에 대한 권리는 시민사회 형성의 중요한 기반이 된다. \"그 자신의 주인으로서, 곧 그의 일신과 행위와 노동의 소유주로서 인간은 그 자신 안에 소유권의 주된 원천을 가지고 있다. 따라서 발명과 기술을 통하여 삶의 편익을 개선했을 때, 그가 자신을 부양하고 편리하게 하기 위해서 사용한 것의 대부분을 이루는 것은 전적으로 그의 것이며, 다른 사람과의 공유물이 아니다.\" 하지만 인간 사회는 인간의 이기적인 속성 탓에 타인의 정당한 권리와 재산을 침해하여 그들을 부당하게 지배하려는 일종의 전쟁 상태로 빠지게 된다. 이 전쟁 상태를 해소하기 위하여 사람들은 이제 상호 협약을 맺어 공동체를 세우고, 그 협약에 스스로 종속하게 된다. 시민으로부터 주권을 위임받은 정부는 입법권과 집행권을 가지고서 공공선을 위하여 주어진 권한을 행사하며, 이에 동의한 시민들은 제정된 법에 의해 보호도 받고 또한 그 법에 복종도 하는 것이다. 이렇게 로크도 사회의 형성을 루소와 유사하게 하나의 계약적 연대에 의한 것으로 바라보았는데, 루소보다 더 구체적인 제도적 장치와 과정까지 이론적으로 고찰함으로써 오늘날 시민사회 국가이론의 대체적인 윤곽을 그려내게 되었다. 2) 사회정의론 롤즈의 사회정의론은 강의내용을 참조하시기 바랍니다.",
      "frontmatter": {}
    },
    "03-FILE IO": {
      "path": "/06.university/system-programminguniversity/03-file-io/",
      "filename": "03-FILE IO",
      "content": "open() creat() close() read() write() lseek() Disk I/O efficiency dup(), dup2() stat(), fstat(), lstat() access() 파일 디스크립터 프로세스가 실행될때 기본적으로 여는 파일은 3개 각각 0(STDIN), 1(STDOUT) ,2(STDERR) man -s 2 open : 과 같이 메뉴얼 파일을 볼 수 있다 open int open (const char *pathname, int flags); int open (const char *pathname, int flags, mode_t mode); int open (const char *pathname, int flags); int open (const char *pathname, int flags, mode_t mode); 파일 디스크립터 값을 반환 -1 은 실패 flags : 파일을 열 때 취해지는 구체적 행동을 기술한다 파일 엑세스 플레그(필수 플래그) : 1개 필수 O_RDONLY(00) : 읽기 모드 (파일 엑세스 플레그) O_WRONLY(01) : 쓰기 모드 (파일 엑세스 플레그) O_RDRW(02) : 읽기 쓰기 모드 (파일 엑세스 플레그) 파일 생성 플래그 File Creation Flags (선택 플래그) : 생성과정에 영향 O_CREAT: 파일이 존재하지 않을 경우 새 파일을 생성합니다. O_EXCL: O_CREAT 와 함께 사용되며, 파일이 이미 존재하면 오류를 발생시킵니다. 이를 통해 파일의 중복 생성을 방지합니다. O_TRUNC: 파일이 이미 존재할 경우, 파일 내용을 비웁니다. O_CLOEXEC : 파일 디스크립터가 생성될 때, 해당 프로세스가 exec() 시스템 호출을 통해 새로운 프로그램으로 전환될 때 자동으로 닫히도록 설정 O_DIRECTORY : 이 플래그는 지정된 경로가 반드시 디렉토리여야 함을 나타냅니다. 만약 경로가 디렉토리가 아닐 경우, open() 호출은 실패합니다. O_NOCTTY : 이 플래그는 열린 파일이 제어 터미널이 되지 않도록 합니다. O_NOFOLLOW : 이 플래그는 심볼릭 링크를 따라가지 않도록 설정합니다. 만약 지정된 경로가 심볼릭 링크인 경우, open() 호출은 실패합니다. O_TMPFILE : 이름 없는 임시 파일 생성 파일 상태 플래그 (File Status Flags) (선택 플래그) : 파일이 열린 후의 I/O 동작에 영향을 미침 O_APPEND: 파일을 추가 모드로 열어, 매번 쓰기 전에 파일 끝으로 이동 O_ASYNC: 입출력이 가능할 때 SIGIO 신호를 생성하는 비동기 모드 활성화 O_DIRECT: 캐시 영향을 최소화한 I/O 수행 O_DSYNC: 데이터 일관성 보장 동기화 쓰기 모드 O_LARGEFILE: 큰 파일을 열 수 있도록 허용 O_NOATIME: 파일 접근 시간을 업데이트하지 않음 O_NONBLOCK: 비차단 모드로 파일 열기 O_PATH: 파일을 열지 않고 파일 디스크립터만 반환 O_SYNC: 파일 데이터와 메타데이터가 모두 동기화된 상태에서 쓰기 완료 ... 등등 모드 : mode_t = unsigned int (/usr/include/sys/types.h 에 선언되어 있음) Flag에 O_CREAT를 지정한 경우에만 필요 0-8비트는 파일의 보호모드 9-11 비트는 sticky 비트 공유모드 S_IRUSR (읽기 권한, 소유자): 소유자에게 읽기 권한 부여 S_IWUSR (쓰기 권한, 소유자): 소유자에게 쓰기 권한 부여 S_IXUSR (실행 권한, 소유자): 소유자에게 실행 권한 부여 S_IRGRP (읽기 권한, 그룹): 그룹에게 읽기 권한 부여 S_IWGRP (쓰기 권한, 그룹): 그룹에게 쓰기 권한 부여 S_IXGRP (실행 권한, 그룹): 그룹에게 실행 권한 부여 S_IROTH (읽기 권한, 기타): 기타 사용자에게 읽기 권한 부여 S_IWOTH (쓰기 권한, 기타): 기타 사용자에게 쓰기 권한 부여 S_IXOTH (실행 권한, 기타): 기타 사용자에게 실행 권한 부여 S_ISUID (셋 사용자 ID 비트): 셋 사용자 ID 비트 설정 S_ISGID (셋 그룹 ID 비트): 셋 그룹 ID 비트 설정 S_ISVTX (스티키 비트): 스티키 비트 설정 create int creat(const char *pathname, mode_t mode); creat( pathname, mode ); open ( pathname, OWRONLY OCREAT O_TRUNC, mode); 2개는 기능이 동일 이때 소유자 모드는 변경하지 않는다 close 열린 파일 닫기 read ssize_t read (int filedes, void *buf, size_t nbytes); buf 읽은 데이터를 담을 메모리의 시작 주소 nbytes 읽을 바이트 수 write ssize_t write (int filedes, void *buf, size_t nbytes); read 와 반대 copy.c lseek",
      "frontmatter": {
        "tags": [
          "system-programing",
          "system-call"
        ],
        "series": "system programming(university)",
        "series_weight": "3",
        "date": "2024-09-05T23:00:00+09:00",
        "lastmod": "2025-06-03T09:59:32+09:00"
      }
    },
    "04-FILE 조작": {
      "path": "/06.university/system-programminguniversity/04-file-조작/",
      "filename": "04-FILE 조작",
      "content": "link() / unlink() / remove() / rename() umask() chmod() / fchmod() truncate() / ftruncate() utime() Directory mkdir() / opendir() / readdir() / closedir() / rewinddir() / rmdir() chdir() / getcwd() Symbolic link sync() / fsync() 파일 시스템 Pasted image 20241005230119 Pasted image 20241005230383 Boot block First block of the file system Program to run system is located Super block Boot 다음 영역 file system 관리 정보 size of file system info for free block info for i-node file type access permission bits file size Time stamp Number of links pointers to data blocks for the file stat 구조체의 field들은 i-node에서 읽어온다 block to modify that super block is modified etc resides(상주하다) in kernel memory space in DRAM Needed to be consistent with the super block in disk Use \"sync\" system call (also a user cmd) i-node block list Super block 다음의 영역 Consists of many i-nodes 1 i-node / 1 file Identified by the i-node no. File 관리에 필요한 정보 저장 Data block Last area in the file system Contains the data for general files and directory files. general files directory files file name i-node numbers for files hard link Link from directory block to i-node Use i-node number to make the link (Linux command ‘ln’) Hard link는 같은 file system 내부에서만 연결 가능 만약 디렉토리일 경우 inode block 의 inode 값은 특정 디렉토리의 data block 상의 주소를 가리킴 data block 디렉토리 file 내부에 특정 파일의 inode 를 가리키는 주소값이 들어있음 inode block 의 inode 가 특정 파일의 data block 상의 주소를 가리킴 Pasted image 20241005231545 Pasted image 20241005231693 Pasted image 20241006010455 i-node 2549 link count 최소 3 i-node 1267 link count 최소 2 루트 디렉토리의 경우 부모디렉토리(..)가 자기자신(.) 과 동일한 inode 를 가리킴 link count of i-node number of directory entries that is pointing to an i-node stat 구조체의 st_link 링크 카운트(link count)는 파일 시스템에서 특정 i-node가 얼마나 많은 디렉토리 엔트리와 연결되어 있는지를 나타내는 값 즉 나(i-node)를 가리키는 주소들 갯수 방금 생성된 디렉토리 link count 가 기본적으로 2인 이유 상위 부모의 inode 가 가리키는 주소에 data block 에 디렉토리 정보(디렉토리 엔트리)에 나의 inode주소 값이 있다 나의 inode 가 가리키는 주소에 data block 에 디렉토리 정보에 (.) 이라는 나의 inode 를 가리키는 inode주소 값이 있다 Symbolic link Linux command ‘% ln –s /usr/lib lib’ ex) lrwxrwxrwx 1 root 7 Sep 25 07:14 lib -> /usr/lib File name: lib Contents of file: /usr/lib File size: 7 bytes The contents of file (data block) is the pathname of the target link Operations on file Deleting a file If a file is deleted in a directory block, link count of i-node decreases If link count become zero, both i-node and data blocks of a file are deleted 일반 파일 삭제 과정 상위 디렉토리 정보(directory entry)에 나의 inode 주소값이 적혀있는 것을 지운다 자연스레 나를 가리키는 디렉토리 정보가 사라졌으므로 link count 값이 1 줄어든다 만약 link count 값이 0 이면 나의 inode 주소를 가리키는 주소를 가진 개체들이 없으므로 data block 의 값을 삭제해도 된다 Moving a file Linux command: mv Directory entry is changed without changing the i-node and data block of file 파일 권한 Pasted image 20241006063697 umask : 가린다의 의미 mask 에 파일을 생성할 때 umask 값을 사용하여 적절히 생성한다 특수 권한 Pasted image 20241006193734 리눅스 파일 권한 Set UID 파일에 Set-UID 비트가 설정되면 다른 사용자가 파일을 실행했을때 해당 사용자의 권한이 아닌 파일의 소유자 권한으로 실행. Set-UID 비트를 설정하기 위해 소유자 허가권에 s를 추가하거나 앞에 4를 붙여줌. (ex 4750 => rwsr-x---) Set GID 파일에 Set-GID 비트가 설정되면 다른 사용자가 파일을 실행했을때 해당 사용자의 권한이 아닌 그룹의 권한으로 실행 Set-GID 비트를 설정하기 위해 소유자 허가권에 s를 추가하거나 앞에 2를 붙여줌. (ex 4750 => rwsr-x---) Sticky-Bit 디렉토리만 설정 가능 sticky-bit가 설정된 디렉토리에 파일을 생성하면 해당 파일은 생성한 사람의 소유가 되며, 오직 소유자와 root에게만 해당 파일에 대한 삭제 및 변경의 권한이 있다. 스티키비트 추가하기위해 문자방식인경우 t를 사용하고 숫자방식인경우 1 사용. link int link (const char *cur_path, const char *new_path); This call makes a new link/directory entry (newpath) to an existing file (curpath) 같은 i-node를 가리키는 directory entry가 하나 더 만들어짐 해당 i-node의 link count가 하나 증가 Return values Success: 0, link count increases Error: -1 Parameter cur_path: current hard link (or pathname) new_path: new hard link (or pathname) 보통 link count가 0이 되면 해당 파일은 삭제되지만, 다른 프로세스가 사용중인 파일이 삭제되는 것을 방지하기 위해 link count가 0에 도달했더라도 그 파일을 open 한 프로세스의 수가 0 이상이면 삭제하지 않는다 파일이 close 될 때 kernel은 그 파일을 open 한 프로세스 수가 0 인지 확인하고, 0 일 경우 link count가 0 인지 확인하고 삭제한다 unlink remove int remove (const char *pathname); if mathname is nomal file => remove = unlink if mathnaem is dir file => remove = rmdir rename int rename(const char *oldpath, const char *newpath); rename() 함수는 파일의 이름을 변경하며, 필요시 디렉토리 간에 이동할 수 있습니다. 이 함수는 다음과 같은 특성을 가지고 있습니다: 하드 링크와의 영향: 다른 하드 링크(예: link(2) 를 사용하여 생성된 링크)에 대해서는 영향을 미치지 않습니다. 열린 파일 디스크립터: oldpath 에 대한 열린 파일 디스크립터는 영향을 받지 않습니다. rename() 작업이 성공하는지 여부는 여러 가지 제약 조건에 따라 결정됩니다. 아래는 관련된 오류 상황입니다: newpath가 이미 존재하는 경우: rename() 은 원자적으로 대체되므로, 이 시점에서 다른 프로세스가 newpath 를 접근하려고 할 때 파일이 없다고 발견되는 상황은 없습니다. 그러나 oldpath 와 newpath 가 모두 파일을 참조하는 상태가 발생할 수 있는 시간 여유가 있습니다. oldpath와 newpath가 동일한 하드 링크인 경우: 두 경로가 동일한 파일을 참조하고 있다면, rename() 은 아무 작업도 하지 않고 성공 상태를 반환합니다. newpath가 존재하지만 작업이 실패하는 경우: 어떤 이유로든 작업이 실패하면, rename() 은 newpath 의 인스턴스를 그대로 유지합니다. oldpath가 디렉토리를 참조하는 경우: 이 경우, newpath 는 존재하지 않거나 비어 있는 디렉토리를 지정해야 합니다. oldpath가 심볼릭 링크를 참조하는 경우: 링크가 이름이 변경됩니다. 만약 newpath 가 심볼릭 링크를 참조한다면, 해당 링크는 덮어씌워집니다. umask mode_t umask(mode_t newmask); newmask: bitwise OR SIRUSR, SIWUSR, S_IXUSR SIRGRP, SIWGRP, S_IXGRP SIROTH, SIWOTH, S_IXOTH",
      "frontmatter": {
        "tags": [
          "system-programing",
          "system-call"
        ],
        "series": "system programming(university)",
        "series_weight": "4",
        "date": "2024-10-05T22:52:00+09:00",
        "lastmod": "2025-06-27T19:57:04+09:00"
      }
    },
    "05-PROCESS": {
      "path": "/06.university/system-programminguniversity/05-process/",
      "filename": "05-PROCESS",
      "content": "Pasted image 20241027181337 qexec() 이후에 오는 문자에 의해 구별되는 표시는 다음의 의미가 있다 l -> 인자 정보를 개개의 문자열 데이터를 가르키는 포인터 arg0, arg1……. argn으로 전달한다 v -> 인자 정보를 개개의 문자열 데이터를 가리키는 포인터 배열의 선두주소 argv로 전달한다 e -> envp 정보를 전달한다 p -> p를 사용하는 경우, 실행할 파일이름을 환경 변수 PATH로 지정한 디렉토리 안에서 찾아내어 실행한다 execve() 축약어: exec + v (vector) + e (environment) 설명: \"exec\"는 실행을 의미하며, \"v\"는 인수를 배열 형태로 받고, \"e\"는 환경 변수를 배열 형태로 받는다는 것을 나타냅니다. execv() 축약어: exec + v (vector) 설명: \"exec\"는 실행을 의미하며, \"v\"는 인수를 배열 형태로 받는다는 것을 나타냅니다. 환경 변수를 설정할 수 없습니다. execvp() 축약어: exec + v (vector) + p (path) 설명: \"exec\"는 실행을 의미하며, \"v\"는 인수를 배열 형태로 받고, \"p\"는 PATH 환경 변수를 통해 파일을 찾는다는 것을 나타냅니다. execle() 축약어: exec + l (list) + e (environment) 설명: \"exec\"는 실행을 의미하며, \"l\"은 인수를 개별적으로 받는다는 것을 나타내고, \"e\"는 환경 변수를 배열 형태로 받는다는 것을 나타냅니다. execl() 축약어: exec + l (list) 설명: \"exec\"는 실행을 의미하며, \"l\"은 인수를 개별적으로 받는다는 것을 나타냅니다. 환경 변수를 설정할 수 없습니다. execlp() 축약어: exec + l (list) + p (path) 설명: \"exec\"는 실행을 의미하며, \"l\"은 인수를 개별적으로 받고, \"p\"는 PATH 환경 변수를 통해 파일을 찾는다는 것을 나타냅니다. 정리 함수 이름 축약어 설명 인수 형태 환경 변수 지원 execve exec + v (vector) + e (environment) 파일 이름 + argv[] + envp[] 지원 execv exec + v (vector) 파일 경로 + argv[] 지원하지 않음 execvp exec + v (vector) + p (path) 파일 이름 + argv[] 지원하지 않음 execle exec + l (list) + e (environment) 파일 이름 + arg1, arg2, ... + NULL 지원 execl exec + l (list) 파일 경로 + arg1, arg2, ... + NULL 지원하지 않음 execlp exec + l (list) + p (path) 파일 이름 + arg1, arg2, ... + NULL 지원하지 않음",
      "frontmatter": {
        "tags": [
          "system-programing"
        ],
        "series": "system programming(university)",
        "series_weight": "5",
        "date": "2024-10-27T18:04:00+09:00",
        "lastmod": "2024-10-27T18:04:00+09:00"
      }
    },
    "07-SIGNAL": {
      "path": "/06.university/system-programminguniversity/07-signal/",
      "filename": "07-SIGNAL",
      "content": "",
      "frontmatter": {
        "tags": [
          "system-programing"
        ],
        "series_weight": "7",
        "date": "2024-11-12T15:11:00+09:00",
        "lastmod": "2024-11-12T15:11:00+09:00"
      }
    },
    "lab5 과제 추가 설명": {
      "path": "/06.university/system-programminguniversity/lab5-과제-추가-설명/",
      "filename": "lab5 과제 추가 설명",
      "content": "파일의 이름을 바꾸는 프로그램을 작성하는데, - 파일의 이름을 같은 디렉토리 안에서 바꾸거나 - 다른 디렉토리로 이동하여 바꾸는 기능이 모두 구현되어야 합니다. - 4-1 강의자료에서 학습한 시스템 호출들을 사용하여 작성하기 바랍니다. manual rename rename() 함수는 파일의 이름을 변경하며, 필요시 디렉토리 간에 이동할 수 있습니다. 이 함수는 다음과 같은 특성을 가지고 있습니다: 하드 링크와의 영향: 다른 하드 링크(예: link(2) 를 사용하여 생성된 링크)에 대해서는 영향을 미치지 않습니다. 열린 파일 디스크립터: oldpath 에 대한 열린 파일 디스크립터는 영향을 받지 않습니다. 성공 여부 결정 rename() 작업이 성공하는지 여부는 여러 가지 제약 조건에 따라 결정됩니다. 아래는 관련된 오류 상황입니다: newpath가 이미 존재하는 경우: rename() 은 원자적으로 대체되므로, 이 시점에서 다른 프로세스가 newpath 를 접근하려고 할 때 파일이 없다고 발견되는 상황은 없습니다. 그러나 oldpath 와 newpath 가 모두 파일을 참조하는 상태가 발생할 수 있는 시간 여유가 있습니다. oldpath와 newpath가 동일한 하드 링크인 경우: 두 경로가 동일한 파일을 참조하고 있다면, rename() 은 아무 작업도 하지 않고 성공 상태를 반환합니다. newpath가 존재하지만 작업이 실패하는 경우: 어떤 이유로든 작업이 실패하면, rename() 은 newpath 의 인스턴스를 그대로 유지합니다. oldpath가 디렉토리를 참조하는 경우: 이 경우, newpath 는 존재하지 않거나 비어 있는 디렉토리를 지정해야 합니다. oldpath가 심볼릭 링크를 참조하는 경우: 링크가 이름이 변경됩니다. 만약 newpath 가 심볼릭 링크를 참조한다면, 해당 링크는 덮어씌워집니다.",
      "frontmatter": {
        "tags": [
          "system-programing"
        ],
        "series": "system programming(university)",
        "date": "2024-10-06T03:47:00+09:00",
        "lastmod": "2024-10-06T03:47:00+09:00"
      }
    },
    "리눅스 링킹": {
      "path": "/06.university/system-programminguniversity/리눅스-링킹/",
      "filename": "리눅스 링킹",
      "content": "리눅스 시스템 콜 예시 y = sin(x) 코드가 있을 때 #include <math.h> 를 선언하지 않아도 컴파일 시에 /usr/lib/libm.a 파일을 링크 시켜주면 실행 가능하다 단 선언은 해야하므로 double sin(double x); 코드를 적어주어야 한다 gcc -lm program.c 실행 시에 컴파일 된다 옵션은 linking math 이다",
      "frontmatter": {
        "tags": [
          "linux"
        ],
        "series": "system programming(university)",
        "series_weight": "1",
        "date": "2024-09-03T11:57:00+09:00",
        "lastmod": "2024-09-03T11:57:00+09:00"
      }
    },
    "Untitled": {
      "path": "/06.university/mju_ecs-project/untitled/",
      "filename": "Untitled",
      "content": "%20image%2020250513062286.png) [고건혁] [오전 1:16] 포워딩 방식을 지금 하려고 하니까 생각난 궁금증이 일단 ttyd컨테이너는 외부 접근을 차단하고 포워딩 방식으로 한다고 하면 도커 컨테이너(커스텀컨테이너)에 대한 접근은 어떤 식으로 하는건가요? 아래 사진과 같은 방식중 첫번째 방식을 사용하려고 하였습니다 Pasted image 20250513064647 [고건혁] [오전 1:17] 포워딩 방식을 사용하는 이유가 단순히 다른 사용자가 남의 컨테이너에 접근하는데 그냥 localhost:9000치고 들어가버리면 안되니까 그런건데 예를 들어서 ttyd웹 환경에서 안하고 다른사람이 남의 컨테이너 포트 번호를 아는상태에서 VsCode나 로컬 터미널 환경에서 접근하는건 어떻게 막는건가요? 상단에 설명한 것 처럼 허용하는 ip 만 접근 가능한거여서 포워딩 방식으로 접근하지 않는 한 접근 할 수 없습니다 [고건혁] [오전 2:26] 포워딩 방식이 여전히 spring에서는 ws을 전달하지 못하는 문제가 있어서 새로 구현한 방식이 일단 기존 학교 msi나 lms같이 처음부터 모든 포트를 닫고 사용자가 인증에 성공한 경우에만 지속적으로 ping을 보내서 포트를 열어두고 사용자가 사용을 중지하면 포트를 닫아서 보안을 유지하는 방식으로 변경했습니다 기본구조가 이제 사용자가 로그인->컨테이너 생성(생성될때 포트 할당 및 맵핑만 하고 닫아둠)->사용자가 로그인한 토큰을 가진채로 internal/unlock/{\\\\}에 인증->연결된 ttyd포트 (localhost:9000같은 주소)주소 응답->사용자를 internal/ping/{\\\\}으로 넘기고 지속적으로 핑응답하도록->사용자는 ping페이지와 localhost탭이 같이 켜져있으면 지속적으로 사용가능->ping탭이 닫히면 3분안에 포트를 다시 닫음 이런식으로 만들었습니다 3가지 방법중 3번 방법인 것 같은데 사용자가 터미널을 사용하고 있을 때는 외부 사람들도 접근 가능한거 아닌가요 Pasted image 20250513065092 [고건혁] [오전 2:28] 근데 구현은 일단했는데 포트 닫는 port DROP명령어가 macOS에는 없어서 현재 테스트가 안됩니다..아마 저희가 올리는 서버는 리눅스 기반이면 \"iptables -D INPUT -p tcp —dport \" + port + \" -j DROP\"이 명령어 실행이 가능해서 (관리자 권한으로 실행되어야함)가능할거 같긴한데 지금은 모르겠네요.. 이거는 해봐야 알 수 있을거 같은데 was 에서 관리자권한이 필요한 명령이 실행가능한지는 모르겠어요 [고건혁] [오전 6:21] 컨테이너 상세 상태 stats연결 해보여고 했는데 현재 docker-java에서 2375포트 통해서 데몬 열고 받아오는 수 밖에 없어서 그걸 초간격으로 주기적으로 실행하면 기존에 열린 포트랑 충돌 나서 불가능 할거 같아요 이 말이 정확히 무슨 말인지 이해가 안되는데 한 번에 여러 개의 stats 연결을 동시에 여는 게 어려움 이런 말인가요? <- 이 문제는 조금 중요한 거 같아서 회의를 한번 해봐야 할 거 같아요 [고건혁] [오전 6:26] 기본적인 상태 전송은 어차피 ping방식으로 해서 요청을 보내니까 그 요청의 응답으로 상태 전송하면 좋을거 같아서 ping엔트포인트에 응답으로 pong상태를 보내는 식으로 만들고 있습니다 status 또는 stat 을 poling 또는 long Poiling 방식으로 구현하신다는 거죠?",
      "frontmatter": {
        "date": "2025-06-03T06:05:16+09:00",
        "lastmod": "2025-06-04T06:11:20+09:00"
      }
    },
    "mju ecs 해야할 것": {
      "path": "/06.university/mju_ecs-project/mju-ecs-해야할-것/",
      "filename": "mju ecs 해야할 것",
      "content": "수정사항 PortAllocator (포트 배정 서비스 추가) 단순 http 요청 docker status 추가 용어 정리 (용어 혼재 사용) hostPort -> containerPo 미해결 과제 포트 연결시 TCP 인지 UDP 인지도 선택여부에 두어야 하는가? 컨테이너 삭제시 ttyd 또한 삭제가 필요 현재는 그대로 방치됨 도커 볼륨 관련 현재 container.js 에서 너무 많은 일을 하고 있어 new_container.js 와 container.js 로 분리해줘 container table 과 ttyd table container table 과 ttyd table 2개로 분리할 필요 없이 1개의 테이블에 있어도 되는 거 아닌가? (실시간 status 시) tydContainerId; //ttyd 컨테이너 ID private String ttydHostPort; //ttyd 호스트 포트 // ttyd 컨테이너의 컨테이너 포트의 경우 고정되어 있으므로 필요 없음 필요 status status 에 ttyd 접근 포트 필요함 나 ttyd 실행시 포트 설정 => 직접 포트 접근 막기 https://chat.qwen.ai/c/633a43de-01b2-4739-abbd-13139669c810 명령어 이해 상세 -p 127.0.0.1:hostPort:containerPort 로컬에서만 접근 가능 host url 이 달라져도 localhost 로만 접근 docker template 만들기 docker document 만들기 서버 클라이언트 api 호환 마지막 건혁님 forword endpoint status hostname vscode (version 1.100) 현재 도커 터미널 관련 아키텍쳐 터미널에 접근하는 것을 ttyd 로 했는데 1개의 터미널당 1개의 ttyd 컨테이너가 필요하고 ttyd 가 각 컨테이너의 프록시 역할을 해 이것은 아키텍쳐 적으로 너무 복잡해 이것을 고치기 위해 websocket 프론트 부분은 terminal.html?containerId={containerId} 프론트 부분은 xterm.js 로 구성 백엔드 부분은 spring websocket 으로 구성 (특정 도커와 적절히 연결) 마지막에 기존 ttyd 의존성 완전 삭제 백엔드 WebSocket 설정 WebSocketConfig.java 터미널 핸들러 작성 TerminalWebSocketHandler.java 컨테이너 터미널 세션 관리 클래스 DockerTerminalSessionManager.java 실제 터미널 실행 클래스 DockerTerminalSession.java Docker Java 클라이언트 확장 기능 DockerJavaClient.java - 일부 메서드 추가 OutputStream 커스텀 클래스 프론트엔드",
      "frontmatter": {
        "date": "2025-06-03T06:05:16+09:00",
        "lastmod": "2025-06-04T06:11:13+09:00"
      }
    },
    "mjuecs 2": {
      "path": "/06.university/mju_ecs-project/mjuecs-2/",
      "filename": "mjuecs 2",
      "content": "컨테이너 기반 컴퓨팅 제공 서비스 프로젝트 설계서 개요 본 문서는 AWS ECS(Elastic Container Service)와 유사한 사용자 맞춤형 컨테이너 기반 컴퓨팅 자원 제공 서비스를 구축하기 위한 상세 설계안을 기술한다. 사용자는 본 서비스를 통해 독립된 Docker 컨테이너 환경을 할당받아 원하는 애플리케이션을 실행할 수 있다. 본 서비스의 가장 큰 기술적 특징은 데이터베이스(DB)의 역할을 사용자 인증 정보 관리에만 국한하고, 컨테이너의 생성, 상태, 소유권 등 모든 운영 정보는 각 Docker 호스트로부터 실시간으로 조회하여 메인 API 서버(Python Flask)의 인메모리 캐시에 저장 및 관리하는 것이다. 컨테이너와 사용자의 매핑은 엄격한 컨테이너 네이밍 규칙을 통해 이루어진다. 이는 Docker 호스트의 실제 상태를 시스템의 유일한 진실 공급원(Source of Truth)으로 삼아 DB와 실제 상태 간의 불일치 가능성을 원천적으로 제거하려는 설계 의도를 반영한다. 주요 기능: 사용자 가입 및 인증 사용자별 Docker 컨테이너 생성, 조회, 시작, 중지, 삭제 컨테이너 자원 제한 (CPU, 메모리) 컨테이너 자동 배치 (스케줄링) 사용자별 컨테이너 생성 개수 제한 주기적인 컨테이너 상태 검사 및 정책 기반 관리 시스템 아키텍처 구성 요소 물리적/가상 머신 구성 (최초 제안 기반): 컴퓨터 1 (제어 플레인): 사용자 인증 데이터베이스: PostgreSQL 또는 MySQL 사용. 사용자 계정 정보만 저장. 메인 API 서버: Python Flask 기반으로 개발. 모든 비즈니스 로직, Docker 호스트 연동, 인메모리 캐시 관리 수행. 웹 서버/리버스 프록시: Nginx 사용. SSL/TLS 종료, 로드 밸런싱(단일 Flask 인스턴스 환경에서는 주로 정적 파일 서빙 및 리버스 프록시 역할), 요청 포워딩. 컴퓨터 2, 3, 4 ... (데이터 플레인 - Docker 호스트): Docker Engine 설치 및 실행. 실제 사용자 컨테이너들이 구동되는 환경. host 상태를 주기적으로 확인 및 전송할 수 있는 서비스( Grafana , Prometheus , node-exporter ) 필요에 따라 수평적으로 확장 가능. 소프트웨어 스택: OS: Linux 백엔드: Python (Flask 프레임워크) 웹 서버: Nginx 컨테이너 기술: Docker Engine 데이터베이스: PostgreSQL 또는 MySQL 라이브러리: docker (Python Docker SDK), APScheduler (백그라운드 작업), bcrypt (비밀번호 해시), JWT 관련 라이브러리 (예: PyJWT ). 데이터 흐름 사용자 요청: 클라이언트(웹 UI 또는 API 클라이언트)가 서비스 API를 호출한다. Nginx: 요청을 수신하여 필요한 경우 SSL/TLS 처리를 수행하고, Flask API 서버로 요청을 전달한다. Flask API 서버 (인증): 인증이 필요한 요청의 경우, 요청 헤더의 JWT 토큰을 검증한다. 로그인/가입 시에는 DB의 사용자 테이블을 조회/수정한다. Flask API 서버 (컨테이너 관련 요청): 조회/상태 확인: 관리 중인 컨테이너 정보는 자체 인메모리 캐시에서 조회한다. 생성/삭제/제어: 스케줄링 로직에 따라 대상 Docker 호스트를 결정하고, 해당 호스트의 Docker Engine API를 직접 호출하여 작업을 수행한다. 작업 결과 및 최신 상태는 즉시 인메모리 캐시에 반영된다. Docker 호스트: Flask API 서버의 요청에 따라 컨테이너를 생성, 시작, 중지, 삭제하고 그 결과를 반환한다. 응답: Flask API 서버는 처리 결과를 Nginx를 통해 클라이언트에게 반환한다. 네트워크 구성 Flask API 서버 모든 docker host 의 ip 와 접근가능한 포트를 구성파일에 기록해둔다 `` { \"server1\":{ \"ip\" : \"192.168.0.56\", \"port_range\" : [ 10000, 11999 ] }, \"server2\":{ \"ip\" : \"192.168.0.57\", \"port_range\" : [ 12000, 13999 ] }, \"server3\":{ \"ip\" : \"192.168.0.58\", \"port_range\" : [ 14000, 15999 ] }, \"server4\":{ \"ip\" : \"192.168.0.56\", \"port_range\" : [ 16000, 17999 ] }, \"server5\":{ \"ip\" : \"192.168.0.56\", \"port_range\" : [ 18000, 19999 ] }, \"server6\":{ \"ip\" : \"192.168.0.56\", \"port_range\" : [ 20000, 11999 ] } } \u0002PROTECTED0\u0003 # 예시: # self.all_containers_cache = {\"docker_id_1\": ContainerInfo(...), \"docker_id_2\": ContainerInfo(...)} # self.host_status_cache = {\"host_A\": HostInfo(...), \"host_B\": HostInfo(...)} ` - **애플리케이션 재시작 시 캐시 복구 (\"웜업\"):** - Flask 애플리케이션 시작 시, 인메모리 캐시는 비어있는 상태이다. - 시작 루틴에서 설정된 모든 Docker 호스트에 접속하여 현재 실행 중인 모든 컨테이너 정보와 호스트 상태를 조회하고, 이를 파싱하여 전체 인메모리 캐시를 재구성한다. - 이 과정은 호스트 및 컨테이너 수에 따라 시간이 소요될 수 있으며, 완료 전까지 서비스는 불완전한 정보를 제공할 수 있다. **4.3. 컨테이너-사용자 매핑 (라벨 기반)** owneruserid=[userid] [userid] 는 컨테이너를 생성한 사용자의 ID이다. API 서버는 컨테이너 생성 시 인증된 사용자의 userid - **소유권 판단:** Labels owneruserid 라벨 값을 해당 컨테이너의 owneruserid userid 와 캐시된 컨테이너의 owneruserid - **컨테이너 이름:** 컨테이너 이름은 사용자가 원하는 이름 **4.4. API 엔드포인트 (예시)** /api/auth POST /register POST /login POST /logout /api/containers GET / POST / { \"image\": \"ubuntu:latest\", \"namesuffix\": \"myserver\", \"ports\": {\"80/tcp\": null} } GET / : 특정 컨테이너 상세 정보 조회. (이름은 [userid][namesuffix] DELETE / POST //start POST //stop /api/admin GET /hosts/status GET /cache/inspect POST /cache/refresh --- ### 5. 컨테이너 관리 로직 **5.1. 컨테이너 생성 (사용자 요청 처리 상세)** 1. **사용자 인증:** API 요청 헤더의 JWT 토큰을 검증하여 유효한 사용자인지 확인한다. userid 3. **요청 자원 확인:** 사용자가 요청한 이미지, 포트 매핑, 이름 접미사 등을 파싱한다. 기본 자원 할당량(CPU 2개, 메모리 3GB)을 설정한다. 4. **스케줄링 (호스트 선택):** 사용자가 원하는 host 에 컨테이너를 생성할 수 있도록 한다 [인증된userid][사용자요청namesuffix] 형식으로 기본 이름을 구성한다. 만약 namesuffix /containers/create , 이후 /containers/{id}/start HostConfig 내 CpuQuota , CpuPeriod , Memory 7. **결과 처리 및 캐시 업데이트:** - Docker API로부터 생성 성공/실패 응답을 받는다. - 성공 시: 사용자에게 적절한 성공 메세지를 반환한다 - 실패 시: 사용자에게 적절한 오류 메시지를 반환한다. 8. **동시 생성 요청 제어:** 한 사용자가 동시에 여러 생성 요청을 보내는 것을 방지하기 위해, 사용자별로 \"생성 진행 중\" 플래그를 관리한다 (Flask 세션, 또는 간단한 인메모리 딕셔너리 활용. 분산 환경에서는 Redis 등의 외부 Lock 필요). **5.2. 컨테이너 자원 제한** HostConfig 의 CpuPeriod 와 CpuQuota 를 설정한다. 예를 들어, 2개의 CPU 코어 효과를 내려면 CpuPeriod=100000 (기본값), CpuQuota=200000 HostConfig 의 Memory (바이트 단위)를 설정한다 (예: 3 1024 1024 * 1024 for 3GB). MemorySwap 을 Memory - **디스크 (50GB - 가이드라인):** - Docker 자체만으로 컨테이너별 엄격한 디스크 사용량 쿼터를 적용하는 것은 스토리지 드라이버 및 OS 설정에 따라 복잡하다. - 본 설계에서는 \"50GB\"를 **사용 가능한 총 이미지 크기 + 컨테이너의 쓰기 가능한 레이어에서 사용할 수 있는 예상 작업 공간의 가이드라인**으로 간주한다. - 또는, 컨테이너 생성 시 특정 크기의 Docker 볼륨을 생성하여 마운트하는 방식을 고려할 수 있으나, 이는 볼륨 관리 기능 추가 및 스토리지 플러그인 연동이 필요할 수 있다. - 초기에는 명시적 제한보다는 사용량 모니터링 및 가이드라인 제공에 초점을 맞춘다. **5.3. 일일 검사 및 정책 적용** APScheduler - 작업 내용: 1. 인메모리 캐시의 모든 컨테이너 정보를 순회한다. 2. 각 컨테이너에 대해 정의된 규칙을 검사한다: exited - 예: 관리자가 정의한 특정 금지된 이미지를 사용한 컨테이너 (이미지 이름 검사). - 예: 과도한 네트워크 트래픽을 유발하는 것으로 의심되는 컨테이너 (별도 모니터링 데이터 연동 필요 - 고급). 3. 규칙에 해당하는 컨테이너에 대해 자동 조치(예: 중지, 사용자에게 경고 알림)를 수행하거나 관리자에게 보고한다. 조치 내역은 상세히 로깅한다. --- ### 6. Docker 호스트 관리 config.py APScheduler GET /info GET /containers/json?all=true GET /containers/{id}/stats?stream=false - **호스트 장애 감지:** 폴링 시 특정 호스트로부터 응답이 없거나 오류가 발생하면, 해당 호스트를 \"unreachable\" 상태로 캐시에 표시하고, 이 호스트에서 실행 중이던 컨테이너들도 접근 불가능 상태임을 인지한다. 사용자에게 해당 컨테이너가 일시적으로 사용할 수 없음을 알릴 수 있다. 자동 장애 복구(다른 호스트로 컨테이너 이전)는 본 설계의 초기 범위에는 포함되지 않는다 (복잡도 매우 높음). --- ### 7. 보안 고려 사항 - **API 인증 및 권한 부여:** 모든 API 엔드포인트는 JWT 토큰을 통해 인증된 사용자만 접근 가능하도록 하며, 자신의 자원에만 접근할 수 있도록 철저한 권한 검사를 수행한다. - **입력값 검증:** SQL Injection, Command Injection, XSS 등의 공격을 방지하기 위해 모든 사용자 입력(API 파라미터, 요청 본문)에 대해 Flask 레벨에서 또는 Pydantic, Marshmallow 같은 라이브러리를 사용하여 엄격한 유효성 검사를 수행한다. - **Docker Daemon API 보안:** - 반드시 TLS를 활성화하여 Flask API 서버와 Docker 호스트 간의 모든 통신을 암호화한다. - Docker 호스트의 방화벽에서 Flask 서버의 IP 주소에서만 Docker API 포트(일반적으로 TCP 2376)로의 접근을 허용한다. - **컨테이너 격리 강화:** - 컨테이너 내부에서 실행되는 애플리케이션은 가능한 최소한의 권한을 가진 사용자(non-root user)로 실행되도록 Dockerfile을 작성한다. --cap-drop=ALL ). 필요한 최소한의 capability만 선택적으로 추가한다 ( --cap-add=... - (고급) AppArmor 또는 Seccomp 프로필을 적용하여 컨테이너가 수행할 수 있는 시스템 콜을 제한하고 파일 시스템 접근을 제어하여 보안 계층을 강화할 수 있다. - **이미지 보안:** - 서비스에서 사용할 수 있는 Docker 이미지 목록을 관리자가 사전에 승인하고 제공하는 방식(Curated List)을 강력히 권장한다. - 만약 사용자가 임의의 이미지를 지정할 수 있게 한다면, 이미지 스캔 도구(예: Trivy, Clair)를 사용하여 알려진 보안 취약점을 검사하는 프로세스를 도입하거나, 위험성을 사용자에게 명확히 고지해야 한다. - **네이밍 규칙 악용 방지:** userid userid --- ### 8. 확장성 및 안정성 - **API 서버 (Flask):** - **단일 인스턴스 제약:** 현재 설계(모든 상태를 Flask 앱 메모리에 저장)는 **단일 Flask 인스턴스 환경에 최적화**되어 있다. Gunicorn이나 uWSGI를 사용하여 여러 워커 프로세스를 실행할 수 있지만, 이는 단일 머신 내에서의 병렬 처리 능력 향상이며, 여러 머신으로 Flask 앱을 확장하는 것과는 다르다. - **확장 시 문제점:** 여러 Flask 인스턴스로 확장할 경우, 각 인스턴스가 자신만의 독립된 인메모리 캐시를 가지게 되어 상태 불일치가 발생하고 서비스가 오작동할 수 있다. 이를 해결하려면 Redis와 같은 외부 공유 분산 캐시 시스템을 도입해야 하며, 이는 \"DB 외에 다른 저장소를 사용하지 않겠다\"는 초기 설계 원칙과 상충될 수 있다. **따라서 본 설계는 단일 API 서버 인스턴스 운영을 기본 전제로 한다.** - **데이터베이스:** 사용자 인증용으로만 사용되므로 상대적으로 부하가 적어 초기에는 단일 인스턴스로 충분하다. 사용자 수가 매우 많아질 경우 DB 읽기 복제 등을 고려할 수 있다. - **Docker 호스트:** 필요에 따라 더 많은 Docker 호스트 머신을 추가하여 수평적으로 쉽게 확장할 수 있다. Flask API 서버 설정에 새 호스트 정보를 추가하면 된다. - **애플리케이션 재시작 및 장애:** - Flask API 서버가 재시작되면 인메모리 캐시가 모두 소실되므로, 모든 Docker 호스트를 스캔하여 캐시를 재구축하는 \"웜업\" 과정이 필요하다. 이 시간 동안 서비스 응답이 지연되거나 불완전할 수 있다. - Docker 호스트 자체의 장애는 폴링 메커니즘을 통해 감지된다. 해당 호스트의 컨테이너는 \"unreachable\" 상태로 처리된다. 자동화된 컨테이너 재배치(self-healing)는 구현 복잡도가 높아 현재 설계 범위에는 포함되지 않는다. --- ### 9. 로깅 및 모니터링 - **로깅:** **컨테이너 정보를 DB에 저장하지 않으므로, 모든 활동에 대한 상세한 로깅이 시스템의 상태 추적 및 문제 해결에 매우 중요하다.** - **Flask API 서버 로깅:** 모든 API 요청(엔드포인트, 파라미터, 사용자 ID), 응답(성공/실패, 상태 코드), 주요 내부 동작(캐시 업데이트, 스케줄링 결정, Docker API 호출 내역), 오류 및 예외 상황 등을 파일 또는 표준 출력으로 상세히 기록한다. - **컨테이너 이벤트 로깅:** 컨테이너 생성, 삭제, 시작, 중지 등의 주요 이벤트 발생 시 관련 정보(사용자 ID, 컨테이너 이름, 호스트 등)를 명확히 로깅한다. - **주기적 작업 로깅:** 호스트 상태 폴링 결과, 일일 검사 및 정책 적용 결과 등을 로깅한다. - **중앙 집중식 로깅 시스템 고려:** 운영 규모가 커지면 ELK Stack(Elasticsearch, Logstash, Kibana) 또는 Grafana Loki 같은 중앙 집중식 로깅 시스템을 도입하여 로그를 효율적으로 수집, 검색, 분석하는 것을 고려한다. - **모니터링:** flaskexporter nodeexporter , cAdvisor - **인메모리 캐시 상태:** 캐시 크기, 갱신 빈도, 웜업 시간 등을 모니터링할 수 있는 내부 메트릭을 노출할 수 있다. - **시각화 및 알림:** 수집된 메트릭은 Grafana를 사용하여 시각화하고, 주요 지표에 대한 임계치 기반 알림(Alerting)을 설정하여 문제 발생 시 신속하게 대응한다. --- ### 10. 사용자 인터페이스 (UI) - 사용자가 서비스를 편리하게 이용할 수 있도록 웹 기반 UI를 제공 - UI는 별도의 프론트엔드 프로젝트(예: React, Vue.js, Angular 사용)로 개발하여 API를 호출한다 - **UI 주요 기능:** - 회원가입, 로그인 - 컨테이너 목록 보기 (이름, 상태, 포트 등) - 새 컨테이너 생성 폼 (이미지 선택, 이름 접미사 입력, 포트 설정 등) - 컨테이너 제어 버튼 (시작, 중지, 삭제) - 컨테이너 터미널 연결 - 컨테이너 로그 보기 (옵션) --- ### 11. 주요 도전 과제 및 한계점 (본 설계 방식 채택 시) - **전적인 메모리 의존성 및 상태 유실 위험:** Flask API 서버의 인메모리 캐시에 모든 컨테이너 운영 상태가 의존한다. 서버 장애 또는 재시작 시 모든 캐시 정보가 유실되며, Docker 호스트를 전수 조사하여 상태를 재구성하는 \"웜업\" 과정이 필수적이다. 이 과정 동안 서비스의 일시적 불안정성이 발생할 수 있다. - **단일 인스턴스 확장성 제약:** 현재 아키텍처는 API 서버의 수평적 확장이 어렵다. 여러 인스턴스가 각자의 메모리 캐시를 가지면 상태 불일치가 발생하므로, 고가용성 및 부하 분산을 위한 일반적인 웹 서비스 확장 전략 적용에 한계가 있다. (외부 공유 캐시 도입 시 설계 원칙 변경) 하지만 본 프로젝트는 1개의 망에서만 동작하며 동시접속자 수나 사용자 수가 300 명 이내 설계이므로 상관없다 - **데이터 영속성 및 감사 추적의 어려움:** 컨테이너가 시스템에서 삭제되면, 해당 컨테이너의 과거 존재 이력이나 상세 설정 정보가 메모리에서 완전히 사라진다. 문제 발생 시 과거 상태를 추적하거나 정교한 감사 로그를 확보하기 위해서는 매우 철저하고 별도의 로깅 시스템에 의존해야 한다. - **성능 병목 가능성:** 관리하는 Docker 호스트 및 전체 컨테이너 수가 크게 증가할 경우, 주기적인 전체 상태 폴링, 대규모 캐시 관리, 메모리 내에서의 빈번한 필터링 작업 등이 Flask API 서버의 성능 병목 지점이 될 수 있다. --- 본 설계는 \"DB에는 최소한의 정보만, 모든 운영 상태는 실시간 조회 및 메모리 캐시\"라는 핵심 원칙 하에 작성되었다. 이로 인해 얻는 실시간성과 단순함의 이면에는 위에서 언급된 도전 과제들이 존재함을 명확히 인지하고, 시스템 개발 및 운영 단계에서 이에 대한 면밀한 고려와 대비가 필요하다. ## 컨테이너 기반 컴퓨팅 제공 서비스 프로젝트 설계서 분석 및 제언 보고서 ### 1. 서론 본 보고서는 제시된 \"컨테이너 기반 컴퓨팅 제공 서비스 프로젝트 설계서\"(이하 설계서)에 대한 심층 분석을 제공하고, 잠재적인 문제점, 개선 방안, 그리고 추가적으로 고려해야 할 사항들을 다각적으로 제시하는 것을 목적으로 합니다. 설계서의 핵심은 AWS ECS와 유사한 컨테이너 관리 플랫폼을 구축하되, 데이터베이스의 역할을 사용자 인증 정보 관리에만 국한하고, 컨테이너 관련 모든 운영 정보는 Docker 호스트로부터 실시간으로 조회하여 API 서버의 인메모리 캐시에 저장 및 관리하는 것입니다. 이 방식은 Docker 호스트를 시스템의 '단일 진실 공급원(Source of Truth)'으로 삼아 데이터 불일치 가능성을 원천적으로 제거하려는 독창적인 접근 방식을 취하고 있습니다. 본 보고서는 설계서의 주요 특징과 장점을 먼저 살펴본 후, 아키텍처, 데이터 관리, 보안, 확장성, 안정성 등 다양한 측면에서 발생할 수 있는 도전 과제와 한계점을 분석할 것입니다. 이를 바탕으로 시스템의 완성도와 지속 가능성을 높이기 위한 구체적인 개선 방안과 추가 고려 사항을 제안하여, 성공적인 프로젝트 구현에 기여하고자 합니다. 특히, \"단일 API 서버 인스턴스\" 및 \"300명 이내 사용자\"라는 초기 운영 규모 제약을 고려하여 현실적인 분석과 제안을 드리도록 하겠습니다. ### 2. 설계서 주요 특징 및 장점 분석 설계서는 몇 가지 주목할 만한 특징과 그에 따른 장점을 가지고 있습니다. - **Source of Truth로서의 Docker 호스트 및 실시간 상태 반영:** 컨테이너의 실제 상태 정보를 각 Docker 호스트에서 직접 가져와 인메모리 캐시에 반영하는 방식은 이론적으로 데이터베이스와 실제 상태 간의 불일치 문제를 원천적으로 방지할 수 있습니다. 이는 시스템 상태의 정확성과 신뢰성을 높이는 데 기여합니다. DB는 오직 사용자 인증이라는 명확하고 제한된 역할만 수행하므로, 시스템의 복잡도가 낮아지고 특정 컴포넌트에 대한 의존성이 줄어듭니다. - **간결한 데이터 모델 및 DB 부하 최소화:** 사용자 인증 정보만을 DB에 저장함으로써, DB 스키마가 매우 단순해지고 DB 관련 작업 부하가 현저히 줄어듭니다. 컨테이너의 빈번한 상태 변경, 생성, 삭제 등의 이벤트가 DB 트랜잭션을 유발하지 않으므로 DB 성능 병목 현상 발생 가능성을 최소화합니다. [userid] 를 포함하고, owneruserid docker - **명확한 역할 분담:** 제어 플레인(API 서버, DB, 웹 서버)과 데이터 플레인(Docker 호스트)의 역할이 비교적 명확하게 구분되어 있어, 각 구성 요소의 독립적인 관리 및 확장이 용이한 구조의 기반을 마련하고 있습니다. 이러한 특징들은 특히 초기 개발 단계에서 빠른 프로토타이핑과 구현을 가능하게 하며, 소규모 환경에서는 충분히 효과적으로 운영될 수 있는 잠재력을 가지고 있습니다. ### 3. 설계서의 잠재적 문제점 및 한계점 심층 분석 설계서의 독창적인 접근 방식은 여러 장점을 제공하지만, 동시에 몇 가지 중요한 잠재적 문제점과 한계점을 내포하고 있습니다. **3.1. 인메모리 캐시 의존성의 위험 및 상태 유실** - **단일 장애점 (Single Point of Failure):** API 서버의 인메모리 캐시는 시스템의 모든 운영 상태를 담고 있는 핵심 요소입니다. 만약 API 서버(Flask 애플리케이션)에 장애가 발생하거나 재시작될 경우, 캐시된 모든 정보가 유실됩니다. 이는 서비스 전체의 즉각적인 중단 또는 심각한 기능 장애로 이어집니다. \"단일 API 서버 인스턴스\"를 전제로 하므로, 이 서버의 안정성이 전체 시스템의 안정성과 직결됩니다. - **웜업(Warm-up) 시간 및 서비스 불안정성:** API 서버 재시작 시, 모든 Docker 호스트에 접속하여 현재 실행 중인 컨테이너 정보를 수집하고 캐시를 재구성하는 \"웜업\" 과정이 필수적입니다. 관리하는 호스트 및 컨테이너 수가 증가함에 따라 이 웜업 시간은 길어질 수 있으며, 이 시간 동안 API 서버는 불완전한 정보를 제공하거나 정상적인 서비스 처리가 불가능할 수 있습니다. 300명 사용자 규모에서는 컨테이너 수가 수백 개에 이를 수 있으며, 각 컨테이너 정보를 상세히 가져오는 과정은 수십 초에서 수 분이 소요될 가능성이 있습니다. - **데이터 영속성 부재:** 인메모리 캐시는 휘발성이므로, 컨테이너가 삭제된 후에는 해당 컨테이너의 과거 설정, 상태 변화 이력, 통계 정보 등이 시스템에서 완전히 사라집니다. 이는 감사 추적, 장애 원인 분석, 사용량 기반 과금 정책 수립 등에 심각한 제약을 초래합니다. 로깅에 전적으로 의존해야 하지만, 로그 데이터만으로는 정형화된 과거 상태 조회가 어렵습니다. **3.2. 확장성 제약** - **API 서버 수평 확장 불가:** 현재 설계는 API 서버가 모든 상태 정보를 자체 메모리에 가지므로, 여러 인스턴스로 수평 확장(scale-out)하는 것이 불가능합니다. 각 인스턴스가 독립적인 캐시를 가지면 상태 불일치가 발생하여 시스템이 오작동하게 됩니다. 사용자가 300명 이내로 제한되어 있고 단일 망에서 동작한다고 언급되었지만, 향후 서비스 성장에 따른 트래픽 증가나 처리 용량 증설 요구에 유연하게 대응하기 어렵습니다. 단일 인스턴스의 처리 용량을 넘어서는 요청이 발생하면 성능 저하 및 서비스 장애로 이어질 수 있습니다. stats **3.3. 데이터 관리 및 감사 추적의 어려움** - **상세 로깅의 중요성 및 부담:** 설계서에서도 언급했듯이, 컨테이너 정보를 DB에 저장하지 않으므로 모든 활동에 대한 상세한 로깅이 필수적입니다. 이는 로깅 시스템 설계, 로그 데이터 저장 및 관리, 검색 및 분석에 대한 추가적인 부담으로 작용합니다. 로그 데이터의 양이 방대해질 수 있으며, 필요한 정보를 효율적으로 추출하고 분석하기 위한 별도의 시스템(예: ELK Stack) 구축 및 운영이 요구됩니다. - **이력 추적의 한계:** 로깅만으로는 특정 시점의 시스템 전체 스냅샷, 특정 컨테이너의 상세한 생명주기 이력, 설정 변경 이력 등을 체계적으로 추적하고 분석하기 어렵습니다. 이는 운영 중 문제 해결이나 사용자 지원 시 어려움으로 작용할 수 있습니다. **3.4. 네트워크 구성 및 포트 관리** - **동적 포트 할당 및 충돌 방지 로직의 복잡성:** 사용자가 요청한 컨테이너 내부 포트를 Docker 호스트의 특정 포트 범위 내에서 동적으로 매핑할 때, 사용 가능한 포트를 정확히 찾아 할당하고 중복 할당을 방지하는 로직이 API 서버 내에 견고하게 구현되어야 합니다. 여러 컨테이너가 동시에 생성 요청될 경우 경쟁 조건(race condition)이 발생하여 포트 충돌이 일어날 수 있습니다. 이에 대한 정교한 동기화 메커니즘이 필요합니다. - **IP 주소 변경 및 호스트 관리의 번거로움:** Docker 호스트의 IP 주소가 변경되거나 호스트가 추가/삭제될 때마다 API 서버의 구성 파일을 수동으로 업데이트해야 합니다. 이는 운영 편의성을 저해하고 오류 발생 가능성을 높입니다. **3.5. 보안 고려 사항 심층 분석** - **Docker API 접근 제어의 중요성:** Flask API 서버만이 Docker 호스트의 API에 접근할 수 있도록 방화벽 설정을 하는 것은 기본이지만, API 서버 자체가 공격자에게 침해당할 경우 모든 Docker 호스트에 대한 통제권을 탈취당할 수 있는 위험이 있습니다. TLS 통신은 필수적이며, 인증서 관리 또한 중요합니다. [userid][namesuffix] )과 라벨( owneruserid )을 통해 소유권을 판단합니다. 만약 이름 생성 규칙이나 라벨 주입 과정에 논리적 허점이 있거나, 사용자가 name_suffix - **이미지 보안 관리:** 사용자가 임의의 Docker 이미지를 사용할 수 있도록 허용한다면, 악성 코드가 포함된 이미지나 보안 취약점이 있는 이미지를 통해 시스템 전체 또는 다른 사용자의 컨테이너에 영향을 미칠 위험이 있습니다. 이미지 검증 프로세스나 승인된 이미지 목록 사용 정책이 강력히 권장됩니다. **3.6. 장애 시나리오 및 복구 전략의 구체성** - **Docker 호스트 장애:** 호스트 장애 시 해당 호스트의 컨테이너가 \"unreachable\"로 표시되는 것은 좋으나, 해당 컨테이너에 의존하는 사용자 서비스는 중단됩니다. 자동 복구(다른 호스트로 이전)는 초기 범위에 없다고 명시되었지만, 장애 발생 시 수동 복구 절차나 사용자 알림, 데이터 복구(만약 컨테이너 내부에 중요한 데이터가 있었다면) 방안에 대한 구체적인 계획이 부족합니다. - **네트워크 장애:** API 서버와 Docker 호스트 간, 또는 Nginx와 API 서버 간 네트워크 문제 발생 시 시스템 전체가 불안정해질 수 있습니다. 이에 대한 감지 및 대응 방안이 필요합니다. **3.7. 컨테이너 자원 관리의 세부 사항** - **디스크 사용량 제한의 모호성:** \"50GB 가이드라인\"은 실제 강제적인 제한이 아니므로, 특정 사용자가 과도한 디스크 공간을 점유하여 호스트 전체의 디스크 부족을 유발할 수 있습니다. Docker 볼륨을 사용한 크기 제한은 관리 복잡성을 증가시키지만, 보다 명확한 통제가 가능합니다. 스토리지 드라이버 수준의 쿼터 설정은 환경에 따라 적용 가능성이 다릅니다. - **컨테이너 자동 배치(스케줄링)의 제한:** 사용자가 직접 호스트를 선택하는 방식은 단순하지만, 호스트의 현재 부하 상태나 자원 가용성을 고려하지 않아 특정 호스트에 부하가 집중될 수 있습니다. 이는 시스템 전체의 효율성을 저해할 수 있습니다. ### 4. 개선 제안 앞서 분석된 문제점과 한계점을 보완하고 시스템의 안정성과 효율성을 높이기 위한 몇 가지 개선 방안을 제안합니다. **4.1. 상태 관리 및 데이터 영속성 강화** - **경량 외부 캐시/저장소 도입 고려 (원칙 수정 가능성 검토):** \"DB 외 다른 저장소 사용 안 함\"이라는 원칙이 시스템의 핵심 제약사항이라면 이를 존중해야 합니다. 그러나 API 서버의 단일 장애점 및 상태 유실 위험을 완화하기 위해, Redis와 같은 인메모리 키-값 저장소를 **캐시의 보조 저장소 또는 짧은 주기의 스냅샷 저장 용도**로 사용하는 방안을 신중히 검토해볼 수 있습니다. - **장점:** API 서버 재시작 시 Redis로부터 빠르게 상태를 복구하여 웜업 시간을 대폭 단축할 수 있습니다. 또한, 분산 락(Distributed Lock) 구현이 용이해져 동시성 제어 로직을 단순화할 수 있습니다. - **트레이드오프:** 초기 설계 원칙에서 벗어나며, Redis 관리 포인트가 추가됩니다. - **대안 (원칙 유지 시):** API 서버 시작 시 병렬/비동기 웜업 로직을 고도화하고, 웜업 진행 중임을 명확히 API 응답에 표시하여 사용자 혼란을 줄입니다. 주기적으로 인메모리 캐시의 스냅샷을 파일 시스템에 백업하고, 재시작 시 이 스냅샷을 우선 로드 후 Docker 호스트와 동기화하는 방안도 고려할 수 있습니다. (단, 파일 I/O 성능 및 일관성 문제 고려 필요) - **이벤트 소싱(Event Sourcing) 패턴의 경량화된 적용:** 컨테이너 생성, 삭제, 상태 변경 등의 주요 이벤트를 단순 로그 이상의 구조화된 형태로 파일 또는 경량 메시지 큐(예: SQLite를 활용한 로컬 큐)에 기록합니다. 이는 완전한 이벤트 소싱 시스템은 아니지만, 장애 발생 시 특정 컨테이너의 이력을 추적하거나 상태를 재구성하는 데 도움이 될 수 있습니다. **4.2. 안정성 및 가용성 향상 (단일 인스턴스 환경 내에서)** - **API 서버 프로세스 관리 강화:** Gunicorn 또는 uWSGI 사용 시 워커 프로세스 수를 적절히 설정하고, 워커 프로세스 비정상 종료 시 자동 재시작 등의 기능을 활성화하여 단일 머신 내에서의 안정성을 최대한 확보합니다. /health/live ), 주요 서브시스템(예: Docker 호스트 연결성, 캐시 상태 - /health/ready gevent 나 asyncio **4.3. 로깅 및 모니터링 시스템 고도화** - **구조화된 로깅 (Structured Logging):** JSON이나 key-value 형태의 구조화된 로그를 사용하여 로그 파싱 및 분석의 효율성을 높입니다. 로그 내용에는 타임스탬프, 사용자 ID, 컨테이너 ID, 요청 ID, 이벤트 유형, 성공/실패 여부, 오류 메시지 등을 명확히 포함합니다. - **중앙 집중식 로깅 시스템 도입:** 설계서에서 언급된 ELK Stack 또는 Grafana Loki + Promtail 조합은 300명 사용자 규모에서도 장기적으로 로그 관리의 효율성을 크게 높여줍니다. 초기에는 파일 기반 로깅으로 시작하더라도, 향후 확장을 염두에 두고 로그 포맷을 표준화하는 것이 좋습니다. - **인메모리 캐시 모니터링 지표 추가:** 캐시 크기(항목 수, 메모리 점유량), 웜업 소요 시간, 캐시 적중률(만약 캐시 미스가 발생할 수 있는 로직이 있다면), 폴링 주기별 처리 시간 등의 내부 메트릭을 Prometheus 등을 통해 노출하여 캐시 동작 상태를 면밀히 관찰합니다. **4.4. 네트워크 및 포트 관리 개선** threading.Lock - **호스트 정보 외부 설정 및 동적 로딩:** Docker 호스트 목록을 JSON 설정 파일에서 읽어오되, API 서버가 특정 신호(예: SIGHUP)를 받거나 관리자 API를 통해 이 설정을 다시 로드할 수 있도록 하여, API 서버 재시작 없이 호스트 정보를 업데이트할 수 있는 유연성을 확보합니다. **4.5. 보안 강화** - **API 서버 보안 강화:** - 모든 API 엔드포인트에 대해 입력값 유효성 검증(길이, 타입, 허용 문자 등)을 철저히 수행합니다. (Pydantic, Marshmallow 등 활용) - 요청 속도 제한(Rate Limiting) 및 악의적 요청 패턴 감지/차단 기능을 Nginx 또는 API 게이트웨이(만약 도입한다면) 수준에서 적용합니다. - JWT 토큰 탈취에 대비한 짧은 만료 시간 설정 및 리프레시 토큰 메커니즘 도입을 고려합니다. (로그아웃 시 서버 측 토큰 블랙리스트 관리는 단일 인스턴스에서는 가능하나, 복잡도 증가) - **컨테이너 격리 강화 심화:** --cap-drop=ALL 외에, no-new-privileges --read-only - AppArmor/Seccomp 프로필은 설정 및 관리가 복잡할 수 있으나, 높은 수준의 보안이 필요하다면 검토할 가치가 있습니다. 초기에는 적용하지 않더라도 향후 강화 방안으로 남겨둘 수 있습니다. - **비밀(Secrets) 관리:** Docker API 접근을 위한 TLS 인증서/키, DB 패스워드 등의 민감 정보는 코드나 설정 파일에 하드코딩하지 않고, 환경 변수, 또는 HashiCorp Vault와 같은 외부 비밀 관리 도구를 통해 안전하게 주입합니다. (단일 서버 환경에서는 환경 변수나 암호화된 설정 파일이 현실적일 수 있음) **4.6. 컨테이너 관리 로직 심화** - **스케줄링 유연성 확보:** 사용자가 호스트를 직접 선택하는 기능 외에, \"자동 선택\" 옵션을 제공하여 API 서버가 호스트의 현재 자원 가용성(CPU, 메모리, 실행 중인 컨테이너 수 등 - 캐시된 정보 기반)을 고려하여 최적의 호스트를 추천하거나 자동으로 배치하는 기능을 추가할 수 있습니다. 이는 사용자 편의성을 높이고 시스템 자원 활용 효율을 개선합니다. - **디스크 사용량 제한의 현실적 접근:** docker system df 또는 개별 컨테이너의 docker exec df -h overlay2 )는 프로젝트 쿼터와 유사한 크기 제한 옵션( size`)을 제공할 수 있으나, 이는 Docker 버전 및 시스템 환경에 따라 지원 여부나 안정성이 다를 수 있어 충분한 테스트가 필요합니다. 이미지 크기 사전 고지: 컨테이너 생성 시 사용될 이미지의 크기를 사용자에게 미리 알려주고, 남은 예상 작업 공간을 안내하는 것도 도움이 됩니다. 사용자 경험(UX) 개선 사항 실시간 알림: 컨테이너 생성 완료, 시작/중지, 오류 발생 등의 주요 이벤트 발생 시 웹 UI를 통해 사용자에게 실시간으로 알림을 제공합니다. (WebSockets 활용) 컨테이너 생성 템플릿: 자주 사용되는 애플리케이션 유형(예: 웹 서버, DB)에 대한 사전 정의된 컨테이너 설정 템플릿을 제공하여 사용자가 더 쉽게 컨테이너를 생성할 수 있도록 지원합니다. 상세한 오류 메시지 및 문제 해결 가이드: API 오류 발생 시, 단순한 HTTP 상태 코드 외에 사용자 친화적인 오류 메시지와 가능한 해결 방법을 안내합니다. 추가 고려 사항 라이선스 및 비용: 사용되는 모든 오픈소스 소프트웨어의 라이선스를 확인하고 준수해야 합니다. 향후 클라우드 환경으로 확장하거나 상용 솔루션을 도입할 경우 관련 비용을 고려해야 합니다. 개발 및 운영 인력의 기술 스택: 현재 설계는 Python, Docker, Linux 등에 대한 숙련도를 요구합니다. 팀원의 기술 역량을 고려하여 현실적인 기술 선택과 교육 계획이 필요합니다. 테스트 전략: 단위 테스트, 통합 테스트, API 테스트, 부하 테스트 등 각 단계별 테스트 전략을 수립하고 자동화하여 시스템의 안정성을 확보해야 합니다. 특히 인메모리 캐시 로직, Docker 호스트 연동 로직, 동시성 제어 로직 등은 철저한 테스트가 필요합니다. 향후 기능 확장 로드맵: 초기 버전 출시 이후 고려할 수 있는 기능들(예: 영구 볼륨 관리, 사용자 정의 네트워크, CI/CD 연동, 고급 모니터링 대시보드, 과금 시스템 연동 등)에 대한 장기적인 로드맵을 구상해두는 것이 좋습니다. 이는 현재 아키텍처 설계 시 미래의 확장성을 어느 정도 고려해야 할지 판단하는 데 도움이 됩니다. 결론 제시된 \"컨테이너 기반 컴퓨팅 제공 서비스 프로젝트 설계서\"는 Docker 호스트를 단일 진실 공급원으로 삼고 API 서버의 인메모리 캐시를 적극적으로 활용하는 독창적이고 흥미로운 접근 방식을 제시합니다. 이 설계는 데이터 일관성 확보와 DB 의존성 최소화라는 명확한 장점을 가지며, 특히 명시된 \"단일 API 서버 인스턴스, 300명 이내 사용자\"라는 제한된 초기 운영 환경에서는 충분히 실현 가능하고 효율적일 수 있습니다. 하지만, 인메모리 캐시에 대한 높은 의존성은 API 서버 장애 시 상태 유실, 웜업 시간, 수평 확장 제약 등의 본질적인 한계를 내포하고 있습니다. 또한, 데이터 영속성 부재로 인한 감사 추적의 어려움, 폴링 부하 관리, 보안 강화 등은 신중하게 다뤄야 할 도전 과제입니다. 본 보고서에서 제안된 개선 방안들은 이러한 문제점들을 완화하고 시스템의 안정성, 효율성, 보안성, 그리고 사용자 경험을 향상시키는 데 초점을 맞추고 있습니다. 특히, 경량 외부 캐시/저장소의 제한적 도입 고려, 비동기 처리 강화, 정교한 로깅 및 모니터링 시스템 구축, 보안 계층 심화, 그리고 보다 유연한 컨테이너 관리 로직 추가 등은 시스템의 완성도를 높이는 데 기여할 수 있을 것입니다. 가장 중요한 것은 설계의 핵심 원칙과 현실적인 제약 조건 사이에서 균형을 찾는 것입니다. 초기 설계의 단순함과 명확성을 유지하면서도, 잠재적인 위험 요소를 최소화하고 향후 성장에 대비할 수 있는 유연성을 확보하는 방향으로 설계를 발전시켜 나가는 것이 바람직합니다. 철저한 테스트와 점진적인 기능 개선을 통해 안정적이고 사용자 친화적인 컨테이너 서비스를 성공적으로 구축할 수 있기를 기대합니다.",
      "frontmatter": {
        "date": "2025-06-04T06:29:24+09:00",
        "lastmod": "2025-06-04T16:11:54+09:00",
        "share_link": "https://share.note.sx/v4psgxdd#Y5O2qjdKYsrASdE/rxbiWrMGN9Q/BipEsTZ5Dln3e3s",
        "share_updated": "2025-06-10T04:59:34+09:00"
      }
    },
    "mjuecs project 발전방향": {
      "path": "/06.university/mju_ecs-project/mjuecs-project-발전방향/",
      "filename": "mjuecs project 발전방향",
      "content": "aws 의 ecs 서비스와 같이 컴퓨팅 을 제공 유저 정보만 db 에 기록하고 나머지는 모두 docker demon 에 요청 정보로 처리한다 유저 id 는 평문 저장 비밀번호는 암호화 저장 물리적 컴퓨터로 해당 서비스를 서비스한다 컴퓨터1 : db, python flask, nginx , 컴퓨터2 : docker 구동, host 상태 서버 컴퓨터3 : docker 구동, host 상태 서버 즉 2개의 프로젝트가 개발되어야 한다 매인 프로젝트 사용자별 docker 컨테이너 생성 조회 삭제 서비스 (어려움) host 의 상태 정보를 요청 받으면 보내주는 서비스 (쉬움) python 웹서버 설정란에 docker host ip 를 적고 해당 ip 를 통해 컨테이너를 생성 조회 삭제를 한다 3초에 한번씩 모든 docker 를 구성하는 host 컴퓨터들에게 요청을 보내서 메모리에 저장(캐시) 사용자는 생성할때 마다 특정 규칙에 따라 생성한다 (ex 1인당 컨테이너 2개등등) 이때 저장된 캐시가 갱신된것을 확인하고 갱신됬으면 특정 규칙에 따라 검사하고 생성 허용을 할 지 안할지 정한다 초기 사용자가 컨테이너를 생성할 때 여러개의 docker host 중에 적절한 서버를 고르고 해당 서버에서 컨테이너를 생성할 수 있도록 한다 컨테이너 생성시에는 유저는 1번의 생성 요청이 끝나야 다른 생성 요청을 할 수 있다 사용자별 컨테이너 자원제한 2cpu, 3memory, 50GB disk 하루에 한번씩 사용자들의 컨테이너를 검사하고 규칙에 따라 컨테이너를 정지할지 말지 고를 수 있다",
      "frontmatter": {
        "date": "2025-06-04T06:11:20+09:00",
        "lastmod": "2025-06-04T06:29:24+09:00"
      }
    },
    "mjuecs project": {
      "path": "/06.university/mju_ecs-project/mjuecs-project/",
      "filename": "mjuecs project",
      "content": "물리 환경 db,backend,frontend 컴퓨터 1대 여러대의 docker host 로그인 처리 나의 경우는 mju 에 로그인이 가능한 사람을 기준으로 했지만 어떠한 방식으로 로그인처리를 할 지 고를 수 있어야함 api METHOD endpoint 설명 POST /api/docker/run POST /api/docker/start POST /api/docker/restart /api/docker/stop /api/docker/status /api/auth/login /api/auth/ 필요 요구 사항 사용자가 새로운 컨테이너를 만들려고 할 때 docker host 컴퓨터들의 상태 (cpu, memory, ...) 을 보고 그곳에 컨테이너를 생성 가능(일종의 aws 의 리전개념) 사용자가 백엔드는 백엔드에서 관리하고 있는 모든 host 의 정보(사용량, 컨테이너 사용량 등등)를 순차적으로 돌아가며 가져와서 메모리에 저장해두었다가 사용자에게 갱신해 주면 됨 back 엔드에서는 사용자 정보만 db 에 저장하고 컨테이너 상태와 같은것들은 가져오지 않아야 한다 사용자 마다 전체 컨테이너의 개수 max 제한이 있어야 함 사용자가 1개의 컨테이너를 생성중에 있을 때는 컨테이너가 만들어지고 난 후 생성가능 컨테이너에 터미널 접근을위해 websocket 을 사용 터미널 창은 tab 으로 구성되며 첫 탭은 컨테이너 log 만 존재하는 tab 으로 상호작용은 없는 탭 사용자는 초기 화면에서 자신의 컨테이너 상태를 확인 할 수 있어야 한다(실시간 sse 활용) 사용자는 특정 컨테이너 정보 창에서 상세한 컨테이너 stats 을 확인할 수 있어야 한다(실시간 sse 활용) front 의 경우 onepage 로 만들어 구성이 쉽도록 한다",
      "frontmatter": {
        "date": "2025-06-03T06:05:16+09:00",
        "lastmod": "2025-06-11T15:06:32+09:00"
      }
    },
    "mjuecs ubuntu 문서": {
      "path": "/06.university/mju_ecs-project/mjuecs-ubuntu-문서/",
      "filename": "mjuecs ubuntu 문서",
      "content": "🐧 mjuecs ubuntu (Ubuntu 22.04 기반) Docker 컨테이너 가이드 이미지 출처: ubuntu (Docker Hub)) 컨테이너 생성 1-1. 템플릿 선택 mjuecs ubuntu 템플릿을 선택합니다. 이 템플릿은 Ubuntu 22.04를 기반으로 하며, SSH 서버와 사용자 계정 생성 custom 이 되어 있습니다 1-2. 환경변수 설정 컨테이너 생성 시 다음 환경변수를 설정해야 합니다: 환경변수 설명 ROOT_PASSWORD 루트 계정의 비밀번호 (예: rootpass123 ) USERNAME 생성할 일반 사용자 계정명 (예: student01 ) PASSWORD 해당 사용자 비밀번호 (예: studentpass123 ) Pasted image 202![3002975.png) 이 설정에 따라 컨테이너 최초 실행 시 자동으로 사용자 계정이 생성되고 sudo 권한이 부여됩니다. 1-3. 컨테이너 정보 접근 컨테이너가 생성된 후, 해당 컨테이너 정보 페이지로 이동합니다. 1-4. 웹터미널 접근 준비 웹터미널 접근 비밀번호 복사 버튼을 클릭하여 복사합니다. 그 후 웹터미널 열기 버튼을 클릭하여 웹터미널을 실행합니다. 1-5. 웹터미널 로그인 아이디: 학번 비밀번호: 위에서 복사한 웹터미널 접근 비밀번호 VS Code에서 원격 SSH 연결 해당 컨테이너는 ssh 설정과 유저 생성이 자동으로 이루어지며 바로 vscode 연결이 가능합니다 2-1. VS Code 확장 설치 VS Code 실행 Extensions(확장) 메뉴에서 Remote - SSH 검색 및 설치 2-2. SSH 접속 vscode 에서 ctrl + p 접근 이후 ssh 입력 새로운 host 연결Pasted image 20250523105725 컨테이너 생성시 입력한 사용자 이름, 외부 접근포트의 경우 컨테이너 정보창 외부 접근 포트 번호 Pasted image 20250523105879 config 는 아무거나 설정 (전역 설정, 유저 설정) 연결 Pasted image 20250523110037 비빌번호 입력 -> 접속 완료 2-3. 이후 접속 VS Code 하단의 녹색 버튼 클릭 → Remote-SSH: Connect to HostPasted image 20250523004373 mjuecs.gonetis.com 선택 비밀번호 입력 → 접속 완료",
      "frontmatter": {
        "date": "2025-06-03T06:05:16+09:00",
        "lastmod": "2025-06-04T06:11:18+09:00"
      }
    },
    "mjuecs 발표 스크립트": {
      "path": "/06.university/mju_ecs-project/mjuecs-발표-스크립트/",
      "filename": "mjuecs 발표 스크립트",
      "content": "mjuecs 호스트 비밀번호 564738291 sejonng univ auth github shinnk.iptime.org 에서 ppt 다운 명지대 전산정보원 안녕하세요 5조 발표 시작하겠습니다 먼저 목차입니다 설명... 프로젝트 주제 선정이유 입니다 해당 프로젝트는 컴공또는 정통 학생들이 학교 수업 또는 프로젝트 진행중 발생할 수 있는 여러 개발환경의 세팅의 어려움을 해결하기 위해 해당 주제를 선정하였습니다 일반적으로 개발환경의 세팅의 어려운 예시는 다음과 같습니다 먼저 아카텍쳐 차이로 인해 발생하는 문제인데요 가장 먼저 예시를 들면 oracle database 입니다 해당 소프트웨어의 경우 database 수업시 표준적으로 사용하는 db 임에도 불구하고 상용프로그램이면서 intel, amd 진영에서 사용하는 아키텍쳐인 x64 cisc 기반의 컴퓨터를 지원하는데 최근 점점 risc 기반의 arm 기반 아키텍쳐를 개인 컴퓨터로 사용하는 경우가 늘고 있습니다 특히 mac 의 경우 apple silicon 이 도입되면서 해당 x64 기반의 소프트웨어가 명령어 해석기로 돌아간다고 해도 완벽히 작동하는 것이 아니므로 해당 oracle database 는 구동되지 못합니다 또한 system programming 실습시 리눅스 systemcall 을 배우는 방식으로 수업이 진행되는데 일반적인 systemcall 의 경우 동일하지만 sys_write 와 같은 systemcall 은 번호가 다르게 배정되어있어 미묘한 차이가 발생합니다 이러한 문제는 저수준 수업을 들으면 들을 수록 더 크게 다가오는데요 컴퓨터 하드웨어 수업시 오랬동안 산업 표준으로 여겨저온 x64 기반의 어셈블리를 배우게 되는데 타 아키텍쳐와는 완전히 문법이 달라 개인컴퓨터에서 실습조차 구동하기가 쉽지 않다는 문제가 있습니다 지금까지 타 아키텍쳐로 발생하는 문제만들 이야기 했지만 개발과정에서 많이 사용하는 서버측 os 인 리눅스와 개인 pc 에서 사용하는 window macos와 같이 운영체제 가 달라 발생하는 여러 개발환경의 어려움도 발생합니다 이것은 저의 개인적 경헙인데요 프로젝트 진행시 외부에 db 를 공유해 사용한다면 dummy data test 또한 쉽게 가능하다는 장점이 있습니다 위의 여러가지 문제를 통해 저희는 명지대 학생만 무료로 접근할 수 있는 컨테이너 기반 클라우드 서비스를 기획하였습니다 실제 프로젝트 구현내용및 원래 목표와의 차이 입니다 해당 프로젝트를 통해 저희는 실제로 사용하기 위해 학교 인프라를 그대로 사용하여 학교내에서 ip 를 받는등 여러 시도들을 해보았는데요 하지만 여러가지 조건들에 막혀 초기 계획인 학교내에 해당 프로젝트를 실행한다는 것은 실패하였습니다. 어떠한 문제조건에 막혀 프로젝트를 진행하지 못하였는제 설명드리면 왼쪽에 보이는 이미지가 학교에서 실제로 사용하고 있는 공인 IP 목록 입니다 명지대 전산원 신청서를 통해 2048개의 공인 ip 중 1개를 받아서 사용할 수 있 단 조건이 필요합니다 전화를 통해 질의한 결과 명지대학교라는 네트워크 망내부에서 공인 ip 를 받기 위해서는 외부 인바운드 시에는 방화벽을 통과해야 하는데 해당 방화벽을 완전 open 하는 것은 불가능하다는 답변을 받았습니다 저희 프로젝트의 경우 학생이 컨테이너를 생성하는 경우 포트를 2개 씩 동적으로 배정받는데 이렇게 방화벽이 존재한다면 프로젝트의 진행이 차질히 생깁니다 그래서 조건을 바꾸어서 학교 내에서 와이파이를 잡을때 만이라도 구동이 된다면 어떨까 했을때는 오른쪽을 보시면 되는데요 오른쪽은 현재 학교 네트워크 망을 간단히 그려본 것입니다 10.10.xx.xx 대역의 망을 중심으로 하위에 MJUWLAN, 강의실 마다 존재하는 y5401 등등의 iptime 공유기 NAT 가 존재합니다 만약 저희가 10.10.xx.xx 망의 ip를 받을 수 있다면 학교내에서는 사용할 수 있다는 결론이 나오지만 10.10.xx.xx 를 받는 것또한 실패하였습니다 그래서 현재는 home 서버 형태로 구성되어 있는 상태입니다 프로젝트 진행시 가장 어려웠던 점 및 해결과정입니다 학교 수업중 여러번 다루어지는 주제인 Coherency 문제입니다 db 에서 특정 행을 조회할때나 운영체제에서 cache 를 접근할 때 많이 다루어지는 유명한 문제인데 접근하는 정보가 진실임을 보장하지 않을 때 inconsistency 가 발생하는 문제입니다 저희도 똑같은 문제를 경험했는데요. docker deamon 과 db 에서 조회하는 정보가 다를때 인데요 모든 crud 작업을 docker deamon 에서 직접 처리하도록 한다면 해당 문제를 발생하지 않을 것인데 이렇게만 처리한다면 docker deamon 에 큰 부담을 주게 됩니다 그림은 정상 프로세스 진행과 비정상 프로세스 진행입니다 .. ... 이러한 문제를 저희는 여러 논리를 붙여서 해결했는데요 하지만 이러한 해결 접근은 근본적인 해결책은 아니라고 느꼈습니다 해당 문제를 해결할 때 사용하는 주된 용어가 source of truth 라는 개념인데요 접근하는 정보가 진실임을 보장하는 의미 입니다 기존 db 를 사용자 인증 관리에만 국한하고 컨테이너의 생성, 상태, 소유권 등 모든 운영 정보는 각 Docker 호스트로부터 실시간으로 조회하여 메인 API 서버(Python Flask)의 인메모리 캐시에 저장 및 관리하며 . 컨테이너와 사용자의 매핑은 엄격한 컨테이너 네이밍 규칙을 통해 이루어진다. 이는 Docker 호스트의 실제 상태를 시스템의 유일한 진실 공급원(Source of Truth)으로 삼아 DB와 실제 상태 간의 불일치 가능성을 원천적으로 제거하려는 설계의도 입니다 2번째는 프로젝트 시연할 때 받은 피드백인데요 현재 이미지의 경우 저희가 만들어둔 것 이외에는 수동으로 만들어야 하는데 커뮤니티 기능을 활성화 하여 사용자끼리 공유가 되게 만들면 더 좋겠다는 생각을 하였습니다 마지막으로 보안 이유 및 법적 검토인데요 최근 skt 보안 사고 발생으로 한번 고민해볼 만한 주제라고 판단되어 해당 주제를 들고 와봤는데요 먼저 저희 프로젝트의 login 프로세스를 짚고 나머지를 설명드리겠습니다 ... 이러한 방식으로 프로세스가 진행되는데 해당 아이디어와 비슷하게 구현한 세종대 github 를 발견하였고 제작자에게 문의를 하여보았습니다 사이트를 보여주면서 질의 내용의 결론은 이렇습니다 학번을 db 에 저장하면서 부터 개인정보 처리자로 인식된다 저희가 만든 프로젝트는 개인정보 보호법과 해당 학교의 이용약관 보안 정책에 대한 이행의무를 가진다 학교의 이용약관 및 보안정책 이행에 있어서는 문제가 없고 개인정보 보호법에는 약간의 수정을 거친다면 문제가 없도록 만들수 있다 위의 결론은 법적 자문을 받지는 않았기 때문데 보증을 할 수 없지만 큰 축에서는 엇나가지 않을 것이라고 판단됩니다 ... 저희 프로젝트는 사용자의 비밀번호를 저장하지 않지만 찾아보는 김에 정리를 해보았는데요 ... 지금까지 들어주셔서 감사합니다",
      "frontmatter": {
        "date": "2025-06-11T15:04:45+09:00",
        "lastmod": "2025-06-11T15:05:01+09:00"
      }
    },
    "mjuecs 최종 발표": {
      "path": "/06.university/mju_ecs-project/mjuecs-최종-발표/",
      "filename": "mjuecs 최종 발표",
      "content": "실제 시연 => oracle database 연결, 김직수 교수 클라우드 등등 기술적 검토 로그인시 명지대 서버와 통신하는 과정 법적 검토 => 최근 skt 보안 사고로 인해 이것을 조금 확실하게 따질 필요가 있어 보임 실제 프로덕트를 운영하더라도 수익이 없고 학생들이 한 프로젝트를 제제하는 경우는 잘 없음(세종대) 하지만 초기에는 학번만 저장하면 실제 서비스해도 법적인 문제가 발생하지 않을 것이라는 생각을 하였음 현제 해당 프로젝트는 학교의 비밀번호 아이디를 입력하여 로그인 할 수 있도록 만들어 두었음 비밀번호의 경우 당연히 민감한 개인정보라고 생각하여 초기 설계부터 완전히 저장하지 않는 방향으로 설계하였음 하지만 학교의 학번이 민감한 개인정보인 경우 이것이 법적으로 문제가 될수 있다는 생각을 하였고 해당하는 학교에 로그인이 가능한 프로젝트를 구현한 세종대 github 프로젝트(sejong-univ-auth) 에 직접 질문을 통해 법적 내용을 공유받아 해당 내용을 공유하려고함 실제로 내가 질문한 내용 이슈 접근방식 제공모듈 자체는 개인정보 처리자로 인식되지 않아 상관없지만 해당 모듈(방법론)을 사용하여 프로젝트를 진행할 경우 개인정보 처리자로 인식이 되며 관련 법규에 대한 제제를 받는다 서비스 사업자는 개인정보 법률 및 해당대학교 이용약관 보안정책에 대한 이행의무를 가진다 현재 학번을 database 평문저장하고 있음 개인정보 법률 -> 정확한 판단이 불가 아이디 -> github 등 여러가지 사이트가 이미 아이디를 공개적인 검색으로 사용 학번 단독으로 저장되었는데 이 경우 개인정보로 보아도 되는가? 실제 명지대 보안 정책 -> 학교의 보안정책은 괜찮다 3장 6조 1항 학교는 이용자가 이 약관의 의무를 위반하거나 서비스의 정상적인 운영을 방해한 경우, 경고, 일시정지, 영구이용정지 등으로 서비스 이용을 단계적으로 제한할 수 있다. 동일 계정으로 계속 요청을 하면 일정 시간(10분) 자동으로 막힘으로 고려할 필요가 없다 3장 9조 5항 아호 학교의 동의 없이 영리를 목적으로 서비스를 사용하는 행위 해당 법적인 검토를 통해 실제 프로덕션 상황에서 실제 어떠한 문제가 발생할 수 있는지 간접적인 경험이 되었음",
      "frontmatter": {
        "date": "2025-06-03T06:05:16+09:00",
        "lastmod": "2025-06-03T06:40:30+09:00"
      }
    },
    "mjuecs 프로젝트 발표": {
      "path": "/06.university/mju_ecs-project/mjuecs-프로젝트-발표/",
      "filename": "mjuecs 프로젝트 발표",
      "content": "캡스톤 디자인1 최종 발표 MJUECS 목차 프로젝트 주제 선정 이유 및 구현하려고 했던 목표 실제 프로젝트 구현 내용 및 원래 목표와의 차이 (목표 대비 미흡한 점 혹은 초과 달성한 성과 등) 프로젝트 진행시 가장 어려웠던 점 및 해결 과정 프로젝트 결과 아쉬웠다고 생각되는 점 및 성과라고 생각되는 점 Futre Works (고도화 방안) 기타 프로젝트 성과를 잘 설명할 수 있는 사항들 (자유롭게) 프로젝트 주제 선정 이유 및 구현하려고 했던 목표 [!warning] 컴공 또는 정통 수업에서 실습시 개발환경 구성하기가 쉽지 않는 경우가 많음 아키텍쳐 문제 apple silicon 은 aarch64 기반, database 수업시 oracle database 기반으로 수업이 많이 진행됨 oracle databse 의 경우 amd64 아키텍쳐만 지원 클라우드 컴퓨팅(하둡, spark system 이미지), system 최근은 이러한 경우가 많이 사라졌지만 여전히 이와 같이 아키텍쳐에 종족적인 경우가 많음 system programming 에서 시스템 콜 실습시에 sys_write 번호는 x86-64에서 1, AArch64에서도 64입니다. 컴퓨터 구조론, 컴퓨터 아키텍쳐와 같은 저수준을 다루는 수업에서는 일반적으로 오랬동안 산업계에서 절대적인 위치를 가져왔던 amd64 아키텍쳐가 기본적으로 사용됨 자동화 문제 [!warning] 프로젝트 진행시에 공동 db 를 만들어 두고 개발시 dummy data 같은 것들을 미리 넣어두고 사용하면 테스트시 편리함 [!Goal] 목표 : 명지대 학생만 무료로 사용할 수 있는 컨테이너 기반 클라우스 서비스 실제 프로젝트 구현 내용 및 원래 목표와의 차이 학교 망 내부에서 해당 프로젝트를 구성해서 실제 구동 및 유비보수 비용이 들지 않도록 만들려고 했지만 학교 내부에서 방화벽 없이 사용할 수 있는 ip 를 받지 못함 [!info] 학교 공인 IP 대역 서브넷 마스크 : 255.255.248.0 IP 개수 : 2(32 - 21) = 2048개 (첫 번째와 마지막 주소 포함) 네트워크 주소 : 117.17.150.0 , 브로드캐스트 주소 : 117.17.157.255 , 사용 가능한 호스트 주소 범위 : 117.17.150.1 ~ 117.17.157.254 학교 내부망의 경우 10.xx.xx.xx 망을 중심으로 Tree 형태로 구성되어 있다 하위에 192.168 사설 망(Y5401, Y5445, MJUWLAN)들이 존재 397x306 399x295 프로젝트 진행시 가장 어려웠던 점 및 해결 과정 컨테이너의 상태를 db 에서 관리하고 있는데 도커 데몬 출력과의 데이터 불일치가 일어날 가능성이 존재 사용자가 터미널에서 컨테이너를 강제로 정지시킬때 kill -9 1 해당 문제가 발생 정상 프로세스 docker demon 에서 컨테이너의 상태가 up, db 또한 컨테이너의 상태가 up 상태라고 저장되어 있음 사용자가 웹 화면에서 정지 버튼을 누름 docker 컨테이너 상태가 정지됨 하지만 db 또한 stop 상태로 업데이트 비정상 프로세스 docker demon 에서 컨테이너의 상태가 up, db 또한 컨테이너의 상태가 up 상태라고 저장되어 있음 사용자가 터미널에서 kill -9 1 입력 최상위 프로세스를 종료 (bare machine 의 init 프로세스는 아니지만 컨테이너 최상위 프로세스임) docker 컨테이너 상태가 정지됨 하지만 db 에는 그대로 up 상태임을 나타냄 해결 해당 문제를 전형적인 Cache Inconsistency 와 비슷한 문제 로 판단하여 여러 논리들을 붙여서 해결했지만 근본적인 해결 책이 아니라고 느낌, source of Truth 아키텍쳐로 설계해야 옳다는 결론 Futre Works (고도화 방안) 데이터베이스(DB)의 역할을 사용자 인증 정보 관리에만 국한하고, 컨테이너의 생성, 상태, 소유권 등 모든 운영 정보는 각 Docker 호스트로부터 실시간으로 조회하여 백엔드 서버측의 인메모리 캐시에 올려두고(캐싱) 저장 및 관리 컨테이너와 사용자의 매핑은 엄격한 컨테이너 네이밍 규칙 또는 container labeling 기능을 활용하여 이는 Docker 호스트의 실제 상태를 시스템의 유일한 진실 공급원(Source of Truth)으로 삼아 DB와 실제 상태 간의 불일치 가능성을 원천적으로 제거하려는 설계 docker host 를 메인 서버와 분리하여 scale out 이 가능하게 설계한다면 이것은 네트워크 3계층 네트워크 계층의 데이터 플레인 계층과 제어 플레인 계층의 구분과 역할에 많은 공통점이 있다고 판단함 네트워크 에서의 영감을 받아 해당 프로젝트를 고도화 하고자 하는 방향이 있음 커뮤니티 기능을 활성화 하여 image 템플릿을 사용자간 공유 업로드 하여 사용할 수 있다면 더욱 활성화가 가능할 것으로 보임 참고 : mjuecs 발전 방향 및 고도화 아키텍쳐 설계서 기타 프로젝트 성과를 잘 설명할 수 있는 사항들 [!danger] 법적 검토 : 최근 SKT 의 보안 사고 이슈 현재 해당 프로젝트의 기능중 하나인 학교 아이디와 비밀번호로 로그인이 가능하게 하였는데 594x304 해당 기능과 비슷하게 세종대에서 구현한 모델이 있어 소개하고 해당 프로젝트를 설계 및 구현한 사람에게 법률 관련 문의를 질의함 학교 로그인 프로젝트 github [!conclusion] 해당 프로젝트는 개인정보 보호법과 해당 학교의 이용약관 및 보안정책에 대한 이행의무를 가진다 학교의 이용약관에서는 문제가 없다고 판단이 되고 개인정보 보호법에는 약간의 수정을 거친다면 문제가 없을 수 있다고 판단된다 하지만 해당 판단은 법률지식이 없는 공대생 입장이므로 법률 전문가의 질의가 필요하다고 판단된다 따져봐야할 지점 : 사용자의 학번은 db 에 평문으로 저장 비밀번호의 경우 암호화 저장이 맞지만 아이디의 경우 평문저장이 문제가 되지 않는다 (해당 id(학번)은 웹사이트내에서 사용됨으로 평문으로 필요) 하지만 사용자의 id 가 db 에 저장되는 순간 법적으로 개인정보 처리자로 인식, 또한 학교구성원의 정보 임으로 학교 이용약관의 이행의무 또한 가지고 있음 학교 정보 처리 이용 약관 3장 6조 1항 학교는 이용자가 이 약관의 의무를 위반하거나 서비스의 정상적인 운영을 방해한 경우, 경고, 일시정지, 영구이용정지 등으로 서비스 이용을 단계적으로 제한할 수 있다. => 의도적으로 서버측에 과도한 요청을 날리는 문제인데 동일 계정으로 서버측에 요청을 하면 일정 시간 자동으로 막힘 따로 제한할 필요가 없다 3장 9조 5항 아호 학교의 동의 없이 영리를 목적으로 서비스를 사용하는 행위 => 영리를 목적으로 사용하지 않으므로 상관이 없음 사용자의 정보를 저장하는 경우 사용자에게 개인정보 처리 방침을 공시하여야 한다 개인정보 보호법 제29조 (안전조치 의무) 개인정보 처리자는 개인정보가 분실·도난·유출·변조·훼손되지 않도록 안전성 확보에 필요한 기술적·관리적 조치를 해야 함. [!tip] 별점 -> 비밀번호 저장 방침 사용자의 정보중 비밀번호를 저장하려고 하는 경우 정보통신망법 제28조 (개인정보의 보호조치) 이용자의 개인정보를 보호하기 위해 비밀번호는 암호화하여 저장해야 하고, 해킹이나 유출 우려가 없도록 접근통제·방화벽 구축 등의 조치를 해야 함. 개인정보의 기술적·관리적 보호조치 기준 (행정안전부 고시) 비밀번호는 복호화가 불가능한 단방향 암호화(SHA-256 이상)로 저장할 것. 가능하면 비밀번호 정책(복잡성, 최소 길이, 변경 주기) 도 설정해야 함.",
      "frontmatter": {
        "date": "2025-06-08T06:31:36+09:00",
        "lastmod": "2025-06-10T08:56:43+09:00",
        "share_link": "https://share.note.sx/40dcqfcd#WJgwu/w+tND+y5yf+3CeNUsc4IeeJO9zEFKy8VG77N0",
        "share_updated": "2025-06-10T10:22:45+09:00"
      }
    },
    "mjuecs 프로젝트": {
      "path": "/06.university/mju_ecs-project/mjuecs-프로젝트/",
      "filename": "mjuecs 프로젝트",
      "content": "mux 프로젝트 개요 mju_ecs는 명지대 학생들을 대상으로 컨테이너 컴퓨팅 자원을 빌려주는 서비스 입니다. aws 의 컨테이너 서비스인 ecs 서비스와 일정 부분 비슷하지만 무료 유료 부분, 규모에서 차이가 있습니다 📈 docker api 컨테이너 상태(Stats) 실시간 보기 curl --unix-socket /var/run/docker.sock http://localhost:2375/containers/{containerId}/stats CPU 사용률, 메모리, 네트워크 I/O, 디스크 I/O 등의 실시간 데이터를 스트리밍 방식으로 반환합니다. 로컬에서 Docker API 사용해보기 (Unix 소켓 기반): # 컨테이너 목록 보기 curl --unix-socket /var/run/docker.sock http://localhost/containers/json # 특정 컨테이너 로그 보기 curl --unix-socket /var/run/docker.sock http://localhost/containers/my_container/logs?stdout=1 # 컨테이너 시작 curl -X POST --unix-socket /var/run/docker.sock http://localhost/containers/my_container/start 배포 사용자가 docker 그룹에 존재하는가 sudo usermod -aG docker $USER my-ttyd-docker 이미지가 존재하는가 => docker build -t my-ttyd-docker . h2 기본 db 를 만들었는가 java -cp h2-2.3.232.jar org.h2.tools.Shell h2 database 사용법 select * from STUDENT; select * from DOCKER_CONTAINER; select * from TTYD_CONTAINER;",
      "frontmatter": {
        "date": "2025-03-13T17:00:00+09:00",
        "lastmod": "2025-06-04T06:11:17+09:00"
      }
    },
    "mjuecs-system-programming-ubuntu 이미지 설명": {
      "path": "/06.university/mju_ecs-project/mjuecs-system-programming-ubuntu-이미지-설명/",
      "filename": "mjuecs-system-programming-ubuntu 이미지 설명",
      "content": "mjuecs-system-programming-ubuntu 이미지 설명 본 이미지는 mjuecs-ubuntu 이미지에서 system-programming 실습시 필요할 거 같은 패키지들을 추가로 설치해놓은 추가 변형 이미지 입니다 아래는 해당 패키지에 대한 설명이 추가 되어 있습니다. 📦 시스템 프로그래밍을 위한 리눅스 패키지 목록 패키지명 설명 주요 용도 build-essential C/C++ 컴파일 및 빌드에 필요한 핵심 도구 묶음 기본 개발 환경 구축 gcc GNU C Compiler C 언어 소스 코드 컴파일 g++ GNU C++ Compiler C++ 언어 소스 코드 컴파일 make Makefile 기반 자동 빌드 도구 여러 파일로 구성된 프로젝트 빌드 관리 gdb GNU Debugger 실행 중인 프로그램 디버깅 (변수, 메모리, 스택 등 확인) valgrind 메모리 디버깅 및 성능 분석 도구 메모리 누수, 잘못된 접근 감지 strace 시스템 콜 추적 도구 프로그램이 어떤 시스템 호출을 사용하는지 분석 ltrace 공유 라이브러리 함수 호출 추적 외부 라이브러리 함수 호출 내역 확인 pkg-config 라이브러리 정보 제공 도구 컴파일 시 필요한 옵션, 경로 등을 자동으로 불러옴 libssl-dev OpenSSL 개발 라이브러리 SSL/TLS 관련 네트워크 프로그래밍에 필요 manpages-dev 개발자용 매뉴얼 페이지 ( man 2 , man 3 ) 시스템 콜과 C 라이브러리 함수 문서 제공 💡 설치 명령어 예시: sudo apt update sudo apt install build-essential gcc g++ make gdb valgrind strace ltrace pkg-config libssl-dev manpages-dev 🔍 각 패키지의 활용 예시 (간단히) 패키지 예제 사용법 설명 gcc gcc hello.c -o hello C 코드 컴파일 make make all Makefile 기반 전체 빌드 gdb gdb ./hello → run , break main , step 디버깅 시작, 중단점 설정 등 valgrind valgrind --leak-check=yes ./hello 메모리 누수 체크 strace strace ./hello 실행 중 시스템 콜 확인 ltrace ltrace ./hello 외부 라이브러리 함수 호출 보기 pkg-config gcc prog.c -o prog $(pkg-config --cflags --libs openssl) OpenSSL 사용 시 편리한 컴파일 옵션 자동 삽입 manpages-dev man 2 read 또는 man 3 printf 시스템 콜( read )이나 라이브러리 함수( printf ) 설명 보기",
      "frontmatter": {
        "date": "2025-06-03T06:05:16+09:00",
        "lastmod": "2025-06-04T06:11:18+09:00"
      }
    },
    "oracle database": {
      "path": "/06.university/mju_ecs-project/oracle-database/",
      "filename": "oracle database",
      "content": "Oracle Database Docker 컨테이너 가이드 이미지 출처: gvenzl/oracle-xe (Docker Hub)) 컨테이너 생성 1-1. 템플릿 선택 Oracle Database 11, 18, 21 중 하나를 선택합니다. 1-2. 환경변수 설정 필수: ORACLE_PASSWORD 환경변수에 관리자 비밀번호를 입력 나머지 변수는 선택 📷 예시ed%20image%2020250517080578.png) 1-3. 컨테이너 정보 접근 컨테이너 정보 페이지로 이동합니다. 1-4. 웹터미널 접근 준비 웹터미널 접근 비밀번호 복사 버튼을 클릭하여 비밀번호를 복사 이후 웹터미널 열기 버튼 클릭 1-5. 웹터미널 로그인 아이디: 학번 비밀번호: 위에서 복사한 웹터미널 접근 비밀번호 사용자 및 DB 생성 변수 설명 아래 명령어들에서 사용하는 변수들은 자신에게 맞게 대체합니다. 변수명 설명 $ORACLE_PASSWORD 컨테이너 생성 시 설정한 관리자 비밀번호 $USER_NAME 생성할 사용자 이름 $USER_PASSWORD 생성할 사용자 비밀번호 $ACCESS_IP DB 접근용 IP 주소(mjuecs.gonetis.com) $ACCESS_PORT DB 접근용 포트 번호 2-1. SQL\\*Plus 접속 터미널에서 관리자 계정으로 SQL\\*Plus 접속: sqlplus sys/$ORACLE_PASSWORD as SYSDBA 2-2. 사용자 생성 및 권한 부여 ✅ Oracle 11 버전인 경우: CREATE USER $USER_NAME IDENTIFIED BY $USER_PASSWORD; GRANT CREATE SESSION TO $USER_NAME; GRANT CONNECT, RESOURCE TO $USER_NAME; GRANT CREATE MATERIALIZED VIEW TO $USER_NAME; ALTER USER $USER_NAME DEFAULT TABLESPACE users QUOTA UNLIMITED ON users; ✅ Oracle 18 / 21 버전인 경우: ALTER SESSION SET \"_ORACLE_SCRIPT\"=true; CREATE USER $USER_NAME IDENTIFIED BY $USER_PASSWORD; GRANT CREATE SESSION TO $USER_NAME; GRANT CONNECT, RESOURCE TO $USER_NAME; GRANT CREATE MATERIALIZED VIEW TO $USER_NAME; ALTER USER $USER_NAME DEFAULT TABLESPACE users QUOTA UNLIMITED ON users; 외부 연결 확인 설정한 IP(url)와 포트를 이용해 Oracle 클라이언트 도구 또는 SQL Developer로 접속 테스트 📷 예시 Pasted image 20250522221749",
      "frontmatter": {
        "date": "2025-04-02T22:00:00+09:00",
        "lastmod": "2025-06-04T06:11:19+09:00"
      }
    },
    "ryugod 프로젝트2": {
      "path": "/06.university/mju_ecs-project/ryugod-프로젝트2/",
      "filename": "ryugod 프로젝트2",
      "content": "mysql template 자료 : https://hub.docker.com/_/mysql postgres template 자료 : https://hub.docker.com/_/postgres mongoDB template 자료 : https://hub.docker.com/_/mongo ... API 컨테이너 생성 : POST /api/{userId}/container 요청 본문: { \"image\": \"ubuntu:22.04\", \"type\": \"template\", // 또는 \"manual\" 현재는 menual 만 사용 \"container_port\" : \"3306\", // 컨테이너 내부에서 사용하는 서비스 포트 22는 자동 \"env\" : [] // mysql 의 경우 MYSQL_ROOT_PASSWORD 변수가 필수로 필요하다 } 응답: { \"status\" : \"success\", \"container_id\": \"c123456789\", \"container_name\": \"MjuEcs-username-1\" } 터미널 세션 API : POST /api/terminals/create",
      "frontmatter": {
        "date": "2025-03-15T10:30:00+09:00",
        "lastmod": "2025-03-15T10:30:00+09:00"
      }
    },
    "컨테이너 종류": {
      "path": "/06.university/mju_ecs-project/컨테이너-종류/",
      "filename": "컨테이너 종류",
      "content": "컨테이너는 2가지 종류가 있다 실행하고 바로 꺼지는 컨테이너 실행하면 계속 백그라운드로 살아있는 컨테이너 ubuntu 가 바로 꺼지는 컨테이너 이므로 it 옵션을 통해서 bash 셸의 출력을 내가 사용하고 있는 터미널로 붙이게 되면 계속 사용할 수 있다 단 이것도 ctrl + d 를 통해 나가게 되면 종료되어 버린다 터미널을 붙여야 실행되는 컨테이너 docker run -it --name myubuntu ubuntu /bin/bash (exit) or (ctrl + d) docker start -ai myubuntu OR docker run -it --name myubuntu ubuntu /bin/bash (exit) or (ctrl + d) docker start -it myubuntu OR docker run -d --name myubuntu ubuntu sleep infinity docker exec -it myubuntu bash exit docker stop myubuntu # 프로세스 1 번을 종료 즉 연결된 것이 없다면 완전 정지 docker start myubuntu # 다시 실행 docker stop myubuntu # 정지 docker rm myubuntu # 컨테이너 삭제 3번방식이 가장 적절한 것 같다 터미널을 붙이지 않아도 실행되는 컨테이너 docker start --name my",
      "frontmatter": {
        "date": "2025-03-13T20:31:00+09:00",
        "lastmod": "2025-06-27T19:51:46+09:00"
      }
    },
    "컨테이너 포트 할당": {
      "path": "/06.university/mju_ecs-project/컨테이너-포트-할당/",
      "filename": "컨테이너 포트 할당",
      "content": "현재 코드는 순차적으로 포트를 검사하는 방식을 사용하고 있기 때문에, 특히 BASE_PORT 에서 시작하여 MAX_PORT 까지의 범위가 넓을 경우 검색 시간이 오래 걸릴 수 있습니다. 이를 개선하기 위해 다음과 같은 방법들을 고려할 수 있습니다. 랜덤 포트 할당 순차적으로 검사하는 대신, 주어진 범위 내에서 랜덤하게 포트를 선택하여 검사하면 평균적으로 더 빠르게 사용 가능한 포트를 찾을 수 있습니다. 수정된 코드 import java.util.Random; /** * 주어진 범위에서 사용 가능한 포트를 랜덤하게 찾습니다. * * @param startPort 시작 포트 * @param endPort 종료 포트 * @return 사용 가능한 포트 번호, 없으면 -1 반환 */ private int findAvailablePort(int startPort, int endPort) { Random random = new Random(); int attempts = 0; int maxAttempts = (endPort - startPort + 1); // 최대 시도 횟수 while (attempts < maxAttempts) { int port = startPort + random.nextInt(endPort - startPort + 1); if (isPortAvailable(port)) { return port; } attempts++; } return -1; // 사용 가능한 포트가 없는 경우 } 장점 순차 검색보다 평균적으로 더 빠르게 사용 가능한 포트를 찾을 가능성이 높습니다. 랜덤성을 통해 특정 포트에 대한 경쟁 상태를 줄일 수 있습니다. 단점 매우 드물지만, 모든 포트가 사용 중인 경우 최악의 경우에도 많은 시도가 필요할 수 있습니다. 할당된 포트 캐싱 이미 할당된 포트를 관리하는 데이터 구조(예: Set<Integer> )를 사용하여, 중복 검사를 피하고 빠르게 사용 가능한 포트를 찾을 수 있습니다. 수정된 코드 import java.util.HashSet; import java.util.Set; private static final Set<Integer> allocatedPorts = new HashSet<>(); /** * 주어진 범위에서 사용 가능한 포트를 빠르게 찾습니다. * * @param startPort 시작 포트 * @param endPort 종료 포트 * @return 사용 가능한 포트 번호, 없으면 -1 반환 */ private synchronized int findAvailablePort(int startPort, int endPort) { for (int port = startPort; port <= endPort; port++) { if (!allocatedPorts.contains(port) && isPortAvailable(port)) { allocatedPorts.add(port); // 할당된 포트를 캐싱 return port; } } return -1; // 사용 가능한 포트가 없는 경우 } /** * 컨테이너 종료 시 할당된 포트를 해제합니다. * * @param port 해제할 포트 */ private synchronized void releasePort(int port) { allocatedPorts.remove(port); } 사용 예시 컨테이너가 종료될 때 할당된 포트를 해제하는 로직을 추가합니다: // ttydLauncherService에서 컨테이너 종료 시 호출 releasePort(port); 장점 이미 할당된 포트를 관리하므로 불필요한 검사를 피할 수 있습니다. 동기화( synchronized )를 통해 멀티스레드 환경에서도 안전하게 포트를 관리할 수 있습니다. 단점 메모리에 할당된 포트 목록을 유지해야 하며, 프로그램이 종료되면 목록이 사라집니다. 운영체제의 임시 포트 활용 Java의 ServerSocket(0) 은 운영체제가 자동으로 사용 가능한 포트를 할당하도록 요청합니다. 이 방식은 가장 빠르고 간단하지만, 할당된 포트가 범위( 9000 ~ 36000 ) 내에 있는지 보장되지 않습니다. 수정된 코드 private int findAvailablePortInRange(int startPort, int endPort) { try (ServerSocket serverSocket = new ServerSocket(0)) { // OS가 임시 포트 할당 int port = serverSocket.getLocalPort(); if (port >= startPort && port <= endPort) { return port; } } catch (IOException e) { // 무시 } return -1; // 범위 내의 포트를 할당할 수 없는 경우 } 장점 운영체제가 가장 빠르게 사용 가능한 포트를 제공합니다. 별도의 포트 검사 로직이 필요 없습니다. 단점 할당된 포트가 지정된 범위( 9000 ~ 36000 )를 벗어날 가능성이 있습니다. 특정 범위 내에서만 포트를 사용해야 하는 경우 적합하지 않을 수 있습니다. 병렬 처리 포트 검사를 병렬로 수행하여 성능을 향상시킬 수 있습니다. 예를 들어, 여러 스레드가 동시에 다른 포트를 검사하도록 할 수 있습니다. 수정된 코드 import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.atomic.AtomicInteger; private int findAvailablePortParallel(int startPort, int endPort) { AtomicInteger availablePort = new AtomicInteger(-1); ExecutorService executor = Executors.newFixedThreadPool(10); // 10개의 스레드 사용 for (int port = startPort; port <= endPort; port++) { int currentPort = port; executor.submit(() -> { if (availablePort.get() == -1 && isPortAvailable(currentPort)) { availablePort.set(currentPort); } }); } executor.shutdown(); while (!executor.isTerminated()) { // 모든 작업이 완료될 때까지 대기 } return availablePort.get(); } 장점 병렬 처리를 통해 포트 검색 속도를 크게 향상시킬 수 있습니다. 단점 복잡도가 증가하며, 과도한 스레드 사용으로 인해 리소스를 많이 소비할 수 있습니다. 결론 가장 적합한 방법은 요구사항과 환경에 따라 다릅니다: 빠른 해결: 랜덤 포트 할당( Random )을 사용하세요. 안정성과 효율성: 할당된 포트를 캐싱( Set )하는 방법을 추천합니다. 범위 제한이 필요 없는 경우: 운영체제의 임시 포트( ServerSocket(0) )를 활용하세요. 대규모 환경: 병렬 처리를 사용하여 성능을 최적화하세요. 추가적인 질문이나 요구사항이 있다면 말씀해주세요! 😊",
      "frontmatter": {}
    },
    "학교 네트워크 망 traceroute": {
      "path": "/06.university/mju_ecs-project/학교-네트워크-망-traceroute/",
      "filename": "학교 네트워크 망 traceroute",
      "content": "학교 강의실별 와이파이 접속시 [ ~/source/projects/ryugod master ] $ traceroute google.com traceroute to google.com (172.217.161.238), 64 hops max, 40 byte packets 1 192.168.0.1 (192.168.0.1) 4.016 ms 10.352 ms 4.851 ms 2 117.17.157.1 (117.17.157.1) 12.000 ms 5.915 ms 5.870 ms 3 10.10.13.254 (10.10.13.254) 12.100 ms 3.874 ms 8.998 ms 4 202.30.111.4 (202.30.111.4) 3.789 ms 8.036 ms 3.908 ms 5 202.30.111.20 (202.30.111.20) 7.132 ms 3.753 ms 8.286 ms mjuwlan 와이파이 접속시 [ ~/source/projects/ryugod master ] $ traceroute google.com traceroute to google.com (142.250.76.142), 64 hops max, 40 byte packets 1 192.168.42.253 (192.168.42.253) 5.708 ms 4.067 ms 3.655 ms 2 10.10.10.254 (10.10.10.254) 5.003 ms 4.274 ms 3.686 ms 3 * * *",
      "frontmatter": {
        "date": "2025-06-20T02:31:54+09:00",
        "lastmod": "2025-06-27T19:51:49+09:00"
      }
    },
    "dockerpty __init__": {
      "path": "/06.university/mju_ecs-project/dockerpty-document/dockerpty-__init__/",
      "filename": "dockerpty __init__",
      "content": "아래는 제공된 코드를 기반으로 한 dockerpty 모듈의 구조화된 문서입니다. 이 문서에는 개요, 사용 방법 및 함수에 대한 자세한 설명이 포함되어 있습니다. dockerpty 문서 dockerpty 는 Docker 컨테이너와 의사 터미널(PTY) 기능을 사용하여 상호 작용할 수 있는 Python 라이브러리입니다. 이 라이브러리를 사용하면 현재 프로세스 내에서 컨테이너의 PTY를 제어할 수 있습니다. 이 라이브러리는 특히 Docker 컨테이너 내에서 대화형 명령을 실행하거나 PTY 지원을 통해 컨테이너에 연결하는 데 유용합니다. 목차 개요 설치 사용 방법 PTY로 컨테이너 시작하기 컨테이너에서 명령 실행하기 Exec 세션 시작하기 API 참조 start exec_command start_exec 개요 dockerpty 라이브러리는 PTY를 통해 Docker 컨테이너와 상호 작용할 수 있는 고급 인터페이스를 제공합니다. 이 라이브러리는 대화형 세션을 시작하고, 컨테이너 내에서 명령을 실행하며, 입출력 스트림을 관리하는 과정을 단순화합니다. 이 라이브러리는 Docker API 위에 구축되었으며 PTY 작업을 처리하는 복잡성을 추상화합니다. 주요 기능: 컨테이너에서 대화형 세션 시작. PTY 지원을 통해 컨테이너 내에서 명령 실행. 표준 입력, 출력 및 오류 스트림 관리. 설치 dockerpty 를 사용하려면 다음 의존성을 설치해야 합니다: Python 3.x Docker SDK for Python ( docker 패키지) pip 를 사용하여 dockerpty 를 설치할 수 있습니다: pip install dockerpty 사용 방법 PTY로 컨테이너 시작하기 start 함수를 사용하면 실행 중인 컨테이너에 연결하고 현재 프로세스 내에서 해당 컨테이너의 PTY를 제어할 수 있습니다. import docker import dockerpty client = docker.from_env() container = client.containers.run('ubuntu', detach=True, tty=True) dockerpty.start(client, container) 매개변수: client : Docker 클라이언트 인스턴스. container : Docker 컨테이너 객체 또는 ID. interactive : 대화형 모드를 활성화할지 여부 (기본값: True ). stdout , stderr , stdin : 표준 스트림에 사용할 파일 객체 (선택 사항). logs : 컨테이너 로그를 포함할지 여부 (기본값: None ). 컨테이너에서 명령 실행하기 exec_command 함수는 Docker exec API를 사용하여 컨테이너 내에서 명령을 실행합니다. import docker import dockerpty client = docker.from_env() container = client.containers.run('ubuntu', detach=True, tty=True) dockerpty.exec_command(client, container, command='bash') 매개변수: client : Docker 클라이언트 인스턴스. container : Docker 컨테이너 객체 또는 ID. command : 컨테이너 내에서 실행할 명령. interactive : 대화형 모드를 활성화할지 여부 (기본값: True ). stdout , stderr , stdin : 표준 스트림에 사용할 파일 객체 (선택 사항). Exec 세션 시작하기 start_exec 함수는 Docker exec API를 사용하여 생성된 기존 exec 세션을 시작합니다. import docker import dockerpty client = docker.from_env() container = client.containers.run('ubuntu', detach=True, tty=True) # Exec 인스턴스 생성 exec_id = client.api.exec_create(container.id, 'bash') # Exec 세션 시작 dockerpty.start_exec(client, exec_id) 매개변수: client : Docker 클라이언트 인스턴스. exec_id : Exec 인스턴스의 ID. interactive : 대화형 모드를 활성화할지 여부 (기본값: True ). stdout , stderr , stdin : 표준 스트림에 사용할 파일 객체 (선택 사항). API 참조 start def start(client, container, interactive=True, stdout=None, stderr=None, stdin=None, logs=None): 지정된 컨테이너의 PTY와 대화형 세션을 시작합니다. 매개변수: client : Docker 클라이언트 인스턴스. container : Docker 컨테이너 객체 또는 ID. interactive : 대화형 모드를 활성화할지 여부 (기본값: True ). stdout , stderr , stdin : 표준 스트림에 사용할 파일 객체 (선택 사항). logs : 컨테이너 로그를 포함할지 여부 (기본값: None ). exec_command def exec_command(client, container, command, interactive=True, stdout=None, stderr=None, stdin=None): Docker exec API를 사용하여 컨테이너 내에서 명령을 실행합니다. 매개변수: client : Docker 클라이언트 인스턴스. container : Docker 컨테이너 객체 또는 ID. command : 컨테이너 내에서 실행할 명령. interactive : 대화형 모드를 활성화할지 여부 (기본값: True ). stdout , stderr , stdin : 표준 스트림에 사용할 파일 객체 (선택 사항). start_exec def start_exec(client, exec_id, interactive=True, stdout=None, stderr=None, stdin=None): Docker exec API를 사용하여 생성된 기존 exec 세션을 시작합니다. 매개변수: client : Docker 클라이언트 인스턴스. exec_id : Exec 인스턴스의 ID. interactive : 대화형 모드를 활성화할지 여부 (기본값: True ). stdout , stderr , stdin : 표준 스트림에 사용할 파일 객체 (선택 사항). 참고 사항 대화형 모드: 대화형 모드를 사용하려면 컨테이너를 tty=True 옵션으로 시작해야 합니다. 오류 처리: Docker API와 상호 작용할 때 항상 예외를 처리하여 런타임 오류를 방지하세요. 의존성: 이 라이브러리를 사용하려면 docker Python 패키지를 설치하고 적절히 구성해야 합니다. 이 문서는 dockerpty 라이브러리를 사용하는 데 필요한 포괄적인 가이드를 제공합니다. 추가 정보는 소스 코드 또는 공식 Docker API 문서를 참조하세요.",
      "frontmatter": {}
    },
    "dockerpty io": {
      "path": "/06.university/mju_ecs-project/dockerpty-document/dockerpty-io/",
      "filename": "dockerpty io",
      "content": "문서: dockerpty/io.py 모듈 분석 개요 이 문서는 dockerpty/io.py 파일의 주요 기능과 클래스에 대해 설명합니다. 이 파일은 Docker에서 사용되는 PTY(가상 터미널)와 관련된 입출력 작업을 관리하는 데 사용됩니다. 주요 기능은 파일 디스크립터를 다루고, 데이터 스트림을 읽고 쓰며, 멀티플렉싱된 데이터를 처리하는 것입니다. 주요 함수 및 클래스 1 set_blocking(fd, blocking=True) 기능: 파일 디스크립터( fd )의 블로킹 모드를 설정하거나 해제합니다. 매개변수: fd : 파일 디스크립터 객체. blocking : 블로킹 모드를 활성화할지 여부 (기본값: True ). 반환값: 원래의 블로킹 상태를 반환합니다. 2 select(read_streams, write_streams, timeout=0) 기능: 주어진 읽기 및 쓰기 스트림 중에서 준비된 스트림을 선택합니다. 매개변수: read_streams : 읽기 가능한 스트림 목록. write_streams : 쓰기 가능한 스트림 목록. timeout : 타임아웃 시간 (초 단위). 반환값: 읽기 및 쓰기 가능한 스트림의 튜플을 반환합니다. 예외 처리: POSIX 시그널로 인해 select() 가 중단될 경우 빈 리스트를 반환합니다. 클래스 상세 설명 1 Stream 기능: 파일 디스크립터를 추상화한 클래스입니다. 소켓과 파일 모두 일관된 방식으로 읽고 쓸 수 있도록 지원합니다. 주요 메서드: fileno() : 파일 디스크립터 번호를 반환합니다. set_blocking(value) : 블로킹 모드를 설정합니다. read(n=4096) : 최대 n 바이트의 데이터를 읽습니다. write(data) : 데이터를 버퍼에 저장하고 비동기적으로 씁니다. do_write() : 버퍼에 있는 데이터를 가능한 만큼 씁니다. needs_write() : 쓰기를 대기 중인 데이터가 있는지 확인합니다. close() : 스트림을 닫습니다. 특징: 복구 가능한 I/O 오류를 처리합니다 ( ERRNO_RECOVERABLE 참조). 2 Demuxer 기능: Docker에서 사용되는 멀티플렉싱된 데이터를 디멀티플렉싱하여 읽습니다. 주요 메서드: read(n=4096) : 멀티플렉싱된 데이터를 디코딩하여 읽습니다. _next_packet_size(n) : 다음 패킷의 크기를 계산합니다. 특징: Docker는 PTY가 없는 경우 8바이트 헤더를 사용하여 데이터를 멀티플렉싱합니다. 첫 4바이트는 스트림 유형(예: stdout, stderr)을 나타내고, 다음 4바이트는 데이터 길이를 나타냅니다. 3 Pump 기능: 두 개의 스트림 간 데이터를 전달하는 \"펌프\" 역할을 합니다. 주요 메서드: flush(n=4096) : 읽기 스트림에서 데이터를 읽고 쓰기 스트림으로 전달합니다. is_done() : 펌프 작업이 완료되었는지 확인합니다. 특징: 읽기 스트림에서 EOF를 받으면 작업이 완료된 것으로 간주합니다. 쓰기 스트림에 대기 중인 데이터가 없어야 작업이 완료됩니다. 사용 예시 # Stream 객체 생성 fd = os.open(\"/path/to/file\", os.O_RDWR) stream = Stream(fd) # 데이터 읽기 data = stream.read(1024) # 데이터 쓰기 stream.write(b\"Hello, World!\") # Demuxer 객체 생성 demuxer = Demuxer(stream) decoded_data = demuxer.read() # Pump 객체 생성 from_stream = Stream(fd1) to_stream = Stream(fd2) pump = Pump(from_stream, to_stream) # 데이터 펌핑 pump.flush() 라이선스 이 코드는 Apache License 2.0 하에 배포됩니다. 자세한 내용은 Apache License를 참조하십시오. 이 문서는 dockerpty/io.py 파일의 주요 기능과 클래스 구조를 한글로 설명하며, 사용자가 이를 이해하고 활용하는 데 도움을 주기 위해 작성되었습니다.",
      "frontmatter": {}
    },
    "dockerpty pty": {
      "path": "/06.university/mju_ecs-project/dockerpty-document/dockerpty-pty/",
      "filename": "dockerpty pty",
      "content": "문서: dockerpty 모듈 개요 제공된 코드는 dockerpty 라는 Python 모듈을 나타내며, 특히 pty.py 파일에 초점을 맞추고 있습니다. 이 문서에서는 모듈의 주요 구성 요소와 기능을 설명합니다. 소개 dockerpty 모듈은 Docker 컨테이너와 의사 터미널(PTY) 인터페이스를 통해 상호작용하는 것을 용이하게 설계되었습니다. 사용자가 로컬 터미널 세션과 유사한 방식으로 Docker 컨테이너 터미널에 연결하고 제어할 수 있도록 합니다. 이 모듈의 주요 목적은 PTY의 라이프사이클을 관리하는 것인데, 호스트와 컨테이너 간의 입력/출력 스트림을 처리하며 터미널 창 크기를 조정하는 작업을 포함합니다. 주요 클래스 및 기능 WINCHHandler 목적: 터미널 창 크기가 변경될 때 동적으로 PTY를 조정하기 위해 SIGWINCH 신호를 처리합니다. 메서드: __init__(self, pty) : PTY를 참조하여 핸들러를 초기화합니다. start(self) : SIGWINCH 신호를 트랩하고 PTY를 조정합니다. stop(self) : SIGWINCH 신호 트랩을 중지하고 이전 신호 핸들러를 복원합니다. __enter__(self) & __exit__(self, *_) : start() 및 stop() 메서드를 호출하는 컨텍스트 관리 메서드입니다. Operation (추상 기본 클래스) 목적: Docker 컨테이너 또는 exec 관련 작업을 위한 추상 기본 클래스로 사용됩니다. 메서드: israw(self, **kwargs) : 해당 작업이 raw 모드로 작동해야 하는지 여부를 결정합니다 (즉, 컨테이너/exec에 TTY가 할당되었는지 확인). start(self, **kwargs) : 작업 실행을 시작합니다 (하위 클래스에서 구현해야 함). resize(self, height, width, **kwargs) : PTY를 조정합니다 (하위 클래스에서 구현해야 함). sockets(self) : 입력/출력 스트림에 대한 소켓을 반환합니다 (하위 클래스에서 구현해야 함). RunOperation 목적: docker run -like 명령을 관리하기 위해 Operation 인터페이스를 구현합니다. 초기화: RunOperation(client, container, interactive=True, stdout=None, stderr=None, stdin=None, logs=None) client : Docker 클라이언트 인스턴스. container : Docker에서 반환된 컨테이너 사전. interactive : 세션이 대화형으로 작동해야 하는지 여부 (기본값: True ). stdout , stderr , stdin : 표준 출력, 오류 및 입력 스트림 (기본값은 sys.stdout , sys.stderr , sys.stdin ). logs : 컨테이너 로그를 포함할지 여부 (설정되지 않으면 폐기 예정 경고 발생). 주요 메서드: start(self, sockets=None, **kwargs) : PTY를 설정하고 호스트와 컨테이너 간 데이터 전송을 시작합니다. israw(self, **kwargs) : 컨테이너가 tty=True 로 시작되었는지 확인합니다. sockets(self) : stdin , stdout , stderr 에 대한 소켓을 반환합니다. resize(self, height, width, **kwargs) : 컨테이너의 PTY를 조정합니다. _container_info(self) : 컨테이너에 대한 자세한 정보를 검색합니다. ExecOperation 목적: docker exec -like 명령을 관리하기 위해 Operation 인터페이스를 구현합니다. 초기화: ExecOperation(client, exec_id, interactive=True, stdout=None, stderr=None, stdin=None) client : Docker 클라이언트 인스턴스. exec_id : client.exec_create 를 사용하여 생성된 exec 인스턴스 ID. interactive : 세션이 대화형으로 작동해야 하는지 여부 (기본값: True ). stdout , stderr , stdin : 표준 출력, 오류 및 입력 스트림 (기본값은 sys.stdout , sys.stderr , sys.stdin ). 주요 메서드: start(self, sockets=None, **kwargs) : exec 프로세스를 시작하고 데이터 펌프를 설정합니다. israw(self, **kwargs) : exec 프로세스가 tty=True 로 시작되었는지 확인합니다. sockets(self) : exec 프로세스의 모든 I/O 스트림에 대한 단일 소켓을 반환합니다. resize(self, height, width, **kwargs) : exec 프로세스의 PTY를 조정합니다. is_process_tty(self) : exec 프로세스에 TTY가 할당되어 있는지 확인합니다. _exec_info(self) : exec 인스턴스에 대한 자세한 정보를 검색합니다. PseudoTerminal 목적: Docker 컨테이너 또는 exec 프로세스를 위한 PTY의 라이프사이클을 관리합니다. 초기화: PseudoTerminal(client, operation) client : Docker 클라이언트 인스턴스. operation : RunOperation 또는 ExecOperation 인스턴스. 주요 메서드: sockets(self) : 기본 작업의 sockets() 메서드에 위임합니다. start(self, sockets=None) : PTY 세션을 시작하고, 호스트와 컨테이너 간 입력/출력 스트림을 관리하며, 터미널 크기를 조정합니다. resize(self, size=None) : 현재 터미널 크기 또는 지정된 크기에 따라 PTY를 조정합니다. _hijack_tty(self, pumps) : 호스트와 컨테이너 간 데이터를 읽고 쓰기 위한 주요 루프를 관리합니다. 헬퍼 함수 exec_create 목적: 컨테이너에 대해 exec 인스턴스를 생성합니다. 시그니처: exec_create(client, container, command, interactive=True) client : Docker 클라이언트 인스턴스. container : 대상 컨테이너. command : 컨테이너 내에서 실행할 명령. interactive : 세션이 대화형으로 작동해야 하는지 여부 (기본값: True ). 종속성 표준 라이브러리: sys : 표준 입력/출력/오류 스트림에 접근하기 위해. signal : SIGWINCH 신호를 처리하기 위해. warnings : 폐기 경고를 발행하기 위해. ssl.SSLError : SSL 관련 오류를 처리하기 위해. 내부 모듈: dockerpty.io : 저수준 I/O 작업 및 스트림 관리를 처리합니다. dockerpty.tty : 터미널 조작 유틸리티를 제공합니다. 예제 사용법 import docker from dockerpty import PseudoTerminal # Docker 클라이언트 초기화 client = docker.Client() # 컨테이너 생성 container = client.create_container( image='busybox:latest', stdin_open=True, tty=True, command='/bin/sh', ) # 컨테이너의 PTY 시작 pty = PseudoTerminal(client, RunOperation(client, container)) pty.start() 결론 dockerpty 모듈은 Docker 컨테이너와 의사 터미널을 통한 상호작용을 위한 견고한 프레임워크를 제공합니다. PTY를 활용하여 컨테이너화된 애플리케이션을 터미널 기반 워크플로우로 원활히 통합할 수 있으며, 로컬 터미널 세션과 유사한 사용자 경험을 보장합니다.",
      "frontmatter": {
        "date": "2025-06-20T02:31:54+09:00",
        "lastmod": "2025-09-09T23:10:58+09:00"
      }
    },
    "dockerpty tty": {
      "path": "/06.university/mju_ecs-project/dockerpty-document/dockerpty-tty/",
      "filename": "dockerpty tty",
      "content": "dockerpty: tty.py 모듈 문서 dockerpty: tty.py 모듈은 터미널(TTY) 장치와 상호 작용하기 위한 유틸리티를 제공하며, 특히 가상 터미널(PTY)과 관련된 작업에 유용합니다. 이 모듈은 TTY 크기를 확인하거나 터미널을 일시적으로 raw 모드로 전환하는 기능을 포함하고 있습니다. 이러한 기능은 Docker 컨테이너 또는 PTY와 TTY 간 데이터를 스트리밍해야 하는 환경에서 유용하게 사용됩니다. 아래는 모듈의 구성 요소와 사용 방법에 대한 자세한 설명입니다. 함수 size(fd) 설명 제공된 파일 디스크립터와 연결된 TTY의 크기를 (행, 열) 형태의 튜플로 반환합니다. 파일 디스크립터는 일반적으로 TTY의 표준 출력 스트림( stdout )을 나타내야 합니다. TTY 크기를 확인할 수 없는 경우 (예: 파일 디스크립터가 TTY가 아닌 경우), 함수는 None 을 반환합니다. 매개변수 fd : 유효한 파일 디스크립터를 가진 파일 객체 (예: sys.stdout ). 파일 디스크립터는 TTY와 연결되어 있어야 합니다. 반환 값 (rows, cols) : TTY의 행과 열 수를 나타내는 튜플. None : TTY 크기를 확인할 수 없는 경우. 예제 코드 import sys from dockerpty.tty import size tty_size = size(sys.stdout) if tty_size: rows, cols = tty_size print(f\"TTY 크기: 행={rows}, 열={cols}\") else: print(\"TTY가 아니거나 크기를 확인할 수 없습니다.\") 클래스 Terminal 설명 Terminal 클래스는 터미널을 일시적으로 raw 모드로 전환하기 위한 컨텍스트 관리자 인터페이스를 제공합니다. Raw 모드에서는 줄 버퍼링 및 문자 처리가 비활성화되며, PTY에서 TTY로 데이터를 직접 스트리밍할 때 유용합니다. 생성자 Terminal(fd, raw=True) 매개변수: fd : 유효한 파일 디스크립터를 가진 파일 객체 (예: sys.stdin ). 이는 터미널의 입력 스트림을 나타냅니다. raw : 터미널이 raw 모드로 동작할지 여부를 나타내는 플래그. 기본값은 True . 속성: fd : 터미널과 연결된 파일 디스크립터. raw : 터미널이 raw 모드로 동작할지 여부. original_attributes : raw 모드로 전환하기 전의 원래 터미널 속성을 저장합니다. 초기값은 None . 메서드 start() 터미널이 TTY이고 raw 플래그가 True 인 경우, 터미널을 raw 모드로 전환합니다. 원래의 터미널 속성을 나중에 복원하기 위해 저장합니다. 동작: 파일 디스크립터가 TTY가 아니거나 raw 가 False 인 경우 아무 작업도 수행하지 않습니다. 그렇지 않으면 현재 터미널 속성을 저장하고 tty.setraw() 를 사용하여 raw 모드로 전환합니다. stop() 터미널 속성을 raw 모드로 전환하기 전의 원래 상태로 복원합니다. 동작: original_attributes 가 None 이 아닌 경우, termios.tcsetattr() 를 사용하여 TCSADRAIN 플래그와 함께 터미널 속성을 복원합니다. 그렇지 않으면 아무 작업도 수행하지 않습니다. israw() 터미널이 raw 모드로 설정되어 있는 경우 True 를 반환하고, 그렇지 않으면 False 를 반환합니다. __enter__() with 블록에 진입할 때 호출됩니다. start() 메서드를 호출하여 터미널을 raw 모드로 준비합니다. __exit__(*_args) with 블록을 종료할 때 호출됩니다. stop() 메서드를 호출하여 터미널을 원래 상태로 복원합니다. __repr__() Terminal 객체의 문자열 표현을 반환합니다. 파일 디스크립터와 raw 모드 상태를 포함합니다. 예제 코드 import sys from dockerpty.tty import Terminal # Terminal 컨텍스트 관리자를 사용하여 stdin을 일시적으로 raw 모드로 전환 with Terminal(sys.stdin, raw=True): print(\"터미널이 이제 raw 모드입니다. Ctrl+C를 눌러 종료하세요.\") # raw 모드가 필요한 작업 수행 # 예: 입력을 한 문자씩 즉시 읽기 while True: char = sys.stdin.read(1) if char == '\\x03': # Ctrl+C break print(f\"누른 키: {char}\") print(\"터미널이 원래 상태로 복원되었습니다.\") 주요 개념 Raw 모드 Raw 모드는 줄 버퍼링 및 문자 처리를 비활성화하여 터미널과 직접 상호 작용할 수 있도록 합니다. 이는 입력을 즉시 처리해야 하는 대화형 셸과 같은 애플리케이션에서 유용합니다. TTY 크기 TTY 크기는 텍스트를 표시하기 위한 행과 열의 수로 표현됩니다. 이 정보는 출력을 올바르게 포맷하거나 터미널 기반 애플리케이션의 레이아웃을 조정하는 데 사용됩니다. 컨텍스트 관리자 Terminal 클래스는 Python의 컨텍스트 관리자 프로토콜( __enter__ 및 __exit__ )을 활용하여 사용 후 터미널 설정이 제대로 복원되도록 보장합니다. 예외가 발생하더라도 설정이 복원됩니다. 의존성 dockerpty: tty.py 모듈은 다음 표준 라이브러리 모듈에 의존합니다: os : 운영 체제와 상호 작용하기 위한 함수를 제공합니다 (예: 파일 디스크립터가 TTY인지 확인). termios : POSIX 터미널 제어 함수에 접근하여 터미널 속성을 조작합니다. tty : 터미널 모드를 설정하기 위한 도우미 함수를 제공합니다 (예: raw 모드로 전환). fcntl : 파일 제어 작업에 접근하여 TTY 크기를 쿼리합니다. struct : fcntl.ioctl() 에서 반환된 바이너리 데이터를 해석합니다. 라이선스 이 모듈은 Apache License, Version 2.0에 따라 배포됩니다. 전체 라이선스 텍스트는 http://www.apache.org/licenses/LICENSE-2.0에서 확인할 수 있습니다. 작성자 Chris Corbyn ( chris@w3style.co.uk ) 이 문서는 dockerpty: tty.py 모듈의 구성 요소와 이를 효과적으로 사용하는 방법에 대한 종합적인 개요를 제공합니다.",
      "frontmatter": {}
    },
    "Schema_Diagram 및 보고서": {
      "path": "/06.university/databaseuniversity/schema_diagram-및-보고서/",
      "filename": "Schema_Diagram 및 보고서",
      "content": "Pasted image 20240522033715 열쇠표시 : primary key Pasted image 20240522234781 좌측 : FK, 우측:PK 목표로 하는 데이터베이스 시스템에 대한 설명 마트에 방문하는 고객의 거래를 관리하기 위한 db 시스템 데이터베이스 스키마 다이어그램 표현 (“Schema_Diagram.pdf” 참조) 상단에 위치 데이터베이스를 구성하는 테이블들의 명세 표현 (“테이블정의서.xls” 참조) 파일로 첨부 주요 질의서(Query) 세트 및 각각에 대한 설명 {??}시에 사는 고객들의 이름과 나이를 조회하시오 select customer_name, age from customer where address like '%{??}시%'; {??} 종류의 상품의 이름을 조회하시오. select product_name from product where class = '{??}'; 고객이 거래 1회당 산 {??}의 갯수의 평균을 조회하시오. SELECT SUM(quantity) AS total_quantity FROM transaction_item WHERE product_id = ( SELECT product_id FROM product WHERE product_name = ? {??} 형식 YYYY-MM-DD 일자에 방문한 고객들의 이름을 조회하시오. SELECT c.customer_name FROM customer c INNER JOIN visit v ON c.customer_id = v.customer_id WHERE TRUNC(v.visit_date) = DATE '{??}'; {??} 이름 손님이 머무른 시간의 평균을 조회하시오. SELECT AVG(t.transaction_date - v.visit_date) * 24 AS \"avg_time(hour)\" from ( select customer_name, visit_id, visit_date from customer inner join visit on customer.customer_id = visit.customer_id where customer_name = '{??}' ) v inner join transaction t on v.visit_id = t.visit_id; {??} 고객의 구매 내역 조회 SELECT c.customer_name, p.product_name, ti.quantity, p.price, (ti.quantity * p.price) AS total_price FROM customer c JOIN visit v ON c.customer_id = v.customer_id JOIN transaction t ON v.visit_id = t.visit_id JOIN transaction_item ti ON t.transaction_id = ti.transaction_id JOIN product p ON ti.product_id = p.product_id WHERE c.customer_name = '고객 이름' ORDER BY t.transaction_date DESC; 웹 인터페이스 Pasted image 20240606154867 Pasted image 20240606154860 Pasted image 20240606154852 Pasted image 20240606154844 Pasted image 20240606154833 Pasted image 20240606155195 Pasted image 20240606155242 Pasted image 20240606155222 Pasted image 20240606155231 Pasted image 20240606155386 Pasted image 20240606155467 Pasted image 20240606155482 결론 및 시사점 쿼리문을 만드는데에는 크게 어렵지 않았지만 설계를 하는것이 훨씬 어려웠던것 같다 설계에서 테이블간의 단순하게 한개의 PK FK 관계로 연결되다 보니 쿼리문이 길어지고 복잡했던것 같다 추가 생성및 제거 DDL CREATE TABLE customer ( customer_id INT PRIMARY KEY, customer_name VARCHAR2(50) NOT NULL, age INT NOT NULL, address VARCHAR2(100), enrollment_date DATE DEFAULT CURRENT_DATE ); CREATE TABLE visit ( visit_id INT PRIMARY KEY, customer_id INT, transportation varchar(30) check ( transportation in ('car','public transportation','walk')), visit_date DATE DEFAULT CURRENT_DATE, FOREIGN KEY (customer_id) REFERENCES customer(customer_id) ); CREATE TABLE product ( product_id INT PRIMARY KEY, product_name VARCHAR2(50) NOT NULL, class VARCHAR2(15) check ( class in ('grain', 'dairy', 'meet', 'fish', 'vagitable', 'fruit', 'favorite food') ), price INT NOT NULL ); -- 음식 마트 종류: 유제품, 곡물, 과일, 채소, 고기, 생선, 가공식품 순으로 나열하였습니다. -- CREATE TABLE transaction ( transaction_id INT PRIMARY KEY, visit_id INT NOT NULL, transaction_date DATE DEFAULT CURRENT_DATE, FOREIGN KEY (visit_id) REFERENCES visit(visit_id) ); CREATE TABLE transaction_item ( item_id INT PRIMARY KEY, transaction_id INT, product_id INT NOT NULL, quantity INT NOT NULL, FOREIGN KEY (transaction_id) REFERENCES transaction(transaction_id), FOREIGN KEY (product_id) REFERENCES product(product_id) ); -- 만들어 보자 예시로 -- 1. customer 테이블 시간은 마음대로 하되 순서대로 TRUNCATE TABLE customer; INSERT INTO customer (customer_id, customer_name, age, address, ENROLLMENT_DATE) VALUES (1, '홍길동', 30, '서울시 강남구', TO_DATE('2024-04-30 10:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO customer (customer_id, customer_name, age, address, ENROLLMENT_DATE) VALUES (2, '김철수', 25, '용인시 처인구', TO_DATE('2024-04-30 10:30:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO customer (customer_id, customer_name, age, address, ENROLLMENT_DATE) VALUES (3, '이영희', 28, '서울시 송파구', TO_DATE('2024-04-30 11:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO customer (customer_id, customer_name, age, address, ENROLLMENT_DATE) VALUES (4, '박민수', 35, '용인시 처인구', TO_DATE('2024-04-30 11:30:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO customer (customer_id, customer_name, age, address, ENROLLMENT_DATE) VALUES (5, '최유리', 22, '서울시 용산구', TO_DATE('2024-04-30 12:00:00', 'YYYY-MM-DD HH24:MI:SS')); -- 2. visit 테이블 TRUNCATE TABLE visit; INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (1, 1, 'car', TO_DATE('2024-05-01 11:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (2, 2, 'public transportation', TO_DATE('2024-05-01 12:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (3, 3, 'walk', TO_DATE('2024-05-02 13:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (4, 4, 'car', TO_DATE('2024-05-02 20:10:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (5, 5, 'public transportation', TO_DATE('2024-05-02 21:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (6, 1, 'walk', TO_DATE('2024-05-02 21:30:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (7, 2, 'car', TO_DATE('2024-05-03 15:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (8, 3, 'public transportation', TO_DATE('2024-05-03 20:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (9, 4, 'walk', TO_DATE('2024-05-04 08:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (10, 5, 'car', TO_DATE('2024-05-04 11:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (11, 1, 'public transportation', TO_DATE('2024-05-06 12:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (12, 2, 'walk', TO_DATE('2024-05-08 11:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (13, 3, 'car', TO_DATE('2024-05-08 13:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (14, 4, 'public transportation', TO_DATE('2024-05-08 21:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (15, 5, 'walk', TO_DATE('2024-05-10 09:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (16, 1, 'car', TO_DATE('2024-05-10 10:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (17, 2, 'public transportation', TO_DATE('2024-05-10 11:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (18, 3, 'walk', TO_DATE('2024-05-10 12:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (19, 4, 'car', TO_DATE('2024-05-10 19:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO visit (visit_id, customer_id, transportation, visit_date) VALUES (20, 5, 'public transportation', TO_DATE('2024-05-10 21:00:00', 'YYYY-MM-DD HH24:MI:SS')); -- 3. product 테이블 TRUNCATE TABLE product; INSERT INTO product (product_id, product_name, class, price) VALUES (1, '쌀', 'grain', 2000); INSERT INTO product (product_id, product_name, class, price) VALUES (2, '우유', 'dairy', 1500); INSERT INTO product (product_id, product_name, class, price) VALUES (3, '소고기', 'meet', 10000); INSERT INTO product (product_id, product_name, class, price) VALUES (4, '방어', 'fish', 8000); INSERT INTO product (product_id, product_name, class, price) VALUES (5, '당근', 'vagitable', 1200); INSERT INTO product (product_id, product_name, class, price) VALUES (6, '사과', 'fruit', 3000); INSERT INTO product (product_id, product_name, class, price) VALUES (7, '피자', 'favorite food', 5000); INSERT INTO product (product_id, product_name, class, price) VALUES (8, '빵', 'grain', 2500); INSERT INTO product (product_id, product_name, class, price) VALUES (9, '요거트', 'dairy', 1600); INSERT INTO product (product_id, product_name, class, price) VALUES (10, '돼지고기', 'meet', 11000); INSERT INTO product (product_id, product_name, class, price) VALUES (11, '고등어', 'fish', 8500); INSERT INTO product (product_id, product_name, class, price) VALUES (12, '양배추', 'vagitable', 1300); INSERT INTO product (product_id, product_name, class, price) VALUES (13, '바나나', 'fruit', 3500); INSERT INTO product (product_id, product_name, class, price) VALUES (14, '라면', 'favorite food', 5500); INSERT INTO product (product_id, product_name, class, price) VALUES (15, '옥수수', 'grain', 2700); -- 4. transaction 테이블 TRUNCATE TABLE transaction; INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (1, 1, TO_DATE('2024-05-01 12:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (2, 2, TO_DATE('2024-05-01 12:34:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (3, 3, TO_DATE('2024-05-02 14:00:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (4, 4, TO_DATE('2024-05-02 20:45:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (5, 5, TO_DATE('2024-05-02 21:45:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (6, 6, TO_DATE('2024-05-02 22:00:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (7, 7, TO_DATE('2024-05-03 16:30:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (8, 8, TO_DATE('2024-05-03 21:00:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (9, 9, TO_DATE('2024-05-04 09:30:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (10, 10, TO_DATE('2024-05-04 12:00:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (23, 3, TO_DATE('2024-05-02 15:30:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (11, 11, TO_DATE('2024-05-06 12:00:00', 'YYYY-MM-DD HH24:MI:SS')); INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (12, 12, TO_DATE('2024-05-08 11:45:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (13, 13, TO_DATE('2024-05-08 15:00:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (14, 14, TO_DATE('2024-05-08 22:30:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (15, 15, TO_DATE('2024-05-10 09:45:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (24, 4, TO_DATE('2024-05-02 21:30:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (16, 16, TO_DATE('2024-05-10 12:30:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (17, 17, TO_DATE('2024-05-10 12:45:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (18, 18, TO_DATE('2024-05-10 14:00:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (19, 19, TO_DATE('2024-05-10 20:30:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 INSERT INTO transaction (transaction_id, visit_id, transaction_date) VALUES (20, 20, TO_DATE('2024-05-10 22:30:00', 'YYYY-MM-DD HH24:MI:SS')); -- 수정 -- 5. transaction_item 테이블 TRUNCATE TABLE transaction_item; -- transaction_item 무작위 테이블 데이터 생성 BEGIN FOR transaction_id IN 1..20 LOOP FOR i IN 1..(1 + DBMS_RANDOM.VALUE(0, 4)) LOOP INSERT INTO transaction_item (item_id, transaction_id, product_id, quantity) VALUES (transaction_id * 10 + i, transaction_id, ROUND(DBMS_RANDOM.VALUE(1, 15)), ROUND(DBMS_RANDOM.VALUE(1, 5))); END LOOP; END LOOP; END; / COMMIT; / -- 제거 -- DROP TABLE transaction_item; -- DROP TABLE transaction; -- DROP TABLE product; -- DROP TABLE visit; -- DROP TABLE customer;",
      "frontmatter": {
        "description": "대학 수업 database 에서 배운 모든 내용 정리",
        "series": "database(university)",
        "date": "2024-05-21T18:51:00+09:00",
        "lastmod": "2024-05-21T18:51:00+09:00"
      }
    },
    "데이터베이스 sql": {
      "path": "/06.university/databaseuniversity/데이터베이스-sql/",
      "filename": "데이터베이스 sql",
      "content": "SQL DDL (data definition laguage) 쿼리 Create Table Drop Table Truncate Table Alter Table alter table r {add, add constraint, modifiy, drop colume, set unused, drop unused colume} A rename Data Type Constraint (NOT NULL, DEFAULT, CHECK, REFERENCE) DML (data manipulation language) select select from where group by having order by delete insert update Domain Types char(n) : 고정 길이 문자열 varchar(n) : 가변 길이 문자열 int : integer 정수 하위 집합 smallint : 작은 int numeric(p, d) : p=숫자 전체 길이, d=소수점 아래자리수 ex) numeric(3,1) : 0.1 부터 99.9 까지 저장 가능 더 정밀한 소수점은 버려짐 real, double precision : 부동 소수점 숫자 float(n) : n 자리 정밀도 소수 MySQL TINYINT: 1바이트 정수 (-128 ~ 127 또는 0 ~ 255) SMALLINT: 2바이트 정수 (-32,768 ~ 32,767 또는 0 ~ 65,535) MEDIUMINT: 3바이트 정수 (-8,388,608 ~ 8,388,607 또는 0 ~ 16,777,215) INT (INTEGER): 4바이트 정수 (-2,147,483,648 ~ 2,147,483,647 또는 0 ~ 4,294,967,295) BIGINT: 8바이트 정수 (-9,223,372,036,854,775,808 ~ 9,223,372,036,854,775,807 또는 0 ~ 18,446,744,073,709,551,615) FLOAT: 4바이트 부동 소수점 수 (가변 소수점) DOUBLE: 8바이트 부동 소수점 수 (가변 소수점) DECIMAL (NUMERIC): 고정 소수점 수 (정밀도와 소수 자릿수 지정 가능) PostgreSQL SMALLINT: 2바이트 정수 (-32,768 ~ 32,767) INTEGER (INT): 4바이트 정수 (-2,147,483,648 ~ 2,147,483,647) BIGINT: 8바이트 정수 (-9,223,372,036,854,775,808 ~ 9,223,372,036,854,775,807) REAL: 4바이트 부동 소수점 수 (가변 소수점) DOUBLE PRECISION: 8바이트 부동 소수점 수 (가변 소수점) NUMERIC: 고정 소수점 수 (정밀도와 소수 자릿수 지정 가능) DECIMAL: NUMERIC 의 동의어 Oracle NUMBER: 가변 정밀도와 소수 자릿수를 갖는 숫자 (정밀도와 소수 자릿수 지정 가능) BINARY_FLOAT: 4바이트 부동 소수점 수 (가변 소수점) BINARY_DOUBLE: 8바이트 부동 소수점 수 (가변 소수점) FLOAT: NUMBER 의 동의어로 사용되며, 정밀도 지정 가능 CREATE TABLE DDL CREATE TABLE 테이블이름 ( 컬럼이름 datatype [DEFAULT기본값] [컬럼제약조건], 컬럼이름 datatype [DEFAULT기본값] [컬럼제약조건], … [테이블 제약조건] … ); table 이름은 reserbed word 는 사용 불가 이름의 시작은 문자로 그리고 A~Z, a~z, 0~9, _, $, # 문자만 허용 culumn 제약 조건 : 컬럼 제약조건: \\[CONSTRAINT이름\\] constraint_type table 제약 조건 : 테이블 제약조건: \\[CONSTRAINT이름\\] constraint_type(column,..) 제약조건 이름을 넣지 않으면 시스템이 알아서 임의 의 제약 조건의 이름을 만들어준다 제약조건 NOT NULL : 컬럼 제약조건만 가능 테이블 제약조건을 가능 하지 않음 UNIQUE : NULL 의 중복은 괜찮음, 자동으로 인덱스 생성 PRIMARY KEY = NOT NULL + UNIQUE 복합 컬럼의 경우 순서가 인덱스 생성시 사용되게 되고 자주 사용하는 column 을 순서가 앞쪽에 배치되어야 search 가 빠르다 #ModificationRequired (인덱스 원리를 넣어보자) FOREIGN KEY : 참조 무결성 제약 참조하는 PK 가 사라질 때 행동 양식을 정해 줄 수 있다 뒤에 붙여주면 된다 정하지 않고 PK 를 삭제하려고 할 때 그 PK 값을 참조하는 FK 가 참조오류가 발생해서 오류를 내뱉는다 ON DELETE CASCADE: 해당하는 FK를 가진 참조행도 삭제 ON DELETE SET NULL: 해당하는 FK를 NULL로 바꿈(PK 는 NULL 이 안되지만 이렇게 해서 정보를 유지시킨다 db 의 지속성에 문제를 일으키므로 주의해서 사용해야 한다) CHECK : 사용자 임의 조건 제약조건 추가 ALTER TABLE테이블이름 ADD CONSTRAINT … NOT NULL은 추가하지 못함 ALTER TABLE emp ADD CONSTRAINT emp_mgr_fk FOREIGN KEY(mgr) REFERENCES emp(empno); 제약조건 삭제 ALTER TABLE테이블이름 DROP CONSTRAINT제약조건이름 PRIMARY KEY의 경우 FK 조건이 걸린 경우에는 CASCADE로 삭제 해야함 ALTER TABLE book DROP CONSTRAINT c_emp_u; ALTER TABLE dept DROP PRIMARY KEY CASCADE; subquery 를 통한 테이블 생성 CREATE TABLE empSALES AS SELECT * FROM emp WHERE job = ‘SALES’; -- 일반적으로 임시 테이블을 만들 때 사용된다 -- not null 제약 조건만 상속 pk fk 제약 조건의 경우 사라진다 -- pk fk 가 들어가면 다른 테이블에 영향을 끼치기 때문에 의도적인 동작이다 ALTER TABLE DDL 컬럼 이름, constraint, datatype, default 등등 컬럼의 속성이 변경 대상이다 ADD : ALTER TABLE 테이블명 ADD (추가할 칼럼명과 속성들); => ALTER TABLE PLAYER2 ADD (ADDRESS VARCHAR2(80)) MODIFIY (이름을 제외한 컬럼의 속성 변경) : ALTER TABLE 테이블명 MODIFY (칼럼명1 데이터 유형 [DEFAULT 식] [NOT NULL], 칼럼명2 데이터 유형 ...); => ALTER TABLE PLAYER2 MODIFY (JOIN_YYYY VARCHAR2(8) DEFAULT '20020129' NOT NULL); DROP COLUMN : ALTER TABLE 테이블명 DROP COLUMN 삭제할 칼럼명; => ALTER TABLE PLAYER2 DROP COLUMN ADDRESS; ADD CONSTRAINT : ALTER TABLE 테이블명 ADD CONSTRAINT 제약조건명 (칼럼명); => ALTER TABLE PLAYER2 ADD CONSTRAINT PLAYER_FK2 FOREIGN KEY (TEAM_ID) REFERENCES TEAM(TEAM_ID); DROP CONSTRAINT : ALTER TABLE 테이블명 DROP CONSTRAINT 제약조건명 => ALTER TABLE PLAYER2 DROP CONSTRAINT PLAYER_FK2; NOT NULL 제약조건과 다른 제약조건의 차이 CREATE TABLE 예시테이블 ( 컬럼명1 데이터타입 [NOT] NULL, 컬럼명2 데이터타입deafault 10, ... CONSTRAINT 제약조건이름 PRIMARY KEY (컬럼명1, 컬럼명2), CONSTRAINT 제약조건이름 FOREIGN KEY (컬럼명2) REFERENCES 다른테이블(컬럼명), CONSTRAINT 제약조건이름 UNIQUE (컬럼명3), CONSTRAINT 제약조건이름 CHECK (조건식) ); 과 같이 table 수준의 제약조건에서는 NOT NULL 조건 부여 불가 ALTER TABLE PLAYER2 DROP CONSTRAINT PLAYER_FK2; ALTER TABLE table_name MODIFY column_name data_type NOT NULL; 과 같이 drop contraint 구문에서도 NOT NULL 조건 부여 불가 modify 구문에서 변경해야함 RENAME TABLE DDL RENAME : RENAME 변경전_테이블명 TO 변경후_테이블명 => ALTER TABLE PLAYER2 RENAME COLUMN PLAYER_ID TO TEMP_ID; DROP TABLE DDL Delete, Truncate, Drop Pasted image 20240515030865 컬럼 컬럼 추가 ALTER TABLE 테이블명 ADD (추가할 칼럼명); ALTER TABLE book ADD (pubs VARCHAR2(50)); 컬럼 수정 ALTER TABLE 테이블명 MODIFY (칼럼명1 데이터 유형 \\[DEFAULT 식\\] \\[NOT NULL\\], 칼럼명2 데이터 유형 ...) ALTER TABLE PLAYER2 MODIFY (JOIN_YYYY VARCHAR2(8) DEFAULT '20020129' NOT NULL) 컬럼 이름 수정 ALTER TABLE 테이블명 RENAME COLUMN 변경해야 할 칼럼명 TO 새로운 칼럼명; ALTER TABLE PLAYER2 RENAME COLUMN PLAYER_ID TO TEMP_ID; 컬럼 삭제 ALTER TABLE 테이블명 DROP COLUMN 삭제할 칼럼명; ALTER TABLE book DROP COLUMN author ; UNUSED 컬럼 (SET UNUSED -> DROP UNUSED COLUMES) ALTER TABLE boosek SET UNUSED (author); ALTER TABLE book DROP UNUSED COLUMNS; 임시로 컬럼을 사용하지 않는 상태로 대기해 두었다가 이후 안정성 확보 이후 완전히 지운다 이 쿼리는 오라클만 지원한다 comment dbms 는 테이블에 또는 컬럼에 즉 attributte 에 상세정보와 같은 comment 를 달 수 있다 oracle, postgresql comment 만들기 --생성시 에는 주석을 만들수 없음 COMMENT ON TABLE your_table_name IS 'your_comment'; -- table 주석달기 COMMENT ON COLUMN your_table_name.your_column_name IS 'your_comment'; -- column 주석달기 mysql commnet 만들기 create table test( pid INT NOT NULL COMMENT '제품ID', -- 생성시 열에 commnet ... ) comment '임시 테이블입니다' -- 생성시 table 에 commnet ALTER TABLE products COMMENT '상품정보'; -- 변경시 table에 comment ALTER TABLE products MODIFY prod_name VARCHAR(100) NULL COMMENT '제품이름'; -- 변경시 열에 commnet 확인은 모든 dbms 가 각각 다름 기타 DDL truncate table rename drop table select DML -- select 기본 구조 SELECT [ALL | DISTINCT] 열_리스트 [FROM 테이블_리스트] [USING(열_리스트) | ON 조건] [WHERE 조건] [GROUP BY 열_리스트 [HAVING 조건]] [ORDER BY 열_리스트 [ASC | DESC]]; sql 실행 순서 Pasted image 20240418150733 FROM절 (join) 기본적으로 FROM문에서 작동 Pasted image 20240514190508 cross join : cartasian product : 모든 조합 , 또는 join 이라고 적어도 같은말 sql 에서는 cross join 이다 inner join == null 이 안나오게 조건을 만족하는 모든 튜플 결국 의미상 theta join 과 비슷 theta join == 임의의 조건에 의한 조인 결국 의미상 inner join 과 비슷 equi join == theta join 의 조건이 같다는 조건일 떄 {left,right,full} outer join == 조건을 만족하지 않아도 null 과 함께 나옴 outer join 의 대한 이해/데이터베이스%20용어%20정의%20및%20relation%20algebra.md#^a02478) natural join == equi join(=) && 동일한 column명 중복 제거 self join 자기 자신과 조인 as 구문을 통해 임시로 attribute 이름을 만들 수 있다 old-name as new-name 의미의 명확성을 위해 조인 조건은 on 또는 using() 문을 통해 완성하자 theta join 이면서 equi join 이 아닌 경우 : 한 테이블에는 사람들의 정보가, 다른 테이블에는 도시의 정보가 저장되어 있다고 가정해봅시다. 이때, 사람들의 나이가 특정 도시에 속한 사람들의 평균 나이보다 큰 경우만을 선택하는 조건을 사용하여 두 테이블을 조인하고 싶다면, 이러한 경우에는 θ-join을 사용할 수 있습니다. 즉, \"=\" 대신에 \">\" 연산자를 조인 조건으로 사용합니다. WHERE 문 추가 조건 연산 문자열 매치 where name like '%dar%' % : 문자열과 매칭 _ : 하나의 문자와 매칭 like '100\\%' escape '\\' : excape 를 통해 special 문자를 일반 문자로 처리할 수 있다 즉 100% 문자열와 완벽히 일치하는 value 는 true 로 반환된다 null null 산술연산 = null ex) 5 + null = null null 비교 연산 = unknown : 새로운 진리값 만들어짐 OR (unknown or true) =true (unknown or false) =unknown (unknown or unknown) =unknown AND (true and unknown) =unknown (false and unknown) =false (unknown and unknown) =unknown NOT (not unknown) =unknown [!NOTE] 참조 where 절에서 unknown 값이 나오면 false 와 동일하게 처리한다 즉 where salary = null 이라고 쓰면 모든 값의 비교에서 unknown 이 반환되고 모든값이 false 와 같이 처리된다 그러므로 where salary is null 에서의 (is null)과 같은 새로운 is, is not 조건 연산자가 만들어졌다 sql 연산자 우선순위 Arithmetic operators : 산술 연산 Concatenation operator Comparison conditions : 비교 연산 IS \\[NOT\\] NULL, LIKE, \\[NOT\\] IN \\[NOT\\] BETWEEN NOT logical condition AND logical condition OR logical condition group by group by 에 참여하지 않은 attribute 는 논리상 select 문에서 그냥은 참여할 수 없다 having, order by 도 마찬가지 aggreate function parameter 로써는 가능하다 aggregate function 여러 행을 한행으로 결과치를 만들어 낸다 여러행을 한행으로 만드는 성질 때문에 1행만 나오게 되고 많은 경우에 실제 사용시에는 group by 와 함께 사용되게 된다 위의 where 절의 null 의 설명을 참조: count 를 제외한 avg, min, max, sum 의 경우에는 필연적으로 산술연산 또는 비교연산이 들어가게 되고 이러한 점 때문에 count 를 제외한 연산자의 경우에는 null 값을 제거하고 연산하게 된다 count 의 경우 count(\\*) => null 포함 모든 값 갯수 count(column 명) => null 제외 값의 개수 salary null null 10000 20000 10000 select count(*), count(salary), count(distinct salary), avg(salary), from '위의 테이블' -- 결과 : 5, 3, 2 having Aggregation 결과에 대해 다시 condition을 적용할 때 사용 order by 특이하게 논리상 이후에 수행될 select 문에 select student.name \"학생이름\" 처럼 alias 한 이름이 사용될 수 있다 group by 절에서 select 문의 alias 를 사용하지 못한다 그런데 order by 에서는 사용 가능하다 SELECT 문 산술 연산 사용 가능 set operator 두 질의에 결과에 대한 연산 수행 다른 연산과는 다르게 중복을 제거한다 중복을 제거하려면 union all 같이 all 을 붙인다 Pasted image 20240514220581 UNION \\[ ALL \\] : 합집합 EXCEPT \\[ ALL ]: 차집합 (minus : 연산은 몇몇 dbms는 지원하지 않는다(posgresql, msql) execpt 연산이 표준) INTERSECT \\[ALL] : 교집합 [!NOTE] 참조 다른 연산과는 다르게 중복을 제거한다 중복을 제거하려면 union all 같이 all 을 붙인다 r 의 어떤 튜플x가 m번 존재, s의 어떤 튜플 x가 n 번 존재한다면 r union s => x 1회 등장 r uinon all s => x m+n회 등장 nested subquery 쿼리 내부의 쿼리 select A1,A2, ...,An from r1,r2, ...,rm where P 이러한 sql 문이 있을 때 $A_i$ : 단일 값을 반환하는 subquery 로 대체될 수 있다 $r_i$ : 유효한 subquery 로 대체될 수 있다 p : B < [not] in,> (subquery) 형태의 표현식에 들어갈 수 있다 각각에 subquery 의 위치에 따라 가능한 형태가 다르고 표현도 다르다 이러한 것들의 별칭 또한 존재한다 Scalar subquery select 문에서 함수처럼 단일값을 반환하는 query select dept_name, ( select count(*) from instructor where department.dept_name =instructor.dept_name ) as num_instructors from department; deptname numinstructors 물리학과 6 컴공학과 12 철학과 8 from에서 tuple 이 하나하나씩 넘어 올 때 마다 dept_name 내부에 들어가는 값이 일종의 parameter 처럼 계속 바뀌면서 다른 출력을 낸다 inline view 임시 공간에 테이블을 생성하여 사용하는 View 와 비슷함 -- 평균 급여 42000 이상인 부서와 그 평균 급여 select dept_name,avg_salary from (select dept_name, avg (salary) as avg_salary from instructor group by dept_name ) where avg_salary > 42000; where 내부의 suquery 주로 테스트 용도로 사용됨 집합 \\[not\\] in 비교 some, all, \\[not\\] exits, unique for set membership : \\[not\\] in => 집합용 Pasted image 20240514230545 for set comparisons : some, all 비교용 Pasted image 20240514230562 Pasted image 20240514230502 Pasted image 20240514230528 for set cardinality : with 절 from 절 subquery inline view 의 발전된 방향으로서 재사용성: WITH 절을 통해 정의된 Common Table Expression(CTE)는 쿼리 내에서 여러 번 참조될 수 있습니다. 이는 복잡한 쿼리를 여러 부분으로 나누어 각 부분을 한 번만 계산하고, 그 결과를 여러 번 재사용함으로써 성능을 향상시킬 수 있습니다. 가독성과 유지 보수: 쿼리가 더 읽기 쉽고 이해하기 쉬워지므로, 성능 최적화를 위한 수정이 필요할 때 쉽게 변경할 수 있습니다. 잘 구조화된 쿼리는 성능 문제를 진단하고 해결하는 데도 도움이 됩니다. 실행 계획 최적화: 데이터베이스의 쿼리 최적화기는 WITH 절을 사용함으로써 더 효율적인 실행 계획을 생성할 수 있습니다. 이는 데이터베이스 시스템에 따라 다르지만, WITH 절이 제공하는 명확한 논리적 구조는 최적화기가 더 좋은 결정을 내리는 데 도움이 될 수 있습니다. with max_budget (value) as (select max(budget) from department) select department.dept_name from department,max_budget where department.budget = max_budget.value; INSERT INTO DML INSERT INTO 테이블이름 [(컬럼리스트)] VALUES (값리스트) Pasted image 20240514230503 UPDATE DML UPDATE 테이블이름 SET 변경내용 [WHERE조건] Pasted image 20240514235163 DELETE DML DELETE FROM 테이블이름 [WHERE조건]; DML 참고 Pasted image 20240514235350 Function Character Function : LOWER : 문자열를 받아서 모두 소문자로 UPPER : 문자열를 받아서 모두 소문자로 CONCAT : 문자열 2 개를 받아서 이어준다 SUBSTR(문자, 시작인덱스, 길이) : substring 시작 인덱스 부터 길이만큼 추출 LENGTH : 문자열을 받아서 길이를 반환 from dual 를 사용 LTRIM : LTRIM('xxxYYZZxYZ','x') = 'YYZZxYZ' RTRIM : RTRIM('XXYYzzXYzz','z') = 'XXYYzzXY' RTRIM('XXYYZZXYZ ') = 'XXYYZZXYZ' 우측 공백제거 TRIM : TRIM('x' FROM 'xxYYZZxYZxx') = 'YYZZxYZ' ASCII Number function ABS : Absolute 숫자의 절대값을 반환합니다. 예를 들어, -5의 절대값은 5입니다 MOD : Modulo 두 숫자를 나눈 나머지를 반환합니다. 예를 들어, 10 MOD 3은 1을 반환합니다. ROUND : Round 숫자를 반올림합니다. 예를 들어, ROUND(5.67)은 6을 반환합니다. TRUNC : Truncate 소수점 이하를 버립니다. 예를 들어, TRUNC(5.67)은 5를 반환합니다. SIGN : 숫자의 부호를 반환합니다. 양수이면 1, 음수이면 -1, 0이면 0을 반환합니다. CHR/CHAR : 아스키 코드 값을 해당 문자로 변환합니다. 예를 들어, CHR(65)는 'A'를 반환합니다. CEIL : 주어진 숫자보다 크거나 같은 가장 작은 정수를 반환합니다. 예를 들어, CEIL(4.2)는 5를 반환합니다. FLOOR : 주어진 숫자보다 작거나 같은 가장 큰 정수를 반환합니다. 예를 들어, FLOOR(4.8)는 4를 반환합니다. EXP : Exponential 주어진 숫자의 자연 지수 함수 값을 반환합니다. 예를 들어, EXP(1)은 약 2.718을 반환합니다. LOG : Logarithm 주어진 숫자의 로그 값을 반환합니다. 예를 들어, LOG(100, 10)은 2를 반환합니다 (밑이 10일 때). LN : Natural Logarithm 주어진 숫자의 자연 로그 값을 반환합니다. 예를 들어, LN(2.718)은 1을 반환합니다 POWER : 주어진 숫자를 제곱합니다. 예를 들어, POWER(2, 3)은 8을 반환합니다. SIN : COS : TAN : Date function SYSDATE : 데이터베이스 서버의 현재 날짜와 시간 EXTRACT : 날짜나 시간 값에서 특정 부분(연도, 월, 일, 시, 분, 초 등)을 추출합니다. 예를 들어, EXTRACT(YEAR FROM date_column) 은 주어진 날짜에서 연도를 추출합니다. TONUMBER(TOCHAR(d,'DD' 'MM' 'YY')) TO_CHAR(date_column, 'DD') : 날짜에서 일을 문자열로 추출합니다 TO_NUMBER(TO_CHAR(date_column, 'DD')) : 날짜에서 일을 숫자로 변환하여 추출합니다. TO_CHAR(date_column, 'MM') : 날짜에서 월을 문자열로 추출합니다. TO_NUMBER(TO_CHAR(date_column, 'MM')) : 날짜에서 월을 숫자로 변환하여 추출합니다. TO_CHAR(date_column, 'YY') : 날짜에서 연도의 마지막 두 자리를 문자열로 추출합니다. TO_NUMBER(TO_CHAR(date_column, 'YY')) : 날짜에서 연도의 마지막 두 자리를 숫자로 변환하여 추출합니다 Conversion function TO_CHAR : TO_CHAR(date_column, 'YYYY-MM-DD') mysql 안되노 TO_NUMBER TO_NUMBER('12345') TO_DATE / CAST: 문자열을 날짜로 변환합니다 TO_DATE('2024-05-17', 'YYYY-MM-DD') , CAST(some_column AS VARCHAR(20)) CONVERT : 한 데이터 타입을 다른 데이터 타입으로 변환 CONVERT(VARCHAR, some_date, 23) Null-Related function NVL/ISNULL : NVL(column, 'default') default 반환 첫 번째 인수가 NULL인 경우 두 번째 인수를 반환 NULLIF :: NULLIF(a, b) 는 a 와 b 가 같으면 NULL을 반환하고, 다르면 a 를 반환 COALESCE : Coalesce COALESCE(column1, column2, 'default') 는 column1 이 NULL이 아니면 그 값을 반환하고, NULL이면 column2 를 반환하며, column2 도 NULL이면 'default'를 반환합니다. 인수 중에서 첫 번째로 NULL이 아닌 값을 반환합니다 case 표현식 CASE SIMPLE_CASE_EXPRESSION 조건 ELSE 표현절 END CASE SEARCHED_CASE_EXPRESSION 조건 ELSE 표현절 END DECODE(expr, value1, return1 [, value2, return2, ... , default]) --Oracle Only. SELECT ENAME, EMPNO, MGR, CASE some_column WHEN 'value1' THEN 'result1' WHEN 'value2' THEN 'result2' ELSE 'default_result' END FROM EMP; SELECT ENAME, EMPNO, MGR, CASE WHEN some_column > 100 THEN 'Greater than 100' WHEN some_column = 100 THEN 'Equal to 100' ELSE 'Less than 100' END FROM EMP; 두 table의 동일 속성에서 누가 fk 가 될것인가 fk 로 엮여있는 table 에서 pk를 어떻게 설정할 것인가 student 와 instructor 의 advisor 의 예",
      "frontmatter": {
        "tags": [
          "database",
          "sql"
        ],
        "description": "대학 수업 database 에서 배운 모든 내용 정리",
        "series": "database(university)",
        "series_weight": "2",
        "date": "2024-04-19T00:30:00+09:00",
        "lastmod": "2024-04-19T00:30:00+09:00"
      }
    },
    "데이터베이스 디자인": {
      "path": "/06.university/databaseuniversity/데이터베이스-디자인/",
      "filename": "데이터베이스 디자인",
      "content": "나쁜 디자인과 좋은 디자인 $R = in\\dept(ID, name, salary, dept\\name, building, budget)$ 나쁜 디자인 정보의 중복: 중복 데이터는 아래의 이상현상에서 자세히 나타난다 dept_name ->building, budget : functional dependency 관계로 building, budget 은 정보의 중복이 발생할 수 있다 이상 현상: Anomaly Insertion Anomaly:loan-number 없이branch-name 등 insert 불가 Deletion Anomaly: 어떤 branch의 마지막 loan을 delete하면 branch 자체가 사라짐 Update Anomaly: 어떤 특정한 branch의 정보(예: Downtown의assets) 를 update하면 해당되는 튜플들을 모두 업데이트해야 함 한개라도 실패하면 안된다 Decomposition 이 필요하다는 결론 중복 데이터를 최소화 정보를 보존 하면서 분리 함수 종속성 유지 (BCNF) functional dependency 는 현실의 실세계의 돌아가는 법칙을 반영한 것이다 정규화 1NF : 데이터는 원자적이어야 한다(Domain is atomic if its elements are considered to beindivisible) 2NF second nomal form 3NF BCNF 4NF 1정규화 앨범이름 가수명 곡명 1집 에일라 A,B,C,D ... 1FN : 테이블을 구성하는 모든 요소들이 atomic(원자적이다) 즉 분해 불가능하다 위의 엘범 테이블의 경우 곡명에 여러가지 곡이 들어가 있고 만약 각각을 parsing 해서 사용하게 되면 첫번째 정규화 규칙을 지키지 못하게 된다 하지만 여기서 나는 곡명을 파싱하지 않고 그냥 통째로만 사용할 꺼야 라고 한다면 제 1정규화가 되어있지 않다고 말할 수 없다 손실분해 Lossy Decomposition 분해를 할 때 정보를 모두 저장되어 있는 채로 분리할 수 있어야 한다 Branch = (bname, bcity, asset), Loan = (loan#, c_name, amount) 문제점: 상관 관계가 없어짐 (연결 정보의 손실) Called a Connection Trap Branch = (bname, bcity, asset), Loan = (loan#, cname, amount, bcity) natural join시 tuple 증가 information(relationship) 상실 잘못된 상관 관계(b_city) Pasted image 20240613041271 정확하지 않은 정보가 늘어나는 것 또한 손실분해라고 한다 즉 분해를 할 때 Lossless-join Decomposition 을 해야한다 분해를 하면 자연조인시(공통 attribute 가 존재할 때) 본래의 원본 테이블보다 크거나 같다 같을 때 만을 Lossless-join Decomposition 이라고 한다 Pasted image 20240613041954 분해를 할 때 아래의 것을 본래는 고려하면서 분해를 해야한다 Functional Dependencies multivalued dependencies (we will not cover this!) : 어떤 함수 종속도 존재하지 않은 분해의 무손실 분해를 보장할 수 있는 다중값 종속 기능적 종속성 functional dependency a -> b a functionally determines b a, b : set of attributes 집합이 가능하다 two tuples with same a have same b a가 정해지면 반드시 b가 정해진다. a 값이 같은데 b 값이 다를 수 없다 실제 Application의 규칙에 의해 정해짐(실 세계의 규칙을 반영) 예시) student 테이블 student = {ID, name, gen, ...} ID -> name gen ... ID 는 이름 성별 등등을 결정한다 즉 id가 같은 tuple 은 다른 이름을 가질 수 없다 trivial 이라는 것은 customer-name, loan-number →customer-name customer-name →customer-name 당연히 기능적 종속성을 만족하는 관계이다 closure set of functional dependency F : 함수 종속의 집합 F+ : F 로 부터 추론 가능한 모든 집합 attribute set closure (AG)+ 는 AG 로 만들 수 있는 모든 Attrubute 집합이다 AG 가 후보키 인지 확인법 슈퍼키가 맞는가 즉 AG 를 통해 R 을 구성할 수 있는가 AG 의 부분집합이 존재하는가 함수 분해를 통해 무손실 분해를 증명할 수 있다 Lossless-join Decomposition Revisited R1 과 R2 의 교집합 attribute 가 superkey 일 때 원본으로 되돌릴수 있다 즉 분해할 때 superkey 를 각각 가져가야 한다 3단계일때 Boyce-Codd Normal Form BCNF trivial 하지 않은 모든 함수종속에서 결정자가 superKey인 경우 BCNF Third Normal Form: Motivation meterialized view from 의 경우 미리 1번만 조인 시켜서 작동하게 되는데 즉 1회성으로 메모리에 하지만 meterialized view 는 물리적으로 보조메모리에 저장시킨다 BCNF 는 데이터중복을 방지하여 어노몰리 문제가 사라지지만 함수 의존성을 보장하지 못할 수도 있다 그래서 3NF 를 사용하여 데이터 중복이 되더라도 즉 어노몰리 문제가 발생하더라도 함수의존성을 보존하는 것을 목표로 한다 분해 문제 풀어보기 3nf 는 일정수준의 중복이 필연적으로 발생하게 된다",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "description": "대학 수업 database 에서 배운 모든 내용 정리",
        "series": "database(university)",
        "series_weight": "4",
        "date": "2024-06-04T15:11:00+09:00",
        "lastmod": "2024-06-04T15:11:00+09:00"
      }
    },
    "데이터베이스 용어 정의 및 relation algebra": {
      "path": "/06.university/databaseuniversity/데이터베이스-용어-정의-및-relation-algebra/",
      "filename": "데이터베이스 용어 정의 및 relation algebra",
      "content": "기본 용어 정의 attribute : culume(열)의 이름 rable(레이블) domain(도메인) : attribute가 가질수 있는 값(value)들의 집합(null 은 모든 domain 의 원소이다) row == record == tuple : 가로 value 의 집합 entity : 하나의 tuple 을 통해 표현하고 싶은 객체 arity : attribute 의 개수 relation(table) : tuple 의 집합 ( table 을 통해 표현하는 것은 entity 간의 차이를 나타내므로 관계 라고 명칭 ) database : table 의 집합 NULL 은 0 이나 \"\" 이 아님 nonvalue 임 모든 domain 은 명시하지 않는 한 NULL 을 포함 Pasted image 20240312171627 schema : 논리적 구조 일종의 data type == 테이블의 기본적 구조 == attribute 가 모두 동일 모든 attribute 의 domain instance : 특정시점의 entity 들의 상태(dbms 의 상태) Pasted image 20240312171654 key : tuple 을 구별하기 위한 Attribute 조합의 집합 tuple을 구별하지 못해도 만들어진 목적은 tuple 을 구별하기위해 만들어짐 결국 모든 Attribute 의 모든 가능한 부분 집합 Super key : relation 에서 tuple 을 구별하기 위한 unique 한 Attribute 의 집합 attribute 를 여러개 묶던지 tuple 들이 모두 구별만 되면 성립 Candidate Key(후보키) : Superkey 중 minimal 한 key 집합 attribute 를 하나만 빼더라도 tuple 들이 구별이 되지않는 집합의 원소가 최소가 되는 집합 Primary key : Candidate key 중 하나 (relation 을 정의할 떄 선택) NULL 값 부여 불가 Foreign key : 타 table의 value 을 참조하는 Attribute 집합 참조하는 주체는 key 가 아닐 수 있지만 참조되는 객체의 value 는 primary key 의 value reference integrity :참조된 table의 primary key 의 값들 중 하나이거나 NULL만 가능하다 Pasted image 20240312173334 학생 table 에서 super key : {학번, 이름, 주민등록번호} , {학번}, {주민등록번호, 지도교수} .... 등등 candidate key : {학번} , {주민등록번호} primary key : 학번 => db 설계자가 임의 지정 forigen key : 지도교수 교수 table 에서 super key : {교번, 이름} {교번} {교번, 학과} candidate key : {교번} forigen key : 없음 표기 관습 relation 은 소문자 r relation 의 스키마는 대문자 R 속성의 집합은 그리스 영문자 $\\alpha, \\beta$ 속성의 집합이 슈퍼키일 때 K : K 는 r(R) 의 슈퍼키 r 이라고 하면 특정 시점의 r(R) 의 인스턴스 값을 표시한다 Entity - relationship model table 들의 관계로 얽혀 있는 database 를 관계를 중심으로 보기편하게 시각화를 시도하는 모델 위에서 학생과 교수의 table 의 관계를 예시로 들자 학생의 attribute들 중 지도교수라는 attribute 가 있고 들어갈 수 있는 value 는 교수의 primary key 인 교번이 들어가게 된다 이때 이러한 테이블의 관계를 아래처럼 표현한다 Pasted image 20240312174445 둘의 관계는 advisor 관계이며 이 관계를 visual 하게 보여줄 수 있다 database 의 수학적 해석 $공식적으로, 도메인 집합 (D1, D2, \\ldots, D_n)이 주어졌을 때$ $관계 (r)은 (D1 \\times D2 \\times \\ldots \\times D_n)의 부분집합입니다.$ $따라서, 관계는 (n)-튜플 ((a1, a2, \\ldots, an))의 집합이며, 여기서 각 (ai)는 (D_i)에 속합니다.$ database language Data Definition Language (DDL) create Database Schema를 정의하는 언어 create table (table 생성) drop table r alter table r alter table r add A D alter table r drop A ( 지원하는 dbms 별로 없음 ) Data Manipulation Language (DML) modification Database의 data를 조작(schema는 불변)하는 언어 Retrieve(조회) : select {} from {} where ... Insert(저장) : insert into r values (smith, 00102 ...) Delete(삭제) : delete from r where ... Change(변경) : Data Control Language (DCL) system Database의 constraint를 제어하는 언어 priviliege 여기서 말하는 제약조건은 attribute value 의 제약조건(not null 등 )이 아닌 db 사용자 관리 접근 ip 허용 등 아닌 시스템적인 제약조건을 말한다 Procedural vs. nonprocedural sql 이 대표적인 nonprocedural 언어이다 내부적으로는 sql 문은 해석하여 procedural 로 변환하여 순차적 처리를 위한 언어 Transaction Management(트랜젝션) 계좌 A 에서 계좌 B 로 50 달러를 옮기는 상황을 생각해 보자 논리적인 행동 하나로 보이지만 프로그래밍에서는 계좌 A 의 50 달러를 전체 남아있는 계좌 액수 에서 빼는 행동, 계좌 B 에 50 달러를 추가하는 행동 이 있다 이때 A 계좌에 남아있는 계좌의 액수가 부족 또는 네트워크 불완전 등 일련의 문제 발생으로 인해 하나의 행위가 실패했다고 가정하자 이 때 하나의 행동만 성공 적으로 처리되게 된다면 문제가 심각해진다(둘다 못하는 것이 낮다) 그래서 두가지의 행동을 하나로 묶어 실패 할꺼면 둘다 실패하던가 성공할시 둘다 성공해야하는 것으로 두개의 논리적 행동을 하나의 논리적 행동으로 묶어 처리할 필요가 있는데 이렇게 개별적이고 분할할 수 없는 작업으로 구분되는 정보 처리 과정을 Transaction 이라고 한다 DB 또한 이러한 것을 지원한다 정규화 이론 Pasted image 20240312173334 Pasted image 20240314151338 위의 그림과 아래의 그림을 비교해볼때 위는 table을 적절히 나눈 경우 모두 하나의 table 로 관리하는 경우의 예시이다 예를 들어 physics 의 예산을 7000 에서 8000 으로 변경할 때 id 33456 tuple 또한 바꾸어야 하는데 이렇게 중복이 발생하게 되면 성능상 오버헤드를 낳는다 비정규화 된 데이터베이스의 문제 정보가 중복 저장 => 보조 기억 장치 오버헤드 업데이트시 여려개의 tuple 을 건드려야 한다 데이터베이스 설계 방식 ER (Entity - relationship) model 통하여 설계를 접근한다 Normalization Theory(정규화 이론) : 어떤 디자인이 나쁜 디자인 인지 공식화 학교 시스템 데이터베이스/학교%20시스템%20데이터베이스.md) Pasted image 20240315140348 화살표 기준 출발 하는 부분이 Foreign attribute relation algebra Procedural language sql(non procedural) 문이 내부적으로 처리하는 언어 Six basic/primitive operators select: $\\sigma_c(r)$ 선택 => 수평 선택 c 조건을 만족하는 R table tuple 의 부분 집합을 선택한다 project: $\\Pi_a(r)$ 투영 => 수직 선택 cartesian product: $r \\times s$ tuple의 가능한 모든 조합 (동일 attribute 일 경우 r table 의 rB, s table dml sB 라고 표기하여 두집합의 attribute를 다르게 판단한다) rename: $\\rhos(r)$ 또는 $\\rho{s(A1,A2,A3)}(r)$ rename 이름을 다시 짓는다 R을 S로 여기서 table 이름만 재설정 할 수도 attribute 의 이름을 재설정 할 수도 있다 additional operation set intersection(교집합): $r\\cap s = r -(r-s)$ Natural join(자연 조인): $r \\bowtie s = \\sigma{r.A1 = S.A1 \\land r.A2 = s.A2 \\land \\ldots \\land r.An = s.A_n}(R \\times S)$ product 중복 제거 cartecian product 에서 동일 attribute 또는 동일 domain 의 값을 Theta join(세타 조인) Equi join(등가 조인): $r \\bowtie s$, $r\\bowtie\\theta s = \\sigma\\theta(r \\times s)$ select + product 지정된 조건을 만족하는 모든 튜플 조합 특히 조건에 = 등호의 조건이 들어갈 때 equi join 이라고 부른다 Assignment Operation(대입 연산) : $temp1 \\leftarrow r \\times s$ 일종의 변수 설정 Outer join(외부 조인) : ⟕, ⟖, ⟗ 좌측 우측 양쪽 순서로 table 을 null 값이 들아가더라도 join 하여 projection 한다 netraul join 과 inner join 의 차이 netural join 의 경우 동일한 attribute 이름을 기준으로 서로를 묶지만 inner join 의 경우 임의로 어떠한 attribute 를 같은 기준으로 join 할지 선택할 수 있다 outer join 에 대한 이해 ^a02478 cartasian product 의 경우 모든 가능한 조합을 모두 표기하기 때문에 이러한 일이 발생하지 않는다 하지만 동일한 두개의 attribute value 하나의 value 로 join (netural join 등) 할 때 가능하지 않은 조합 null 도 표기하고 싶을 때 문제가 발생한다 즉 정보 손실을 방지하는 조인 연산의 확장 additional operation의 primitive opration 을 통한 구현 $r \\bowtie s = \\pi{R.A1, R.A2, \\ldots, R.An, S.B1, S.B2, \\ldots, S.Bm}(\\sigma{R.Ai = S.Ai}(r \\times s))$ NULL null 의 산술연산은 null 이다 Aggregate function ( 집계 함수 ) 는 avg max 등 계산시에 null을 무시한다 비교 연산에서 NULL 과의 비교는 새로운 진리값 (unknown) 을 반환한다 (\"1 어떠한 그룹(단위)로 연산을 적용할 것이냐 F1(A1): 연산적용을 통해 새로 만들어진 attribute value 들의 평균, 최소, 최대, 합, 개수 를 새로운 attribute 로 하여 table 을 생성 ex) ${dept\\name} \\Gamma{avg(salary)\\ as\\ avg\\sal}(R)$ : deptname, avgsal attribute 를 갖는 table advanced topics Division(): $r\\div s$ table S의 모든 tuple 과 관련있는 R 의 tuple 반환 여러 행으로부터 단일 결과 값을 계산하는데 사용된다 Pasted image 20240319200523 Pasted image 20240319200928 Pasted image 20240319201229 Pasted image 20240406020148 Pasted image 20240406020200 Pasted image 20240406020429 Pasted image 20240509120599 Pasted image 20240409194736 Pasted image 20240408083433 Pasted image 20240408083443 참조 무결성 제약 조건과 외래키 제약 조건 => $참조 무결성 제약 조건 \\supset 외래키 제약 조건$ 제약 조건이란 데이터의 입력 조건을 말한다 외래키 제약 조건 : 참조하는 값이 참조되는 값의 attribute의 domain 중 하나에 나와야 한다 & 참조되는 값은 pk 의 구성 속성중 하나이다 참조 무결성 제약 조건 : 참조하는 값이 참조되는 값의 attribute의 domain 중 하나에 나와야 한다 pk 구성요소라는 추가되는 조건에 의해 외래키 제약 조건이 더 엄격하다 현재 dbms 는 외래키 제약조건은 지원하지만 참조되는 속성이 주 키가 아닌 참조 무결성 제약조건은 언제나 깨질 수가 있다 즉 두 관계의 차집합의 사례에 대해서는 지원하지 않는다 procedural language (relation algebra) 문제 Pasted image 20240325011946 컴퓨터공학과, 교수, 교번 관계는 교수 table 만으로 확인 가능하다 $\\Pi_{\\text{교수.교 번,교수.이름}}(\\sigma_{\\text{교수.학과} = \\text{\"컴퓨터공학과\"}}(교수))$ and 의 표시는 , 가 아닌 $\\land$ 표기가 맞다 학과명 학과장 이름 교수 table 과 학과 table 을 동시에 확인해야 한다 $\\Pi{\\text{교수.학과,교수.이름}}(\\sigma{\\text{교수.학과=학과.학과명,교수.교번=학과.학과장 }}(교수 \\times 학과))$ //product 구현 $\\Pi_{\\text{교수.학과,교수.이름}}({\\text{교수}}\\bowtie{\\text{학과}} )$ // join구현 최적화가 가능한가?? primitive 연산으로는 불가능하지 않나?? 학생 교수의 번호, 이름 => 학생 교수 table 동시 확인(rename 후 합집합) $(\\rho{\\text{학생(번호,이름)}}(\\Pi{학생.학번, 학생.이름}(학생)))\\cup(\\rho{\\text{교수(번호,이름)}}(\\Pi{교수.교번, 교수.이름}(교수)))$ $\\cup, -, \\times, \\bowtie$ binary 연산 이후 table 의 이름은 무었인가?? 교집합 $R \\cap S$ 을 primitive operation 으로만 표현하라 $R \\cup S - (R-S) - (S-R) = R \\cap S$ natural join $R \\bowtie S$ 을 primitive operation 으로만 표현하라 $R \\bowtie S = \\sigma{R.A1 = S.A1 \\land R.A2 = S.A2 \\land \\ldots \\land R.An = S.A_n}(R \\times S)$ 가장 높은 연봉을 받는 교수의 이름을 말하시오 모든사람과 비교시 한번이라도 연봉이 딸리는 사람 $\\Pi{instructor.name}(\\sigma{instructor.salary [!problem] select $A1, A2, A3 \\dots An$ from $r1, r2, r3, \\dots rn$ where P $\\Pi{A1, A2, A3, \\dots An}(\\sigmaP(r1 \\times r2 \\times \\dots \\times r_n))$ [!Problem] select $A1, A2$ sum($A_3$) from $r1, r2, r3 \\dots rm$ where P group by $A1, A2$ ${A1, A2}\\Gamma{sum(A3)}(\\sigmaP(r1 \\times r2 \\times \\dots \\times r_m))$ multiset : 컴퓨터/통신 멀티셋, 동일 항목이 여러 개 출현하는 것을 허락하는 집합체. 예시 1: 최소한 두 종류의 상품을 구매한 고객 찾기 Query 1 $(\\pi{CustomerName}(\\sigma{ProductName=\"Book\"}(Purchases)) \\cap \\pi{CustomerName}(\\sigma{ProductName=\"Laptop\"}(Purchases)))$ Query 2 $(\\pi{customer-name, product-name} (Purchases) \\div \\rho{temp(product-name)} ({(\"Book\"), (\"Laptop\")}))$ 예시 2: 두 지점에서 대출을 받은 고객 찾기 Query 1 (\\pi{CN}(\\sigma{BN=\"East Branch\"}(borrower loan)) \\cap \\pi{CN}(\\sigma{BN=\"West Branch\"}(borrower loan))) Query 2 (\\pi{customer-name, branch-name} (borrower loan) \\div \\rho{temp(branch-name)} ({(\"East Branch\"), (\"West Branch\")}))",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "description": "대학 수업 database 에서 배운 모든 내용 정리",
        "series": "database(university)",
        "series_weight": "01",
        "date": "2024-03-12T15:50:00+09:00",
        "lastmod": "2025-09-05T21:00:03+09:00"
      }
    },
    "데이터베이스 인덱스": {
      "path": "/06.university/databaseuniversity/데이터베이스-인덱스/",
      "filename": "데이터베이스 인덱스",
      "content": "인덱스 : data에 빠르게 접근하기 위한 수단 기본 인덱스 파일(index entriy)의 구조 Pasted image 20240506050531 search key 검색키 : record 를 찾는 데 사용되는 속성이나 속성들의 집합 인덱스의 종류 Ordered indices : record가 정렬된 순서와 동일하게 정렬된 search key Hash indices : buckets 범위 안에서 값이 일정하게 분배로 된어 있다 값이 할댕 인덱스 평가 지표 Access types(접근 유형) 지정된 value 를 통해 즉각적으로 record 를 찾기 지정된 value 를 통해 범위를 통해 record 를 찾기 Access time(접근 시간) : 특정한 data 혹은 data 집합 접근에 걸리는 시간 Insertion time(삽입 시간) 새로운 데이터 삽입 + 인덱스 구조 업데이트 Deletion time(삭제 시간) 데이터 삭제 + 인덱스 구조 업데이트 Space overhead(공간 부담) : 인덱스 구조가 차지하는 추가적인 공간 Ordered index primary index (clustering index) : 검색 키의 값을 파일과 동일하게 정렬된 순서로 저장 일반적인 상황에서는 primary index 의 search key 가 primary key 인 경우가 많지만 무조건 그럴 필요는 없다 secondary index (non clustering index) : 검색 키의 값과 파일의 순서가 동일한 순서가 아님 Dense index, sparse index Dense index : index file(index entry)에 모든 search key 값이 나타난다 access time 적어짐 insetion time, Deletion time 커짐 space overhead 커짐 spase index : index file(index entry)에 search key 값이 단지 몇 개만 나타나다 access time 많아짐 insertion time, Deletion time 작아짐 space overhead 작아짐 두개의 조합 Pasted image 20240506100581 primary index 이면서 dense index : 데이터의 정렬 순서와 맞으면서 모든 search key 값이 index entry 에 출현Pasted image 20240506100547 primary index 이면서 sparce index : 데이터의 정렬 순서와 맞으면서 search key 값이 몇 개만 출현Pasted image 20240506100528 1번과 2번의 비교 즉 dence 와 sparce 의 비교 primary & Dense Pasted image 20240506100547 primary & sparce Pasted image 20240506100528 leafnode 는 덴스인덱스 루트에서 leaf 까지는 미만이면 좌측 이상이면 우측 leaf 에서는 모든 값을 가지고 있다 DENSE leaf 에서 실제 값을 찾을 때는 좌측에 있다 노드의 하나의 크기는 운영체제가 관리하는 block 과 동일하다 tree 의 깊이를 최대한 낮추어야 한다 All paths from root to leaf are of thesame length Each node that is not a root or a leaf has betweenn/2 andn children (n = number of pointers in a node) A leaf node has between (n–1)/2 andn–1 values 모든 루트에서 리프까지의 경로 길이가 동일하다 (All paths from root to leaf are of the same length) 루트나 리프가 아닌 각 노드는 n/2에서 n개의 자식 노드를 가진다 (Each node that is not a root or a leaf has between n/2 and n children, where n is the number of pointers in a node) 리프 노드는 (n–1)/2에서 n–1개의 값을 가진다 (A leaf node has between (n–1)/2 and n–1 values) 특수 경우 (Special cases) a. 루트가 리프가 아닌 경우, 최소한 2개의 자식 노드를 가져야 한다 (If the root is not a leaf, it has at least 2 children) b. 루트가 리프인 경우, 0에서 (n–1)개의 값을 가질 수 있다 (If the root is a leaf, it can have between 0 and (n–1) values) B-tree B+-tree 에 비한 장단점 Advantages of B-Tree indices: may use less tree nodes than a corresponding B+-Tree sometimes possible to find search-key value before reaching leaf node Disadvantages of B-Tree indices: only a small fraction of all search-key values are found early 일부분만 일찍 찾는다 non-leaf nodes are larger, so fan-out is reduced → B-Trees typically have greater depth than corresponding B+-Tree b-tree b+-tree 에 비해 non leaf 노드는 각 key 들의 포인터 뿐만 아니라 자식 노드 포인터도 가지고 있으므로 노드에서 엣지로 나아가는 선(fan out)들이 적어진다 선이 적어진다는 말은 tree 의 높이가 높아지게 되고 이것은 성능상 불리한 점을 가져온다 insertion and deletion are more complicated than B+-Trees implementation is harder than B+-Trees Typically, advantages of B-Trees do not outweigh disadvantages! 멀티키 접근 하나의 key를 기준으로 가져온후 조건을 확인하거나 두개의 key를 각각 가져와서 교집합 하던가 후자가 더 느린 경우가 생긴다 인덱스를 쓴다고 빨라진다고 보장할 수 없다 그래서 composite search keys 생김 두개의 search key를 사용해서 찾으므로 비트맵 인덱스 online analytics 데이터 분석용 db?? b+-tree 도 아니다 성능이 낮은 쿼리는 무었인가 인덱스가 2개에서 쿼리 순서에 따른 문제",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "description": "대학 수업 database 에서 배운 모든 내용 정리",
        "series": "database(university)",
        "series_weight": "3",
        "date": "2024-04-28T02:51:00+09:00",
        "lastmod": "2024-04-28T02:51:00+09:00"
      }
    },
    "데이터베이스 질문 목록": {
      "path": "/06.university/databaseuniversity/데이터베이스-질문-목록/",
      "filename": "데이터베이스 질문 목록",
      "content": "[!question1] unary 연산이후에는 rename 을 제외하고는 table의 이름이 바뀌지 않았다 하지만 $\\cup, -, \\times, \\bowtie$ 등의 binary 연산 이후 table 의 이름은 무었인가요?? [!question] not null 은 제약조건에 속하는데 왜 modify 에서 변경하며 add,drop constraint 구문에서는 not null 변경이 안되는 이유 CREATE TABLE 예시테이블 ( 컬럼명1 데이터타입 [NOT] NULL, 컬럼명2 데이터타입, ... CONSTRAINT 제약조건이름 PRIMARY KEY (컬럼명1, 컬럼명2), CONSTRAINT 제약조건이름 FOREIGN KEY (컬럼명2) REFERENCES 다른테이블(컬럼명), CONSTRAINT 제약조건이름 UNIQUE (컬럼명3), CONSTRAINT 제약조건이름 CHECK (조건식) ); 과 같이 table 수준의 제약조건에서는 NOT NULL 조건 부여 불가 ALTER TABLE PLAYER2 DROP CONSTRAINT PLAYER_FK2; ALTER TABLE table_name MODIFY column_name data_type NOT NULL; 과 같이 drop contraint 구문에서도 NOT NULL 조건 부여 불가 default 구문도 제약조건인건가요?? Pasted image 20240520135017 Pasted image 20240613053258 애초에 말이 안되는 함수 종속성 규칙이지만 여기서 처음부터 F 가 F={이름 -> 시 ; 이름 -> 도} 라고 주어진다면 좌측처럼 분해된다면 lossy decomposition이고 우측이라면 무손실 분해이다 Pasted image 20240613153865 위는 AG가 후보키가 된다는 증명과정 그렇다면 AG 를 찾는 과정은 모든 조합을 다 확인해야 하는다2^6 -1 Pasted image 20240613153803 3NF 의 2번째 정의?? Pasted image 20240613055581 이것의 후보키를 구해보자",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "description": "대학 수업 database 에서 배운 모든 내용 정리",
        "series": "database(university)",
        "date": "2024-12-20T05:30:00+09:00",
        "lastmod": "2024-12-20T05:30:00+09:00"
      }
    },
    "데이터베이스2 ER model": {
      "path": "/06.university/databaseuniversity/데이터베이스2-er-model/",
      "filename": "데이터베이스2 ER model",
      "content": "27, 51, 52page entity 전제조건 : ER 모델 추상화된 모델로서 name = (firstname, middleinitial, last_name) 이렇게 여러가지 속성들을 같이 가지고 있는 속성을 분해하지 않고 1개의 속성으로 나타내도 된다 Pasted image 20241010093184 ER 모델 entity set entity set ~= relation(table) => customer, Account value set ~= domain (entity 가 가질 수 있는 value 들의 집합) => {이름들, A-\\\\\\*} simple attribute vs composite attribute simple : 분해 불가능한 속성 (id, dept) composite : 분해가능한 속성 (이름: 성과 이름 2개로 이루어짐) single-valued attribute vs multivalued attribute single-valued : 아이디, 소속학과 multivalued attribute : 아이들{철수, 영희} Derived attributes : 계산하기 전 : 만나이 = (현재 날짜 - 생일) ER 모델 Relationship Relationship Sets : Entity들의 상호연관관계 => depositor Role : Relationship sets에서 각 entity의 역할 => access-date ER 모델 Weak entity set Pasted image 20241010105936 1 to many, partial total 자체적으로 독립하기 어려운 집합 weak entity set 의 master entity 가 정해저야 정해진다 weak entity set 의 pk = master entity set 기본키 + 구별자 식별과 존재를 모두 의존 실무모델링에서는 식별관계 weak relation 관계이다 ER 모델 Relationship Cardinality(관계 수) Pasted image 20241010093734 Relationship Cardinality(entiy 가 몇개냐?): 엔터티와 연관될 수 있는 엔터티의 수 : one to many : instructor vs student : 1명의 교수는 여러 학생을 지도한다 반대로 1명의 학생은 1명의 지도교수를 가진다 one to one : 와이프 vs 남편 : 이혼이 없다는 가정하에 ER-model : one 측에 화살표(->), many 측에 직선이 그어진다 erwin : participation 참여율 : 모든 entiy가 relationship에 참여하는 개념인가 왼쪽 그림은 a라는 엔터티 집합은 b라고 하는 엔터티의 집합과의 관계에 partial participation을 하고 있고 b의 집합은 a 집합과의 관계에 total participation을 한다 오른쪽 그림은 a라는 집합은 b라고 하는 엔터티의 집합과의 관계에 total participation을 하고 있고 b의 집합도 a 집합과의 관계에 total participation을 한다 total total 은 dbms 에서 구현 불가능 ER-model : 두줄 직선은 total participation, 한줄직선은 Partial participation 이다 erwin 에서는 : Pasted image 20241010094699 1:many 관계에서 relationship entity set 은 many entity set 에 붙는다 ER diagram for a university database Pasted image 20240923143356 instructor student 둘의 관계는 one to many : 한명의 교수는 여러명의 학생을 지도교수로 하고 있다 한명의 학생은 한명의 지도교수를 가진다 교수는 advisor에 partial 하고 있다 : 교수는 학생을 한명도 지도하지 않을 수 있다 학생은 advisor에 total 하고 있다 : 학생은 지도 교수를 한명도 안 가질 수 있다 strong strong instructor department 둘의 관계는 many to one : 한명의 교수는 하나의 학과를 가지고 있다 하나의 학과는 여러명의 교수가 있다 학과는 inst_dept 에 partial 하고 있다 : 학과는 교수를 가지지 않을 수 있다 교수는 inst_dept 에 total 하고 있다 : 교수는 학과를 무조건 갖는다 many측에 즉 교수측에 dept_name 이 fk 로 들어가면 table 을 따로 만들지 않아도 된다 course section 둘의 관계는 one to many : 한 과목은 여러개의 강좌가 개설될 수 있다 한 강좌는 1개의 과목이다 과목은 course_sec 에 partial 하고 있다 : 과목으로 개설된 강좌가 없을 수 있다 강좌는 course_sec 에 total 하고 있다 : 모든 강좌는 과목을 가진다 strong weak : 과목은 강좌 없이도 구별될 수 있다, 강좌는 과목 없이는 구별될 수 없다 ER diagram -> relation schemes 결론 이진관계기준!!! many to many 다대다 관계 : 관계집합 \bentity 생성시 : 양 엔티티 집합의 기본 키의 합이 관계집합 최소 슈퍼키가 되어 이것을 기본 키로 선택 관계집합 entity 생성안할 때 : 브릿지를 만들지 않는한 불가 one to many 일대다 및 다대일 관계 : 관계집합 entity 생성시 : \"많은 쪽 엔티티 집합\"의 기본 키가 관계 집합 최소 슈퍼키 이것을 기본 키로 사용 관계집합 entity 생성안할 때 : 많은 쪽이 one 측의 기본 키를 가져가서 그것을 왜래키로 사용해도 관계집합이 충분히 표현 one to one 일대일 관계 : 관계집합 entity 생성시 : 참여하는 ㅡ엔티티 집합 중 어느 쪽의 기본키 모두 관계집합 최소 슈퍼키 가능 이것을 기본 키를 선택 관계집합 entity 생성안할 때 : 아무 entity 집합이나 상대측의 pk 를 자신의 fk 로 사용하면 가능 relationship cardinarity의 각각의 경우 many to many && partial partial && strong strong (m:m, p:p, s:s) 양쪽의 attribute를 모두 pk 로 entity set 을 만든다 one to many && partial total && strong strong (o:m, p:t, s:s) many측의 attribute가 pk이자 fk, one 측의 attribute 가 fk 인 entity set 을 만든다 total 이므로 not null 조건이 있어야 한다 many측이 total이면 one측에 있는 pk를 가져다가 many측에 붙인다 on to many && one to many && total total : 불가능 one to many && partial total && strong weak (o:m, p:t, s:w) many 측의 attribute가 pk 이자 fk, one 측의 ER diagram -> relation schemes 예시 중복 속성은 고유한 속성이 아닌데 가지고 있는 entity set 부분을 FK 로 한다 기호 설명 Pasted image 20241007125062 직사각형 (Rectangles): 개체 집합(entity set)을 나타냅니다. 개체 집합의 이름이 직사각형 안에 적혀 있으며, 해당 개체 집합의 모든 속성(attributes)도 나열됩니다. 다이아몬드 (Diamonds): 관계 집합(relationship set)을 나타냅니다. 관계 집합의 이름이 다이아몬드 안에 적혀 있습니다. 비분할 직사각형 (Undivided Rectangles): 관계 집합의 속성을 나타냅니다. 관계의 주요 키(primary key)의 일부인 속성은 밑줄로 표시됩니다. 선 (Lines): 개체 집합과 관계 집합을 연결합니다. 이 선은 개체와 관계 간의 연관성을 나타냅니다. 점선 (Dashed Lines): 관계 집합의 속성과 그 관계 집합을 연결합니다. 속성이 관계에 포함됨을 나타냅니다. 이중선 (Double Lines): 개체가 관계 집합에 완전하게 참여(total participation)하고 있음을 나타냅니다. 즉, 해당 개체는 반드시 이 관계에 포함되어야 합니다. 이중 다이아몬드 (Double Diamonds): 약한 개체 집합(weak entity set)과 연결된 식별 관계 집합(identifying relationship set)을 나타냅니다. 약한 개체는 독립적으로 식별될 수 없고, 반드시 다른 개체와의 관계를 통해 식별됩니다. Pasted image 20240923134175 화살표 one, 직선 many 식별관계 vs 비 식별관계 total participation - total participation 1 many 불가 A <= relation ship = B 이것을 ddl 로 강제할 방법이 없다 goal for decomposition Lossless-join decomposition No redundancy Dependency preservation Pasted image 20241028083795Pasted image 20241028083878",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "description": "대학 수업 database 에서 배운 모든 내용 정리",
        "series": "database(university)",
        "series_weight": "11",
        "date": "2024-09-23T13:20:00+09:00",
        "lastmod": "2024-09-23T13:20:00+09:00"
      }
    },
    "데이터베이스2 정규화": {
      "path": "/06.university/databaseuniversity/데이터베이스2-정규화/",
      "filename": "데이터베이스2 정규화",
      "content": "정규화 시에 제대로 분해 되었는지 확인하는 2가지 개념 손실분해가 일어나지 않게 중복 data 가 존재하는가 (redundency) 종속성이 보존되게 분해 나쁜 디자인 특정 정보를 표현 불가 -> 대출이 없는 지점은 오픈할 수 없다 정보의 중복 -> 동일 지점에서 빌린 여러개의 대출 정보의 손실 anomaly (update, insertion, deletion) update : loan-number 없이 branch-name 등 insert 불가 delete : 어떤 branch의 마지막 loan을 delete하면 branch 자체가 사라짐 update : 어떤 특정한 branch의 정보(예: Downtown의assets)를 update하면 해당하는 튜플들을 모두 업데이트해야 함 좋은 디자인 정보의 중복을 피한다 정보의 관계가 표현된다 무결성 제약 조건 분해를 안하면 업데이트시 여러개의 동시에 업데이트해야하는 문제가 발생 싱크시에 트래픽 발생, 싱크 실패 확율 그렇다면 분해는 쉬운가? 그렇지 않다 손실분해(lossy decomposition) Pasted image 20241019172546 손실 분해의 예시이다 무손실 분해(lossless decomposition) Pasted image 20241019172955 무손실 분해의 예시이다 이것을 보장하게 하는 것을 위해 아래의 함수 종속 개념을 사용하여 보장하게 된다 functional dependencies Pasted image 20241019173166 trivial functional dependencies {주민등록번호} => {주민등록번호, 성별} // 당연한 이야기 {주민등록번호} => {주민등록번호} // 당연한 이야기 {주민등록번호} => 공집합 // 수학적으로 당연한 이야기 정규화 BCNF : 함수 종속 $\\alpha -> \\beta$ 에서 $\\alpha$는 슈퍼키이다 r(R) relation을 BCNF 조건을 어긋나게 하는 함수종속 $\\alpha\\ \\rightarrow\\ \\beta$ 의 각 속성조합인 $r1 =\\alpha\\ \\cup\\ \\beta$ 와 $r2=R\\ - (\\beta\\ -\\ \\alpha)$ 2개로 나눈다 즉 알파를 볼모로 잡고 나머지를 다른 table 로 분리 3정규형 : 함수 종속 $\\alpha -> \\beta$ 에서 $\\alpha$는 슈퍼키 이거나 $\\beta-\\alpha$ 는 후보키에 속한다 정규화 과정 BCNF 목표 lossless join Dependency preservation 1차 정규화 모든 entity 는 원자적이어야 한다 2차 정규화 1차 정규화 완료 모든 key 가 아닌 attrivutes 는 완전하게 PK 종속되어야 한다 3차 정규화 쉬운 정의 2차 정규화 완료 transitively dependent(a -> b && b-> c) 가 없어야 한다",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "description": "대학 수업 database 에서 배운 모든 내용 정리",
        "series": "database(university)",
        "series_weight": "13",
        "date": "2024-10-07T12:57:00+09:00",
        "lastmod": "2024-10-07T12:57:00+09:00"
      }
    },
    "데이터베이스2 쿼리 과정": {
      "path": "/06.university/databaseuniversity/데이터베이스2-쿼리-과정/",
      "filename": "데이터베이스2 쿼리 과정",
      "content": "쿼리 비용 측정 관계 대수 평가 알고리즘 개별 연산을 결합하는 알고리즘 쿼리 비용 측정 단순하게 disk 만 고려함 disk io 가 가장 큰 요소 seek(디스크에서 찾기) + transfer(메모리로 가져오기) seek를 통해 시작 위치를 찾는다(instructor table 의 시작 data 위치) transfer $t_T$ : time to transfer one block assuming for simplicity that write cost is same as read cost typically, cost to write a block is greater than read cost, since data is read back after being written to ensure that the write was successful $t_S$ : time for one seek 크게 selection 과 join 을 쓰기 위해 발행하는 cost 를 계한해 본다 selection $h_i$ = 트리 높이 $hi(tT+t_S)$ : 트리 통과 시간 b : 조건에 들어가는 record 이 차지하는 블록 개수 n : 조건에 들어가는 record 개수 A1 : Linear Search : $ts+br*t_T$ //br 개의 블락에 걸쳐 존재 A1 , Equal on key : $ts+br*t_T$ // 동등하면 평균적으로 1/2 A2 : Primary index, Equal on key : $(hi+1)(tT+t_S)$ // 트리 통과 + 1개의 튜플 찾기 A3 : Primary index, Equal on nonkey (중복 존재 가능): $hi\\times(tT+tS)+b\\times tT + t_S$ A4 : Secondary index, Equal on key $(hi+1)(tT+t_S)$ // 1개만 가능하니까 바로 찾음 A4 : Secondary index, Equal on nonkey $(hi+n)(tT+t_S)$ // n개의 중복 튜플 , cost 큼 A5 : Pirmary index, comparison : $hi\\times(tT+tS)+b\\times tT + t_S$ A6 : Secondary index, comparison : $(hi+n)(tT+t_S)$ // cost 큼 A7 : conjunctive selection using index : 최소 비용 조건을 먼저 이후는 메모리에서 A8 : conjunctive selection composite index : 가능하다면 복합 인덱스 사용 A9 : conjunctive selection by intersection of identifiers : 나오는 리프 노드의 포인터 record 의 교집합 A10 : disjunctive selection by union of identifiers : 리프노트 포인터 합집합 또는 리니어 서치 sorting sorting 사용 이유 output 출력시 사용 join 시에 정렬이 되있을 때 편한 사용 Nested loop join Block Nested loop join index join merge join hash join r = br 개의 블록, nr 개의 튜플 s = bs 개의 블록, ns 개의 튜플 nested-loop join s 관점 nr \\* br 개의 transfer r 관점 b_r 개의 transfer s 관점 n_r 번 seek r 관점 b_r 개 seek block nested-loop join nr 을 br 로 교체",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "description": "대학 수업 database 에서 배운 모든 내용 정리",
        "series": "database(university)",
        "series_weight": "14",
        "date": "2024-10-07T14:19:00+09:00",
        "lastmod": "2024-10-07T14:19:00+09:00"
      }
    },
    "데이터베이스2 프로젝트": {
      "path": "/06.university/databaseuniversity/데이터베이스2-프로젝트/",
      "filename": "데이터베이스2 프로젝트",
      "content": "직원 100 : 70개발자 마케팅, 경영관리, 연구개발 프로젝트 일정기간 투입가능 프로젝트의 직무 PM",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "description": "대학 수업 database 에서 배운 모든 내용 정리",
        "series": "database(university)",
        "date": "2024-11-01T05:39:00+09:00",
        "lastmod": "2024-11-01T05:39:00+09:00"
      }
    },
    "요구사항명세서": {
      "path": "/06.university/databaseuniversity/요구사항명세서/",
      "filename": "요구사항명세서",
      "content": "우리 회사는 100명의 직원을 둔 SI 업체로, 30명의 마케팅, 경영관리 및 연구개발 직원을 제외하 면 70명의 개발자가 월평균 10개 정도의 프로젝트를 수행하고 있다. 개발자들은 프로젝트에 초기부 터 종료 시까지 투입되기도 하고, 프로젝트에 일정 기간만 투입되기도 한다. 프로젝트에 투입되는 개발자들은 경력과 기술 등급에 따라서 PM, PL, 분석자, 설계자, 프로그래머, 테스터 등 다양한 직무를 맡는다. 우리 회사가 수행하는 프로젝트에 대해 프로젝트번호, 프로젝트명, 프로젝트 착수일자/종료일자, 발주처 등을 관리하고, 직원들에 대해서는 직원번호, 직원명, 주민등록번호, 최종학력을 관리하며, 특히 프로젝트 투입 직원들에 대해서는 경력과 경험한 기술(Skill Set)들을 관리하고자 한다. 직원은 회원가입 페이지를 통해 자신의 정보를 데이터베이스에 저장하고 수정/변경할 수 있어야 한다. 직원 사번은 정해진 규칙에 따라서 자동으로 부여하는 것으로 가정하며, 회원가입 시 회원 로그인은 중복 체크 기능을 추가하여 이미 등록된 ID로 회원가입 신청을 할 경우 에러 메시지를 출력하고 중복되지 않는 신규 ID를 입력할 수 있어야 한다. 경영진은 현재 우리 회사가 몇 개의 프로젝트를 수행하고 있고, 직원들이 현재 어느 프로젝트에 몇 명이나 투입되어 있으며, 그들이 각각 어떤 직무를 수행하고 있고, 투입 기간이 어떻게 되는지 등을 체계적으로 관리하길 원하고 있다. 이를 통해서 과거 특정 시점에 어떤 직원이 어느 프로젝트 에 어떤 직무로 참여했었는지도 알 수 있고, 개인별 경력관리는 물론 인센티브 지급을 위한 기초 자료까지 추출할 수 있다. 그러므로 경영진은 일반직원과는 다르게 타 직원들의 정보를 검색할 수 있는 권한이 있어야 한다. 직원들이 참여한 각 프로젝트에서는 프로젝트 종료 시점에 평가를 시행한다. 평가에는 고객 평가, PM 평가, 동료 평가 등이 존재한다. 각 평가는 평가자와 피평가자가 존재하고 평가 항목으로는 업무 수행 평가, 커뮤니케이션 능력 평가가 있으며, 각 평가 항목 당 평점과 평가 내용을 관리한다. 고객 평가는 고객이 프로젝트에 참여한 참여사 직원들에 대해서 평가하는 것을 말한다. 프로젝트를 종 료하는 시점에 PM이 주관하여 고객사의 담당자로부터 평가서를 의뢰하고 결과를 회사에 보고해야 한다. 동료 평가는 프로젝트에 참여한 각 멤버들이 자기 이외의 프로젝트 참여자에 대해서 평가하 는 것이다. 이 중에서 PM 평가는 PM이 프로젝트팀원을 평가하는 것을 말한다. 이러한 평가 결과는 회사 내부에서 인사고과와 인사 평가의 근거 자료로 활용된다 추가) 1. 설계단계에서 ER 다이어그램 마케팅 경영관리 연구개발 개발자테스터 이것은 슈퍼 서브 타입으로 표현한다 2. 평가는 고객 PM 동료 이것은 슈퍼 서브로 표현 발주처는 fk 즉 발주처 entity set 이 존재 3. MM 관계는 지양 4. 1 to M and total total 관계는 지양 5. 추가 엔티티 = 조원 수 서브타입 추가는 금지 6. 마지막 데모 시나리오 = 조원수 / 2 => 2개 이상의 table 을 조합, 추가 엔티티를 1개의 데모에는 무조건 사용 , 성능 향상 기법(인덱싱, metirial view, 반정규화) person(100) 마케팅 경영관리 연구개발 개발자(70) PM PL 분석자 설계자 프로그래머 테스터 project id name start_date end_date 발주처 member 직원번호 직원명 주민등록번호 최종학력 평가 평가자 피평가자 추가 요구사항 설계단계에서 ER 다이어그램 마케팅 경영관리 연구개발 개발자테스터 이것은 슈퍼 서브 타입으로 표현한다 평가는 고객 PM 동료 이것은 슈퍼 서브로 표현 발주처는 fk 즉 발주처 entity set 이 존재 MM 관계는 지양 1 to M and total total 관계는 지양 추가 엔티티 = 조원 수 서브타입 추가는 금지 마지막 데모 시나리오 = 조원수 / 2 => 2개 이상의 table 을 조합, 추가 엔티티를 1개의 데모에는 무조건 사용 , 성능 향상 기법(인덱싱, metirial view, 반정규화) 프로젝트 하나에 참여하는 직원은 모두 개발자인가? ER 다이어그램을 설계하기 위해서 주어진 정보에 기반하여 엔터티 세트와 관계를 정의할 수 있습니다. 다음은 이를 기반으로 한 엔터티와 관계입니다. 엔터티 세트 (Entity Sets) 직원 (Employee) 속성: 직원 ID, 이름, 직책, 부서 등 프로젝트 (Project) 속성: 프로젝트 ID, 프로젝트 이름, 시작일, 종료일 등 고객 (Client) 속성: 고객 ID, 고객 이름, 연락처 등 평가 (Evaluation) 속성: 평가 ID, 평가 날짜, 평가 내용 등 평가 항목 (Evaluation Criteria) 속성: 항목 ID, 항목 이름 (업무 수행, 커뮤니케이션 능력 등) 관계 (Relationships) 직원 - 프로젝트 (Participates) 관계: 직원은 여러 프로젝트에 참여할 수 있으며, 각각의 프로젝트에 여러 직원이 참여할 수 있음 (다대다 관계) 프로젝트 - 고객 (Evaluates) 관계: 고객은 특정 프로젝트에 대해 평가를 수행함 (일대다 관계) 프로젝트 - PM (Managed By) 관계: 각 프로젝트는 하나의 PM에 의해 관리됨 (일대일 관계) 직원 - 평가 (Receives) 관계: 직원은 여러 평가를 받을 수 있으며, 각 평가는 여러 직원에 대해 수행될 수 있음 (다대다 관계) 평가 - 평가 항목 (Includes) 관계: 각 평가는 여러 평가 항목을 포함할 수 있음 (일대다 관계) 직원 - 평가 (Gives) 관계: 직원은 다른 직원에 대한 평가를 수행함 (다대다 관계) 요약 직원: 여러 프로젝트에 참여 (Participates) 프로젝트: 고객에 의해 평가됨 (Evaluates), PM에 의해 관리됨 (Managed By) 평가: 여러 평가 항목을 포함 (Includes), 직원에 의해 수행됨 (Gives) Employee ↔ EmployeeSkill: 1:N (하나의 직원이 여러 기술을 가질 수 있음) SkillSet ↔ EmployeeSkill: 1:N (하나의 기술이 여러 직원에게 연결될 수 있음)",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "description": "대학 수업 database 에서 배운 모든 내용 정리",
        "series": "database(university)",
        "date": "2024-12-23T02:30:00+09:00",
        "lastmod": "2024-12-23T02:30:00+09:00"
      }
    },
    "프로젝트 sample data 통계": {
      "path": "/06.university/databaseuniversity/프로젝트-sample-data-통계/",
      "filename": "프로젝트 sample data 통계",
      "content": "개발자 70 명 나머지 30명 월별 평균 10개 정도의 프로젝트 진행 Pasted image 20241118201393 SELECT EXTRACT(YEAR FROM month) AS PROJECT_YEAR, EXTRACT(MONTH FROM month) AS PROJECT_MONTH, COUNT(p.project_id) AS PROJECT_COUNT FROM (SELECT ADD_MONTHS(TRUNC(DATE '2021-01-01', 'MM'), LEVEL - 1) AS month FROM dual CONNECT BY LEVEL <= 47) m LEFT JOIN project p ON (p.start_date < LAST_DAY(m.month) + 1 AND (p.end_date > LAST_DAY(m.month) OR p.end_date IS NULL)) GROUP BY EXTRACT(YEAR FROM month), EXTRACT(MONTH FROM month) ORDER BY PROJECT_YEAR, PROJECT_MONTH;",
      "frontmatter": {
        "tags": [
          "sql",
          "database"
        ],
        "description": "대학 수업 database 에서 배운 모든 내용 정리",
        "series": "database(university)",
        "date": "2024-11-18T18:07:00+09:00",
        "lastmod": "2024-11-18T18:07:00+09:00"
      }
    },
    "학교 시스템 데이터베이스": {
      "path": "/06.university/databaseuniversity/학교-시스템-데이터베이스/",
      "filename": "학교 시스템 데이터베이스",
      "content": "table mean student 학생 instructor 교수 advisor 지도관계 department 학과 course 과목 prereq 선이수관계 section 특정 강좌 teaches 강의교수 takes 수강 classroom 강의실 time_slot 시간표 우측부터 설명 student(학생) 말그대로 학생 ID=PK dept_name(학과명)=FK advisor(지도관계) 학생과 교수의 지도관계 s_id(학생ID)=PK, FK i_id(교수ID)=FK 학생의 지도교수가 아직 배정 받지 못한 경우가 있을 수 있음 NULL 값 부여불가 기능을 사용 s_id 만 PK 이므로 모든 학생이 들어가고 학생의 중복은 불가능하다 instructor(교수) department(학과) course(과목): 학과에 존재하는 모든 과목 prereq(선이수 관계) : 학과에 존재하는 과목의 모든 선이수 관계 모든 값이 PK 이면서 FK couseid, prereqid 묶어서 하나의 PK section(강의) : course 와 달리 실제 강의를 의미함 teaches(강의 교수) classroom(강의실) tasks(수강신청 목록) prereq 의 두 attribute 가 하나의 PK 를 이루는가 즉 집합인가? superkey candidatekey primarykey foreignkey => 이것만 하나의 attribute 인가?? teaches -- 학교 시스템 데이터베이스 생성 DDL 스크립트 -- primary key만 존재하는 table부터 생성해야 foreign key가 존재하는 table을 생성할 때 문제가 없다. 즉 생성의 순서가 존재한다 --department table --student table --instuctor table --advisor table --course table --prereq table --classroom table --time_slot table --section table --takes table --teaches table -- 지울떄는 반대로 DROP Table `teaches`; DROP Table `takes`; DROP Table `section`; DROP Table `time_slot`; DROP Table `classroom`; DROP Table `prereq`; DROP Table `course`; DROP Table `advisor`; DROP Table `instructor`; DROP Table `student`; DROP Table `department`; --생성 CREATE TABLE `department` ( `dept_name` varchar(20), `building` varchar(15), `budget` numeric(12,2) CHECK (`budget` > 0), PRIMARY KEY (`dept_name`) ) CREATE TABLE `student` ( `ID` varchar(5), `name` varchar(20) NOT NULL, `dept_name` varchar(20), `tot_cred` numeric(3,0) CHECK (`tot_cred` >= 0), PRIMARY KEY (`ID`), FOREIGN KEY (`dept_name`) REFERENCES `department` (`dept_name`) ON DELETE SET NULL ) CREATE TABLE `instructor` ( `ID` varchar(5), `name` varchar(20) NOT NULL, `dept_name` varchar(20), `salary` numeric(8,2) CHECK (salary > 29000), PRIMARY KEY (`ID`), FOREIGN KEY (`dept_name`) REFERENCES `department` (`dept_name`) ON DELETE SET NULL ) CREATE TABLE `advisor`( `s_ID` varchar(5), `i_ID` varchar(5), PRIMARY KEY (`s_ID`), FOREIGN KEY (`i_ID`) REFERENCES instructor (ID) ON DELETE SET NULL, FOREIGN KEY (`s_ID`) REFERENCES student (ID) ON DELETE CASCADE ) CREATE TABLE `course` ( `course_id` varchar(8), `title` varchar(50), `dept_name` varchar(20), `credits` numeric(2,0) CHECK (credits > 0), PRIMARY KEY (`course_id`), FOREIGN KEY (`dept_name`) REFERENCES `department` (`dept_name`) ON DELETE SET NULL ) CREATE TABLE `prereq` ( `course_id` varchar(8), `prereq_id` varchar(8), PRIMARY KEY (`course_id`,`prereq_id`), FOREIGN KEY (`course_id`) REFERENCES `course` (`course_id`), FOREIGN KEY (`prereq_id`) REFERENCES `course` (`course_id`) ) CREATE TABLE `classroom` ( `building` varchar(15), `room_number` VARCHAR(7), `capacity` numeric(4,0), PRIMARY KEY (`building`,`room_number`) ) CREATE TABLE `time_slot`( `time_slot_id` varchar(4), `day` varchar(1), `start_hr` numeric(2) CHECK (start_hr >= 0 AND start_hr < 24), `start_min` numeric(2) CHECK (start_min >= 0 AND start_min < 60), `end_hr` numeric(2) CHECK (end_hr >= 0 AND end_hr < 24), `end_min` numeric(2) CHECK (end_min >= 0 AND end_min < 60), PRIMARY KEY (`time_slot_id`, `day`, `start_hr`, `start_min`) ) CREATE TABLE `section` ( `course_id` varchar(8), `sec_id` varchar(8), `semester` varchar(6) CHECK (semester in ('Fall', 'Winter', 'Spring', 'Summer')), `year` numeric(4,0) CHECK (year > 1701 and year < 2100), `building` varchar(15), `room_number` varchar(7), `time_slot_id` varchar(4), PRIMARY KEY (`course_id`,`sec_id`,`semester`,`year`), FOREIGN KEY (`course_id`) REFERENCES `course` (`course_id`) ON DELETE CASCADE, FOREIGN KEY (`building`,`room_number`) REFERENCES `classroom` (`building`,`room_number`) ON DELETE SET NULL ) CREATE TABLE `takes` ( `ID` varchar(5), `course_id` varchar(8), `sec_id` varchar(8), `semester` varchar(6), `year` numeric(4,0), `grade` varchar(2), PRIMARY KEY (`ID`,`course_id`,`sec_id`,`semester`,`year`), FOREIGN KEY (`course_id`,`sec_id`,`semester`,`year`) REFERENCES `section` (`course_id`,`sec_id`,`semester`,`year`) ON DELETE CASCADE, FOREIGN KEY (`ID`) REFERENCES `student` (`ID`) ON DELETE CASCADE ) CREATE TABLE `teaches`( `ID` varchar(5), `course_id` varchar(8), `sec_id` varchar(8), `semester` varchar(6), `year` numeric(4,0), PRIMARY KEY (`ID`, `course_id`, `sec_id`, `semester`, `year`), FOREIGN KEY (`course_id`,`sec_id`, `semester`, `year`) REFERENCES `section` (`course_id`, `sec_id`, `semester`, `year`) ON DELETE CASCADE, FOREIGN KEY (ID) REFERENCES `instructor` (`ID`) ON DELETE CASCADE ) INSERT INTO classroom VALUES('Packard','101',500); INSERT INTO classroom VALUES('Painter','514',10); INSERT INTO classroom VALUES('Taylor','3128',70); INSERT INTO classroom VALUES('Watson','100',30); INSERT INTO classroom VALUES('Watson','120',50); INSERT INTO department VALUES('Biology','Watson',90000); INSERT INTO department VALUES('Comp. Sci.','Taylor',100000); INSERT INTO department VALUES('Elec. Eng.','Taylor',85000); INSERT INTO department VALUES('Finance','Painter',120000); INSERT INTO department VALUES('History','Painter',50000); INSERT INTO department VALUES('Music','Packard',80000); INSERT INTO department VALUES('Physics','Watson',70000); INSERT INTO course VALUES('BIO-101','Intro. to Biology','Biology',4); INSERT INTO course VALUES('BIO-301','Genetics','Biology',4); INSERT INTO course VALUES('BIO-399','Computational Biology','Biology',3); INSERT INTO course VALUES('CS-101','Intro. to Computer Science','Comp. Sci.',4); INSERT INTO course VALUES('CS-190','Game Design','Comp. Sci.',4); INSERT INTO course VALUES('CS-315','Robotics','Comp. Sci.',3); INSERT INTO course VALUES('CS-319','Image Processing','Comp. Sci.',3); INSERT INTO course VALUES('CS-347','Database System Concepts','Comp. Sci.',3); INSERT INTO course VALUES('EE-181','Intro. to Digital Systems','Elec. Eng.',3); INSERT INTO course VALUES('FIN-201','Investment Banking','Finance',3); INSERT INTO course VALUES('HIS-351','World History','History',3); INSERT INTO course VALUES('MU-199','Music Video Production','Music',3); INSERT INTO course VALUES('PHY-101','Physical Principles','Physics',4); INSERT INTO instructor VALUES('10101','Srinivasan','Comp. Sci.',65000); INSERT INTO instructor VALUES('12121','Wu','Finance',90000); INSERT INTO instructor VALUES('15151','Mozart','Music',40000); INSERT INTO instructor VALUES('22222','Einstein','Physics',95000); INSERT INTO instructor VALUES('32343','El Said','History',60000); INSERT INTO instructor VALUES('33456','Gold','Physics',87000); INSERT INTO instructor VALUES('45565','Katz','Comp. Sci.',75000); INSERT INTO instructor VALUES('58583','Califieri','History',62000); INSERT INTO instructor VALUES('76543','Singh','Finance',80000); INSERT INTO instructor VALUES('76766','Crick','Biology',72000); INSERT INTO instructor VALUES('83821','Brandt','Comp. Sci.',92000); INSERT INTO instructor VALUES('98345','Kim','Elec. Eng.',80000); INSERT INTO section VALUES('BIO-101','1','Summer',2017,'Painter','514','B'); INSERT INTO section VALUES('BIO-301','1','Summer',2018,'Painter','514','A'); INSERT INTO section VALUES('CS-101','1','Fall',2017,'Packard','101','H'); INSERT INTO section VALUES('CS-101','1','Spring',2018,'Packard','101','F'); INSERT INTO section VALUES('CS-190','1','Spring',2017,'Taylor','3128','E'); INSERT INTO section VALUES('CS-190','2','Spring',2017,'Taylor','3128','A'); INSERT INTO section VALUES('CS-315','1','Spring',2018,'Watson','120','D'); INSERT INTO section VALUES('CS-319','1','Spring',2018,'Watson','100','B'); INSERT INTO section VALUES('CS-319','2','Spring',2018,'Taylor','3128','C'); INSERT INTO section VALUES('CS-347','1','Fall',2017,'Taylor','3128','A'); INSERT INTO section VALUES('EE-181','1','Spring',2017,'Taylor','3128','C'); INSERT INTO section VALUES('FIN-201','1','Spring',2018,'Packard','101','B'); INSERT INTO section VALUES('HIS-351','1','Spring',2018,'Painter','514','C'); INSERT INTO section VALUES('MU-199','1','Spring',2018,'Packard','101','D'); INSERT INTO section VALUES('PHY-101','1','Fall',2017,'Watson','100','A'); INSERT INTO teaches VALUES('10101','CS-101','1','Fall',2017); INSERT INTO teaches VALUES('10101','CS-315','1','Spring',2018); INSERT INTO teaches VALUES('10101','CS-347','1','Fall',2017); INSERT INTO teaches VALUES('12121','FIN-201','1','Spring',2018); INSERT INTO teaches VALUES('15151','MU-199','1','Spring',2018); INSERT INTO teaches VALUES('22222','PHY-101','1','Fall',2017); INSERT INTO teaches VALUES('32343','HIS-351','1','Spring',2018); INSERT INTO teaches VALUES('45565','CS-101','1','Spring',2018); INSERT INTO teaches VALUES('45565','CS-319','1','Spring',2018); INSERT INTO teaches VALUES('76766','BIO-101','1','Summer',2017); INSERT INTO teaches VALUES('76766','BIO-301','1','Summer',2018); INSERT INTO teaches VALUES('83821','CS-190','1','Spring',2017); INSERT INTO teaches VALUES('83821','CS-190','2','Spring',2017); INSERT INTO teaches VALUES('83821','CS-319','2','Spring',2018); INSERT INTO teaches VALUES('98345','EE-181','1','Spring',2017); INSERT INTO student VALUES('00128','Zhang','Comp. Sci.',102); INSERT INTO student VALUES('12345','Shankar','Comp. Sci.',32); INSERT INTO student VALUES('19991','Brandt','History',80); INSERT INTO student VALUES('23121','Chavez','Finance',110); INSERT INTO student VALUES('44553','Peltier','Physics',56); INSERT INTO student VALUES('45678','Levy','Physics',46); INSERT INTO student VALUES('54321','Williams','Comp. Sci.',54); INSERT INTO student VALUES('55739','Sanchez','Music',38); INSERT INTO student VALUES('70557','Snow','Physics',0); INSERT INTO student VALUES('76543','Brown','Comp. Sci.',58); INSERT INTO student VALUES('76653','Aoi','Elec. Eng.',60); INSERT INTO student VALUES('98765','Bourikas','Elec. Eng.',98); INSERT INTO student VALUES('98988','Tanaka','Biology',120); INSERT INTO takes VALUES('00128','CS-101','1','Fall',2017,'A'); INSERT INTO takes VALUES('00128','CS-347','1','Fall',2017,'A-'); INSERT INTO takes VALUES('12345','CS-101','1','Fall',2017,'C'); INSERT INTO takes VALUES('12345','CS-190','2','Spring',2017,'A'); INSERT INTO takes VALUES('12345','CS-315','1','Spring',2018,'A'); INSERT INTO takes VALUES('12345','CS-347','1','Fall',2017,'A'); INSERT INTO takes VALUES('19991','HIS-351','1','Spring',2018,'B'); INSERT INTO takes VALUES('23121','FIN-201','1','Spring',2018,'C+'); INSERT INTO takes VALUES('44553','PHY-101','1','Fall',2017,'B-'); INSERT INTO takes VALUES('45678','CS-101','1','Fall',2017,'F'); INSERT INTO takes VALUES('45678','CS-101','1','Spring',2018,'B+'); INSERT INTO takes VALUES('45678','CS-319','1','Spring',2018,'B'); INSERT INTO takes VALUES('54321','CS-101','1','Fall',2017,'A-'); INSERT INTO takes VALUES('54321','CS-190','2','Spring',2017,'B+'); INSERT INTO takes VALUES('55739','MU-199','1','Spring',2018,'A-'); INSERT INTO takes VALUES('76543','CS-101','1','Fall',2017,'A'); INSERT INTO takes VALUES('76543','CS-319','2','Spring',2018,'A'); INSERT INTO takes VALUES('76653','EE-181','1','Spring',2017,'C'); INSERT INTO takes VALUES('98765','CS-101','1','Fall',2017,'C-'); INSERT INTO takes VALUES('98765','CS-315','1','Spring',2018,'B'); INSERT INTO takes VALUES('98988','BIO-101','1','Summer',2017,'A'); INSERT INTO takes VALUES('98988','BIO-301','1','Summer',2018,NULL); INSERT INTO advisor VALUES('00128','45565'); INSERT INTO advisor VALUES('12345','10101'); INSERT INTO advisor VALUES('23121','76543'); INSERT INTO advisor VALUES('44553','22222'); INSERT INTO advisor VALUES('45678','22222'); INSERT INTO advisor VALUES('76543','45565'); INSERT INTO advisor VALUES('76653','98345'); INSERT INTO advisor VALUES('98765','98345'); INSERT INTO advisor VALUES('98988','76766'); INSERT INTO time_slot VALUES('A','M',8,0,8,50); INSERT INTO time_slot VALUES('A','W',8,0,8,50); INSERT INTO time_slot VALUES('A','F',8,0,8,50); INSERT INTO time_slot VALUES('B','M',9,0,9,50); INSERT INTO time_slot VALUES('B','W',9,0,9,50); INSERT INTO time_slot VALUES('B','F',9,0,9,50); INSERT INTO time_slot VALUES('C','M',11,0,11,50); INSERT INTO time_slot VALUES('C','W',11,0,11,50); INSERT INTO time_slot VALUES('C','F',11,0,11,50); INSERT INTO time_slot VALUES('D','M',13,0,13,50); INSERT INTO time_slot VALUES('D','W',13,0,13,50); INSERT INTO time_slot VALUES('D','F',13,0,13,50); INSERT INTO time_slot VALUES('E','T',10,30,11,45); INSERT INTO time_slot VALUES('E','R',10,30,11,45); INSERT INTO time_slot VALUES('F','T',14,30,15,45); INSERT INTO time_slot VALUES('F','R',14,30,15,45); INSERT INTO time_slot VALUES('G','M',16,0,16,50); INSERT INTO time_slot VALUES('G','W',16,0,16,50); INSERT INTO time_slot VALUES('G','F',16,0,16,50); INSERT INTO time_slot VALUES('H','W',10,0,12,30); INSERT INTO prereq VALUES('BIO-301','BIO-101'); INSERT INTO prereq VALUES('BIO-399','BIO-101'); INSERT INTO prereq VALUES('CS-190','CS-101'); INSERT INTO prereq VALUES('CS-315','CS-101'); INSERT INTO prereq VALUES('CS-319','CS-101'); INSERT INTO prereq VALUES('CS-347','CS-101'); INSERT INTO prereq VALUES('EE-181','PHY-101'); 임의 데이터 무작위 생성 참조 import sqlite3 import random import string conn = sqlite3.connect('test_db.sqlite') cursor = conn.cursor() cursor.execute(''' CREATE TABLE IF NOT EXISTS test_table ( id INTEGER PRIMARY KEY, name TEXT, value INTEGER ) ''') for i in range(10000): name = ''.join(random.choices(string.ascii_letters, k=5)) # 랜덤 이름 생성 value = random.randint(1, 100) # 랜덤 값 생성 cursor.execute('INSERT INTO test_table (name, value) VALUES (?, ?)', (name, value)) conn.commit() conn.close()",
      "frontmatter": {
        "tags": [
          "database"
        ],
        "description": "대학 수업 database 에서 배운 모든 내용 정리",
        "series": "database(university)",
        "series_weight": "1",
        "date": "2024-03-21T01:14:00+09:00",
        "lastmod": "2024-03-21T01:14:00+09:00"
      }
    },
    "university operating system quiz": {
      "path": "/06.university/os/university-operating-system-quiz/",
      "filename": "university operating system quiz",
      "content": "pcb(process control block) 에 포함된 정보 process state process number program counter and registers memory limits list of open files 6주차 CPU를 연속적으로 사용하는 시간을 CPU burst 라고 한다 스케줄링 알고리즘의 목적으로 바람직한것 cpu 사용율을 최대화 throughput 을 최대화 average wating time 최대화 스케줄링에 대해 맞는것 preemtive 는 강제 중단 가능 non-preemtive 는 강제 중단 불가능 최근 운영체제는 preemtive 를 주로 사용 non-preemtive 알고리즘 SJF FCFS non-preemtive 에서 가장 average wating time 이 가장 작은 것은 shortest job first exponential moving average 과거의 데이터가 현재의 값에 영향을 미치고 최근 데이터가 더 영향이 더 큰 경우에 사용한다 값의 변화의 추이를 예측하는 데 사용된다 preemtive 스케줄링 SRTF Priorty Round Robin priorty 스케줄링의 단점 => starvation, 기아 7주차 멀티레벨 queue 스케줄링에서는 각 큐가 일정비율로 CPU 를 할당받는다 => True process aging 을 구현할 수 있는 스케줄링 방법 => multilevel feedback queue 스케줄링 윈도우는 preemptive scheduling 을 사용 동시성 문제에 대해 맞는것 두개이상의 프로세스가 같은 리소스를 동시에 접근하려 할 때 발생 CPU 코어가 하나인 경우에도 발생한다 race condition 때문에 발생하기도 한다 critical section problem 으로 이해 할 수 있다 동시성 문제는 프로세스가 같은 변수를 동시에 읽으려고 할 때도 발생한다 => False critical section 은 하나의 프로세스만 들어갈 수 있다는 조건은 mutual exclution critical section에 아무도 없는데 계속 대기하는 상황은 Progress critical section 문제를 해결하고자 할때 Mutual exclution 을 만족하지 못하는 이유 locked = 0 or 1 lock 방식 알고리즘의 경우 여러 프로세스가 동시에 critical section에 들어 갈 수 있기 때문 9주차 lock 방식은 mutual exclusion, bounded Waiting 만족하지 못한다 peterson solution의 한계는 critical section problem 이 이미 해결되었다고 가정한 후에 풀었다 critical section problem 를 해결하기 위해 interrupt 를 비활성화 하는 방법의 한계는 cpu 가 여러개인 경우 적용할 수 없고 결국 시스템의 성능이 저하된다 semaphore 접근을 위해 들어갈때 wait, 나갈 때 signal 을 부른다 semaphore 가 critical section 에 들어가기 위해 지속적으로 semaphore 의 현재값을 체크 할 수 있다 이것을 busy-wating(spin lock) 이라고 한다 semaphore 의 값이 -2 일 때 대기자 리스트에 2개의 프로세스가 대기중이다 중간고사 폰 노이만 구조가 이전과 다른점 : 코드영역(program)이 하드웨어가 아닌 메모리로 올라간다(소프트웨어 개념의 탄생) 캐쉬를 통해 성능 향상이 가능한 이유 : priciple of locality : 비슷한 시간에 지역적으로 접근한다 10주차 dining philosopher problem 에서 Deadlock 이 발생 -> 이것의 해결방법 항상 적어도 한명의 철학자가 배가 고프지 않으면 된다 => Circular Wait 옆사람이 쥐고 있는 젓가락을 뺏어올 수 있으면 된다 => no preemtion 젓가락 두개를 동시에 가져올 수 없으면 젓가락을 가져가지 않으면 된다 => hold and wait deadlock 해결책 prevention 은 deadlock 이 발생할 수 있는 원인을 애초에 없애 버리는 기술이다 o avoid 기술은 deadlock 이 이미 발생했을 때 피해를 최소화 하는 방법이다 x => detection Detection 기술은 한 프로세스에게 리소스를 허용하면 deadlock 이 발생하는지 미리 감지하는 기술이다 x => avoid 다른문서 출력중인데 출력명령을 내리기위해 대기할 필요 없음 스풀링 4가지 조건은 필요조건( 충분조건 x , 필요충분조건 x ) 12 주차 logical address space 맞는 설명 cpu 가 사용하는 주소 컴퓨터에 장착되어 있는 실제 메모리 크기 보다 클 수 있다 => virtual memory 실제와는 다른 추상적인 주소이다 MMU 는 레지스터에 저장된 base 주소를 더해서 메모리 상의 실제 주소를 찾아낸다 fixed partitioning 방법 에서는 프로세스 당 정해진 크기의 메모리가 할당되므로 프로세스가 사용하지 않는 공간이 생길 수 있는데 이 공간을 internal(내부) fragmentation(단편화) 이라고 부른다 반면 dynamic patritioning 방법에서는 프로세스에게 할당되는 공간 사이사아에 남는 공간이 생기는데 이른 external(외부) fragmentation(단편화) 라고 부른다 dynamic partitioning의 메모리 할당 전략에 대해서 맞는 것 First-fit: 요청 크기를 만족하는 첫 번째 가용 공간을 할당합니다 (위에서부터 탐색 가정). Best-fit: 요청 크기를 만족하면서 가장 크기가 비슷한 (즉, 남는 공간이 가장 적은) 가용 공간을 할당합니다. Worst-fit: 요청 크기를 만족하면서 가장 크기가 큰 가용 공간을 할당합니다. paging 기법 맞는 것 logical address 와 physical address space 를 분리하여 메모리를 유연하게 관리한다 cpu 가 바라보는 가용 용량과 실제 가용한 메모리 공간이 무관햊니다 page table 을 효과적으로 저장하고 읽어오기 우해서는 하드웨어의 도움이 필요하다 TLB(transiation lookaside buffer) 에 대해 맞는 것을 고르시오 캐시의 종류이야 하드웨어로 구현되어 있음 associative memory",
      "frontmatter": {
        "tags": [
          "university",
          "operating-system"
        ],
        "date": "2025-04-22T11:00:00+09:00",
        "lastmod": "2025-06-03T06:40:43+09:00"
      }
    },
    "university operating system": {
      "path": "/06.university/os/university-operating-system/",
      "filename": "university operating system",
      "content": "Computer Organization 폰 노이만 구조 폰 노이만 구조가 이전과 다른점 : 코드영역(program)이 하드웨어가 아닌 메모리로 올라간다(소프트웨어 개념의 탄생) 메모리 계층 구조 위 내용은 컴퓨터 시스템에서 메모리 계층 구조(Memory Hierarchy)와 관련된 개념을 설명하고 있습니다. 특히, 프로세서가 메모리를 참조할 때 나타나는 시간적/공간적 지역성(Temporal and Spatial Locality)과 이를 기반으로 한 메모리 접근 패턴의 효율성을 다룹니다. 아래에서 각 항목을 상세히 설명하겠습니다. Memory References Cluster in Time and Space 이 문장은 메모리 참조가 시간적 지역성(Temporal Locality)과 공간적 지역성(Spatial Locality)이라는 두 가지 특성을 보인다는 것을 의미합니다. (1) 시간적 지역성 (Temporal Locality) 정의: 최근에 참조된 데이터가 다시 참조될 가능성이 높음. 예시: 프로그램이 특정 변수를 반복적으로 사용하는 경우, 해당 변수는 캐시 또는 레지스터에 남아 있는 것이 유리함. 응용: 캐시 메모리를 통해 자주 사용되는 데이터를 유지하여 성능을 향상시킴. (2) 공간적 지역성 (Spatial Locality) 정의: 어떤 메모리 주소가 참조되면 그 근처의 주소들도 곧 참조될 가능성이 높음. 예시: 배열이나 연속된 데이터 구조를 처리할 때, 프로세서는 연속된 메모리 위치를 순차적으로 접근함. 응용: 캐시 라인(Cache Line)을 통해 한 번에 여러 데이터를 미리 가져오는 방식으로 활용됨. Data is Organized So That the Percentage of Accesses to Each Successively Lower Level is Substantially Less Than That of the Level Above 이 문장은 메모리 계층 구조의 기본 원칙을 설명합니다. 메모리 계층 구조는 다음과 같은 특징을 가집니다: (1) 메모리 계층 구조 컴퓨터 시스템은 다양한 종류의 메모리를 사용하며, 각 메모리는 속도, 크기, 비용의 관점에서 차이가 있음. 일반적으로 메모리 계층은 다음과 같이 구성됩니다: 레지스터(Register): 가장 빠르지만 크기가 작음. 캐시(Cache): L1, L2, L3 캐시로 나뉘며, 속도와 크기가 점진적으로 증가. RAM(Main Memory): 큰 용량을 제공하지만 상대적으로 느림. 디스크(Storage): 가장 느리지만 매우 큰 용량을 제공. (2) 접근 비율(Access Rate) 메모리 계층 구조에서는 더 낮은 수준의 메모리로 갈수록 접근 비율이 급격히 감소합니다. 예를 들어, 프로세서가 필요로 하는 데이터의 대부분은 L1 캐시에서 해결되며, L2 캐시로 넘어가는 비율은 그보다 적고, RAM으로 넘어가는 비율은 더욱 줄어듦. 이러한 접근 비율의 차이는 지역성 원칙 덕분에 가능해짐. (3) 효율성 메모리 계층 구조는 빠른 메모리를 적절히 활용하여 전체 시스템의 성능을 극대화합니다. 예를 들어, L1 캐시가 필요한 데이터를 제공할 확률이 90%이고, L2 캐시가 추가로 8%를 처리한다면, RAM에서 직접 데이터를 읽어야 하는 경우는 2%에 불과하게 됩니다. Can Be Applied Across More Than Two Levels of Memory 메모리 계층 구조는 단순히 두 개의 메모리 레벨(예: 캐시와 RAM)에만 적용되는 것이 아니라, 여러 레벨에 걸쳐 적용될 수 있습니다. (1) 다중 메모리 레벨 현대 컴퓨터 시스템에서는 여러 레벨의 메모리가 존재하며, 각 레벨은 서로 다른 특성을 가짐: L1 캐시: 가장 빠르지만 작은 용량. L2 캐시: L1보다 느리지만 더 큰 용량. L3 캐시: L2보다 더 느리지만 더 큰 용량. RAM: 캐시보다 느리지만 매우 큰 용량. SSD/HDD: 가장 느리지만 영구 저장이 가능. (2) 데이터 이동 데이터는 필요한 경우에 따라 위계적으로 이동합니다. 예를 들어: 프로세서가 데이터를 요청하면 먼저 L1 캐시를 확인. L1 캐시에 없으면 L2 캐시를 확인. L2 캐시에도 없으면 L3 캐시, RAM, 디스크 순으로 검색. (3) 성능 최적화 각 메모리 레벨의 용량과 속도를 적절히 조합하여 전체 시스템의 성능을 최적화합니다. 예를 들어, 캐시 미스(Cache Miss)가 발생할 때마다 더 느린 메모리로 접근해야 하므로, 캐시 히트율(Cache Hit Rate)을 높이는 것이 중요합니다. 요약 및 결론 위 내용은 메모리 계층 구조와 지역성 원칙을 중심으로, 프로세서가 메모리를 참조할 때 나타나는 패턴과 이를 활용한 시스템 설계 원리를 설명합니다. 핵심 포인트는 다음과 같습니다: 시간적/공간적 지역성은 메모리 참조 패턴의 기본 원칙이며, 이를 통해 캐시와 같은 고속 메모리를 효과적으로 활용할 수 있음. 메모리 계층 구조는 다양한 레벨의 메모리를 조합하여 속도와 용량의 균형을 맞춤. 접근 비율은 상위 메모리 레벨로 갈수록 높고, 하위 레벨로 갈수록 낮아짐. 이러한 원칙은 여러 레벨의 메모리에 걸쳐 적용되며, 전체 시스템의 성능을 최적화하는 데 기여함. 이러한 개념은 현대 컴퓨터 아키텍처에서 매우 중요한 역할을 하며, 캐시 설계, 메모리 관리, 성능 최적화 등 다양한 분야에서 활용됩니다. cache Invisible to the processors, programmer, OS 번역: 프로세서, 프로그래머, 운영체제(OS)에 보이지 않음. 설명: 이 문장은 특정 하드웨어나 메커니즘이 투명(Transparent)하다는 것을 의미합니다. \"보이지 않는다\"는 것은 해당 메커니즘이 프로세서, 프로그래머, 또는 운영체제에 의해 직접적으로 인식되거나 제어되지 않는다는 뜻입니다. 예를 들어, 캐시(Cache)는 프로세서와 메인 메모리 사이에서 데이터를 관리하는 하드웨어지만, 프로그래머나 운영체제가 이를 명시적으로 제어할 필요가 없습니다. 캐시는 자동으로 동작하며, 프로세서의 성능을 향상시키기 위해 배경에서 작동합니다. Interacts with other memory management hardware 번역: 다른 메모리 관리 하드웨어와 상호작용함. 설명: 이 문장은 해당 메커니즘이 독립적으로 동작하지 않고, 다른 메모리 관리 하드웨어와 협력하여 동작한다는 것을 강조합니다. 예를 들어: 캐시는 메인 메모리(RAM), 디스크, 또는 다른 계층적 메모리와 상호작용합니다. 캐시 미스(Cache Miss)가 발생하면, 다음 레벨의 메모리(예: L2 캐시 또는 RAM)에서 데이터를 가져옵니다. 이러한 상호작용은 메모리 계층 구조(Memory Hierarchy)의 핵심 원칙 중 하나입니다. Reasons for its existence: 번역: 그 존재 이유: 설명: 여기서 \"그\"는 앞서 언급된 메커니즘(예: 캐시)을 의미합니다. 이 문장은 해당 메커니즘이 왜 필요한지에 대한 이유를 설명하기 위한 서론입니다. 아래에 나오는 세 가지 이유를 통해 이 메커니즘이 왜 중요한지를 구체적으로 설명합니다. Processor must access memory at least once per instruction cycle 번역: 프로세서는 최소한 매 명령어 사이클마다 메모리에 접근해야 함. 설명: 프로세서는 프로그램을 실행하기 위해 명령어(Instruction)를 읽고 실행해야 합니다. 명령어는 일반적으로 메모리(예: RAM)에 저장되어 있습니다. 따라서, 프로세서는 매 사이클마다 메모리에 접근하여 명령어를 가져와야 합니다. 문제는, 메모리 접근 속도가 프로세서의 처리 속도보다 훨씬 느릴 수 있다는 점입니다. 이를 해결하기 위해, 고속 메모리(예: 캐시)를 사용하여 메모리 접근 속도를 개선합니다. Processor execution is limited by memory cycle time 번역: 프로세서의 실행은 메모리 사이클 시간에 의해 제한됨. 설명: 메모리 사이클 시간(Memory Cycle Time)은 메모리가 데이터를 읽거나 쓰는 데 걸리는 시간을 의미합니다. 메모리 사이클 시간이 길면, 프로세서가 데이터를 가져오거나 저장하는 데 시간이 많이 소요됩니다. 이로 인해 프로세서는 메모리 대기 시간(Memory Latency) 때문에 성능이 저하될 수 있습니다. 이를 해결하기 위해, 고속 메모리(예: 캐시)를 사용하여 메모리 접근 지연을 줄이고 프로세서 성능을 극대화합니다. Exploit the principle of locality with a small, fast memory 번역: 작고 빠른 메모리를 통해 지역성 원칙을 활용함. 설명: 지역성 원칙(Locality Principle)은 프로세서가 메모리에 접근할 때 나타나는 패턴을 설명합니다: 시간적 지역성(Temporal Locality): 최근에 사용된 데이터는 다시 사용될 가능성이 높습니다. 공간적 지역성(Spatial Locality): 특정 메모리 위치 근처의 데이터도 곧 사용될 가능성이 높습니다. 이러한 지역성을 활용하기 위해, 작고 빠른 메모리(예: 캐시)를 사용합니다. 캐시는 자주 사용되는 데이터를 임시로 저장하여, 프로세서가 메인 메모리에 직접 접근하지 않고도 데이터를 빠르게 가져올 수 있도록 합니다. 이는 메모리 접근 지연을 줄이고, 전체 시스템 성능을 향상시킵니다. 요약 및 결론 이 내용은 캐시(Cache)와 같은 메모리 관리 메커니즘의 중요성과 그 존재 이유를 설명합니다. 각 줄의 요점은 다음과 같습니다: 투명성: 캐시는 프로세서, 프로그래머, 운영체제에 보이지 않으며, 자동으로 동작합니다. 상호작용: 캐시는 다른 메모리 관리 하드웨어(예: RAM, 디스크)와 상호작용하여 데이터를 관리합니다. 존재 이유: 프로세서는 매 명령어 사이클마다 메모리에 접근해야 합니다. 메모리 사이클 시간이 프로세서 성능을 제한합니다. 캐시는 지역성 원칙을 활용하여 메모리 접근 지연을 줄이고 성능을 향상시킵니다. $\\boxed{\\text{결론적으로, 이 내용은 캐시와 같은 메모리 관리 메커니즘이 왜 필요한지를 명확히 설명합니다.}}$ cache vs main memory 캐시와 메인 메모리의 구조는 서로 다르며, 각각의 구성 요소는 데이터를 저장하고 관리하는 방식에서 차이를 보입니다. 아래에서 캐시와 메인 메모리의 구조를 비교하며 설명하겠습니다. 캐시(Cache)의 구조 캐시는 고속으로 동작하는 작은 메모리로, 메인 메모리에서 자주 사용되는 데이터를 임시로 저장합니다. 캐시의 구조는 다음과 같은 주요 요소로 구성됩니다: (1) Line Number 정의: 캐시 내부의 특정 위치를 식별하는 번호. 설명: 캐시는 여러 개의 라인(Line)으로 나뉘어 있으며, 각 라인은 메인 메모리의 데이터 블록을 저장할 수 있는 공간입니다. 예를 들어, 캐시가 64개의 라인을 가지고 있다면, 각 라인은 0 부터 63 까지의 고유 번호를 가집니다. 프로세서가 데이터를 요청할 때, 캐시는 이 번호를 사용하여 해당 데이터가 어느 라인에 있는지 확인합니다. (2) Tag 정의: 캐시 라인에 저장된 데이터가 메인 메모리의 어느 위치에 속하는지를 나타내는 정보. 설명: 캐시는 메인 메모리의 일부 데이터만 복사해 두기 때문에, 캐시 라인에 저장된 데이터가 메인 메모리의 어디에 해당하는지 알아야 합니다. 이를 위해 Tag 필드가 사용됩니다. Tag는 메인 메모리 주소의 상위 비트를 저장하여, 해당 데이터가 메인 메모리의 어느 블록인지 식별합니다. 예를 들어, 메인 메모리 주소가 0x1234 이고, 캐시 라인에 저장된 Tag 값이 0x12 라면, 이 데이터는 메인 메모리의 0x12xx 영역에 속함을 의미합니다. (3) Block 정의: 캐시 라인에 저장된 데이터 묶음. 설명: 캐시 라인에는 블록(Block)이라는 단위로 데이터가 저장됩니다. 블록은 메인 메모리에서 연속된 데이터(예: 여러 워드)를 포함하며, 일반적으로 4~64바이트 크기로 구성됩니다. 예를 들어, 블록 크기가 8워드(=32바이트)라면, 한 라인에는 8개의 워드가 저장됩니다. (4) Block Length (k Words) 정의: 블록에 포함된 워드의 개수. 설명: 블록 길이는 캐시 라인에 저장될 수 있는 데이터의 크기를 결정합니다. 예를 들어, 블록 길이가 k = 8 이라면, 한 라인에는 8개의 워드가 저장됩니다. 블록 길이는 메모리 접근 효율성을 높이기 위해 설계되며, 공간적 지역성(Spatial Locality)을 활용합니다. 메인 메모리(Main Memory)의 구조 메인 메모리는 시스템에서 가장 큰 용량을 가진 기본 메모리로, 프로그램과 데이터를 저장합니다. 메인 메모리의 구조는 다음과 같은 주요 요소로 구성됩니다: (1) Block 정의: 메인 메모리에서 연속된 데이터를 묶은 단위. 설명: 메인 메모리는 데이터를 블록(Block) 단위로 관리합니다. 블록은 캐시와 마찬가지로 연속된 데이터 묶음을 의미하며, 일반적으로 4~64바이트 크기로 구성됩니다. 예를 들어, 메인 메모리의 특정 위치에서 8워드(=32바이트)를 하나의 블록으로 정의할 수 있습니다. (2) Memory Address 정의: 메인 메모리의 특정 위치를 식별하는 주소. 설명: 메인 메모리의 모든 데이터는 고유한 주소를 가집니다. 주소는 일반적으로 바이트 단위로 지정되며, 예를 들어 0x1234 와 같은 형식으로 표현됩니다. 프로세서가 데이터를 요청할 때, 메인 메모리 주소를 사용하여 해당 데이터를 찾습니다. (3) Word Length 정의: 메인 메모리에서 처리되는 기본 데이터 단위의 크기. 설명: 워드는 메인 메모리에서 읽거나 쓸 수 있는 최소 데이터 단위입니다. 워드 길이는 시스템 아키텍처에 따라 다릅니다. 예를 들어: 32비트 시스템에서는 워드 길이가 4바이트(32비트). 64비트 시스템에서는 워드 길이가 8바이트(64비트). 메인 메모리는 워드 단위로 데이터를 읽고 씁니다. 캐시와 메인 메모리의 구조 차이 구분 캐시(Cache) 메인 메모리(Main Memory) 목적 메인 메모리보다 빠르게 데이터를 제공하기 위한 임시 저장소. 전체 프로그램과 데이터를 저장하기 위한 기본 메모리. 용량 매우 작음 (KB ~ MB 단위). 매우 큼 (GB 단위). 속도 매우 빠름 (나노초 단위). 상대적으로 느림 (캐시보다 약 10~100배 느림). 구성 요소 Line Number, Tag, Block, Block Length(k Words). Block, Memory Address, Word Length. 데이터 관리 단위 블록(Block) 단위로 데이터를 저장하며, Tag를 통해 메인 메모리 위치를 식별. 메모리 주소(Memory Address)를 기반으로 데이터를 관리. 지역성 활용 시간적/공간적 지역성을 활용하여 성능을 최적화. 지역성 원칙을 직접적으로 활용하지 않음. 요약 및 결론 캐시와 메인 메모리는 목적과 구조가 서로 다릅니다: 캐시: 고속으로 동작하며, 메인 메모리의 일부 데이터를 임시로 저장. Line Number, Tag, Block, Block Length로 구성. 지역성 원칙(Temporal & Spatial Locality)을 활용하여 성능을 최적화. 메인 메모리: 대용량으로 동작하며, 전체 프로그램과 데이터를 저장. Block, Memory Address, Word Length로 구성. 캐시보다 느리지만, 모든 데이터를 저장할 수 있는 충분한 용량을 제공. $\\boxed{\\text{결론적으로, 캐시와 메인 메모리는 각각의 역할과 구조적 특징이 다르며, 이들의 협력으로 시스템 성능이 극대화됩니다.}}$ cache design 캐시 설계는 컴퓨터 시스템의 성능에 큰 영향을 미치는 중요한 요소입니다. 캐시는 메인 메모리보다 빠르게 데이터를 제공하기 위해 사용되며, 이를 효율적으로 설계하기 위해서는 여러 가지 설계 요소를 고려해야 합니다. 아래에서 각 항목을 매우 상세히 설명하겠습니다. Cache Size (캐시 크기) 정의: 캐시 크기는 캐시가 저장할 수 있는 데이터의 총량을 의미합니다. 일반적으로 KB(킬로바이트) 또는 MB(메가바이트) 단위로 표현됩니다. 영향: 성능: 캐시 크기가 클수록 더 많은 데이터를 저장할 수 있으므로, 캐시 히트(Cache Hit) 확률이 증가합니다. 예: 32KB 캐시와 64KB 캐시를 비교했을 때, 64KB 캐시는 더 많은 데이터를 저장할 수 있어 성능이 개선될 가능성이 큽니다. 비용: 캐시 크기가 클수록 하드웨어 비용이 증가합니다. 고속 SRAM(Small Random Access Memory)을 사용하므로, 큰 캐시는 비용과 전력 소비를 증가시킵니다. 지연 시간(Latency): 캐시 크기가 너무 커지면, 탐색(Search) 시간이 증가할 수 있습니다. 설계 고려사항: 캐시 크기를 결정할 때는 비용, 성능, 전력 소비 간의 균형을 고려해야 합니다. 일반적인 캐시 크기: L1 캐시: 8KB ~ 64KB L2 캐시: 256KB ~ 8MB L3 캐시: 4MB ~ 32MB 이상 Block Size (블록 크기) 정의: 블록 크기는 캐시 라인(Line)에 저장되는 데이터의 양을 의미합니다. 일반적으로 워드(Word) 단위로 정의되며, 4~64바이트 범위에서 설정됩니다. 영향: 공간적 지역성(Spatial Locality): 블록 크기가 클수록 연속된 데이터를 한 번에 가져올 수 있으므로, 공간적 지역성을 효과적으로 활용할 수 있습니다. 예: 배열이나 연속된 데이터 구조를 처리할 때 유리함. 캐시 오버헤드: 블록 크기가 클수록 캐시 내부에 저장할 수 있는 라인(Line) 수가 줄어듭니다. 예: 32KB 캐시에서 블록 크기가 16바이트일 때와 64바이트일 때를 비교하면, 후자의 경우 더 적은 수의 라인이 존재하게 됩니다. 캐시 미스(Cache Miss): 블록 크기가 작으면 캐시 미스 발생 횟수가 증가할 수 있지만, 불필요한 데이터를 로드하지 않아 메모리 대역폭을 절약할 수 있습니다. 설계 고려사항: 블록 크기를 결정할 때는 공간적 지역성과 캐시 용량 사이의 균형을 고려해야 합니다. 일반적인 블록 크기: L1 캐시: 16~64바이트 L2/L3 캐시: 64~128바이트 Mapping Function (매핑 함수) 정의: 매핑 함수는 메인 메모리 주소가 캐시의 어느 라인(Line)에 저장될지를 결정하는 규칙입니다. 종류 및 특징: Direct Mapping (직접 매핑): 메인 메모리의 특정 블록이 캐시의 특정 라인에만 매핑됩니다. 장점: 구현이 간단하고, 하드웨어 비용이 적음. 단점: 충돌(Collision)이 자주 발생하여 성능 저하 가능. 예: 두 개의 자주 사용되는 블록이 같은 캐시 라인을 공유하면, 계속해서 서로 교체되는 상황이 발생할 수 있음. Fully Associative Mapping (완전 연관 매핑): 메인 메모리의 블록이 캐시의 어느 라인에든 저장될 수 있습니다. 장점: 충돌이 거의 없으며, 캐시 공간을 효율적으로 사용. 단점: 구현이 복잡하고, 탐색 시간(Search Time)이 길어질 수 있음. Set Associative Mapping (집합 연관 매핑): 캐시를 여러 집합(Set)으로 나누고, 각 집합 내에서는 블록이 자유롭게 매핑됩니다. 예: 2-way Set Associative는 각 집합에 2개의 라인이 존재하며, 블록이 이 중 하나에 저장됨. 장점: Direct Mapping과 Fully Associative Mapping의 장점을 결합. 단점: 하드웨어 복잡도가 증가. 설계 고려사항: 매핑 방식은 충돌 최소화와 구현 복잡도 사이의 균형을 고려해야 합니다. 일반적으로 L1 캐시는 Direct Mapping, L2/L3 캐시는 Set Associative Mapping을 사용합니다. Replacement Algorithm (교체 알고리즘) 정의: 캐시가 가득 찼을 때, 새로운 데이터를 저장하기 위해 기존 데이터를 어떤 순서로 교체할지를 결정하는 알고리즘입니다. 종류 및 특징: LRU (Least Recently Used): 가장 오랫동안 사용되지 않은 데이터를 교체. 장점: 시간적 지역성을 잘 반영. 단점: 구현이 복잡하고, 하드웨어 비용이 증가. FIFO (First In First Out): 가장 먼저 들어온 데이터를 교체. 장점: 구현이 간단. 단점: 최근에 사용된 데이터를 잘못 교체할 수 있음. Random Replacement: 임의로 데이터를 선택하여 교체. 장점: 구현이 간단. 단점: 성능이 불규칙할 수 있음. 설계 고려사항: 교체 알고리즘은 성능과 구현 복잡도 사이의 균형을 고려해야 합니다. 일반적으로 LRU가 가장 많이 사용되지만, 하드웨어 비용을 절약하기 위해 FIFO나 Random Replacement도 사용될 수 있습니다. Write Policy (쓰기 정책) 정의: 프로세서가 데이터를 쓰는 방법을 결정하는 정책입니다. 종류 및 특징: Write-Through: 데이터를 캐시와 메인 메모리에 동시에 씁니다. 장점: 메인 메모리와 항상 일관성을 유지. 단점: 쓰기 작업이 많을 경우 성능 저하. Write-Back: 데이터를 캐시에만 쓰고, 메인 메모리에는 나중에 업데이트. 장점: 쓰기 작업이 적어 성능 향상. 단점: 메인 메모리와 일관성 문제 발생 가능. 설계 고려사항: 쓰기 정책은 데이터 일관성과 성능 사이의 균형을 고려해야 합니다. 일반적으로 Write-Back이 더 많이 사용되지만, 실시간 시스템에서는 Write-Through가 선호될 수 있습니다. Number of Cache Levels (캐시 레벨 수) 정의: 캐시 레벨 수는 계층적 메모리 구조에서 사용되는 캐시의 개수를 의미합니다. 일반적으로 L1, L2, L3 캐시로 구성됩니다. 영향: L1 캐시: 가장 빠르지만 용량이 작습니다. 프로세서와 직접 연결되어 있으며, 주로 명령어와 데이터를 분리하여 저장합니다(Instruc2on Cache와 Data Cache). L2 캐시: L1보다 느리지만 용량이 큽니다. L1 캐시 미스가 발생했을 때 데이터를 제공합니다. L3 캐시: L2보다 더 느리지만 용량이 큽니다. 여러 코어가 공유하며, 전체 시스템의 성능을 지원합니다. 설계 고려사항: 캐시 레벨 수는 성능과 비용 사이의 균형을 고려해야 합니다. 일반적으로 현대 CPU는 L1, L2, L3 캐시를 모두 사용합니다. 요약 및 결론 캐시 설계는 다음과 같은 요소들을 종합적으로 고려해야 합니다: 캐시 크기: 성능과 비용의 균형을 맞춰야 함. 블록 크기: 공간적 지역성을 활용하면서 캐시 용량을 최적화. 매핑 함수: 충돌 최소화와 구현 복잡도 사이의 균형. 교체 알고리즘: 성능과 구현 복잡도 사이의 균형. 쓰기 정책: 데이터 일관성과 성능 사이의 균형. 캐시 레벨 수: 계층적 구조를 통해 성능과 비용의 균형을 맞춤. $\\boxed{\\text{결론적으로, 캐시 설계는 다양한 요소를 조합하여 시스템 성능을 최적화하는 복잡한 과정입니다.}}$ cache mapping function 아래는 주어진 내용을 정확히 번역하고, 각 문장을 상세히 설명한 것입니다. 번역 Mapping Function 블록이 어느 캐시 위치를 차지할지를 결정한다. 두 가지 제약 조건이 설계에 영향을 미침 한 블록을 읽어올 때, 다른 블록은 교체되어야 할 수도 있다. 매핑 함수가 더 유연할수록, 캐시를 탐색하는 데 필요한 회로가 더 복잡해진다. 상세한 설명 Mapping Function (매핑 함수) 정의: 매핑 함수는 메인 메모리의 특정 블록이 캐시의 어느 위치에 저장될지를 결정하는 규칙입니다. 예를 들어, 메인 메모리 주소 0x1234 에 해당하는 데이터가 캐시의 어느 라인(Line)에 저장될지 매핑 함수가 결정합니다. 목적: 캐시는 메인 메모리보다 작기 때문에, 모든 메인 메모리 데이터를 동시에 저장할 수 없습니다. 따라서, 매핑 함수는 어떤 데이터가 캐시에 저장될지, 그리고 캐시 내에서 어디에 저장될지를 결정합니다. 중요성: 매핑 함수는 캐시의 성능과 효율성을 크게 좌우합니다. 잘못 설계된 매핑 함수는 충돌(Collision)을 증가시키고, 캐시 히트(Cache Hit) 확률을 낮출 수 있습니다. 두 가지 제약 조건 (1) When one block is read in, another may have to be replaced 번역: 한 블록을 읽어올 때, 다른 블록은 교체되어야 할 수도 있다. 설명: 캐시는 용량이 제한적이므로, 새로운 데이터를 저장하려면 기존 데이터를 교체해야 하는 경우가 발생합니다. 이 과정은 캐시 교체 정책(Replacement Policy)에 따라 수행됩니다. 예: LRU(Least Recently Used), FIFO(First In First Out), Random Replacement 등. 문제점: 만약 자주 사용되는 데이터가 교체된다면, 성능 저하가 발생할 수 있습니다. 특히, 직접 매핑(Direct Mapping) 방식에서는 충돌(Collision)이 자주 발생하여 교체가 비효율적으로 이루어질 가능성이 큽니다. (2) The more flexible the mapping function, the more complex is the circuitry required to search the cache 번역: 매핑 함수가 더 유연할수록, 캐시를 탐색하는 데 필요한 회로가 더 복잡해진다. 설명: 매핑 함수의 유연성은 캐시의 성능을 향상시킬 수 있지만, 그만큼 하드웨어 구현이 복잡해집니다. 유연성(Flexibility): 직접 매핑(Direct Mapping): 가장 단순한 방식으로, 메인 메모리 블록이 캐시의 특정 라인에만 매핑됩니다. 장점: 구현이 간단하고, 탐색 시간이 짧음. 단점: 충돌이 자주 발생하여 성능 저하 가능. 완전 연관 매핑(Fully Associative Mapping): 메인 메모리 블록이 캐시의 어느 라인에든 저장될 수 있습니다. 장점: 충돌이 거의 없으며, 캐시 공간을 효율적으로 사용. 단점: 탐색(Search)이 복잡해져서 하드웨어 비용이 증가. 집합 연관 매핑(Set Associative Mapping): 캐시를 여러 집합(Set)으로 나누고, 각 집합 내에서는 블록이 자유롭게 매핑됩니다. 장점: Direct Mapping과 Fully Associative Mapping의 장점을 결합. 단점: 하드웨어 복잡도가 증가. 결론: 매핑 함수의 유연성을 높일수록, 캐시 탐색(Search)을 위한 비교 및 제어 회로가 더 복잡해집니다. 이를 위해 추가적인 하드웨어 리소스(예: 비교기, 제어 논리)가 필요하며, 이는 전력 소비와 비용을 증가시킵니다. 요약 및 결론 매핑 함수(Mapping Function)는 메인 메모리 블록이 캐시의 어느 위치에 저장될지를 결정하는 중요한 역할을 합니다. 잘못 설계된 매핑 함수는 충돌을 증가시키고, 캐시 성능을 저하시킬 수 있습니다. 두 가지 제약 조건: 교체 문제: 캐시가 가득 찼을 경우, 새로운 데이터를 저장하기 위해 기존 데이터를 교체해야 합니다. 이 과정은 캐시 교체 정책에 따라 수행됩니다. 복잡성 문제: 매핑 함수가 더 유연할수록, 캐시 탐색을 위한 하드웨어 회로가 더 복잡해집니다. 이는 비용과 전력 소비를 증가시키는 요인이 됩니다. 최적화: 매핑 함수는 성능, 구현 복잡도, 비용 사이의 균형을 고려하여 설계해야 합니다. 일반적으로 현대 컴퓨터 시스템에서는 Set Associative Mapping 방식이 널리 사용되며, 이는 성능과 구현 복잡도 사이의 적절한 균형을 제공합니다. $\\boxed{\\text{결론적으로, 매핑 함수는 캐시 설계에서 매우 중요한 요소이며, 성능과 복잡성 사이의 균형을 맞추는 것이 핵심입니다.}}$ cache mapping function replace algorithm 아래는 주어진 내용을 정확히 번역하고, 각 문장을 상세히 설명한 것입니다. 번역 Replacement Algorithm (교체 알고리즘) 새로운 블록이 캐시에 로드될 때, 어느 블록을 교체할지를 선택합니다. Least Recently Used (LRU) 알고리즘: 효과적인 전략은 캐시에서 가장 오랫동안 참조되지 않은 블록을 교체하는 것입니다. 이를 위해 하드웨어 메커니즘이 필요하며, 이는 가장 최근에 사용되지 않은 블록을 식별하는 역할을 합니다. 상세한 설명 Replacement Algorithm (교체 알고리즘) 정의: 교체 알고리즘은 캐시가 가득 찼을 때, 새로운 데이터를 저장하기 위해 기존 데이터 중 어떤 블록을 교체할지를 결정하는 규칙입니다. 캐시는 용량이 제한적이므로, 새로운 데이터를 저장하려면 기존 데이터를 제거해야 하는 경우가 발생합니다. 예: LRU(Least Recently Used), FIFO(First In First Out), Random Replacement 등. 목적: 교체 알고리즘은 캐시 히트(Cache Hit) 확률을 최대화하면서 성능을 유지하기 위한 핵심 요소입니다. 잘못된 교체 알고리즘은 자주 사용되는 데이터를 잘못 교체하여 성능 저하를 초래할 수 있습니다. Least Recently Used (LRU) Algorithm (1) Chooses which block to replace when a new block is to be loaded into the cache 번역: 새로운 블록이 캐시에 로드될 때, 어느 블록을 교체할지를 선택합니다. 설명: LRU 알고리즘은 \"최근에 사용되지 않은 데이터\"를 교체 대상으로 선택합니다. 예를 들어, 캐시가 가득 찼을 때, 가장 오랫동안 참조되지 않은 블록을 제거하고 새로운 데이터를 저장합니다. 이 방식은 시간적 지역성(Temporal Locality)을 잘 반영하며, 자주 사용되는 데이터를 캐시에 유지할 가능성이 높습니다. (2) An effective strategy is to replace a block that has been in the cache the longest with no references to it 번역: 효과적인 전략은 캐시에서 가장 오랫동안 참조되지 않은 블록을 교체하는 것입니다. 설명: LRU 알고리즘의 핵심 아이디어는 시간적 지역성을 활용하는 것입니다. 최근에 사용된 데이터는 다시 사용될 가능성이 높습니다. 따라서, 오랫동안 참조되지 않은 데이터는 앞으로도 사용될 가능성이 낮다고 가정합니다. 예를 들어, 다음과 같은 시나리오를 고려해 보겠습니다: 캐시에 세 개의 블록(A, B, C)이 있고, A는 최근에 참조되었지만, B와 C는 오랫동안 참조되지 않았습니다. 이 경우, LRU 알고리즘은 B 또는 C를 교체 대상으로 선택합니다. (3) Hardware mechanisms are needed to identify the least recently used block 번역: 이를 위해 하드웨어 메커니즘이 필요하며, 이는 가장 최근에 사용되지 않은 블록을 식별하는 역할을 합니다. 설명: LRU 알고리즘을 구현하기 위해서는 하드웨어 지원이 필요합니다. 구현 방법: 카운터 기반 방식: 각 캐시 라인에 타임스탬프(시간 정보)를 할당하여, 마지막 참조 시간을 추적합니다. 예: 캐시 라인이 참조될 때마다 카운터 값을 업데이트하고, 가장 오래된 카운터 값을 가진 라인을 교체 대상으로 선택합니다. 스택 기반 방식: 스택(Stack) 자료구조를 사용하여, 가장 최근에 참조된 블록을 스택의 맨 위로 이동시키고, 가장 오래된 블록은 스택의 맨 아래에 위치하도록 관리합니다. 문제점: LRU 알고리즘은 매우 효과적이지만, 하드웨어 구현이 복잡하고 비용이 많이 듭니다. 특히, 캐시 크기가 클수록 탐색(Search)과 관리(Management)가 어려워집니다. 요약 및 결론 교체 알고리즘: 새로운 데이터를 저장하기 위해 기존 데이터 중 어떤 블록을 교체할지를 결정하는 중요한 역할을 합니다. 잘못된 교체 알고리즘은 자주 사용되는 데이터를 잘못 교체하여 성능 저하를 초래할 수 있습니다. LRU 알고리즘: 가장 오랫동안 참조되지 않은 블록을 교체 대상으로 선택합니다. 시간적 지역성을 잘 반영하며, 자주 사용되는 데이터를 캐시에 유지할 가능성이 높습니다. 하지만, 하드웨어 구현이 복잡하고 비용이 많이 드는 단점이 있습니다. 하드웨어 메커니즘: LRU 알고리즘을 구현하기 위해서는 하드웨어 지원(예: 카운터, 스택)이 필요합니다. 이를 통해 가장 최근에 사용되지 않은 블록을 식별하고, 효율적으로 교체할 수 있습니다. \\boxed{\\text{결론적으로, LRU 알고리즘은 캐시 성능을 극대화하는 데 효과적이지만, 구현 복잡도와 비용이 높다는 점을 고려해야 합니다.}} cache write policy 아래는 주어진 내용을 정확히 번역하고, 각 문장을 상세히 설명한 것입니다. 번역 Write Policy (쓰기 정책) 블록이 업데이트될 때마다 발생할 수 있음. 블록이 교체될 때 발생할 수 있음. 쓰기 작업을 최소화함. 메인 메모리를 오래된 상태로 남겨둠. 상세한 설명 Write Policy (쓰기 정책) 정의: 쓰기 정책은 프로세서가 데이터를 수정(쓰기)할 때, 해당 데이터가 캐시와 메인 메모리에 어떻게 반영될지를 결정하는 규칙입니다. 캐시와 메인 메모리는 항상 일관성을 유지해야 하므로, 쓰기 정책은 매우 중요합니다. 대표적인 쓰기 정책으로는 Write-Through와 Write-Back이 있습니다. 1-1. \"Can occur every time the block is updated\" 번역: 블록이 업데이트될 때마다 발생할 수 있음. 설명: 이는 Write-Through 정책을 설명합니다. Write-Through: 프로세서가 데이터를 수정할 때마다, 해당 데이터를 캐시와 메인 메모리에 동시에 반영합니다. 장점: 메인 메모리와 캐시가 항상 일관성을 유지합니다. 데이터 손실 위험이 적습니다. 단점: 매번 메인 메모리에 쓰기를 수행하므로, 성능 저하가 발생할 수 있습니다. 특히, 쓰기 작업이 많은 경우 메인 메모리 액세스 시간이 병목 현상을 일으킬 수 있습니다. 1-2. \"Can occur when the block is replaced\" 번역: 블록이 교체될 때 발생할 수 있음. 설명: 이는 Write-Back 정책을 설명합니다. Write-Back: 프로세서가 데이터를 수정할 때, 해당 데이터는 캐시에만 저장됩니다. 메인 메모리에는 캐시 블록이 교체될 때만 데이터가 반영됩니다. 장점: 쓰기 작업이 메인 메모리에 직접 전달되지 않으므로, 성능이 향상됩니다. 특히, 쓰기 작업이 많은 경우 유리합니다. 단점: 메인 메모리와 캐시 간의 일관성 문제가 발생할 수 있습니다. 예를 들어, 시스템이 갑작스럽게 종료되면, 캐시에만 저장된 데이터가 손실될 수 있습니다. 1-2-1. \"Minimizes write operations\" 번역: 쓰기 작업을 최소화함. 설명: Write-Back 정책은 쓰기 작업을 최소화하는 데 초점을 맞춥니다. 캐시에만 데이터를 저장하고, 메인 메모리에는 블록 교체 시에만 데이터를 반영합니다. 따라서, 메인 메모리에 대한 쓰기 작업 횟수를 줄여 성능을 개선합니다. 반면, Write-Through 정책은 매번 메인 메모리에 쓰기를 수행하므로, 쓰기 작업이 많아질 가능성이 큽니다. 1-2-2. \"Leaves main memory in an obsolete state\" 번역: 메인 메모리를 오래된 상태로 남겨둠. 설명: Write-Back 정책에서는 메인 메모리가 캐시와 동기화되지 않은 상태로 남아 있을 수 있습니다. 예를 들어, 캐시에 수정된 데이터가 저장되어 있지만, 메인 메모리는 아직 이전 상태를 유지하고 있을 수 있습니다. 이를 오래된 상태(Obsolete State)라고 합니다. 이러한 문제는 다음과 같은 경우에 발생할 수 있습니다: 다른 프로세서나 장치가 메인 메모리에서 데이터를 읽으려 할 때, 잘못된 데이터를 참조할 가능성. 시스템 장애가 발생하면, 캐시에만 저장된 데이터가 손실될 가능성. 이를 해결하기 위해, Dirty Bit라는 메커니즘이 사용됩니다. Dirty Bit: 캐시 블록이 수정되었음을 표시하는 비트입니다. Dirty Bit가 설정된 블록은 메인 메모리로 데이터가 반영되어야 함을 나타냅니다. 요약 및 결론 쓰기 정책: 쓰기 정책은 캐시와 메인 메모리 간의 데이터 일관성을 유지하는 중요한 역할을 합니다. 대표적인 정책으로는 Write-Through와 Write-Back이 있습니다. Write-Through: 데이터를 매번 캐시와 메인 메모리에 동시에 반영합니다. 메인 메모리와 항상 일관성을 유지하지만, 성능 저하가 발생할 수 있습니다. Write-Back: 데이터를 캐시에만 저장하고, 메인 메모리에는 블록 교체 시에만 반영합니다. 쓰기 작업을 최소화하여 성능을 향상시키지만, 메인 메모리가 오래된 상태로 남아 있을 수 있습니다. 메인 메모리의 오래된 상태: Write-Back 정책에서는 메인 메모리가 캐시와 동기화되지 않은 상태로 남아 있을 수 있습니다. 이를 해결하기 위해 Dirty Bit와 같은 메커니즘이 사용됩니다. $\\boxed{\\text{결론적으로, 쓰기 정책은 데이터 일관성과 성능 사이의 균형을 맞추는 중요한 요소입니다.}}$ OS microkernel System Structure 아래는 주어진 내용을 정확히 번역하고, 각 문장을 상세히 설명한 것입니다. 번역 Microkernel System Structure (마이크로커널 시스템 구조) 커널에서 가능한 많은 기능을 \"사용자 공간\"으로 이동시킴. 사용자 모듈 간의 통신은 메시지 전달(Message Passing)을 통해 이루어짐. 장점: 마이크로커널을 확장하기 쉽다. 새로운 아키텍처로 운영 체제를 포팅하기 쉽다. 더 신뢰할 수 있다 (커널 모드에서 실행되는 코드가 적음). 더 안전하다. 단점: 사용자 공간과 커널 공간 간 통신의 성능 오버헤드. 상세한 설명 Microkernel System Structure (마이크로커널 시스템 구조) (1) Moves as much from the kernel into \"user\" space 번역: 커널에서 가능한 많은 기능을 \"사용자 공간\"으로 이동시킴. 설명: 마이크로커널(Microkernel)은 최소한의 핵심 기능만 커널에 남기고, 나머지 기능은 사용자 공간(User Space)으로 이동시키는 설계 방식입니다. 커널은 기본적으로 다음과 같은 최소한의 기능만 수행합니다: 프로세스 간 통신(Inter-Process Communication, IPC) 낮은 수준의 메모리 관리 스케줄링(Scheduling) 하드웨어 추상화 파일 시스템, 장치 드라이버, 네트워크 스택 등은 사용자 공간에서 실행됩니다. 이는 Monolithic Kernel(모놀리식 커널)과 대비되며, 모놀리식 커널은 모든 서비스를 커널 공간에서 실행합니다. (2) Communication takes place between user modules using message passing 번역: 사용자 모듈 간의 통신은 메시지 전달(Message Passing)을 통해 이루어짐. 설명: 마이크로커널에서는 커널과 사용자 공간 간, 또는 사용자 공간 내부의 모듈 간 통신이 메시지 전달을 통해 이루어집니다. 메시지 전달(Message Passing): 데이터를 패킷 형태로 전송하여 다른 모듈과 통신하는 방식입니다. 예: 파일 시스템 요청이나 장치 드라이버와의 상호작용. 메시지 전달은 커널이 중개 역할을 하며, 이를 통해 각 모듈이 독립적으로 실행될 수 있습니다. 이 방식은 모듈 간의 결합도를 낮추고, 시스템의 유연성을 높이는 데 기여합니다. Benefits (장점) (1) Easier to extend a microkernel 번역: 마이크로커널을 확장하기 쉽다. 설명: 마이크로커널은 최소한의 기능만 커널에 남기고, 나머지 기능은 사용자 공간에서 실행됩니다. 따라서, 새로운 서비스나 기능을 추가하려면 커널을 수정하지 않고 사용자 공간에서 새로운 모듈을 작성하면 됩니다. 예: 새로운 파일 시스템을 추가하거나, 새로운 장치 드라이버를 개발할 때 커널 재컴파일이 필요하지 않습니다. (2) Easier to port the operating system to new architectures 번역: 새로운 아키텍처로 운영 체제를 포팅하기 쉽다. 설명: 마이크로커널은 최소한의 하드웨어 의존적 코드만 커널에 포함합니다. 따라서, 새로운 하드웨어 아키텍처로 운영 체제를 이식(Porting)할 때, 커널 부분만 수정하면 됩니다. 사용자 공간의 모듈들은 하드웨어와 직접적인 연관이 없으므로, 재사용 가능합니다. (3) More reliable (less code is running in kernel mode) 번역: 더 신뢰할 수 있다 (커널 모드에서 실행되는 코드가 적음). 설명: 커널 모드에서 실행되는 코드는 시스템 전체에 영향을 미칠 수 있으므로, 버그가 발생할 경우 심각한 문제가 발생할 수 있습니다. 마이크로커널은 커널 모드에서 실행되는 코드를 최소화하므로, 잠재적인 버그가 발생할 확률이 줄어듭니다. 또한, 사용자 공간에서 실행되는 코드는 문제가 발생해도 시스템 전체가 다운되지 않으며, 복구가 용이합니다. (4) More secure 번역: 더 안전하다. 설명: 마이크로커널은 권한 분리를 강화합니다. 사용자 공간에서 실행되는 모듈들은 제한된 권한으로 동작하며, 커널에 접근할 수 없습니다. 따라서, 악성 코드가 시스템 전체에 영향을 미치는 위험을 줄일 수 있습니다. Detriments (단점) (1) Performance overhead of user space to kernel space communication 번역: 사용자 공간과 커널 공간 간 통신의 성능 오버헤드. 설명: 마이크로커널에서는 사용자 공간과 커널 공간 간 통신이 메시지 전달을 통해 이루어집니다. 이 과정에서 다음과 같은 성능 오버헤드가 발생할 수 있습니다: 컨텍스트 전환(Context Switching): 사용자 모드와 커널 모드 간 전환이 필요함. 데이터 복사(Data Copying): 메시지를 전달하기 위해 데이터를 복사해야 함. 이러한 오버헤드는 특히 고성능이 요구되는 환경에서 문제가 될 수 있습니다. 반면, 모놀리식 커널은 모든 서비스가 커널 공간에서 실행되므로, 이러한 오버헤드가 적습니다. 요약 및 결론 마이크로커널 구조: 커널에서 가능한 많은 기능을 사용자 공간으로 이동시켜, 최소한의 핵심 기능만 커널에 남깁니다. 사용자 모듈 간 통신은 메시지 전달을 통해 이루어집니다. 장점: 확장성이 뛰어나며, 새로운 서비스를 쉽게 추가할 수 있습니다. 새로운 하드웨어 아키텍처로 이식하기 쉽습니다. 커널 모드에서 실행되는 코드가 적어 신뢰성과 보안성이 높습니다. 단점: 사용자 공간과 커널 공간 간 통신의 성능 오버헤드가 발생합니다. 특히, 컨텍스트 전환과 데이터 복사로 인해 성능 저하가 발생할 수 있습니다. \\boxed{\\text{결론적으로, 마이크로커널 구조는 유연성과 보안성을 강화하지만, 성능 오버헤드라는 단점을 가지고 있습니다.}} Monolithic System Structure 아래는 주어진 내용을 정확히 번역하고, 각 문장을 상세히 설명한 것입니다. 번역 Monolithic System Structure (모놀리식 시스템 구조) 모든 운영체제 기능을 단일 커널 블록에 포함시킴. 구성 요소들이 직접 상호작용함. 장점: 빠름. 단점: 복잡한 커널. 유연하지 않음. 상세한 설명 Monolithic System Structure (모놀리식 시스템 구조) (1) Put every OS functions into the single block of kernel 번역: 모든 운영체제 기능을 단일 커널 블록에 포함시킴. 설명: 모놀리식 커널(Monolithic Kernel)은 운영체제의 모든 핵심 기능(예: 프로세스 관리, 메모리 관리, 파일 시스템, 장치 드라이버 등)을 하나의 큰 커널 내부에서 실행합니다. 이는 커널이 모든 서비스를 직접 처리한다는 것을 의미하며, 사용자 공간과 커널 공간의 명확한 분리가 없습니다. 예를 들어, Linux 커널은 모놀리식 구조를 따르며, 대부분의 기능이 커널 내부에서 동작합니다. (2) Parts directly interact 번역: 구성 요소들이 직접 상호작용함. 설명: 모놀리식 커널에서는 커널 내부의 다양한 구성 요소(예: 메모리 관리, 프로세스 스케줄링, 파일 시스템)가 서로 직접적으로 호출하거나 상호작용할 수 있습니다. 예: 파일 시스템 모듈이 메모리 관리 모듈을 직접 호출하여 데이터를 읽거나 쓸 수 있습니다. 이러한 직접적인 상호작용은 성능을 극대화하지만, 코드 간의 결합도(Coupling)가 높아져 유지보수가 어렵게 만듭니다. Benefits (장점) (1) Fast 번역: 빠름. 설명: 모놀리식 커널은 모든 기능이 커널 내부에서 실행되므로, 사용자 공간과 커널 공간 간의 컨텍스트 전환(Context Switching)이나 메시지 전달(Message Passing)이 필요하지 않습니다. 따라서, 성능이 매우 빠릅니다. 예: 마이크로커널 구조에서는 사용자 공간과 커널 공간 간 통신으로 인해 오버헤드가 발생하지만, 모놀리식 커널에서는 이러한 오버헤드가 없습니다. 특히, 고성능이 요구되는 환경(예: 서버, 임베디드 시스템)에서 유리합니다. Detriments (단점) (1) Complex kernel 번역: 복잡한 커널. 설명: 모놀리식 커널은 모든 기능이 하나의 커널 내부에서 실행되므로, 코드 규모가 매우 큽니다. 또한, 각 구성 요소 간의 직접적인 상호작용으로 인해 코드 간의 의존성이 높아지고, 유지보수가 어려워집니다. 예: 새로운 기능을 추가하거나 수정하려면 전체 커널을 재컴파일해야 할 수 있습니다. 이러한 복잡성은 버그(Bug) 발생 가능성을 높이며, 디버깅(Debugging)이 어려워질 수 있습니다. (2) Inflexible 번역: 유연하지 않음. 설명: 모놀리식 커널은 확장성(Extensibility)이 낮습니다. 새로운 하드웨어 아키텍처나 기능을 추가하려면 커널 코드를 수정하고 재컴파일해야 합니다. 또한, 특정 모듈(예: 파일 시스템, 장치 드라이버)을 독립적으로 개발하거나 테스트하기 어렵습니다. 반면, 마이크로커널 구조는 사용자 공간에서 모듈을 독립적으로 실행할 수 있으므로, 더 유연합니다. 요약 및 결론 모놀리식 커널 구조: 모든 운영체제 기능을 단일 커널 내부에 포함시키고, 구성 요소들이 직접 상호작용합니다. 이는 성능을 극대화하지만, 복잡성과 유연성 부족이라는 단점을 가지고 있습니다. 장점: 사용자 공간과 커널 공간 간 통신 오버헤드가 없으므로, 성능이 매우 빠릅니다. 단점: 커널이 매우 복잡하고, 유지보수가 어렵습니다. 확장성이 낮아 새로운 기능이나 하드웨어 아키텍처를 추가하기 어렵습니다. \\boxed{\\text{결론적으로, 모놀리식 커널 구조는 성능이 뛰어나지만, 복잡성과 유연성 부족이라는 단점이 있습니다.}} module 아래는 주어진 내용을 정확히 번역하고, 각 문장을 상세히 설명한 것입니다. 번역 Modules (모듈) 대부분의 현대 운영체제는 커널 모듈을 구현합니다. 객체 지향적 접근 방식을 사용합니다. 각 핵심 구성 요소는 분리되어 있습니다. 각 구성 요소는 알려진 인터페이스를 통해 다른 구성 요소와 통신합니다. 각 구성 요소는 필요할 때 커널 내에서 동적으로 로드됩니다. 전체적으로, 계층 구조와 유사하지만 더 유연합니다. 상세한 설명 Modules (모듈) (1) Most modern operating systems implement kernel modules 번역: 대부분의 현대 운영체제는 커널 모듈을 구현합니다. 설명: 커널 모듈(Kernel Module)은 운영체제의 특정 기능을 독립적으로 구현한 소프트웨어 구성 요소입니다. 이는 운영체제의 핵심 기능을 확장하거나 수정하기 위해 사용됩니다. 예: Linux 커널은 다양한 모듈(예: 파일 시스템, 장치 드라이버 등)을 지원하며, 필요에 따라 동적으로 로드하거나 언로드할 수 있습니다. (2) Uses object-oriented approach 번역: 객체 지향적 접근 방식을 사용합니다. 설명: 커널 모듈은 객체 지향 프로그래밍(Object-Oriented Programming, OOP) 원칙을 따르며 설계됩니다. 각 모듈은 독립적인 객체처럼 동작하며, 명확한 경계와 역할을 가집니다. 객체 지향적 특징: 캡슐화(Encapsulation): 각 모듈은 자신의 데이터와 기능을 캡슐화하여 외부로부터 보호합니다. 재사용성(Reusability): 모듈은 다른 시스템에서도 재사용될 수 있습니다. 확장성(Extensibility): 새로운 기능을 추가하기 쉽습니다. (3) Each core component is separate 번역: 각 핵심 구성 요소는 분리되어 있습니다. 설명: 커널 모듈은 서로 독립적으로 설계되며, 각각의 모듈이 특정 기능을 담당합니다. 예: 파일 시스템 모듈, 네트워크 스택 모듈, 장치 드라이버 모듈 등이 각각 분리되어 있습니다. 이러한 분리는 코드 간의 결합도(Coupling)를 낮추고, 유지보수성을 높이는 데 기여합니다. (4) Each talks to the others over known interfaces 번역: 각 구성 요소는 알려진 인터페이스를 통해 다른 구성 요소와 통신합니다. 설명: 모듈 간의 상호작용은 표준화된 인터페이스를 통해 이루어집니다. 예: 파일 시스템 모듈이 메모리 관리 모듈과 통신할 때, 미리 정의된 API(Application Programming Interface)를 사용합니다. 이러한 방식은 모듈 간의 의존성을 줄이고, 독립적인 개발 및 테스트를 가능하게 합니다. (5) Each is loadable as needed within the kernel 번역: 각 구성 요소는 필요할 때 커널 내에서 동적으로 로드됩니다. 설명: 커널 모듈은 동적 로딩(Dynamic Loading)을 지원하므로, 필요할 때만 메모리에 로드됩니다. 예: USB 장치를 연결하면 관련된 드라이버 모듈이 동적으로 로드되고, 장치를 제거하면 모듈이 언로드됩니다. 이러한 동적 로딩은 리소스 사용을 최적화하고, 커널 크기를 줄이는 데 도움이 됩니다. Overall, similar to layers but with more flexible (1) Similar to layers 번역: 계층 구조와 유사함. 설명: 커널 모듈은 계층 구조(Layered Structure)와 유사한 방식으로 작동합니다. 각 모듈은 특정 계층에서 실행되며, 하위 계층과 상호작용합니다. 예: 하드웨어 추상화 계층(Hardware Abstraction Layer, HAL)은 하위 계층(하드웨어)과 상호작용하고, 상위 계층(파일 시스템)에 서비스를 제공합니다. (2) More flexible 번역: 더 유연함. 설명: 커널 모듈은 계층 구조보다 더 유연합니다. 이유: 독립성: 각 모듈은 독립적으로 개발, 테스트, 로드, 언로드될 수 있습니다. 확장성: 새로운 모듈을 쉽게 추가하거나 기존 모듈을 수정할 수 있습니다. 동적 로딩: 모듈이 필요할 때만 로드되므로, 리소스를 효율적으로 사용할 수 있습니다. 예: 마이크로커널 구조와 유사한 유연성을 가지지만, 성능 오버헤드가 적습니다. 요약 및 결론 커널 모듈: 커널 모듈은 운영체제의 특정 기능을 독립적으로 구현한 소프트웨어 구성 요소입니다. 객체 지향적 접근 방식을 사용하여 설계되며, 각 모듈은 분리되어 있고 표준화된 인터페이스를 통해 상호작용합니다. 유사점: 커널 모듈은 계층 구조와 유사하게 작동하지만, 더 유연합니다. 장점: 유연성: 독립적인 개발, 테스트, 로드, 언로드가 가능합니다. 확장성: 새로운 기능을 쉽게 추가하거나 수정할 수 있습니다. 효율성: 동적 로딩을 통해 리소스 사용을 최적화합니다. $\\boxed{\\text{결론적으로, 커널 모듈은 현대 운영체제에서 유연성과 확장성을 제공하는 중요한 구성 요소입니다.}}$ PCB, TCB /** * 🔹 프로세스 상태 상수 * 프로세스(또는 스레드)가 가질 수 있는 상태를 나타냅니다. */ #define TASK_RUNNING 0 // 실행 중이거나 실행 가능한 상태 (큐에 등록됨) #define TASK_INTERRUPTIBLE 1 // 인터럽트 가능한 대기 상태 (시그널에 의해 깨어날 수 있음) #define TASK_UNINTERRUPTIBLE 2 // 인터럽트 불가능한 대기 상태 (예: 디스크 IO 대기) #define TASK_STOPPED 4 // 멈춤 상태 (SIGSTOP 등으로 멈춤) #define TASK_TRACED 8 // 디버거에 의해 추적 중인 상태 /** * 🔹 Forward Declaration * 아래 구조체들은 다른 헤더 파일에서 정의되며, 여기서는 포인터 사용을 위해 선언만 합니다. */ struct files_struct; // 열린 파일 디스크립터 정보 struct fs_struct; // 파일 시스템 관련 정보 (현재 디렉토리 등) struct signal_struct; // 시그널 핸들러 및 대기 시그널 목록 struct sched_entity; // CFS 스케줄러에 사용되는 스케줄링 엔티티 struct mm_struct; // 메모리 공간 정보 (코드, 스택, 힙 등 관리) struct vm_area_struct; // 가상 메모리 영역(VMA) 정보 struct cred; // 사용자/그룹 ID 및 권한 정보 (보안) /** * 🔹 struct thread_struct - 스레드 실행 컨텍스트 * CPU 레지스터 상태 및 실행 위치를 저장하는 구조체입니다. * 컨텍스트 스위칭(다른 스레드/프로세스로 전환) 시 사용됩니다. */ struct thread_struct { unsigned long sp; // 스택 포인터 (스택 전환용) unsigned long ip; // 명령 포인터 (프로그램 카운터) unsigned long fs, gs; // TLS(Thread Local Storage)에 사용되는 세그먼트 레지스터 unsigned long cr2, trap_no; // 페이지 폴트 발생 시 주소 및 예외 번호 unsigned long debugreg[8]; // 하드웨어 디버깅용 레지스터 (브레이크포인트 설정용) unsigned long kernel_gs_base; // 커널 모드에서 GS 레지스터 백업용 struct fpu fpu; // FPU 상태 (SSE / AVX 등 부동소수점 연산 처리) }; /** * 🔹 struct mm_struct - 메모리 관리 구조체 * 하나의 프로세스가 사용하는 전체 메모리 공간 정보를 저장합니다. * 코드 영역, 스택, 힙, mmap 등을 포함합니다. */ struct mm_struct { struct vm_area_struct *mmap; // 가상 메모리 영역(VMA) 연결 리스트 unsigned long start_code, end_code; // 코드 영역 시작 및 끝 주소 unsigned long start_stack; // 스택 시작 주소 unsigned long start_brk, brk; // 힙(heap)의 시작 주소 및 현재 크기 }; /** * 🔹 struct task_struct - 프로세스 또는 스레드의 전체 정보 * Linux 커널에서 하나의 실행 단위(스레드 또는 프로세스)를 나타내는 핵심 구조체입니다. * PCB(Process Control Block) + TCB(Thread Control Block)를 모두 포함합니다. */ struct task_struct { pid_t pid; // 고유한 스레드 ID (PID) pid_t tgid; // 스레드 그룹 ID — 같은 프로세스 내 모든 스레드가 공유 volatile long state; // 프로세스 상태: RUNNING, INTERRUPTIBLE, STOPPED, TRACED 등 struct list_head tasks; // 전체 프로세스 리스트에 연결된 노드 struct mm_struct *mm; // 메모리 관리 구조체 (코드/힙/스택 포함) struct files_struct *files; // 열린 파일 디스크립터 테이블 struct fs_struct *fs; // 파일 시스템 관련 정보 (현재 디렉토리 등) struct signal_struct *signal; // 시그널 처리 정보 및 대기 중인 시그널 목록 struct task_struct *real_parent; // 실제 부모 프로세스 (디버거가 아님) struct task_struct *parent; // 트레이싱 부모 (예: 디버거) struct list_head children; // 자식 프로세스 리스트 struct list_head sibling; // 형제 프로세스(같은 부모를 가진 다른 자식) 리스트 unsigned int flags; // 다양한 플래그 (PF_EXITING, PF_KTHREAD 등) int prio, static_prio, normal_prio; // 스케줄링 우선순위 정보 struct sched_entity se; // CFS 스케줄러용 스케줄링 엔티티 struct thread_struct thread; // CPU 레지스터 및 실행 컨텍스트 struct cred *cred; // 권한 정보 (UID, GID 등 보안 정보) u64 utime, stime; // 사용자 모드 시간, 커널 모드 시간 struct rcu_head rcu; // RCU 동기화용 구조체 (Read-Copy-Update) }; IPC shared Memory Message Passing Direct vs Indirect 프로세스들이 서로를 직접 지정하여 메시지를 전송하거나 수신 하는 방식입니다. 메시지는 메일박스 (Mailbox) 또는 포트(Port)를 통해 전달됩니다. , 프로세스 자신이 아니라 공유된 mailbox를 통해 통신 합니다. Blocking (Synchronous) vs Non-blocking (Asynchronous) 동기적 전송 vs 비동기적 전송 ㅇ IPC SHARE 아래는 mmap() 함수의 각 인자에 대한 상세한 설명과, 각각 사용할 수 있는 가능한 옵션, 그리고 예시입니다. 🔧 mmap() 함수 원형 void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset); 📌 목적: mmap() 은 파일이나 디바이스를 프로세스 주소 공간에 메모리 매핑하여 읽거나 쓸 수 있도록 해주는 시스템 콜입니다. 파일을 메모리처럼 접근 가능하게 함 → 빠른 입출력 처리 가능 공유 메모리를 통해 여러 프로세스 간 통신(IPC) 가능 ✅ 각 인자 설명 및 가능한 값들 void *addr 요청하는 시작 주소 (NULL 권장) NULL: 커널이 적절한 위치에 할당 특정 주소 지정: 해당 주소 근처에 매핑 시도 (실제로 잘 사용되지 않음) 값 의미 NULL 커널이 자동으로 주소 선택 0x12345678 특정 주소에 매핑 시도 (권장되지 않음) size_t length (또는 siz) 매핑할 데이터 크기 (바이트 단위) 보통 페이지 크기(4KB 등)의 배수로 설정해야 함 💡 예: 4096, 8192, 16384 등 int prot - Protection (접근 권한) 메모리 영역에 대해 허용할 접근 권한 지정 플래그 의미 PROT_NONE 접근 금지 PROT_READ 읽기 허용 PROT_WRITE 쓰기 허용 PROT_EXEC 실행 허용 예시 조합 PROT_READ | PROT_WRITE // 읽기/쓰기 모두 가능 PROT_READ // 읽기 전용 PROT_EXEC // 실행 가능 코드 매핑 int flags - Mapping options 매핑 방식(공유 여부, 파일 기반/익명 등) 결정 플래그 의미 MAP_SHARED 수정사항이 다른 프로세스와 공유됨 MAP_PRIVATE Copy-on-write 방식으로 복사본 사용 MAP_ANONYMOUS 파일 없이 메모리만 생성 (file descriptor = -1 ) MAP_FIXED addr 강제 지정 (사용 권장 X) MAP_FILE 일반 파일 매핑 (기본값, 생략 가능) MAP_POPULATE 페이지 폴트를 피하기 위해 사전에 페이지 할당 MAP_HUGETLB , MAP_HUGE_2MB , MAP_HUGE_1GB huge page 사용 (성능 최적화) 자주 조합되는 패턴 // 파일 기반 공유 매핑 MAP_SHARED // 익명 공유 메모리 (IPC에 유용) MAP_SHARED | MAP_ANONYMOUS // 읽기 전용 파일 매핑 MAP_PRIVATE int fd - File Descriptor 매핑할 파일의 디스크립터 ( open() 으로 열어야 함) MAP_ANONYMOUS 를 사용하면 -1 을 넣음 값 의미 -1 익명 메모리 (파일 없음) open(\"file.txt\", O_RDONLY) 실제 파일 매핑 off_t offset 파일 내에서 몇 번째 바이트부터 매핑할 것인지 반드시 페이지 경계(page-aligned)여야 함 (즉, 4096의 배수) 값 의미 0 파일 맨 처음부터 매핑 4096 4KB 이후부터 매핑 offset % getpagesize() != 0 오류 발생 🧪 예시 코드 정리 ✅ 예제 1: 익명 공유 메모리 생성 (프로세스 간 공유 가능) char *buffer = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0); ✅ 예제 2: 파일 전체 매핑 (읽기 전용) int fd = open(\"data.txt\", O_RDONLY); char *file_data = mmap(NULL, file_size, PROT_READ, MAP_PRIVATE, fd, 0); ⚠️ 주의사항 length 는 페이지 크기의 배수여야 함 (보통 getpagesize() 리턴값인 4096) 성공 시 매핑된 주소 반환, 실패 시 (void *)-1 반환 반드시 munmap() 으로 해제 필요 📋 정리표 인자 설명 가능한 값 예시 addr 매핑 시작 주소 NULL 또는 임의 주소 length 매핑할 크기 (byte) 4096, 8192 등 prot 접근 권한 PROT_READ , PROT_WRITE , PROT_EXEC , PROT_NONE flags 매핑 속성 MAP_SHARED , MAP_PRIVATE , MAP_ANONYMOUS , MAP_FIXED fd 파일 디스크립터 open() 결과 or -1 offset 파일 오프셋 0 , 4096 , ... (페이지 경계) 필요하다면 실제 파일 매핑, huge page 사용 예제, 또는 에러 처리까지 포함해 드릴 수 있어요! CPU scheduling 번역 임계구역 문제(Critical-Section Problem)의 요구사항 상호 배제(Mutual Exclusion) => 서로 배타적으로 실행해야 한다 프로세스 Pi가 자신의 임계구역(Critical Section)에 있을 경우, 다른 모든 프로세스는 자신의 임계구역을 실행할 수 없다. 진행(Progress) => 아무도 안쓰면 쓰게 해야 한다 현재 어떤 프로세스도 임계구역을 실행하고 있지 않고, 일부 프로세스들이 자신의 임계구역에 들어가기를 원할 때, 다음 프로세스 선택은 무기한으로 지연될 수 없다. 유한 대기(Bounded Waiting) => 대기시간의 제한(ex => 30ms 이상 기다리면 보장) 한 프로세스가 자신의 임계구역에 들어가려고 요청한 후, 그 요청이 허용되기 전까지 다른 프로세스들이 임계구역에 들어가는 횟수에는 제한이 있어야 한다. 가정: 각 프로세스는 0이 아닌 속도로 실행된다. n개의 프로세스 간의 상대적인 속도에 대한 가정은 없다. Requirements of Critical-Section Prob 중간고사 범위 설명 임계구역 문제는 다중 프로세스 환경에서 공유 자원(예: 변수, 파일 등)에 동시에 접근하는 것을 제어하기 위해 중요한 개념이다. 여러 프로세스가 동시에 임계구역에 진입하면 데이터 일관성(Data Consistency) 문제가 발생할 수 있기 때문에, 이를 방지하기 위한 세 가지 주요 요구사항이 있다. 상호 배제(Mutual Exclusion) => 서로 배타적으로 실행해야 한다 이 조건은 임계구역 문제의 핵심이다. 두 개 이상의 프로세스가 동시에 임계구역에 들어가는 것을 방지하여, 공유 자원에 대해 안전한 접근을 보장한다. 예를 들어, 두 프로세스가 동시에 같은 데이터를 수정하려고 하면 데이터가 손상될 수 있으므로, 이를 방지하기 위해 한 번에 하나의 프로세스만 임계구역에 진입하도록 해야 한다. 진행(Progress) => 아무도 안쓰면 쓰게 해야 한다 이 조건은 시스템의 효율성을 보장하기 위한 것이다. 현재 임계구역을 사용 중인 프로세스가 없고, 임계구역에 진입하려는 프로세스가 있을 경우, 시스템은 적절히 다음 프로세스를 선택해야 한다. 만약 특정 프로세스가 계속해서 임계구역 진입을 차단한다면, 이는 \"교착 상태(Deadlock)\" 또는 \"기아 상태(Starvation)\"로 이어질 수 있다. 따라서 진행 조건은 이러한 문제를 방지하기 위해 필요하다. 유한 대기(Bounded Waiting) => 대기시간의 제한(ex => 30ms 이상 기다리면 보장) 이 조건은 공정성을 보장하기 위한 것이다. 특정 프로세스가 임계구역에 진입하려고 요청한 후, 요청이 허용되기 전에 다른 프로세스들이 임계구역에 진입할 수 있는 횟수는 제한되어야 한다. 예를 들어, 어떤 프로세스가 무한히 기다리는 상황(기아 상태)이 발생하지 않도록 하기 위해, 각 프로세스는 일정 횟수 내에 임계구역에 진입할 기회를 가져야 한다. 가정 비영속적 실행(Nonzero Speed): 모든 프로세스는 반드시 실행되며, 정지된 상태로 남아 있지 않음을 의미한다. 상대적 속도에 대한 가정 없음: 프로세스들의 실행 속도는 서로 다를 수 있으며, 특정 프로세스가 항상 더 빠르거나 느리다고 가정하지 않는다. 이는 실제 운영 체제 환경에서 다양한 요인(예: CPU 스케줄링, 우선순위 등)에 의해 프로세스 속도가 달라질 수 있기 때문이다. 결론 임계구역 문제의 세 가지 요구사항은 공유 자원에 대한 안전하고 효율적이며 공정한 접근을 보장하기 위해 설계되었다. 이를 통해 다중 프로세스 환경에서 데이터 일관성을 유지하면서도 성능과 공정성을 균형 있게 관리할 수 있다. 핵심 메시지: 임계구역 문제를 해결하기 위해서는 상호 배제, 진행, 유한 대기라는 세 가지 요구사항을 모두 충족해야 하며, 이는 운영 체제와 분산 시스템에서 매우 중요한 개념이다. 임계구역 문제(Critical Section Problem)의 요구사항과 해결책 분석 임계구역 문제는 다중 프로세스 환경에서 공유 자원에 대한 안전한 접근을 보장하기 위해 설계된 개념입니다. 이를 해결하기 위한 세 가지 주요 요구사항은 상호 배제(Mutual Exclusion), 진행(Progress), 그리고 유한 대기(Bounded Waiting)입니다. 아래에서는 각 해결책이 이러한 요구사항을 충족하는지 분석하겠습니다. Lock 사용 (첫 번째 시도) => 키를 가져가는 방식 use lock 코드: shared int locked = 0; // Process 0 do { while (locked == 1); // 다른 프로세스가 임계구역을 사용 중인지 확인 locked = 1; // 잠금 설정 critical section // 임계구역 실행 locked = 0; // 잠금 해제 remainder section // 나머지 코드 실행 } while (true); // Process 1 do { while (locked == 1); locked = 1; critical section locked = 0; remainder section } while (true); 요구사항 분석: 상호 배제(Mutual Exclusion): X locked 변수를 통해 두 프로세스가 동시에 임계구역에 진입하는 것을 방지하려고 합니다. 그러나 경쟁 상태(Race Condition)가 발생할 가능성이 있습니다. 예를 들어, 두 프로세스가 동시에 while (locked == 1) 조건을 통과하고 바로 다음 줄에서 locked = 1 을 실행하면, 두 프로세스가 동시에 임계구역에 진입할 수 있습니다. 결론: 상호 배제를 완벽히 보장하지 못합니다. 진행(Progress): O 한 프로세스가 임계구역을 빠져나가면 즉시 다른 프로세스가 진입할 수 있습니다. 결론: 진행 조건 충족 유한 대기(Bounded Waiting): X 특정 프로세스가 무한히 기다리는 상황이 발생할 수 있습니다.( 보장하지 못함 계속 새치기 당할 수 있음 ) 결론: 유한 대기를 보장하지 못합니다. 턴(Turn) 기반 접근법 (두 번째 시도) => 끝나면 상대에게 양보 take turn 코드: shared int turn = 0; // Process 0 do { while (turn != 0); // 자신의 차례인지 확인 critical section // 임계구역 실행 turn = 1; // 턴을 상대 프로세스에게 넘김 remainder section // 나머지 코드 실행 } while (true); // Process 1 do { while (turn != 1); critical section turn = 0; remainder section } while (true); 요구사항 분석: 상호 배제(Mutual Exclusion): O turn 변수를 통해 두 프로세스가 동시에 임계구역에 진입하는 것을 방지합니다. 결론: 상호 배제를 보장합니다. 진행(Progress): X 한 프로세스가 임계구역을 자주 사용해야 하는 경우에도 반드시 턴이 돌아올 때까지 기다려야 합니다. 예를 들어, Process 0이 임계구역을 빠져나간 후 더 이상 임계구역에 진입하지 않아도, Process 1은 턴이 돌아오기 전까지 대기해야 합니다. 누구는 임계구역에 들어갈 수 있어야 한다 결론: 진행 조건을 충족하지 못합니다. 유한 대기(Bounded Waiting): O 턴이 순환적으로 변경되므로 모든 프로세스는 유한한 시간 내에 임계구역에 진입할 수 있습니다. 한개의 프로세스가 계속 할 수 없다 결론: 유한 대기를 보장합니다. 의도(Intentions) 확인 (세 번째 시도) => 상대가 들어가고 싶으면 무조건! 양보 check intentions 코드: shared int flag[2] = {false, false}; // Process 0 do { flag[0] = true; // 임계구역 진입 의도 표시 while (flag[1] == true); // 상대 프로세스가 임계구역에 진입하려고 하는지 확인 critical section // 임계구역 실행 flag[0] = false; // 의도 해제 remainder section // 나머지 코드 실행 } while (true); // Process 1 do { flag[1] = true; while (flag[0] == true); critical section flag[1] = false; remainder section } while (true); 요구사항 분석: 상호 배제(Mutual Exclusion): O flag 배열을 통해 두 프로세스가 동시에 임계구역에 진입하는 것을 방지합니다. 결론: 상호 배제를 보장합니다. 진행(Progress): X 두 프로세스가 동시에 flag[me] = true 를 실행하면, 서로가 상대방의 의도를 확인하고 무한히 대기하는 데드락(Deadlock) 상태에 빠질 수 있습니다. 결론: 진행 조건을 충족하지 못합니다. 유한 대기(Bounded Waiting): O 한개의 프로세스가 계속 새치기가 불가능하다 (데드락의 발생 여부와 무관) 결론: 유한 대기를 보장 Peterson's Solution (최종 해결책) 코드: shared int turn, flag[2]; // Process 0 do { flag[0] = true; // 임계구역 진입 의도 표시 turn = 1; // 턴을 상대 프로세스에게 넘김 while (flag[1] && turn == 1); // 상대 프로세스가 임계구역에 진입 중이고 턴이 자신이 아닌 경우 대기 critical section // 임계구역 실행 flag[0] = false; // 의도 해제 remainder section // 나머지 코드 실행 } while (true); // Process 1 do { flag[1] = true; turn = 0; while (flag[0] && turn == 0); critical section flag[1] = false; remainder section } while (true); 요구사항 분석: 상호 배제(Mutual Exclusion): flag 와 turn 변수를 결합하여 두 프로세스가 동시에 임계구역에 진입하는 것을 방지합니다. 결론: 상호 배제를 보장합니다. 진행(Progress): 한 프로세스가 임계구역을 사용하지 않을 경우, 다른 프로세스가 즉시 진입할 수 있습니다. 결론: 진행 조건을 충족합니다. 유한 대기(Bounded Waiting): 각 프로세스는 최대 한 번의 대기만으로 임계구역에 진입할 수 있습니다. 결론: 유한 대기를 보장합니다. 결론 Peterson's Solution은 상호 배제, 진행, 유한 대기라는 세 가지 요구사항을 모두 충족하는 완벽한 해결책입니다. 이는 단순하면서도 강력한 알고리즘으로, 다중 프로세스 환경에서 안전하고 효율적인 공유 자원 관리를 가능하게 합니다. 핵심 메시지: 임계구역 문제를 해결하기 위해서는 세 가지 요구사항을 모두 충족해야 하며, Peterson's Solution은 이를 달성하는 대표적인 방법입니다. Peterson's Solution 의 성립을 위한 필요 가정 \\[SKIP\\] Peterson's Solution은 상호 배제(Mutual Exclusion), 진행(Progress), 유한 대기(Bounded Waiting)라는 세 가지 요구사항을 충족하는 알고리즘이지만, 이는 몇 가지 중요한 가정이 성립할 때만 가능합니다. 이러한 가정이 깨지면 Peterson's Solution도 요구사항을 충족하지 못할 수 있습니다. 아래에서 Peterson's Solution이 요구사항을 충족하기 위한 특정 조건들을 설명하겠습니다. 메모리 모델과 원자성(Atomicity) Peterson's Solution은 공유 변수( flag 배열과 turn )에 대한 연산이 원자적(Atomic)으로 수행된다는 가정하에 동작합니다. 즉: 각 프로세스가 flag[me] = true , turn = !me , 또는 while (flag[!me] && turn == !me) 같은 연산을 실행할 때, 중간에 다른 프로세스가 끼어들지 않아야 합니다. 만약 메모리 접근이나 변수 갱신이 비원자적으로 이루어진다면, 경쟁 상태(Race Condition)가 발생하여 상호 배제나 진행 조건이 깨질 수 있습니다. 예시: 하드웨어나 컴파일러 최적화로 인해 변수의 쓰기/읽기가 순서대로 처리되지 않는 경우. 예: flag[me] = true 와 turn = !me 사이에 다른 프로세스가 개입하면 의도한 동작이 실패할 수 있음. 해결책: 일반적으로 Peterson's Solution은 순차적 일관성(Sequential Consistency)이라는 메모리 모델을 가정합니다. 이는 모든 메모리 연산이 프로그램 순서대로 수행되고, 모든 프로세스가 동일한 순서로 메모리 변화를 관찰한다는 것을 의미합니다. 현대 시스템에서는 메모리 배리어(Memory Barrier)나 동기화 명령어를 사용하여 원자성을 보장해야 할 수 있습니다. 프로세스의 비선점성(Non-preemption) Peterson's Solution은 두 프로세스가 모두 임계구역 진입을 시도할 때, 프로세스가 중단되지 않고 자신의 코드를 완료할 수 있다는 가정하에 동작합니다. 즉: 프로세스가 스케줄링 중에 강제로 선점(Preempted)되지 않아야 합니다. 만약 한 프로세스가 flag[me] = true 이후 turn = !me 를 실행하기 전에 선점되면, 다른 프로세스가 잘못된 정보를 읽고 임계구역에 동시에 진입할 수 있습니다. 예시: Process 0이 flag[0] = true 를 실행하고 선점당한 후, Process 1이 flag[1] = true 와 turn = 0 을 실행하며 임계구역에 진입할 수 있는 상황이 발생함. 해결책: 운영체제에서 특정 코드 영역을 비선점적으로 실행하도록 보장하거나, 하드웨어 지원을 통해 원자성을 확보해야 합니다. 단일 프로세서 환경 또는 강력한 캐시 일관성 Peterson's Solution은 단일 프로세서 환경이나 강력한 캐시 일관성(Cache Coherence)이 보장되는 다중 프로세서 환경에서만 안전하게 동작합니다. 즉: 모든 프로세서가 동일한 메모리 값을 동일한 순서로 관찰해야 합니다. 만약 각 프로세서가 독립적인 캐시를 사용하고 동기화되지 않은 상태로 데이터를 읽거나 쓴다면, flag 와 turn 값이 불일치하여 오류가 발생할 수 있습니다. 예시: Process 0이 flag[0] = true 를 캐시에 저장했지만, 이 값이 메인 메모리로 플러시되기 전에 Process 1이 읽게 되면, Process 1은 flag[0] 가 여전히 false 라고 판단할 수 있음. 해결책: 캐시 일관성 프로토콜(MESI 등)을 사용하여 모든 프로세서가 동일한 메모리 상태를 유지하도록 보장해야 합니다. 프로세스 속도와 실행 순서 Peterson's Solution은 프로세스 간의 상대적인 실행 속도에 대해 아무런 가정을 하지 않습니다. 그러나 각 프로세스가 반드시 무한히 실행되며 정지하지 않는다는 가정하에 동작합니다. 즉: 프로세스가 무한 루프에 빠지거나 크래시(crash)하지 않아야 합니다. 만약 한 프로세스가 실행 중에 종료되면, 다른 프로세스가 영원히 대기할 수 있습니다. 예시: Process 0이 flag[0] = true 이후 종료되면, Process 1은 while (flag[0] && turn == 0) 에서 무한히 대기할 수 있음. 해결책: 프로세스의 고장을 감지하고 복구할 수 있는 메커니즘을 추가해야 합니다. 프로세스 수 제한 Peterson's Solution은 두 개의 프로세스만 지원하도록 설계되었습니다. 더 많은 프로세스를 지원하려면 알고리즘을 수정해야 합니다. 예시: 3개 이상의 프로세스가 있을 경우, flag 배열과 turn 변수만으로는 충분하지 않으며, 새로운 동기화 메커니즘이 필요합니다. 해결책: Dekker's Algorithm이나 Lamport's Bakery Algorithm과 같은 더 복잡한 알고리즘을 사용하여 다수의 프로세스를 지원할 수 있습니다. 결론 Peterson's Solution이 상호 배제, 진행, 유한 대기를 보장하기 위해서는 다음 조건들이 충족되어야 합니다: 메모리 모델: 메모리 연산이 원자적이고 순차적 일관성을 갖는다. 비선점성: 프로세스가 중단되지 않고 자신의 코드를 완료할 수 있다. 캐시 일관성: 다중 프로세서 환경에서도 모든 프로세서가 동일한 메모리 상태를 관찰한다. 프로세스 신뢰성: 프로세스가 무한히 실행되며 고장 나지 않는다. 프로세스 수 제한: 두 개의 프로세스만 지원한다. 이러한 조건들이 충족되지 않으면 Peterson's Solution도 요구사항을 충족하지 못할 수 있습니다. 따라서 실제 운영 체제나 분산 시스템에서는 더 복잡한 동기화 기법(Hardware Locks, Semaphores, Monitors 등)을 사용하여 이러한 문제를 해결합니다. Readers-Writers Problem 좋아요! 주어진 코드는 Readers-Writers Problem 중에서 \"first readers-writers problem\", 즉: Reader 우선 (Readers-preference) 전략입니다. 즉, 여러 reader가 동시에 읽을 수 있고, writer는 모든 reader가 끝날 때까지 기다려야 합니다. ✅ 문제 정의 요약 여러 reader는 동시에 데이터를 읽을 수 있음. writer는 오직 하나만 접근 가능 먼저온 순서대로 reader 또는 writer 을 허용 🧱 사용되는 세마포어 sem_t mutex; // readcount 변수 보호용 (뮤텍스) sem_t wrt; // writer와 첫 번째 reader 간 경쟁 제어용 int readcount; // 현재 읽고 있는 reader 수 ✅ Reader 프로세스 / 스레드 코드 (수정 후) // READER PROCESS while (TRUE) { wait(&mutex); // mutex 잠금 (readcount 보호) readcount++; // reader 수 증가 if (readcount == 1) { wait(&wrt); // 처음 읽는 reader면 writer 차단 } signal(&mutex); // mutex 해제 // === 실제 데이터 읽기 영역 === printf(\"Reader is reading data\\n\"); // 여기서 데이터 읽기 수행 // ========================== wait(&mutex); // 다시 mutex 잠금 readcount--; // reader 수 감소 if (readcount == 0) { signal(&wrt); // 마지막 reader라면 writer에게 허가 } signal(&mutex); // mutex 해제 } ✍️ Writer 프로세스 / 스레드 코드 // WRITER PROCESS while (TRUE) { wait(&wrt); // writer 잠금 확보 (다른 writer나 reader 모두 없어야 함) // === 실제 데이터 쓰기 영역 === printf(\"Writer is writing data\\n\"); // 여기서 데이터 쓰기 수행 // ========================== signal(&wrt); // writer 작업 완료 후 잠금 해제 } deadlock 상호 배제 (Mutual Exclusion) 한 자원은 동시에 하나의 프로세스만 사용 가능 특정 파일을 여러 프로세스가 동시에 읽을 수 있는 경우는 해당하지 않는다 보유 및 대기 (Hold and Wait) 이미 자원을 가지고 있는 프로세스가 다른 자원을 기다림 2개 이상 자원이 필요한 프로세스 비선점 (No Preemption) 자원은 강제로 빼앗을 수 없고, 프로세스가 자발적으로 반납해야 함 외부에 의해 정지되지 않는 논리과정 순환 대기 (Circular Wait) 프로세스 집합 {P0, P1, ..., Pn}에서, P0는 P1이 가진 자원을 기다리고, P1은 P2의 자원을 기다리며..., 마지막으로 Pn은 다시 P0의 자원을 기다리는 구조 순환 💡 교착 상태 처리 방법 (Dealing with Deadlock) 교착 상태를 다루는 일반적인 접근 방법에는 세 가지가 있습니다: 교착 상태 예방 (Prevent deadlock) 시스템 설계 단계에서 교착 상태가 발생하지 않도록 조건을 강제함 교착 상태 회피 (Avoid deadlock) 런타임 동안 리소스 할당 상황을 분석하여 교착 상태가 생기지 않도록 주의해서 할당 교착 상태 탐지 및 복구 (Detect deadlock and recover) 교착 상태가 발생했음을 감지하고, 이를 해결하는 방식 🛑 교착 상태 예방 (Deadlock Prevention) ✅ 교착 상태 예방의 핵심 아이디어: 교착 상태는 네 가지 필수 조건이 모두 성립할 때 발생합니다. 따라서 이 중 하나라도 성립되지 않게 만들면 교착 상태를 예방할 수 있습니다. 🔁 교착 상태의 4가지 필수 조건: 상호 배제 (Mutual Exclusion): 한 리소스는 동시에 한 프로세스만 사용 가능 보유 대기 (Hold and Wait): 이미 리소스를 가진 프로세스가 다른 리소스를 기다림 비선점 (No Preemption): 리소스는 프로세스가 스스로 반납하기 전에는 강제로 빼앗을 수 없음 순환 대기 (Circular Wait): A→B→C→...→A처럼 순환적으로 리소스를 기다리는 관계 존재 1️⃣ 상호 배제 조건 무효화 (Attacking the Mutual Exclusion Condition) 해결 방안: 어떤 리소스는 여러 프로세스가 동시에 접근 가능하도록 설계 예: 프린터 스풀링(Printer Spooling) 실제 프린터는 프린터 데몬(Daemon)만 접근 사용자는 파일을 스풀 영역에 저장하고 종료 → 동시 접근 방지 단점: 모든 장치가 스풀링 가능한 것은 아님 (예: 키보드, 마우스 등) 리소스를 불필요하게 많이 점유하는 경우도 있음 원칙: 리소스를 반드시 필요할 때만 할당 최소한의 프로세스만 해당 리소스를 점유하도록 제한 2️⃣ 보유 대기 조건 무효화 (Attacking the Hold-and-Wait Condition) 해결 방안: 프로세스가 실행 시작 시 필요한 모든 리소스를 미리 요청 실행 중에는 추가 리소스를 요청하지 못함 장점: 프로세스가 실행 도중 리소스 대기를 하지 않아 교착 상태 발생 불가 단점: 프로세스가 실행 전에 모든 리소스를 정확히 알기 어려움 리소스를 오랫동안 점유하면서 다른 프로세스가 대기하게 됨 (자원 낭비) 변형된 전략: 새로운 리소스를 요청하기 전에 현재 가지고 있는 모든 리소스를 반납 그 후 필요한 모든 리소스를 다시 요청 3️⃣ 비선점 조건 무효화 (Attacking the No Preemption Condition) 해결 방안: 리소스를 강제로 빼앗는 것 (선점) 문제점: 일부 리소스는 중간에 선점하면 문제가 발생 예: 프린터 작업 중간에 리소스를 뺏으면 문서 출력이 불완전하게 됨 예: 데이터베이스 트랜잭션 도중에 중단되면 데이터 불일치 발생 결론: 대부분의 시스템에서는 현실적이지 않은 방법 4️⃣ 순환 대기 조건 무효화 (Attacking the Circular Wait Condition) 해결 방안: 모든 리소스에 고유한 번호 부여 프로세스는 번호가 증가하는 순서대로 리소스를 요청해야 함 예시: R1(1번), R2(2번), R3(3번) 프로세스가 R2를 사용 중이라면 다음에 요청할 수 있는 리소스는 R3 이상만 가능 R1은 요청 불가능 (번호가 작음) 장점: 순환 대기 자체를 방지 비교적 실용적인 방법 단점: 리소스 번호 체계를 잘 설계해야 함 특정 리소스를 반복적으로 사용해야 하는 경우 비효율적일 수 있음 조건 해결 방법 문제점 상호 배제 스풀링, 공유 자원 활용 모든 자원에 적용 불가 보유 대기 초기에 모든 자원 확보 or 반환 후 재요청 자원 낭비, 사전 예측 어려움 비선점 리소스 강제 선점 작업 중단으로 인한 데이터 손실 위험 순환 대기 리소스 순서 규칙 적용 유연성 저하, 번호 체계 복잡 네, 알겠습니다. 제공해주신 운영체제 강의 슬라이드를 바탕으로, 각 슬라이드별 원문, 번역, 그리고 6000자 이상의 매우 상세한 설명을 덧붙여드리겠습니다. Process Synchronization 슬라이드 1: 공유 버퍼 문제 원문 (Original Text) Shared Buffer by Circular Array PC Buffer out in counter Shared Memory r/wr/w Empty if counter == 0 Full if counter == BS Concurrency Problem ! counter++: register1 = counter register1 = register1 + 1 counter = register1 counter--: register2 = counter register2 = register2 - 1 counter = register2 Silberschatz, Galvin and Gagne ©2009 Operating System Concepts – 8th Edition 번역 (Translation) 원형 배열을 이용한 공유 버퍼 PC (생산자/소비자) 버퍼 out in (데이터 인출/삽입 위치) counter (버퍼에 있는 데이터 수) 공유 메모리 읽기/쓰기/쓰기 비어있음 (Empty) if counter == 0 가득 참 (Full) if counter == BS (버퍼 크기) 동시성 문제 발생! counter++ 연산의 내부 동작: register1 = counter register1 = register1 + 1 counter = register1 counter-- 연산의 내부 동작: register2 = counter register2 = register2 - 1 counter = register2 매우 자세한 설명 (Detailed Explanation) 이 슬라이드는 생산자-소비자 문제(Producer-Consumer Problem)라는 고전적인 동시성(Concurrency) 문제를 소개하고 있습니다. 이 문제는 여러 프로세스나 스레드가 공유 자원(Shared Resource)에 동시에 접근할 때 발생할 수 있는 위험을 명확하게 보여줍니다. 구성 요소 분석 Shared Buffer (공유 버퍼): 생산자(Producer)가 데이터를 생성하여 넣는 공간이자, 소비자(Consumer)가 데이터를 가져다 쓰는 공간입니다. 여러 프로세스가 함께 사용하는 '공유 메모리' 영역에 존재합니다. 슬라이드에서는 이 버퍼가 원형 배열(Circular Array)로 구현되었다고 가정합니다. 원형 배열은 배열의 마지막 인덱스 다음에 다시 첫 인덱스로 돌아오는 구조로, 한정된 크기의 버퍼를 효율적으로 사용하는 데 적합합니다. Producer/Consumer (생산자/소비자): 생산자(Producer): 데이터를 생성하여 버퍼에 넣는 역할을 하는 프로세스입니다. 데이터가 꽉 차지 않았다면( counter < BS ), 'in' 포인터가 가리키는 위치에 데이터를 삽입하고 'in' 포인터를 다음 위치로 이동시킵니다. 소비자(Consumer): 버퍼에서 데이터를 가져와 소비하는 역할을 하는 프로세스입니다. 데이터가 비어있지 않다면( counter > 0 ), 'out' 포인터가 가리키는 위치에서 데이터를 꺼내고 'out' 포인터를 다음 위치로 이동시킵니다. in / out 포인터: 버퍼에서 다음 데이터가 삽입될 위치( in )와 다음 데이터가 인출될 위치( out )를 가리키는 변수입니다. counter 변수: 버퍼에 현재 저장된 데이터의 개수를 나타내는 정수 변수입니다. 생산자가 데이터를 넣으면 counter 가 1 증가( counter++ )하고, 소비자가 데이터를 빼가면 counter 가 1 감소( counter-- )합니다. 이 counter 변수 역시 공유 자원입니다. 생산자와 소비자 모두 이 변수를 읽고 수정해야 하기 때문입니다. 동시성 문제의 핵심: Race Condition (경쟁 상태) 슬라이드의 핵심은 counter++ 와 counter-- 연산에서 동시성 문제가 발생할 수 있음을 보여주는 부분입니다. 우리가 C언어 등 고급 언어에서 counter++; 라고 한 줄로 간단하게 작성하는 코드도, 컴파일되어 기계어로 번역되면 실제로는 여러 단계의 명령어로 나뉩니다. 슬라이드에서는 이 과정을 3단계로 나누어 설명합니다. 메모리에서 레지스터로 값을 가져온다: register1 = counter 레지스터의 값을 연산한다: register1 = register1 + 1 레지스터의 값을 다시 메모리에 저장한다: counter = register1 문제는 이 3단계의 연산이 원자적(Atomic)으로 실행되지 않는다는 점입니다. 즉, 1단계와 3단계 사이에 얼마든지 다른 프로세스에게 CPU 제어권이 넘어갈 수 있습니다. 구체적인 시나리오: 가정: counter 의 현재 값은 5입니다. 생산자(P)와 소비자(C)가 거의 동시에 counter 에 접근하려고 합니다. 생산자(P) 실행: register1 = counter 를 실행합니다. 생산자의 register1 에는 5가 저장됩니다. 문맥 교환 (Context Switch) 발생: 생산자가 register1 = register1 + 1 을 실행하기 직전에, 운영체제의 스케줄러가 CPU 제어권을 소비자(C)에게 넘겨줍니다. (예: 타임 슬라이스 만료) 소비자(C) 실행: 소비자는 counter-- 연산을 처음부터 끝까지 모두 실행합니다. register2 = counter (메모리의 counter 는 아직 5이므로, register2 에 5가 저장됨) register2 = register2 - 1 ( register2 는 4가 됨) counter = register2 (메모리의 counter 값이 4로 업데이트됨) 문맥 교환 (Context Switch) 발생: 이제 다시 CPU 제어권이 생산자(P)에게 돌아옵니다. 생산자(P) 실행 재개: 생산자는 이전에 중단되었던 지점부터 실행을 이어갑니다. register1 = register1 + 1 (생산자의 register1 은 5였으므로, 6이 됨) counter = register1 (메모리의 counter 값을 6으로 덮어씁니다.) 결과: 최종 counter 값은 6이 됩니다. 하지만 논리적으로는 생산자가 1을 더하고 소비자가 1을 뺐으므로 원래 값인 5가 되어야 합니다. 데이터의 정합성이 깨진 것입니다. 이러한 상황을 경쟁 상태(Race Condition)라고 부르며, 여러 프로세스가 공유 자원에 동시에 접근하여 조작하려 할 때 실행 순서에 따라 결과가 달라지는 현상을 말합니다. 이 문제를 해결하기 위한 매커니즘이 바로 다음에 나올 임계 구역(Critical Section) 문제입니다. 슬라이드 2: 임계 구역 문제 원문 (Original Text) Critical Section Problem ! Consider system of n processes {p0, p1, … pn-1} ! Each process has a critical section ! If one process in critical section, no other process can ! Each process must ask permission to enter critical section in entry section, may follow critical section with exit section, then remainder section ! Critical section problem is to design protocol to solve this Silberschatz, Galvin and Gagne ©2009 Operating System Concepts – 8th Edition 번역 (Translation) 임계 구역 문제 ! n개의 프로세스 {p0, p1, … pn-1}로 구성된 시스템을 가정하자. ! 각 프로세스는 임계 구역(critical section)을 가지고 있다. ! 만약 한 프로세스가 임계 구역 안에 있다면, 다른 어떤 프로세스도 들어갈 수 없다. ! 각 프로세스는 임계 구역에 진입하기 위해 진입 구역(entry section)에서 허가를 요청해야 하며, 임계 구역 다음에는 퇴출 구역(exit section)이 올 수 있고, 그 이후는 나머지 구역(remainder section)이다. ! 임계 구역 문제란, 바로 이 문제를 해결하기 위한 프로토콜을 설계하는 것이다. 매우 자세한 설명 (Detailed Explanation) 앞선 슬라이드에서 counter 변수와 같은 공유 자원에 접근할 때 발생하는 경쟁 상태를 확인했습니다. 임계 구역 문제(Critical Section Problem)는 이러한 경쟁 상태를 해결하기 위한 문제를 공식적으로 정의한 것입니다. 용어 정의 프로세스(Process): 시스템에서 실행 중인 프로그램을 의미하며, 자신만의 메모리 공간과 자원을 가집니다. 이 슬라이드에서는 여러 개의 프로세스( p0 부터 pn-1 까지)가 협력하며 동작하는 환경을 가정합니다. 임계 구역 (Critical Section): 프로세스 코드 중에서 공유 자원(shared resource)에 접근하는 부분을 말합니다. 앞선 예시에서는 counter++ 또는 counter-- 를 수행하는 코드 블록이 바로 임계 구역에 해당합니다. 이 외에도 공유 변수, 공유 파일, 공유 데이터베이스 등을 조작하는 모든 코드가 임계 구역이 될 수 있습니다. 임계 구역의 핵심 특징은, 두 개 이상의 프로세스가 동시에 실행하면 안 된다는 점입니다. 진입 구역 (Entry Section): 임계 구역에 들어가기 전에, 진입 허가를 요청하고 대기하는 코드 부분입니다. 여기서 다른 프로세스가 이미 임계 구역에 있는지 확인하고, 비어있다면 진입하고, 아니라면 기다리는 로직이 수행됩니다. 퇴출 구역 (Exit Section): 임계 구역에서의 작업을 마친 후, 다른 프로세스가 임계 구역에 진입할 수 있도록 상태를 변경해주는 코드 부분입니다. 예를 들어, \"이제 내가 다 썼으니 다른 프로세스가 들어와도 된다\"는 신호를 보내는 역할을 합니다. 나머지 구역 (Remainder Section): 임계 구역과 관련 없는, 즉 공유 자원에 접근하지 않는 나머지 코드 부분입니다. 이 부분은 다른 프로세스와 동시에 실행되어도 아무런 문제가 없습니다. 문제의 본질 임계 구역 문제의 본질은 \"어떻게 하면 진입 구역(Entry Section)과 퇴출 구역(Exit Section)의 프로토콜(규칙)을 잘 설계해서, 여러 프로세스가 임계 구역을 안전하고 효율적으로 사용하게 할 것인가?\" 입니다. 슬라이드에서 언급된 가장 중요한 규칙은 \"만약 한 프로세스가 자신의 임계 구역에서 실행 중이라면, 다른 어떤 프로세스도 자신의 임계 구역에서 실행될 수 없다\" 는 것입니다. 이를 상호 배제(Mutual Exclusion) 라고 하며, 뒤따르는 슬라이드에서 해결책의 필수 요건 중 하나로 다시 등장합니다. 예를 들어, 여러 사람이 하나의 화장실을 사용한다고 생각해봅시다. 화장실 내부 = 임계 구역 (한 번에 한 명만 사용 가능) 화장실 문을 두드리고 안이 비었는지 확인하는 행동 = 진입 구역 볼일을 다 보고 나와서 문을 열어주는 행동 = 퇴출 구역 자기 자리에서 다른 일을 하는 것 = 나머지 구역 임계 구역 문제는 이 화장실을 여러 사람이 질서 있고 문제없이 사용하기 위한 규칙(프로토콜)을 만드는 것과 같습니다. \"안에 사람이 있으면 기다린다\", \"나올 때는 다음 사람이 쓸 수 있게 한다\"와 같은 규칙을 코드 레벨에서 구현하는 것이 목표입니다. 이 프로토콜은 다음에 설명될 3가지 요구사항(상호 배제, 진행, 한정된 대기)을 반드시 만족해야 합니다. 슬라이드 3: 임계 구역의 일반적인 구조 원문 (Original Text) Critical Section ! General structure of process pi is do { entry section critical section exit section remainder section } while (true); Silberschatz, Galvin and Gagne ©2009 Operating System Concepts – 8th Edition 번역 (Translation) 임계 구역 ! 프로세스 pi의 일반적인 구조는 다음과 같다. do { 진입 구역 (entry section) 임계 구역 (critical section) 퇴출 구역 (exit section) 나머지 구역 (remainder section) } while (true); 매우 자세한 설명 (Detailed Explanation) 이 슬라이드는 앞서 설명한 임계 구역 문제의 해결을 위한 프로세스의 일반적인 코드 구조를 보여줍니다. 이는 개념적인 템플릿으로, 모든 프로세스는 임계 구역에 접근하기 위해 이와 같은 흐름을 따라야 함을 나타냅니다. do-while(true) 루프의 의미 프로세스의 생명주기는 일반적으로 반복적인 작업을 수행합니다. 예를 들어 웹 서버 프로세스는 클라이언트의 요청을 계속해서 받고 처리하며, 워드 프로세서는 사용자의 입력을 계속해서 기다리고 문서를 업데이트합니다. do-while(true) 루프는 이러한 프로세스의 지속적인 실행 사이클을 표현합니다. 프로세스는 종료되지 않는 한, 임계 구역에 접근하고 나머지 작업을 수행하는 이 사이클을 무한히 반복합니다. 각 구역의 역할과 흐름 entry section (진입 구역): 프로세스가 임계 구역에 진입하고자 할 때 가장 먼저 실행되는 코드입니다. 목표: 임계 구역에 진입할 수 있는 '권한' 또는 '잠금(lock)'을 획득하는 것입니다. 동작: 다른 프로세스가 이미 임계 구역 내에서 실행 중인지 확인합니다. 만약 그렇다면, 해당 프로세스가 임계 구역을 빠져나올 때까지 대기해야 합니다. 이 대기하는 방식은 CPU를 계속 소모하며 기다리는 바쁜 대기(busy-waiting)일 수도 있고, 대기 큐에 들어가 잠드는 방식일 수도 있습니다. 이 구역의 설계가 임계 구역 문제 해결의 핵심입니다. critical section (임계 구역): 진입 구역을 성공적으로 통과한 프로세스만이 이 구역의 코드를 실행할 수 있습니다. 상호 배제(Mutual Exclusion)가 보장되어야 하는 구간입니다. 즉, 어떤 시점이든 최대 하나의 프로세스만이 이 구역 안에 있을 수 있습니다. 여기서는 공유 변수 수정, 공유 파일 쓰기, 공유 데이터 구조 변경 등 경쟁 상태를 유발할 수 있는 작업들이 수행됩니다. 앞선 예시에서는 counter++ 또는 counter-- 연산이 여기에 해당합니다. exit section (퇴출 구역): 임계 구역에서의 모든 작업을 마친 프로세스가 실행하는 코드입니다. 목표: 자신이 차지하고 있던 임계 구역을 '해제'하여 다른 대기 중인 프로세스들이 진입할 수 있도록 길을 열어주는 것입니다. 동작: 진입 구역에서 획득했던 '잠금'을 풀어주거나, \"이제 임계 구역이 비었다\"는 상태를 다른 프로세스에게 알려주는 코드가 포함됩니다. 이 구역의 코드가 제대로 실행되지 않으면, 임계 구역이 영원히 잠겨 다른 프로세스들이 무한정 대기하는 교착 상태(Deadlock)가 발생할 수 있습니다. remainder section (나머지 구역): 공유 자원과 관련 없는, 프로세스 고유의 작업을 수행하는 나머지 코드 부분입니다. 이 구역은 상호 배제가 필요 없으므로, 여러 프로세스가 동시에 이 구역의 코드를 실행해도 시스템에 아무런 문제가 발생하지 않습니다. 이 구조는 임계 구역 문제를 해결하기 위한 모든 알고리즘(예: Peterson의 알고리즘, 세마포어, 뮤텍스 등)이 따라야 하는 기본적인 프레임워크입니다. 개발자는 각 구역, 특히 entry section 과 exit section 에 어떤 코드를 넣어야 다음 슬라이드에서 설명할 3가지 요구사항을 모두 만족시킬 수 있을지를 고민해야 합니다. 슬라이드 4: 임계 구역 문제의 요구사항 원문 (Original Text) Requirements of Critical-Section Prob. 1. Mutual Exclusion - If process Pi is in its critical section, then no other processes can be executing in their critical sections 2. Progress - If no process is executing in its critical section and some processes wish to enter their critical section, then the selection of the next process cannot be postponed indefinitely 3. Bounded Waiting - A bound must exist on the number of times that other processes enter critical sections after a process has made a request to enter its critical section and before that request is granted — Assume that each process executes at a nonzero speed — No assumption concerning relative speed of the n processes Silberschatz, Galvin and Gagne ©2009 Operating System Concepts – 8th Edition 번역 (Translation) 임계 구역 문제의 요구사항 1. 상호 배제 (Mutual Exclusion) - 만약 프로세스 Pi가 자신의 임계 구역에서 실행 중이라면, 다른 어떤 프로세스도 자신의 임계 구역에서 실행될 수 없다. 2. 진행 (Progress) - 만약 임계 구역에서 실행 중인 프로세스가 없고, 자신의 임계 구역으로 진입하려는 프로세스들이 있다면, 다음에 진입할 프로세스를 결정하는 것을 무한정 연기할 수 없다. 3. 한정된 대기 (Bounded Waiting) - 어떤 프로세스가 자신의 임계 구역에 진입 요청을 한 후부터 그 요청이 허가될 때까지, 다른 프로세스들이 그들의 임계 구역에 진입하는 횟수에는 한계가 있어야 한다. — 각 프로세스는 0이 아닌 속도로 실행된다고 가정한다. — n개 프로세스들의 상대적인 실행 속도에 대해서는 아무것도 가정하지 않는다. 매우 자세한 설명 (Detailed Explanation) 이 슬라이드는 임계 구역 문제에 대한 '올바른 해법'이 반드시 만족시켜야 할 세 가지 핵심적인 조건과 두 가지 기본 가정을 제시합니다. 이 조건들은 알고리즘의 정확성과 공정성을 보장하는 척도입니다. 세 가지 핵심 요구사항 상호 배제 (Mutual Exclusion): 의미: 이것은 임계 구역 문제의 가장 근본적이고 필수적인 요구사항입니다. 어떤 시점에서든, 오직 하나의 프로세스만이 자신의 임계 구역 코드를 실행할 수 있어야 합니다. 목적: 공유 자원의 데이터 일관성을 보호하고 경쟁 상태(Race Condition)를 원천적으로 방지하는 것입니다. 위반 시: 상호 배제가 깨지면, 앞선 예시에서 counter 값이 뒤죽박죽된 것처럼 데이터가 손상되고 프로그램 전체가 오작동하게 됩니다. 화장실 예시로 비유하면, 여러 사람이 동시에 한 칸에 들어가려는 것과 같습니다. 진행 (Progress): 의미: 이 조건은 두 부분으로 나뉩니다. 임계 구역이 비어있고, 들어가고 싶은 프로세스가 있다면, 반드시 그 중 하나는 들어갈 수 있도록 선택되어야 합니다. 이 선택 과정은 유한한 시간 안에 이루어져야 합니다. 목적: 시스템 전체가 멈추는 교착 상태(Deadlock)를 방지하는 것입니다. 위반 시: 임계 구역은 비어있는데, 들어가고 싶은 프로세스들이 서로 눈치만 보거나 잘못된 로직 때문에 아무도 들어가지 못하고 무한정 대기하는 상황이 발생할 수 있습니다. 예를 들어, 두 프로세스가 서로에게 \"너 먼저 들어가\"라고 양보하다가 결국 아무도 못 들어가는 상황이 '진행' 조건 위반입니다. 또한, 임계 구역과 전혀 상관없는 프로세스가 다음에 들어갈 프로세스를 결정하는 권한을 가져서도 안 됩니다. 오직 들어가고 싶어하는 프로세스들 중에서만 선택이 이루어져야 합니다. 한정된 대기 (Bounded Waiting): 의미: 어떤 프로세스가 임계 구역에 진입하기 위해 요청(예: entry section 실행 시작)을 한 시점부터, 그 요청이 허가되어 실제로 임계 구역에 들어갈 때까지의 대기 시간이 무한하면 안 된다는 뜻입니다. 더 구체적으로는, 다른 프로세스가 새치기하여 임계 구역에 들어가는 횟수에 상한선(bound)이 있어야 합니다. 목적: 특정 프로세스가 무한히 기다리는 기아 상태(Starvation)를 방지하는 것입니다. 위반 시: 상호 배제와 진행 조건은 만족하더라도, 운이 나쁜 특정 프로세스는 다른 프로세스들에게 계속 순서를 빼앗겨 영원히 임계 구역에 들어가지 못할 수 있습니다. 예를 들어, 10개의 프로세스가 있는데 스케줄링 우선순위나 알고리즘의 허점 때문에 항상 1번과 2번 프로세스만 번갈아 가며 임계 구역에 들어가고 3번 프로세스는 영원히 기다리는 상황이 발생하면, 이는 '한정된 대기' 조건을 위반한 것입니다. 두 가지 기본 가정 Assume that each process executes at a nonzero speed (각 프로세스는 0이 아닌 속도로 실행된다): 프로세스가 멈추지 않고 언젠가는 자신의 코드를 계속 실행해 나간다는 가정입니다. 이는 프로세스가 무한 루프에 빠지거나 특정 지점에서 영원히 멈추지 않음을 보장하여, 알고리즘 분석을 가능하게 합니다. No assumption concerning relative speed of the n processes (n개 프로세스들의 상대적인 실행 속도에 대해서는 아무것도 가정하지 않는다): 어떤 프로세스가 다른 프로세스보다 빠르거나 느리다고 가정할 수 없다는 의미입니다. 현대적인 시분할 시스템에서는 문맥 교환이 언제, 어떤 프로세스에게 일어날지 예측할 수 없기 때문에 이는 매우 현실적인 가정입니다. 따라서 설계된 알고리즘은 프로세스들의 실행 속도와 관계없이 항상 올바르게 동작해야 합니다. 이 요구사항들을 기준으로, 다음 슬라이드들에서 제시되는 여러 시도들이 왜 실패하고, 최종적으로 어떤 해결책이 성공하는지를 평가하게 됩니다. 슬라이드 5 & 6: 첫 번째 시도 - Lock 변수 원문 (Original Text) // Slide 5 1st: Use lock shared int locked = 0; do { while (locked == 1); locked = 1; critical section locked = 0; remainder section } while (true); ! Fails to meet ! Solution: Allow only one process to // Slide 6 (with process distinction) Process 0: shared int locked = 0; do { while (locked == 1); locked = 1; critical section locked = 0; remainder section } while (true); Process 1: shared int locked = 0; do { while (locked == 1); locked = 1; critical section locked = 0; remainder section } while (true); 번역 (Translation) // 슬라이드 5 첫 번째 시도: 잠금(lock) 사용 공유 변수 int locked = 0; // 0: 비었음, 1: 잠김 do { while (locked == 1); // 잠겨있으면 계속 대기 locked = 1; // 잠금 설정 임계 구역 locked = 0; // 잠금 해제 나머지 구역 } while (true); ! 요구사항 충족 실패 ! 해결책: 오직 하나의 프로세스만 [진입하도록 허용] // 슬라이드 6 (프로세스 구분) 프로세스 0: 공유 변수 int locked = 0; do { while (locked == 1); locked = 1; 임계 구역 locked = 0; 나머지 구역 } while (true); 프로세스 1: 공유 변수 int locked = 0; do { while (locked == 1); locked = 1; 임계 구역 locked = 0; 나머지 구역 } while (true); 매우 자세한 설명 (Detailed Explanation) 이 두 슬라이드는 임계 구역 문제를 해결하려는 가장 직관적이고 간단한 첫 번째 시도를 보여줍니다. 아이디어는 locked 라는 공유 변수를 사용하여 문에 '자물쇠'를 거는 것과 같습니다. locked == 0 : 문이 열려 있음 (임계 구역이 비어 있음) locked == 1 : 문이 잠겨 있음 (다른 프로세스가 임계 구역 사용 중) 프로세스는 임계 구역에 들어가기 전( entry section )에 locked 가 0인지 확인하고, 0이면 1로 바꾸어 문을 잠근 뒤 진입합니다. 작업을 마치면( exit section ) locked 를 다시 0으로 만들어 문을 열어줍니다. 왜 이 해결책은 실패하는가? 얼핏 보면 완벽해 보이지만, 이 방법은 상호 배제(Mutual Exclusion) 요구사항을 만족시키지 못합니다. 그 이유는 첫 번째 슬라이드에서 counter++ 연산이 여러 단계로 나뉘었던 것과 동일한 문제입니다. entry section 의 두 줄짜리 코드, 즉 while (locked == 1); 과 locked = 1; 은 원자적으로(atomically) 실행되지 않습니다. 상호 배제가 깨지는 시나리오 (두 프로세스가 동시에 진입하는 경우): 가정: locked 의 초기값은 0입니다. 프로세스 0(P0)과 프로세스 1(P1)이 거의 동시에 임계 구역에 진입하려고 합니다. P0 실행: P0가 while (locked == 1); 조건을 검사합니다. 현재 locked 는 0이므로 조건은 거짓이 되고, P0는 while 루프를 빠져나옵니다. 이제 P0는 다음 라인인 locked = 1; 을 실행하려고 합니다. 문맥 교환 (Context Switch) 발생: P0가 locked = 1; 을 실행하기 바로 그 직전에, 운영체제 스케줄러에 의해 문맥 교환이 발생하여 CPU 제어권이 P1에게 넘어갑니다. 이 시점이 치명적인 순간입니다. P1 실행: 이제 P1이 자신의 entry section 을 실행합니다. P1도 while (locked == 1); 조건을 검사합니다. P0가 아직 locked 값을 1로 바꾸지 못했기 때문에, 메모리의 locked 값은 여전히 0입니다. 따라서 P1도 조건을 거짓으로 판단하고 while 루프를 빠져나옵니다. P1은 다음 라인인 locked = 1; 을 실행하여 locked 값을 1로 바꿉니다. P1은 성공적으로 임계 구역에 진입합니다. 문맥 교환 (Context Switch) 발생: CPU 제어권이 다시 P0에게 돌아옵니다. P0 실행 재개: P0는 이전에 중단되었던 지점, 즉 locked = 1; 을 실행할 차례부터 이어갑니다. P0는 locked 값을 1로 설정합니다. (이미 1이었지만, 다시 1로 씁니다.) P0 역시 임계 구역에 진입합니다. 결과: P0과 P1이 동시에 임계 구역 안에 존재하게 됩니다. 이는 상호 배제 원칙을 명백히 위반한 것이며, 이로 인해 공유 자원의 데이터는 손상될 수 있습니다. 이 실패는 값을 확인하는(test) 동작과 값을 설정하는(set) 동작 사이에 인터럽트 가능한 틈(interruptible gap)이 존재하기 때문에 발생합니다. 이 문제를 해결하려면 확인과 설정을 하나의 분리될 수 없는, 원자적인 연산으로 만들어야 합니다. 이것이 바로 뒤에 나올 하드웨어 지원(e.g., TestAndSet )의 필요성으로 이어집니다. 슬라이드 7 & 8: 두 번째 시도 - 차례 지키기 원문 (Original Text) // Slide 7 2nd: Take turns shared int turn = 0; do { while (turn != me); critical section turn = ! me; remainder section } while (true); ! Fails to meet ! Solution: Check if the other process // Slide 8 (with process distinction) Process 0: shared int turn = 0; // `me` is 0 do { while (turn == 1); // wait if it's P1's turn critical section turn = 1; // give turn to P1 remainder section } while (true); Process 1: shared int turn = 0; // `me` is 1 do { while (turn == 0); // wait if it's P0's turn critical section turn = 0; // give turn to P0 remainder section } while (true); 번역 (Translation) // 슬라이드 7 두 번째 시도: 차례 지키기 공유 변수 int turn = 0; do { while (turn != me); // 내 차례가 아니면 대기 임계 구역 turn = ! me; // 상대방에게 차례를 넘김 나머지 구역 } while (true); ! 요구사항 충족 실패 ! 해결책: 다른 프로세스의 [상태를 확인] // 슬라이드 8 (프로세스 구분) 프로세스 0: (`me`는 0) 공유 변수 int turn = 0; do { while (turn == 1); // P1의 차례이면 대기 임계 구역 turn = 1; // P1에게 차례를 넘김 나머지 구역 } while (true); 프로세스 1: (`me`는 1) 공유 변수 int turn = 0; do { while (turn == 0); // P0의 차례이면 대기 임계 구역 turn = 0; // P0에게 차례를 넘김 나머지 구역 } while (true); 매우 자세한 설명 (Detailed Explanation) 이 두 번째 시도는 turn 이라는 공유 변수를 사용하여 두 프로세스가 엄격하게 번갈아가며 임계 구역에 진입하도록 강제하는 방법입니다. 놀이터의 시소를 생각하면 쉽습니다. 한 명이 내려와야 다른 한 명이 올라갈 수 있습니다. turn == 0 : 프로세스 0(P0)의 차례 turn == 1 : 프로세스 1(P1)의 차례 프로세스는 자신의 차례( turn == me )가 될 때까지 기다리고, 임계 구역 사용이 끝나면 상대방에게 차례를 넘겨줍니다 ( turn = !me ). 이 해결책의 평가 상호 배제 (Mutual Exclusion) - 만족 ✔️: turn 변수는 항상 0 또는 1 중 하나의 값만 가질 수 있습니다. 따라서 while (turn != me) 조건은 두 프로세스에 대해 동시에 거짓이 될 수 없습니다. 만약 turn이 0이면 P0만 통과하고 P1은 대기하며, turn이 1이면 P1만 통과하고 P0은 대기합니다. 따라서 두 프로세스가 동시에 임계 구역에 진입하는 것은 불가능합니다. 상호 배제는 완벽하게 지켜집니다. 진행 (Progress) - 실패 ❌: 이것이 이 알고리즘의 치명적인 약점입니다. 진행(Progress) 조건을 위반합니다. 진행 조건은 \"임계 구역이 비어 있고 들어가고 싶은 프로세스가 있다면, 반드시 들어갈 수 있어야 한다\"는 것입니다. 하지만 이 알고리즘은 강제적인 순서 교대 때문에 이 조건을 어깁니다. 진행 조건이 깨지는 시나리오: 초기 상태: turn = 0 . P0가 자신의 차례이므로 임계 구역에 진입합니다. P0는 임계 구역 작업을 마치고 turn = 1 로 설정하여 P1에게 차례를 넘깁니다. 이제 P1의 차례( turn == 1 )입니다. 하지만 P1은 현재 임계 구역에 들어갈 필요가 없고, 자신의 remainder section 에서 오래 걸리는 작업을 수행 중이라고 가정해봅시다. 한편, P0는 자신의 remainder section 작업을 금방 끝내고 다시 임계 구역에 들어가고 싶어합니다. P0는 entry section 의 while (turn == 1); 을 만납니다. turn 은 1이므로 P0는 무한정 대기해야 합니다. 결과: 임계 구역은 명백히 비어있고(아무도 사용 중이지 않음), P0는 임계 구역에 들어가고 싶어하지만, 들어갈 수 없습니다. 단지 P1이 자신의 차례를 사용하지 않았다는 이유만으로 P0의 진입이 막히는 것입니다. 이는 \"다음에 들어갈 프로세스를 결정하는 것을 무한정 연기할 수 없다\"는 진행 조건을 정면으로 위반합니다. 이처럼 한 프로세스의 상태가 다른 프로세스의 진행에 과도하게 영향을 미치는 것을 강결합(tight coupling)이라고도 합니다. 이 해결책은 상호 배제는 해결했지만, 프로세스들이 서로의 발목을 잡게 만들어 시스템의 전체적인 효율성과 처리량을 떨어뜨리는 문제를 낳습니다. 슬라이드 9 & 10: 세 번째 시도 - 의도 확인 원문 (Original Text) // Slide 9 3rd : Check intention shared int flag[2]; do { flag[me] = true; while (flag[!me] == true); critical section flag[me] = false; remainder section } while (true); ! Fails to meet ! Solution: check both // Slide 10 (with process distinction) Process 0: shared int flag[2]; // flag[0] for P0, flag[1] for P1 do { flag[0] = true; // \"I want to enter\" while (flag[1] == true); // Wait if P1 wants to enter critical section flag[0] = false; // \"I'm done\" remainder section } while (true); Process 1: shared int flag[2]; do { flag[1] = true; // \"I want to enter\" while (flag[0] == true); // Wait if P0 wants to enter critical section flag[1] = false; // \"I'm done\" remainder section } while (true); 번역 (Translation) // 슬라이드 9 세 번째 시도: 의도 확인하기 공유 배열 int flag[2]; do { flag[me] = true; // \"나 들어가고 싶어\" 라는 의도를 표시 while (flag[!me] == true); // 상대방이 들어가고 싶어하면 대기 임계 구역 flag[me] = false; // \"나 다 썼어\" 라고 표시 나머지 구역 } while (true); ! 요구사항 충족 실패 ! 해결책: 둘 다 확인하기 // 슬라이드 10 (프로세스 구분) 프로세스 0: 공유 배열 int flag[2]; // flag[0]는 P0용, flag[1]은 P1용 do { flag[0] = true; // \"나 들어가고 싶어\" while (flag[1] == true); // P1이 들어가고 싶어하면 대기 임계 구역 flag[0] = false; // \"나 다 썼어\" 나머지 구역 } while (true); 프로세스 1: 공유 배열 int flag[2]; do { flag[1] = true; // \"나 들어가고 싶어\" while (flag[0] == true); // P0이 들어가고 싶어하면 대기 임계 구역 flag[1] = false; // \"나 다 썼어\" 나머지 구역 } while (true); 매우 자세한 설명 (Detailed Explanation) 이 세 번째 시도는 두 번째 시도의 문제점(한 프로세스가 다른 프로세스를 불필요하게 기다리는 것)을 해결하기 위해 고안되었습니다. turn 변수처럼 차례를 강제하는 대신, 각 프로세스가 자신의 '의도'를 flag 배열을 통해 알리는 방식을 사용합니다. flag[i] = true; : 프로세스 i 가 임계 구역에 들어가고 싶거나, 현재 임계 구역 안에 있음을 의미합니다. flag[i] = false; : 프로세스 i 는 임계 구역에 관심이 없음을 의미합니다. 로직은 간단합니다. 임계 구역에 들어가고 싶으면, 먼저 자신의 flag 를 true 로 설정하여 의도를 밝힙니다. 그리고 상대방의 flag 를 확인해서, 만약 상대방도 true 라면 대기합니다. 상대방이 관심이 없으면( false ), 임계 구역에 진입합니다. 이 해결책의 평가 상호 배제 (Mutual Exclusion) - 만족 ✔️ (단, 특정 조건 하에서만): 이 알고리즘은 상호 배제를 만족하는 것처럼 보입니다. 만약 P0가 while(flag[1])을 통과했다면, 이는 flag[1]이 false라는 뜻입니다. P0가 임계 구역에 있는 동안 flag[0]은 true이므로, P1은 while(flag[0])에 걸려 대기하게 됩니다. 그래서 상호 배제는 지켜집니다. 하지만 아래에서 설명할 교착 상태 문제가 더 치명적입니다. 진행 (Progress) - 실패 ❌: 이 알고리즘은 교착 상태(Deadlock)에 빠질 가능성이 있습니다. 교착 상태란, 두 개 이상의 프로세스가 서로가 가진 자원을 기다리며 영원히 블로킹되는 상황을 말합니다. 이는 \"진행\" 요구사항을 위반하는 대표적인 사례입니다. 교착 상태가 발생하는 시나리오: 프로세스들의 실행 속도에 아무 가정도 할 수 없다는 점을 기억해야 합니다. 초기 상태: flag[0] = false , flag[1] = false . P0 실행: P0가 flag[0] = true; 를 실행합니다. 자신의 진입 의사를 밝혔습니다. 문맥 교환 (Context Switch) 발생: P0가 while (flag[1] == true); 를 실행하기 바로 그 직전에, 문맥 교환이 발생하여 CPU가 P1에게 넘어갑니다. P1 실행: P1도 임계 구역에 들어가고 싶어서 flag[1] = true; 를 실행합니다. 자신의 진입 의사를 밝혔습니다. 이제 P1은 while (flag[0] == true); 를 검사합니다. P0가 이미 flag[0] 을 true 로 설정했으므로, 이 조건은 참이 됩니다. 따라서 P1은 이 while 루프에서 무한정 대기합니다. 문맥 교환 (Context Switch) 발생: CPU가 다시 P0에게 돌아옵니다. P0 실행 재개: P0는 중단되었던 지점, 즉 while (flag[1] == true); 를 실행합니다. P1이 flag[1] 을 true 로 설정했으므로, 이 조건은 참이 됩니다. 따라서 P0 역시 이 while 루프에서 무한정 대기합니다. 결과: P0는 P1이 flag[1] 을 false 로 바꿔주기를 기다리고, P1은 P0가 flag[0] 을 false 로 바꿔주기를 기다립니다. 하지만 두 프로세스 모두 while 루프에 갇혀 임계 구역에 들어갈 수 없으므로, flag 값을 false 로 바꿀 기회는 영원히 오지 않습니다. 이것이 바로 교착 상태입니다. 임계 구역은 비어있지만, 들어가고 싶은 두 프로세스 모두 영원히 기다리게 되므로 '진행' 조건이 깨집니다. 이전 두 시도는 각각 상호 배제와 진행에서 실패했습니다. 이제 이 두 아이디어(차례와 의도)를 결합하여 문제를 해결하려는 시도가 나오게 됩니다. 슬라이드 11 & 12: 피터슨의 해결책 원문 (Original Text) // Slide 11 Peterson’s Solution shared int turn, flag[2]; do { flag[me] = true; turn = ! me; while (flag[!me] && turn == !me); critical section flag[me] = false; remainder section } while (true); ! Provable that 1. Mutual exclusion: 2. Progress: 3. Bounded-waiting: // Slide 12 (with process distinction) Process 0: shared int turn, flag[2]; do { flag[0] = true; turn = 1; while (flag[1] && turn == 1); critical section flag[0] = false; remainder section } while (true); Process 1: shared int turn, flag[2]; do { flag[1] = true; turn = 0; while (flag[0] && turn == 0); critical section flag[1] = false; remainder section } while (true); 번역 (Translation) // 슬라이드 11 피터슨의 해결책 (Peterson’s Solution) 공유 변수 int turn, flag[2]; do { flag[me] = true; // 진입 의사 표시 turn = !me; // 상대방에게 차례를 양보 while (flag[!me] && turn == !me); // 상대방이 원하고, 차례도 상대방 차례라면 대기 임계 구역 flag[me] = false; // 진입 의사 철회 나머지 구역 } while (true); ! 다음이 증명 가능함 1. 상호 배제: 2. 진행: 3. 한정된 대기: // 슬라이드 12 (프로세스 구분) 프로세스 0: 공유 변수 int turn, flag[2]; do { flag[0] = true; turn = 1; // 차례를 P1에게 넘김 while (flag[1] && turn == 1); // P1이 원하고, 차례도 P1이면 대기 임계 구역 flag[0] = false; 나머지 구역 } while (true); 프로세스 1: 공유 변수 int turn, flag[2]; do { flag[1] = true; turn = 0; // 차례를 P0에게 넘김 while (flag[0] && turn == 0); // P0이 원하고, 차례도 P0이면 대기 임계 구역 flag[1] = false; 나머지 구역 } while (true); 매우 자세한 설명 (Detailed Explanation) 피터슨의 해결책은 앞선 두 시도, 즉 flag 변수(의도)와 turn 변수(순서)를 매우 정교하게 결합하여 2개의 프로세스에 대한 임계 구역 문제를 완벽하게 해결하는 소프트웨어적인 방법입니다. 핵심 아이디어: 일단 들어가고 싶다는 의사( flag )를 먼저 밝힌다. ( flag[me] = true; ) 그리고 상대방에게 차례( turn )를 양보한다. ( turn = !me; ) 이것은 매우 관대한 행동처럼 보이지만, 이 알고리즘의 핵심입니다. 대기 조건: 상대방이 임계 구역에 들어갈 의사도 있고( flag[!me] == true ), 실제로 차례도 상대방의 것( turn == !me ) 이라면, 그때만 기다린다. 즉, 내가 들어가고 싶지만, 만약 상대방도 원한다면 기꺼이 양보하겠다는 태도입니다. 왜 이 해결책은 성공하는가? 상호 배제 (Mutual Exclusion) - 만족 ✔️: 두 프로세스 P0와 P1이 동시에 임계 구역에 들어가려면, 두 프로세스 모두 자신의 while 루프 조건을 거짓으로 판단하고 통과해야 합니다. P0가 통과하려면, (flag[1] && turn == 1) 이 거짓이어야 합니다. 즉, flag[1] 이 false 이거나 turn 이 0 이어야 합니다. P1이 통과하려면, (flag[0] && turn == 0) 이 거짓이어야 합니다. 즉, flag[0] 이 false 이거나 turn 이 1 이어야 합니다. 만약 두 프로세스가 거의 동시에 진입을 시도한다면, 각자 flag 를 true 로 설정하고 turn 값을 설정할 것입니다. turn 변수는 공유 변수이므로 P0가 turn = 1 로 설정한 직후 P1이 turn = 0 으로 덮어쓰거나, 그 반대의 경우가 발생합니다. turn 값은 결국 0 또는 1 둘 중 하나만 될 수 있습니다. 만약 마지막 turn 값이 0이라면: P1은 while 조건( flag[0] && turn == 0 )이 참이 되어 대기하게 됩니다. P0만 진입 가능합니다. 만약 마지막 turn 값이 1이라면: P0은 while 조건( flag[1] && turn == 1 )이 참이 되어 대기하게 됩니다. P1만 진입 가능합니다. 따라서 두 프로세스가 동시에 임계 구역에 들어가는 것은 불가능합니다. 진행 (Progress) - 만족 ✔️: 교착 상태가 발생하는지 확인해 봅시다. 두 프로세스가 동시에 while 루프에서 대기하려면, P0는 flag[1] && turn == 1이 참이길 기다리고, P1은 flag[0] && turn == 0이 참이길 기다려야 합니다. 하지만 turn은 동시에 0과 1일 수 없으므로, 두 프로세스가 동시에 while 루프에 갇히는 것은 불가능합니다. 최소한 한쪽은 turn 조건이 거짓이 되어 while 루프를 빠져나갈 수 있습니다. 또한, 만약 한 프로세스(P1)만이 임계 구역에 들어가고 싶다면(flag[1] = true), P0는 flag[0]이 false이므로 P1은 while(flag[0] && turn == 0) 조건에서 flag[0]이 거짓이라 바로 통과할 수 있습니다. 즉, 불필요한 대기가 없습니다. 한정된 대기 (Bounded Waiting) - 만족 ✔️: 기아 상태가 발생하는지 확인해 봅시다. P0가 임계 구역에 진입하기 위해 flag[0] = true를 설정하고 while 루프에서 대기 중이라고 가정합시다. 이는 flag[1]이 true이고 turn이 1이라는 뜻입니다. P1이 임계 구역을 사용하고 있습니다. P1이 임계 구역을 빠져나오면서 flag[1] = false로 설정합니다. 그러면 P0는 while 루프를 빠져나와 임계 구역에 진입할 수 있습니다. 이제 P1이 다시 임계 구역에 들어가고 싶다고 해봅시다. P1은 flag[1] = true로 설정하고 turn = 0으로 설정합니다. P0가 아직 임계 구역에 있는 동안(flag[0] = true), P1은 while (flag[0] && turn == 0) 조건 때문에 대기해야 합니다. turn이 0으로 설정되었기 때문에, P0가 이번 임계 구역 사용을 마치고 나오면, 다음 차례는 반드시 turn 값이 1로 바뀌지 않는 한 P0에게 오지 않습니다. 즉, P1은 P0를 최대 한 번만 \"새치기\"할 수 있습니다. 한번 P0에게 순서가 오면, P1은 P0가 끝날 때까지 기다려야 합니다. 따라서 어떤 프로세스도 무한정 대기하지 않습니다. 피터슨의 해결책은 이 세 가지 요구사항을 모두 만족하는 매우 우아한 알고리즘이지만, 2개의 프로세스에만 적용 가능하다는 한계와, 현대적인 컴퓨터 아키텍처에서는 컴파일러 최적화나 CPU의 명령어 재배치(out-of-order execution) 때문에 코드의 순서대로 동작한다고 보장하기 어려운 문제가 있습니다. 이 때문에 실제 시스템에서는 다음에 나올 하드웨어 기반의 해결책을 주로 사용합니다. 슬라이드 13: 교훈 및 문제점 원문 (Original Text) Lessons ! Need a locking mechanism acquire lock critical section release lock ! Peterson’s algorithm still needs atomic access to shared variables ! Problem about shared variable comes from “ the interruptible gap between get value & set value operations register <- <memory> register = <new value> <memory> <- register “ Make these operations not interruptible, but HOW? Silberschatz, Galvin and Gagne ©2009 Operating System Concepts – 8th Edition 번역 (Translation) 교훈 ! 잠금(Locking) 매커니즘이 필요하다 잠금 획득 (acquire lock) 임계 구역 잠금 해제 (release lock) ! 피터슨의 알고리즘조차도 공유 변수에 대한 원자적 접근이 필요하다. ! 공유 변수에 대한 문제는 다음으로부터 발생한다. “ 값을 가져오는(get) 연산과 값을 설정하는(set) 연산 사이의 인터럽트 가능한 틈 레지스터 ← <메모리> 레지스터 = <새로운 값> <메모리> ← 레지스터 “ 이 연산들을 인터럽트 불가능하게 만들어라, 그런데 어떻게? 매우 자세한 설명 (Detailed Explanation) 이 슬라이드는 지금까지의 논의를 요약하고, 소프트웨어적 해결책의 근본적인 한계를 지적하며 하드웨어 지원의 필요성을 역설합니다. 잠금 매커니즘의 필요성 (Need a locking mechanism) 지금까지 살펴본 모든 시도들은 결국 임계 구역이라는 '방'에 들어가기 전에 '잠금을 획득(acquire lock)'하고, 나온 후에 '잠금을 해제(release lock)'하는 과정으로 요약될 수 있습니다. Acquire Lock: entry section 에 해당하며, 임계 구역에 들어갈 권리를 얻는 과정입니다. Release Lock: exit section 에 해당하며, 권리를 반납하여 다른 프로세스가 들어올 수 있게 하는 과정입니다. 이 acquire/release 구조는 뮤텍스(Mutex), 세마포어(Semaphore) 등 현대 운영체제에서 사용하는 대부분의 동기화 도구의 기본 모델입니다. 문제는 이 acquire 와 release 동작 자체를 어떻게 신뢰성 있게 만드느냐 입니다. 피터슨 알고리즘의 한계와 원자적 접근의 필요성 피터슨의 알고리즘은 수학적으로는 완벽해 보이지만, 현실 세계의 컴퓨터에서는 두 가지 큰 문제에 직면합니다. 원자성 가정의 문제: 피터슨 알고리즘이 올바르게 동작하려면, flag[me] = true 나 turn = !me 같은 개별 명령어가 원자적으로(쪼개지지 않고) 실행된다는 가정이 필요합니다. 즉, turn = 1 이라는 명령어가 실행되는 도중에 다른 프로세스가 turn 값을 읽지 않아야 합니다. 대부분의 아키텍처에서 단순 변수 할당은 원자적이지만, 항상 보장되는 것은 아닙니다. 메모리 모델과 명령어 재배치 문제: 현대의 고성능 CPU와 컴파일러는 성능 향상을 위해 코드의 실행 순서를 임의로 바꿀 수 있습니다(명령어 재배치, Instruction Reordering). 예를 들어, 프로그래머는 아래와 같이 코드를 작성했지만, flag[me] = true; turn = !me; 컴파일러나 CPU가 성능을 위해 순서를 바꿔서 turn = !me; 를 먼저 실행하고 flag[me] = true; 를 나중에 실행할 수도 있습니다. 만약 이런 재배치가 발생하면 피터슨 알고리즘의 모든 논리적 증명은 무너지고, 알고리즘은 오작동하게 됩니다. 결국, 소프트웨어만으로는 이런 하드웨어/컴파일러 수준의 최적화를 통제하며 동기화를 완벽하게 구현하기가 매우 복잡하고 어렵습니다. 문제의 근원: 읽기-수정-쓰기 (Read-Modify-Write)의 비원자성 슬라이드의 마지막 부분은 이 모든 문제의 근원을 다시 한번 명확히 짚어줍니다. 공유 변수와 관련된 문제는 \"값을 읽고(Read/Get), 그 값을 수정하고(Modify), 다시 쓰는(Write/Set)\" 일련의 과정이 한 덩어리로 묶여있지 않고, 그 사이에 인터럽트 가능한 틈(interruptible gap)이 존재하기 때문입니다. register ← <memory> (읽기) register = <new value> (수정) <memory> ← register (쓰기) 첫 번째 시도였던 locked 변수 예제에서 while(locked == 1); (읽기)와 locked = 1; (쓰기) 사이의 틈이 문제를 일으켰습니다. 해결 방향 제시: \"HOW?\" 이러한 \"읽기-수정-쓰기\" 사이클을 아무도 방해할 수 없는, 하나의 원자적인(Atomic) 연산으로 만들 수만 있다면 문제는 간단히 해결됩니다. 하지만 소프트웨어만으로는 이를 구현하기 어렵습니다. 그래서 슬라이드는 \"어떻게(HOW)?\" 라는 질문을 던지며, 하드웨어 수준의 지원이 필요함을 암시합니다. 다음 슬라이드들에서는 이 \"HOW?\"에 대한 하드웨어적인 답변들을 제시합니다. 슬라이드 14: 해결책 - 인터럽트 비활성화 원문 (Original Text) Disabling interrupts ! Uniprocessors – could disable interrupts ! Currently running code would execute without preemption ! Generally too inefficient on multiprocessor systems 4Operating systems using this not broadly scalable Silberschatz, Galvin and Gagne ©2009 Operating System Concepts – 8th Edition &lt;h4>번역 (Translation)&lt;/h4> 인터럽트 비활성화 ! 단일처리기(Uniprocessors) 시스템에서는 인터럽트를 비활성화할 수 있다. ! 현재 실행 중인 코드는 선점(preemption) 없이 실행될 것이다. ! 일반적으로 다중처리기(multiprocessor) 시스템에서는 너무 비효율적이다. 4이 방식을 사용하는 운영체제는 광범위하게 확장 가능하지 않다. 매우 자세한 설명 (Detailed Explanation) 이 슬라이드는 \"읽기-수정-쓰기\" 연산을 원자적으로 만들기 위한 첫 번째 하드웨어적 해결책으로 인터럽트 비활성화(Disabling Interrupts)를 제시합니다. 동작 원리 앞서 문제의 원인은 연산 도중에 문맥 교환(Context Switch)이 발생하기 때문이었습니다. 문맥 교환은 주로 타이머 인터럽트(Timer Interrupt)나 입출력 인터럽트(I/O Interrupt)와 같은 하드웨어 인터럽트에 의해 촉발됩니다. 그렇다면, 임계 구역에 진입하기 전에 시스템의 모든 인터럽트를 비활성화하고, 임계 구역을 빠져나온 후에 다시 인터럽트를 활성화하면 어떨까요? C // 개념적인 코드 do { disable_interrupts(); // 진입 구역 // 임계 구역 critical_section(); // 퇴출 구역 enable_interrupts(); // 나머지 구역 remainder_section(); } while(true); 인터럽트가 비활성화된 동안에는 스케줄러를 호출하는 타이머 인터럽트가 발생하지 않으므로, 현재 CPU를 점유한 프로세스는 누구에게도 방해받지 않고(선점되지 않고) 자신의 코드를 끝까지 실행할 수 있습니다. 즉, 임계 구역 코드가 사실상 원자적으로 실행되는 효과를 얻게 됩니다. 장점과 한계 장점 (단일처리기 시스템에서): 단순하고 확실함: 구현이 매우 간단하면서도 상호 배제를 확실하게 보장합니다. 단점 및 문제점: 다중처리기 시스템에서의 무용성: 이 방법은 단일처리기(Uniprocessor, CPU가 하나인 시스템)에서만 유효합니다. 다중처리기(Multiprocessor, CPU가 여러 개인 시스템) 환경에서는 하나의 CPU에서 인터럽트를 비활성화하더라도, 다른 CPU에서는 여전히 다른 프로세스가 실행될 수 있습니다. 따라서 다른 CPU의 프로세스가 같은 임계 구역에 동시에 접근하는 것을 막을 수 없습니다. 모든 CPU의 인터럽트를 껐다 켜는 것은 통신 오버헤드가 매우 크고 복잡하여 현실적이지 않습니다. 시스템 성능 저하 및 위험성: 인터럽트를 비활성화하는 것은 시스템 전체에 영향을 미치는 매우 강력한 작업입니다. 인터럽트가 꺼져 있는 동안에는 키보드, 마우스, 네트워크 카드 등 다른 모든 하드웨어의 긴급한 요청에 응답할 수 없게 됩니다. 만약 임계 구역 코드가 길어지거나 무한 루프에 빠지면, 시스템 전체가 멈추는 '먹통' 상태가 될 수 있습니다. 사용자 수준 권한 문제: 인터럽트를 제어하는 명령어는 커널 모드(Kernel Mode)에서만 실행 가능한 특권 명령어(Privileged Instruction)입니다. 일반 사용자 프로그램이 이 기능을 마음대로 사용하도록 허용하는 것은 심각한 보안 위험을 초래할 수 있습니다. 운영체제 커널 내부에서나 제한적으로 사용할 수 있는 방법입니다. 결론: 인터럽트 비활성화는 개념은 간단하지만, 현대적인 다중처리기 운영체제에서는 확장성이 떨어지고 위험 부담이 커서 임계 구역 문제의 일반적인 해법으로 사용되지 않습니다. 슬라이드 15, 16, 17: 해결책 - 원자적 명령어 (TestAndSet) 원문 (Original Text) // Slide 15 Atomic instruction shared int locked = false; do { while (locked == true); locked = true; critical section locked = false; remainder section } while (true); Remove gap between TEST and SET!! while( TestAndSet( &locked ) ); Returns the current value and set TRUE if FALSE // Slide 16 TestAndSet Instruction boolean TestAndSet (boolean *target) { boolean rv = *target; if( *target == FALSE ) *target = TRUE; return rv: } TestAndSet Instruction-Better boolean TestAndSet (boolean *target) { boolean rv = *target; *target = TRUE; return rv: } <Value><Value> TRUE // Slide 17 Solution using TestAndSet ! Shared boolean variable lock, initialized to FALSE do { while ( TestAndSet (&lock )); // keep testing until lock is false // critical section lock = FALSE; // remainder section } while (TRUE); 번역 (Translation) // 슬라이드 15 원자적 명령어 (Atomic instruction) 공유 변수 int locked = false; do { while (locked == true); // 문제의 코드 locked = true; // 문제의 코드 임계 구역 locked = false; 나머지 구역 } while (true); TEST(확인)와 SET(설정) 사이의 틈을 제거하라!! while( TestAndSet( &locked ) ); 현재 값을 반환하고, 만약 FALSE였다면 TRUE로 설정한다. // 슬라이드 16 TestAndSet 명령어 boolean TestAndSet (boolean *target) { boolean rv = *target; // 원래 값을 rv에 저장 if( *target == FALSE ) // 원래 값이 FALSE였다면 *target = TRUE; // TRUE로 설정 return rv: // 원래 값 반환 } 더 나은 TestAndSet 명령어 boolean TestAndSet (boolean *target) { boolean rv = *target; // 원래 값을 rv에 저장 *target = TRUE; // 무조건 TRUE로 설정 return rv: // 원래 값 반환 } // 슬라이드 17 TestAndSet을 이용한 해결책 ! 공유 boolean 변수 lock은 FALSE로 초기화된다. do { while ( TestAndSet (&lock )); // lock이 false가 될 때까지 계속 테스트 (실행) // 임계 구역 lock = FALSE; // 나머지 구역 } while (TRUE); 매우 자세한 설명 (Detailed Explanation) 이 슬라이드들은 현대적인 하드웨어에서 임계 구역 문제를 해결하는 표준적인 방법인 원자적 명령어(Atomic Instruction)를 소개합니다. 특히 TestAndSet 이라는 대표적인 명령어를 예로 듭니다. 문제의 재확인과 해결의 방향 (슬라이드 15) 슬라이드 15는 첫 번째 시도였던 lock 변수 방식의 실패 원인이 TEST(확인)와 SET(설정) 사이의 '틈' 때문이었음을 다시 한번 상기시킵니다. 그리고 해결책으로 이 두 동작을 하나의 명령어 TestAndSet 으로 합쳐버리는 아이디어를 제시합니다. 이 명령어는 하드웨어 수준에서 원자성(Atomicity)을 보장받습니다. 즉, TestAndSet 명령어가 실행되는 동안에는 CPU가 다른 어떤 인터럽트나 다른 프로세스의 메모리 접근을 허용하지 않고, 명령어의 모든 내부 동작(읽고-쓰기)이 끝날 때까지 기다립니다. TestAndSet 명령어의 동작 (슬라이드 16) TestAndSet 함수는 메모리 주소를 인자로 받아 다음과 같이 동작합니다. 주어진 주소( target )에 있는 현재 값을 읽어서 임시 변수( rv )에 저장합니다. 주어진 주소( target )에 무조건 TRUE 값을 씁니다. (슬라이드의 'Better' 버전이 일반적인 구현입니다.) 임시 변수( rv )에 저장해 두었던 원래 값을 반환합니다. 이 모든 과정이 하나의 분리 불가능한(indivisible) 하드웨어 명령으로 실행됩니다. TestAndSet 을 이용한 최종 해결책 (슬라이드 17) 이 원자적 명령어를 사용하면 임계 구역의 entry section 을 매우 간결하고 안전하게 구현할 수 있습니다. 공유 변수 lock 을 boolean 타입으로 선언하고 FALSE (잠기지 않음)로 초기화합니다. Entry Section: while (TestAndSet(&lock)); Exit Section: lock = FALSE; 동작 시나리오: Case 1: 임계 구역이 비어있을 때 (lock == FALSE) 프로세스 P0가 TestAndSet(&lock) 을 호출합니다. TestAndSet 은 lock 의 현재 값인 FALSE 를 반환하고, 동시에 lock 의 값을 TRUE 로 원자적으로 변경합니다. while 문은 while(FALSE) 가 되므로, 루프를 즉시 빠져나옵니다. P0는 임계 구역에 진입합니다. 이제 lock 은 TRUE 이므로 다른 프로세스는 들어올 수 없습니다. Case 2: 다른 프로세스가 임계 구역에 있을 때 (lock == TRUE) 프로세스 P1이 TestAndSet(&lock) 을 호출합니다. TestAndSet 은 lock 의 현재 값인 TRUE 를 반환하고, lock 의 값은 계속 TRUE 로 유지됩니다. while 문은 while(TRUE) 가 되므로, P1은 루프를 계속 돌며 대기합니다. (이를 스핀락(Spinlock) 또는 바쁜 대기(Busy-waiting)라고 합니다.) P0가 임계 구역을 빠져나오며 lock = FALSE; 를 실행합니다. 다음 while 루프 차례에서 P1이 다시 TestAndSet(&lock) 을 호출하면, 이제 lock 은 FALSE 이므로 Case 1과 동일하게 동작하여 P1이 임계 구역에 진입하게 됩니다. 평가: 상호 배제 (Mutual Exclusion) - 만족 ✔️: TestAndSet 의 원자성 덕분에, lock 을 확인하고 TRUE 로 설정하는 과정이 쪼개질 수 없으므로 두 프로세스가 동시에 진입하는 것이 원천적으로 불가능합니다. 진행 (Progress) - 만족 ✔️: 임계 구역이 비어있으면( lock=FALSE ), TestAndSet 을 시도하는 첫 번째 프로세스는 반드시 진입에 성공하므로 진행 조건이 만족됩니다. 한정된 대기 (Bounded Waiting) - 실패 ❌: 이 간단한 TestAndSet 구현은 한정된 대기 조건을 만족하지 않습니다. 여러 프로세스가 동시에 대기 중일 때, CPU 스케줄링에 따라 운이 없는 프로세스는 계속해서 기회를 놓치고 기아 상태(Starvation)에 빠질 수 있습니다. 이 문제를 해결하려면 TestAndSet 과 함께 다른 변수(예: 대기 큐나 waiting 배열)를 사용하여 순서를 보장해주는 추가적인 로직이 필요합니다. 하지만 TestAndSet 과 같은 원자적 명령어는 임계 구역 문제를 해결하는 데 필요한 강력하고 필수적인 하드웨어 기반의 빌딩 블록(building block)을 제공한다는 점에서 매우 중요합니다. 운영체제는 이러한 원자적 명령어를 기반으로 세마포어, 뮤텍스 등 더 정교하고 공정한 동기화 도구들을 구현합니다. deadlock 교착 상태 문제 (The Deadlock Problem) 원문 (Original Text): The Deadlock Problem  A set of blocked processes each holding a resource and waiting to acquire a resource held by another process in the set  Example  System has 2 disk drives  P1 and P2 each hold one disk drive and each needs another one 번역 (Translation): 교착 상태 문제  각 프로세스가 자원을 보유한 채 다른 프로세스가 보유한 자원을 얻기 위해 대기하면서 봉쇄된 프로세스들의 집합  예시  시스템에 2개의 디스크 드라이브가 있음  P1과 P2는 각각 하나의 디스크 드라이브를 보유하고 있으며, 각각 다른 디스크 드라이브를 필요로 함 매우 자세한 설명 (Detailed Explanation): 교착 상태(Deadlock)란 다중 프로그래밍 환경에서 두 개 이상의 프로세스가 특정 자원을 할당받은 상태에서 다른 프로세스가 점유하고 있는 자원을 서로 기다릴 때, 결과적으로 어떠한 프로세스도 더 이상 진행할 수 없는 무한 대기 상태에 빠지는 현상을 의미합니다. 🚦 이 슬라이드는 교착 상태의 기본적인 정의와 간단한 예시를 통해 그 개념을 소개하고 있습니다. 핵심 정의: \"A set of blocked processes each holding a resource and waiting to acquire a resource held by another process in the set.\" 이 문장을 분석해보면 교착 상태의 핵심 요소를 파악할 수 있습니다. A set of blocked processes (봉쇄된 프로세스들의 집합): 교착 상태는 단일 프로세스가 아닌, 여러 프로세스들 간의 상호작용에서 발생합니다. 이 프로세스들은 '봉쇄(blocked)' 또는 '대기(waiting)' 상태에 있게 되는데, 이는 다음 단계로 진행하기 위해 필요한 특정 조건을 만족하지 못해 실행을 일시적으로 멈춘 상태를 의미합니다. Each holding a resource (각각 자원을 보유): 교착 상태에 빠진 각 프로세스는 최소 하나 이상의 자원을 이미 점유하고 있습니다. 이 자원은 해당 프로세스가 다음 작업을 수행하기 위해 필요한 요소입니다. Waiting to acquire a resource held by another process in the set (집합 내 다른 프로세스가 보유한 자원을 얻기 위해 대기): 이것이 교착 상태의 핵심적인 순환적 대기 관계를 나타냅니다. 프로세스 A는 프로세스 B가 가진 자원을 기다리고, 동시에 프로세스 B는 프로세스 A가 가진 자원(또는 다른 교착 상태 집합 내 프로세스가 가진 자원)을 기다리는 상황이 발생하는 것입니다. 예시 분석: \"System has 2 disk drives. P1 and P2 each hold one disk drive and each needs another one.\" 이 예시는 교착 상태 발생 과정을 매우 간단명료하게 보여줍니다. 시스템 환경: 사용 가능한 자원: 2개의 디스크 드라이브 (Disk Drive 1, Disk Drive 2) 프로세스: P1, P2 상황 전개: 자원 할당: 프로세스 P1이 Disk Drive 1을 할당받아 사용 중입니다. ( P1 holds Disk Drive 1 ) 프로세스 P2가 Disk Drive 2를 할당받아 사용 중입니다. ( P2 holds Disk Drive 2 ) 이 시점까지는 시스템에 문제가 없습니다. 각 프로세스는 필요한 자원 중 일부를 확보했습니다. 추가 자원 요청: 프로세스 P1은 작업을 완료하기 위해 추가적으로 Disk Drive 2를 필요로 합니다. 하지만 Disk Drive 2는 현재 P2가 보유하고 있습니다. 따라서 P1은 P2가 Disk Drive 2를 놓아줄 때까지 대기 상태에 들어갑니다. ( P1 waits for Disk Drive 2 ) 프로세스 P2도 마찬가지로 작업을 완료하기 위해 추가적으로 Disk Drive 1을 필요로 합니다. 하지만 Disk Drive 1은 현재 P1이 보유하고 있습니다. 따라서 P2는 P1이 Disk Drive 1을 놓아줄 때까지 대기 상태에 들어갑니다. ( P2 waits for Disk Drive 1 ) 교착 상태 발생: 결과적으로 P1은 P2를 기다리고, P2는 P1을 기다리는 순환 대기(Circular Wait) 상황이 발생합니다. 두 프로세스 모두 자신이 원하는 자원을 얻을 수 없으므로 영원히 대기하게 되며, 시스템은 더 이상 이 두 프로세스와 관련된 작업을 진행시키지 못합니다. 이것이 바로 교착 상태입니다. 이러한 교착 상태는 시스템의 성능 저하를 초래하고, 심한 경우 시스템 전체가 멈추는 결과를 가져올 수 있으므로 운영체제는 교착 상태를 예방하거나, 회피하거나, 혹은 탐지하고 회복하는 메커니즘을 가져야 합니다. 이 예시는 교착 상태가 발생하는 가장 기본적인 시나리오를 보여주며, 이후 슬라이드에서 설명될 교착 상태의 발생 조건, 표현 방법, 해결책 등을 이해하는 데 기초가 됩니다. 이 간단한 예시만으로도 교착 상태의 네 가지 필요조건(상호 배제, 점유와 대기, 비선점, 환형 대기) 중 일부가 암시적으로 나타나고 있음을 알 수 있습니다 (예: 디스크 드라이브는 한 번에 한 프로세스만 사용 가능 - 상호 배제, 각 프로세스가 하나를 점유한 채 다른 것을 요구 - 점유와 대기). 시스템 모델 (System Model) 원문 (Original Text): System Model  Resource types R1, R2, . . ., Rm CPU cycles, memory space, I/O devices  Each resource type Ri has Wi instances.  Each process utilizes a resource as follows:  request  use  release 번역 (Translation): 시스템 모델  자원 유형 R1, R2, . . ., Rm CPU 사이클, 메모리 공간, 입출력 장치  각 자원 유형 Ri는 Wi개의 인스턴스(instance)를 가짐.  각 프로세스는 다음과 같이 자원을 활용함:  요청 (request)  사용 (use)  방출 (release) 매우 자세한 설명 (Detailed Explanation): 이 슬라이드는 교착 상태를 논의하기 위한 기본적인 시스템 환경과 자원의 특성, 그리고 프로세스가 자원을 사용하는 일반적인 방식을 정의합니다. 이는 교착 상태의 발생 가능성과 해결 방법을 이해하는 데 필요한 배경 지식을 제공합니다. 🛠️ 자원 유형 (Resource types R1, R2, ..., Rm): 시스템 내에는 다양한 종류의 자원이 존재합니다. 이들을 유형별로 구분하여 R1​,R2​,…,Rm​ 과 같이 표현합니다. 예시: CPU 사이클 (CPU cycles): 프로세스가 명령어를 실행하기 위해 필요한 중앙 처리 장치의 처리 시간입니다. 이는 시간 분할 시스템에서 매우 중요한 자원입니다. 메모리 공간 (Memory space): 프로세스의 코드, 데이터, 스택 등을 저장하기 위한 주 기억장치(RAM)의 공간입니다. 각 프로세스는 실행되기 위해 일정량의 메모리를 필요로 합니다. 입출력 장치 (I/O devices): 프린터, 디스크 드라이브, 네트워크 인터페이스 카드 등과 같이 시스템 외부와의 데이터 교환이나 특수 기능을 수행하는 하드웨어 장치들입니다. 이러한 자원들은 그 성격에 따라 공유 가능 여부, 선점 가능 여부 등이 달라지며, 이는 교착 상태 발생에 영향을 미칩니다. 예를 들어, CPU는 시간 분할을 통해 여러 프로세스가 공유하지만, 특정 시점에는 한 프로세스에 의해 독점적으로 사용될 수 있습니다. 프린터와 같은 장치는 일반적으로 한 번에 하나의 프로세스만 사용할 수 있습니다 (상호 배제). 각 자원 유형 Ri​는 Wi​개의 인스턴스(instance)를 가짐: 각 자원 유형은 하나 이상의 동일한 기능을 수행하는 인스턴스(instance) 또는 단위(unit)를 가질 수 있습니다. Wi​는 자원 유형 Ri​에 속하는 동일한 인스턴스의 개수를 나타냅니다. 예시: 만약 시스템에 2개의 동일한 CPU가 있다면, CPU라는 자원 유형(RCPU​)은 2개의 인스턴스 (WCPU​=2)를 가집니다. 시스템에 4개의 동일한 테이프 드라이브가 있다면, 테이프 드라이브라는 자원 유형(RTape​)은 4개의 인스턴스 (WTape​=4)를 가집니다. 만약 어떤 자원 유형이 오직 하나의 인스턴스만 가진다면 (예: 특별한 그래픽 카드), 해당 자원 유형의 Wi​=1이 됩니다. 자원 인스턴스의 개수는 교착 상태 발생 가능성에 중요한 영향을 미칩니다. 만약 어떤 자원 유형의 인스턴스가 여러 개 있다면, 여러 프로세스가 동시에 해당 유형의 자원을 사용할 수 있으므로 교착 상태 발생 가능성이 낮아질 수 있습니다. 반대로, 인스턴스가 하나뿐인 자원을 여러 프로세스가 동시에 요구하면 교착 상태가 발생하기 쉽습니다. 각 프로세스는 다음과 같이 자원을 활용함 (Each process utilizes a resource as follows): 프로세스가 시스템 내의 자원을 사용하는 일반적인 패턴은 세 단계로 나눌 수 있습니다. 요청 (Request): 프로세스가 특정 작업을 수행하기 위해 자원이 필요한 경우, 운영체제에 해당 자원의 할당을 요청합니다. 만약 요청한 자원의 인스턴스가 즉시 할당될 수 있다면 (즉, 가용 인스턴스가 있다면), 운영체제는 해당 자원을 프로세스에 할당합니다. 만약 요청한 자원이 즉시 할당될 수 없다면 (예: 다른 프로세스가 이미 모든 인스턴스를 사용 중이거나, 가용 인스턴스가 부족한 경우), 요청한 프로세스는 해당 자원을 사용할 수 있을 때까지 대기(wait) 상태로 전환됩니다. 이때, 대기하는 방식은 시스템의 스케줄링 정책에 따라 달라질 수 있습니다 (예: 큐에 저장). 사용 (Use): 프로세스가 요청하여 할당받은 자원을 가지고 특정 작업을 수행합니다. 예를 들어, 프린터 자원을 할당받았다면 문서를 출력하고, CPU 자원을 할당받았다면 명령어를 실행합니다. 이 단계에서 프로세스는 할당된 자원에 대해 배타적인 제어권을 가질 수도 있고 (상호 배제), 다른 프로세스와 공유할 수도 있습니다 (자원의 특성에 따라 다름). 방출 (Release): 프로세스가 자원의 사용을 모두 마치면, 해당 자원을 운영체제에 반납합니다. 이렇게 방출된 자원은 다른 대기 중인 프로세스에 할당될 수 있게 됩니다. 자원의 방출은 일반적으로 프로세스가 작업을 완료했거나, 특정 자원이 더 이상 필요하지 않을 때 자발적으로 이루어집니다. 이러한 요청-사용-방출 (Request-Use-Release) 순서는 모든 자원 활용의 기본적인 사이클입니다. 교착 상태는 주로 '요청' 단계에서 원하는 자원을 얻지 못하고 대기하는 프로세스들이 서로 물고 물리는 관계를 형성할 때 발생합니다. 시스템 모델을 이렇게 정의함으로써, 어떤 조건에서 자원 요청이 실패하고 프로세스들이 대기 상태에 빠지며, 이것이 어떻게 교착 상태로 이어질 수 있는지를 보다 체계적으로 분석할 수 있게 됩니다. 예를 들어, 한정된 수의 인스턴스를 가진 자원에 대해 여러 프로세스가 동시에 '요청'하고, 각자 일부 자원을 '사용' (점유)한 상태에서 추가 자원을 기다리며 '방출'하지 않을 때 교착 상태가 발생할 수 있습니다. 교착 상태 특징 (Deadlock Characterization) 원문 (Original Text): Deadlock Characterization  Mutual exclusion: only one process at a time can use a resource  Hold and wait: a process holding at least one resource is waiting to acquire additional resources held by other processes  No preemption: a resource can be released only voluntarily by the process holding it, after that process has completed its task  Circular wait: there exists a set {P0, P1, …, Pn} of waiting processes such that P0 is waiting for a resource that is held by P1, P1 is waiting for a resource that is held by P2, …, Pn–1 is waiting for a resource that is held by Pn, and Pn is waiting for a resource that is held by P0. 번역 (Translation): 교착 상태 특징 (발생 조건)  상호 배제 (Mutual exclusion): 한 번에 하나의 프로세스만이 자원을 사용할 수 있음  점유와 대기 (Hold and wait): 최소한 하나의 자원을 보유한 프로세스가 다른 프로세스에 의해 보유된 추가 자원을 얻기 위해 대기함  비선점 (No preemption): 자원은 해당 자원을 보유한 프로세스에 의해, 그 프로세스의 작업이 완료된 후 자발적으로만 방출될 수 있음  환형 대기 (Circular wait): 대기 중인 프로세스들의 집합 {P0, P1, …, Pn}이 존재하여, P0는 P1이 보유한 자원을 대기하고, P1은 P2가 보유한 자원을 대기하고, …, Pn–1은 Pn이 보유한 자원을 대기하며, Pn은 P0가 보유한 자원을 대기함. 매우 자세한 설명 (Detailed Explanation): 이 슬라이드는 교착 상태가 발생하기 위해 반드시 동시에 만족되어야 하는 네 가지 조건을 설명합니다. 🕵️‍♂️ 이 네 가지 조건 중 하나라도 만족되지 않으면 교착 상태는 발생하지 않습니다. 따라서 교착 상태를 예방하는 전략은 이 조건들 중 하나 이상을 제거하는 것입니다. 상호 배제 (Mutual Exclusion): 정의: 최소한 하나의 자원이 비공유(non-sharable) 모드로 사용되어야 합니다. 즉, 한 번에 오직 하나의 프로세스만이 해당 자원을 사용할 수 있으며, 다른 프로세스가 그 자원을 사용하려고 하면 요청한 프로세스는 자원이 방출될 때까지 기다려야 합니다. 설명: 만약 모든 자원이 공유 가능하다면 (즉, 여러 프로세스가 동시에 사용할 수 있다면), 교착 상태는 발생하지 않습니다. 예를 들어, 읽기 전용 파일(read-only file)은 여러 프로세스가 동시에 접근하여 읽을 수 있으므로 상호 배제가 필요 없습니다. 그러나 프린터나 쓰기 가능한 파일과 같은 자원은 한 번에 하나의 프로세스만 접근해야 데이터의 일관성이나 장치의 올바른 작동을 보장할 수 있습니다. 이러한 자원들이 바로 상호 배제 조건을 만족시키는 자원들입니다. 교착 상태와의 관계: A 프로세스가 프린터를 사용 중일 때 B 프로세스가 프린터를 사용하려고 하면 B는 대기해야 합니다. 이것 자체는 교착 상태가 아니지만, 교착 상태를 유발할 수 있는 기본 환경을 제공합니다. 점유와 대기 (Hold and Wait): 정의: 프로세스가 최소한 하나의 자원을 점유(holding)하고 있는 상태에서, 다른 프로세스에 의해 점유된 추가적인 자원을 얻기 위해 대기(waiting)해야 합니다. 설명: 프로세스가 필요한 모든 자원을 한 번에 요청하고 할당받는다면 이 조건은 성립하지 않습니다. 하지만 대부분의 프로세스는 실행 도중에 동적으로 자원을 요청합니다. P1이 자원 R1을 이미 할당받아 사용 중인 상태에서, 추가적으로 자원 R2를 요청했는데 R2가 현재 P2에 의해 사용 중이라 P1이 대기하게 되는 상황을 의미합니다. 이때 P1은 R1을 계속 점유하고 있으면서 R2를 기다립니다. 교착 상태와의 관계: 이 조건은 프로세스들이 자원을 부분적으로 확보한 채로 다른 자원을 기다리게 만들어, 자원 할당의 연쇄적인 대기를 유발할 수 있습니다. P1이 R1을 점유하고 R2를 기다리고, P2가 R2를 점유하고 R1을 기다린다면, 이 조건이 상호 배제 및 다른 조건들과 결합하여 교착 상태를 만듭니다. 비선점 (No Preemption): 정의: 이미 할당된 자원은 그 자원을 점유하고 있는 프로세스로부터 강제로 빼앗을 수 없습니다(cannot be preempted). 자원은 오직 해당 자원을 점유한 프로세스가 작업을 완료한 후 자발적으로(voluntarily) 방출할 때만 다른 프로세스가 사용할 수 있습니다. 설명: 만약 운영체제가 필요에 따라 한 프로세스로부터 자원을 강제로 회수하여 다른 프로세스에게 할당할 수 있다면(즉, 선점이 가능하다면), 교착 상태를 해결할 수 있습니다. 예를 들어, CPU는 선점이 가능한 대표적인 자원입니다. 한 프로세스가 CPU를 사용하다가도 우선순위가 더 높은 프로세스가 나타나면 CPU를 빼앗길 수 있습니다. 하지만 프린터로 문서를 인쇄하는 도중에 프린터를 강제로 빼앗으면 인쇄 작업이 망가질 수 있는 것처럼, 많은 자원들은 비선점 특성을 가집니다. 교착 상태와의 관계: 프로세스가 자원을 점유한 채 다른 자원을 기다릴 때, 그 점유한 자원을 놓지 않기 때문에 대기 상태가 지속됩니다. 만약 점유한 자원을 강제로 회수할 수 있다면, 다른 프로세스가 해당 자원을 사용하여 작업을 진행하고 결국에는 대기 중인 프로세스가 필요로 하는 자원을 방출하게 될 가능성이 생깁니다. 환형 대기 (Circular Wait): 정의: 대기하고 있는 프로세스들의 집합 {P0​,P1​,…,Pn​}이 존재하여, P0​는 P1​이 점유한 자원을 기다리고, P1​은 P2​가 점유한 자원을 기다리며, 이러한 관계가 계속 이어져 Pn−1​은 Pn​이 점유한 자원을 기다리고, 마지막으로 Pn​은 P0​가 점유한 자원을 기다리는, 꼬리에 꼬리를 무는 순환적인 대기 형태가 존재해야 합니다. 설명: 이 조건은 앞선 세 가지 조건이 만족될 때 교착 상태가 실제로 발생하게 되는 직접적인 원인이 됩니다. 각 프로세스가 다음 프로세스가 가진 자원을 기다리면서, 결국 처음 프로세스까지 연결되는 사이클이 형성되는 것입니다. 예시: P0​는 P1​이 가진 자원 R1​을 기다림 (P0​→R1​←P1​) P1​은 P2​가 가진 자원 R2​를 기다림 (P1​→R2​←P2​) ... Pn​은 P0​가 가진 자원 R0​를 기다림 (Pn​→R0​←P0​) (여기서 Ri​는 Pi+1​ (또는 Pn​의 경우 P0​)이 점유한 자원을 나타냅니다.) 중요한 점: 이 네 가지 조건은 교착 상태가 발생하기 위한 필요조건(necessary conditions)입니다. 즉, 교착 상태가 발생했다면 이 네 가지 조건은 반드시 성립합니다. 반대로, 이 네 가지 조건이 모두 성립한다고 해서 항상 교착 상태가 발생하는 것은 아닙니다(특히 자원 유형별 인스턴스가 여러 개일 경우). 그러나 자원 유형별 인스턴스가 하나뿐일 경우에는 이 네 가지 조건이 모두 성립하면 반드시 교착 상태가 발생합니다. 이 조건들을 이해하는 것은 교착 상태를 예방하거나, 탐지하고, 회복하는 다양한 전략을 개발하는 데 있어 핵심적인 역할을 합니다. 자원 할당 그래프 (Resource-Allocation Graph) 원문 (Original Text): Resource-Allocation Graph A set of vertices V and a set of edges E.  V is partitioned into two types:  P = {P1, P2, …, Pn}, the set consisting of all the processes in the system  R = {R1, R2, …, Rm}, the set consisting of all resource types in the system  request edge – directed edge Pi ® Rj  assignment edge – directed edge Rj ® Pi 번역 (Translation): 자원 할당 그래프 정점(vertices) V의 집합과 간선(edges) E의 집합으로 구성됨.  V는 두 가지 유형으로 분할됨:  P = {P1, P2, …, Pn}, 시스템 내 모든 프로세스로 구성된 집합  R = {R1, R2, …, Rm}, 시스템 내 모든 자원 유형으로 구성된 집합  요청 간선(request edge) – 방향성 있는 간선 Pi ® Rj  할당 간선(assignment edge) – 방향성 있는 간선 Rj ® Pi 매우 자세한 설명 (Detailed Explanation): 이 슬라이드는 시스템 내의 프로세스와 자원 간의 관계, 특히 자원의 할당 및 요청 상태를 시각적으로 표현하고 분석하기 위한 도구인 자원 할당 그래프(Resource-Allocation Graph, RAG)를 소개합니다. 📊 이 그래프는 교착 상태를 탐지하고 이해하는 데 매우 유용합니다. 자원 할당 그래프의 기본 구성 요소: 자원 할당 그래프는 수학적인 그래프 이론에 기반하며, 정점(Vertices, V)의 집합과 간선(Edges, E)의 집합으로 정의됩니다. 정점의 집합 (V, Vertices): 정점 V는 다시 두 가지 하위 유형의 집합으로 나뉩니다. 프로세스 집합 (P = {P1​,P2​,…,Pn​}): 시스템에서 현재 실행 중이거나 자원을 요청/보유하고 있는 모든 프로세스들을 나타냅니다. 그래프에서는 보통 원(Circle)으로 표현됩니다. 각 원 안에는 해당 프로세스의 식별자(예: P1​,P2​)가 표시됩니다. n은 시스템 내 총 프로세스의 수를 의미합니다. 자원 유형 집합 (R = {R1​,R2​,…,Rm​}): 시스템에 존재하는 모든 자원 유형들을 나타냅니다. CPU, 메모리, 디스크 드라이브, 프린터 등이 이에 해당합니다. 그래프에서는 보통 사각형(Rectangle)으로 표현됩니다. 각 사각형 안에는 해당 자원 유형의 식별자(예: R1​,R2​)가 표시됩니다. m은 시스템 내 총 자원 유형의 수를 의미합니다. 각 자원 유형 Rj​는 여러 개의 인스턴스(instance)를 가질 수 있으며, 이는 사각형 내에 점(dot)으로 표현됩니다. 예를 들어, 자원 유형 Rj​가 4개의 인스턴스를 가지고 있다면, 사각형 Rj​ 안에 4개의 점이 그려집니다. 간선의 집합 (E, Edges): 간선 E는 프로세스와 자원 유형 사이의 관계를 나타내는 방향성 있는(directed) 연결선입니다. 간선은 두 가지 종류가 있습니다. 요청 간선 (Request Edge): Pi​→Rj​ 프로세스 Pi​가 자원 유형 Rj​의 인스턴스를 요청했으나 아직 할당받지 못한 상태임을 나타냅니다. 화살표는 프로세스 Pi​에서 시작하여 자원 유형 Rj​를 향합니다. 이는 \"프로세스 Pi​가 자원 Rj​를 기다리고 있다\"는 의미입니다. 예를 들어, 프로세스 P1​이 프린터 RP​를 사용하기 위해 요청했지만 아직 사용할 수 없는 경우, P1​→RP​ 와 같이 표현됩니다. 할당 간선 (Assignment Edge): Rj​→Pi​ 자원 유형 Rj​의 인스턴스 하나가 프로세스 Pi​에게 이미 할당되어 사용 중임을 나타냅니다. 화살표는 자원 유형 Rj​의 특정 인스턴스(점)에서 시작하여 프로세스 Pi​를 향합니다. (보다 정확히는 자원 유형의 사각형에서 인스턴스를 나타내는 점에서 나와 프로세스로 향하거나, 자원 유형의 사각형에서 프로세스로 향하고 해당 자원 유형에 인스턴스가 여러 개일 경우 어떤 인스턴스가 할당되었는지 명시될 수 있습니다. 텍스트에서는 Rj​→Pi​로 일반화하여 표현) 이는 \"프로세스 Pi​가 자원 Rj​의 인스턴스를 점유하고 있다\"는 의미입니다. 예를 들어, 프로세스 P2​가 디스크 드라이브 RD​의 한 인스턴스를 할당받아 사용 중인 경우, RD​→P2​ 와 같이 표현됩니다. 만약 RD​에 여러 인스턴스가 있다면, 그중 하나가 P2​에게 할당된 것을 의미합니다. 자원 할당 그래프의 중요성: 상태 시각화: 현재 시스템의 자원 할당 상태와 프로세스들의 대기 관계를 한눈에 파악할 수 있게 해줍니다. 교착 상태 분석: 그래프 내에 사이클(cycle)이 존재하는지 여부를 통해 교착 상태 발생 가능성 또는 실제 발생 여부를 판단하는 데 중요한 단서를 제공합니다. 만약 그래프에 사이클이 없다면, 시스템은 교착 상태가 아닙니다. 만약 그래프에 사이클이 존재한다면: 각 자원 유형이 단 하나의 인스턴스만 가지고 있다면, 교착 상태가 반드시 존재합니다. 자원 유형별로 여러 인스턴스가 존재한다면, 사이클의 존재는 교착 상태의 가능성을 의미하지만, 반드시 교착 상태인 것은 아닙니다. 추가적인 분석이 필요합니다. 이처럼 자원 할당 그래프는 복잡한 프로세스와 자원 간의 상호작용을 추상화하고 모델링하여 교착 상태라는 어려운 문제를 체계적으로 접근할 수 있도록 돕는 강력한 도구입니다. 다음 슬라이드들에서는 이 그래프의 구체적인 표현 방식과 예를 통해 교착 상태를 어떻게 식별하는지 더 자세히 보여줄 것입니다. 자원 할당 그래프 (계속) (Resource-Allocation Graph (Cont.)) 원문 (Original Text): Resource-Allocation Graph (Cont.)  Process  Resource Type with 4 instances  Pi requests instance of Rj  Pi is holding an instance of Rj (이 슬라이드는 시각적 표현에 대한 설명으로 보이며, 실제 이미지가 없으므로 텍스트 설명을 기반으로 재구성합니다.) 번역 (Translation): 자원 할당 그래프 (계속)  프로세스  4개의 인스턴스를 가진 자원 유형  Pi가 Rj의 인스턴스를 요청함  Pi가 Rj의 인스턴스를 보유 중임 매우 자세한 설명 (Detailed Explanation): 이 슬라이드는 자원 할당 그래프(RAG)를 구성하는 주요 시각적 요소들을 구체적으로 설명하여, 그래프를 어떻게 그리고 해석하는지에 대한 이해를 돕습니다. 🖼️ 앞서 설명된 정점과 간선의 개념을 실제 그래프에서 어떻게 표현하는지 보여줍니다. 프로세스 (Process): 표현: 자원 할당 그래프에서 프로세스는 일반적으로 원(Circle)으로 표시됩니다. 표기: 원 내부에는 해당 프로세스를 식별하는 이름(예: Pi​,P1​,Pserver​ 등)이 들어갑니다. [도형 예시] 코드 스니펫 graph LR P1((P1)) 위와 같이 P1이라는 프로세스를 원으로 표현합니다. 4개의 인스턴스를 가진 자원 유형 (Resource Type with 4 instances): 표현: 자원 유형은 일반적으로 사각형(Rectangle)으로 표시됩니다. 인스턴스 표현: 사각형 내부에는 해당 자원 유형에 속하는 인스턴스(instance)들을 나타내는 점(dot)들이 그려집니다. 이 슬라이드의 예시에서는 4개의 인스턴스를 가지므로, 사각형 내부에 4개의 점이 있게 됩니다. 표기: 사각형 내부 또는 근처에는 해당 자원 유형의 이름(예: Rj​,Rdisk​,Rprinter​ 등)이 표시됩니다. [도형 예시] 코드 스니펫 graph LR subgraph Rj [Resource Rj] direction LR i1(.) i2(.) i3(.) i4(.) end 위와 같이 Rj​라는 자원 유형을 사각형으로 표현하고, 내부에 4개의 점으로 인스턴스를 나타냅니다. Pi​가 Rj​의 인스턴스를 요청함 (Pi​ requests instance of Rj​): 유형: 요청 간선 (Request Edge) 표현: 프로세스 Pi​에서 자원 유형 Rj​의 사각형을 향하는 화살표(directed edge)로 표시됩니다. 의미: 프로세스 Pi​가 자원 유형 Rj​의 인스턴스 중 하나를 필요로 하며, 현재 할당받기를 기다리고 있음을 의미합니다. 아직 어떤 특정 인스턴스를 요청하는지는 이 간선만으로는 명시되지 않으며, Rj​ 유형의 가용한 인스턴스 중 하나를 기다리는 것입니다. [그래프 예시] 코드 스니펫 graph TD Pi((Pi)) subgraph Rj [Resource Rj] direction LR rj_i1(.) rj_i2(.) end Pi --> Rj 프로세스 Pi​가 자원 유형 Rj​를 요청하는 상황을 나타냅니다. 화살표는 Pi​에서 Rj​로 향합니다. Pi​가 Rj​의 인스턴스를 보유 중임 (Pi​ is holding an instance of Rj​): 유형: 할당 간선 (Assignment Edge) 표현: 자원 유형 Rj​의 사각형 내부에 있는 특정 인스턴스(점)에서 시작하여 프로세스 Pi​를 향하는 화살표(directed edge)로 표시됩니다. 의미: 자원 유형 Rj​의 특정 인스턴스가 프로세스 Pi​에게 할당되어 Pi​가 그 자원을 현재 사용 또는 점유하고 있음을 의미합니다. [그래프 예시] 코드 스니펫 graph TD Pi((Pi)) subgraph Rj [Resource Rj] direction LR rj_i1(.) --- P_holds_instance --> Pi rj_i2(.) end 자원 유형 Rj​의 인스턴스 중 하나(여기서는 첫 번째 점으로 가정)가 프로세스 Pi​에게 할당된 상황을 나타냅니다. 화살표는 Rj​의 인스턴스에서 Pi​로 향합니다. 종합적인 예시: 만약 프로세스 P1​이 자원 R1​(인스턴스 1개)을 점유하고 있으면서, 자원 R2​(인스턴스 2개) 중 하나를 요청하고 있다고 가정해 봅시다. 그리고 프로세스 P2​는 자원 R2​의 인스턴스 중 하나를 점유하고 있다고 가정합니다. 이를 자원 할당 그래프로 표현하면 다음과 같습니다. 코드 스니펫 graph TD P1((P1)) P2((P2)) subgraph R1 [Resource R1] direction LR r1_i1(.) end subgraph R2 [Resource R2] direction LR r2_i1(.) r2_i2(.) end r1_i1 --> P1 // P1이 R1의 인스턴스를 보유 (Assignment Edge) P1 --> R2 // P1이 R2의 인스턴스를 요청 (Request Edge) r2_i1 --> P2 // P2가 R2의 인스턴스 중 하나를 보유 (Assignment Edge) 이처럼 자원 할당 그래프의 시각적 요소를 정확히 이해하면, 시스템의 현재 자원 상태를 명확하게 그림으로 나타낼 수 있습니다. 이 그림을 통해 복잡한 자원 요청 및 할당 관계를 분석하고, 특히 순환적인 대기 관계(사이클)가 형성되는지 여부를 직관적으로 파악하여 교착 상태의 존재 가능성을 진단할 수 있습니다. 예를 들어, 위 예시에서는 아직 명확한 사이클이 보이지 않지만, 만약 P2​가 R1​을 추가로 요청한다면 (P2​→R1​), P1​→R2​←(instance of R2​)←P2​→R1​←(instance of R1​)←P1​ 형태의 사이클이 형성되어 교착 상태가 발생할 수 있음을 시각적으로 확인할 수 있게 됩니다. 이러한 시각화는 교착 상태의 개념을 더욱 명확하게 이해하고, 관련 알고리즘(탐지, 회피 등)의 작동 방식을 파악하는 데 큰 도움이 됩니다. 자원 할당 그래프의 예 (Example of a Resource Allocation Graph) 원문 (Original Text): Example of a Resource Allocation Graph (이 슬라이드는 제목만 있고 실제 그래프 이미지가 없으므로, 일반적인 자원 할당 그래프의 예시를 구성하여 설명합니다.) 번역 (Translation): 자원 할당 그래프의 예 매우 자세한 설명 (Detailed Explanation): 이 슬라이드는 자원 할당 그래프(RAG)가 실제로 어떻게 구성되고 해석될 수 있는지 구체적인 예시를 통해 보여주는 것을 목표로 합니다. 실제 이미지가 제공되지 않았으므로, 교착 상태가 없는 일반적인 상황과 교착 상태가 발생할 수 있는 상황을 포함하는 가상의 예시를 통해 설명하겠습니다. 📝 가상 예시 시나리오: 시스템에 다음과 같은 프로세스와 자원이 있다고 가정합니다. 프로세스: P1​,P2​,P3​ 자원 유형: R1: 인스턴스 1개 (예: 프린터) R2: 인스턴스 2개 (예: 디스크 드라이브) R3: 인스턴스 3개 (예: 메모리 블록) 상황 1: 교착 상태가 없는 경우 P1​은 R1​의 인스턴스를 보유하고 있으며, R2​의 인스턴스 하나를 요청 중입니다. P2​는 R2​의 인스턴스 하나를 보유하고 있으며, R3​의 인스턴스 하나를 요청 중입니다. P3​는 R2​의 다른 인스턴스 하나를 보유하고 있으며, R3​의 인스턴스 하나를 보유 중입니다. (여기서 P3​는 현재 추가 요청 없이 자원을 사용하고 있다고 가정) 자원 할당 그래프 표현 (상황 1): 코드 스니펫 graph TD P1((P1)) P2((P2)) P3((P3)) subgraph R1 [R1 (1 instance)] r1_i1(.) end subgraph R2 [R2 (2 instances)] r2_i1(.) r2_i2(.) end subgraph R3 [R3 (3 instances)] r3_i1(.) r3_i2(.) r3_i3(.) end r1_i1 --> P1 // P1 holds R1 P1 --> R2 // P1 requests R2 r2_i1 --> P2 // P2 holds an instance of R2 P2 --> R3 // P2 requests R3 r2_i2 --> P3 // P3 holds another instance of R2 r3_i1 --> P3 // P3 holds an instance of R3 분석 (상황 1): 프로세스 P1: R1​을 점유하고 R2​를 기다립니다. R2​에는 P2​와 P3​가 각각 하나씩 점유하고 있으므로 현재 가용 인스턴스가 없습니다. P1​은 대기 상태입니다. 프로세스 P2: R2​의 인스턴스 하나를 점유하고 R3​를 기다립니다. R3​는 P3​가 하나 점유하고 있지만, 2개의 가용 인스턴스(r3​i2,r3​i3)가 남아 있습니다. 따라서 P2​는 R3​의 인스턴스를 곧 할당받아 작업을 계속 진행할 수 있습니다. 프로세스 P3: R2​의 인스턴스와 R3​의 인스턴스를 점유하고 있으며, 추가 요청이 없습니다. P3​는 작업을 진행하다가 자원을 방출할 것입니다. 사이클 존재 여부: 위 그래프에는 사이클(cycle)이 존재하지 않습니다. P1​→R2​←P2​→R3​←P3​ 와 같은 경로는 있지만, 닫힌 순환 고리를 형성하지 않습니다. P2​가 R3​를 할당받고 작업을 완료하면, P2​는 R2​의 인스턴스를 방출할 것입니다. 이 방출된 R2​ 인스턴스를 P1​이 할당받을 수 있게 되어 P1​도 진행할 수 있습니다. 따라서 이 시스템은 현재 교착 상태가 아닙니다. 상황 2: 교착 상태가 발생할 수 있는 변경 (가상) 만약 상황 1에서 P3​가 R3​를 점유한 상태에서, 추가로 R1​을 요청한다고 가정해봅시다. (P1​은 이미 R1​을 점유하고 있습니다). 그리고 P2​도 R3​를 할당받지 못하고 대기 중이라고 가정합니다 (예를 들어 R3​의 가용 인스턴스가 없다고 가정하거나, P3​가 P2​보다 먼저 R3​의 남은 인스턴스를 모두 점유하고 있다고 가정). P1​은 R1​을 보유, R2​를 요청. P2​는 R2​의 인스턴스 하나를 보유, R1​을 요청 (원래 R3​ 요청에서 변경). (이 예시는 슬라이드 7의 \"Resource Allocation Graph With A Deadlock\"과 유사하게 만들기 위해 변경) 자원 할당 그래프 표현 (수정된 상황 - 잠재적 교착): 이 상황은 다음 슬라이드인 \"Resource Allocation Graph With A Deadlock\"에서 더 명확하게 다루어질 것이므로, 여기서는 기본적인 그래프 요소들이 어떻게 상호작용하여 복잡한 상태를 나타낼 수 있는지에 초점을 맞춥니다. 일반적인 자원 할당 그래프를 통해 알 수 있는 정보: 자원 점유 상태: 어떤 프로세스가 어떤 자원의 인스턴스를 점유하고 있는지 (Rj​→Pi​ 형태의 할당 간선). 자원 요청 상태: 어떤 프로세스가 어떤 자원 유형의 인스턴스를 기다리고 있는지 (Pi​→Rj​ 형태의 요청 간선). 자원 가용성: 각 자원 유형별로 할당되지 않은 인스턴스가 몇 개 있는지 (사각형 내의 할당되지 않은 점의 개수). 프로세스 대기 여부: 요청 간선이 있는 프로세스는 대기 상태일 가능성이 높습니다 (특히 요청한 자원의 가용 인스턴스가 없을 경우). 잠재적 문제 영역 식별: 특정 자원에 대해 많은 요청 간선이 몰려있거나, 여러 프로세스가 서로의 자원을 기다리는 듯한 패턴이 보이면 교착 상태의 위험을 의심해볼 수 있습니다. 자원 할 deputado 그래프는 시스템의 스냅샷(snapshot)입니다. 시간이 지남에 따라 프로세스가 자원을 요청하고, 할당받고, 방출함에 따라 그래프의 구조는 동적으로 변합니다. 운영체제는 이러한 그래프의 변화를 추적하거나 주기적으로 분석함으로써 교착 상태를 관리할 수 있습니다. 이 예시 슬라이드는 그러한 분석의 기초가 되는 그래프 자체의 구성 방식을 이해시키는 데 목적이 있다고 볼 수 있습니다. 교착 상태를 포함하는 자원 할당 그래프 (Resource Allocation Graph With A Deadlock) 원문 (Original Text): Resource Allocation Graph With A Deadlock (이 슬라이드는 제목만 있고 실제 그래프 이미지가 없으므로, 교착 상태를 명확히 보여주는 전형적인 예시를 구성하여 설명합니다.) 번역 (Translation): 교착 상태를 포함하는 자원 할당 그래프 매우 자세한 설명 (Detailed Explanation): 이 슬라이드는 자원 할당 그래프(RAG)를 사용하여 실제 교착 상태(deadlock)가 발생한 상황을 시각적으로 표현하고 분석하는 방법을 보여줍니다. 🔗 교착 상태의 핵심 특징인 사이클(cycle)이 그래프에 어떻게 나타나는지에 주목해야 합니다. 교착 상태를 보여주는 가상 예시 시나리오: 다음과 같은 프로세스와 자원, 그리고 그들의 상태를 가정합니다. 프로세스: P1​,P2​,P3​ 자원 유형: R1: 인스턴스 1개 R2: 인스턴스 1개 R3: 인스턴스 1개 (단순화를 위해 모든 자원 유형이 단일 인스턴스를 가진다고 가정합니다. 단일 인스턴스 자원의 경우, RAG에 사이클이 존재하면 반드시 교착 상태입니다.) 현재 상태: 프로세스 P1​은 자원 R1​을 보유(holding)하고 있으며, 자원 R2​를 요청(requesting)하고 있습니다. 프로세스 P2​는 자원 R2​를 보유하고 있으며, 자원 R3​를 요청하고 있습니다. 프로세스 P3​는 자원 R3​를 보유하고 있으며, 자원 R1​을 요청하고 있습니다. 자원 할당 그래프 표현: 코드 스니펫 graph TD P1((P1)) P2((P2)) P3((P3)) R1_box[R1 (1 instance)] R2_box[R2 (1 instance)] R3_box[R3 (1 instance)] subgraph R1_box r1_i1(.) end subgraph R2_box r2_i1(.) end subgraph R3_box r3_i1(.) end r1_i1 --> P1 // P1 holds R1's instance P1 --> R2_box // P1 requests R2 r2_i1 --> P2 // P2 holds R2's instance P2 --> R3_box // P2 requests R3 r3_i1 --> P3 // P3 holds R3's instance P3 --> R1_box // P3 requests R1 그래프 분석 및 교착 상태 식별: 사이클(Cycle)의 존재: 위의 자원 할당 그래프를 자세히 살펴보면 다음과 같은 닫힌 경로(cycle)를 발견할 수 있습니다. P1​→R2​(요청)←R2​(인스턴스)←P2​(보유)→R3​(요청)←R3​(인스턴스)←P3​(보유)→R1​(요청)←R1​(인스턴스)←P1​(보유) 간단히 표현하면: P1​requests​R2​held by​P2​requests​R3​held by​P3​requests​R1​held by​P1​ 이 사이클은 프로세스 P1​,P2​,P3​와 자원 R1​,R2​,R3​를 포함하고 있습니다. 교착 상태의 네 가지 조건 만족 여부 확인: 상호 배제 (Mutual Exclusion): 각 자원(R1​,R2​,R3​)은 단일 인스턴스만 가지고 있으므로, 한 번에 하나의 프로세스만 사용할 수 있습니다. 즉, 상호 배제 조건이 만족됩니다. (예: P1​이 R1​을 사용하는 동안 P3​는 R1​을 사용할 수 없습니다.) 점유와 대기 (Hold and Wait): P1​은 R1​을 점유한 채 R2​를 기다립니다. P2​는 R2​를 점유한 채 R3​를 기다립니다. P3​는 R3​를 점유한 채 R1​을 기다립니다. 각 프로세스가 자원을 점유하면서 다른 자원을 대기하고 있으므로, 점유와 대기 조건이 만족됩니다. 비선점 (No Preemption): 문제에서 명시적으로 언급되지는 않았지만, 일반적인 자원(특히 이 예시에서 암시하는 프린터, 파일 등과 유사한 배타적 자원)은 사용 중인 프로세스로부터 강제로 빼앗을 수 없다고 가정합니다. 즉, 비선점 조건이 만족됩니다. P1​이 R1​ 사용을 마칠 때까지 P3​는 R1​을 얻을 수 없습니다. 환형 대기 (Circular Wait): 위에서 식별한 사이클(P1​→R2​←P2​→R3​←P3​→R1​←P1​)이 바로 환형 대기 조건을 명확하게 보여줍니다. P1​은 P2​가 가진 R2​를, P2​는 P3​가 가진 R3​를, P3​는 P1​이 가진 R1​을 기다리고 있습니다. 결론: 교착 상태 발생 모든 자원 유형이 단일 인스턴스를 가지고 있고, 그래프에 사이클이 존재하므로, 이 시스템은 명백히 교착 상태에 빠져 있습니다. P1​은 P2​가 R2​를 놓아주기를 기다립니다. P2​는 P3​가 R3​를 놓아주기를 기다립니다. P3​는 P1​이 R1​을 놓아주기를 기다립니다. 어떤 프로세스도 자신이 점유한 자원을 놓지 않고 다른 자원을 기다리고 있기 때문에, 세 프로세스 모두 영원히 다음 단계로 진행할 수 없습니다. 자원 할당 그래프와 교착 상태의 관계 요약: 사이클 없음 ⇒ 교착 상태 없음: 그래프에 사이클이 없으면 시스템은 교착 상태가 아닙니다. 사이클 존재 + 각 자원 유형별 단일 인스턴스 ⇒ 교착 상태 발생: 그래프에 사이클이 있고, 그 사이클에 포함된 모든 자원 유형이 오직 하나의 인스턴스만 가지고 있다면 시스템은 반드시 교착 상태입니다. (위 예시가 이에 해당) 사이클 존재 + 일부 자원 유형별 다중 인스턴스 ⇒ 교착 상태 가능성 존재: 그래프에 사이클이 있지만, 사이클에 포함된 자원 유형 중 일부가 여러 인스턴스를 가지고 있다면 교착 상태일 수도 있고 아닐 수도 있습니다. 이 경우는 추가적인 분석이 필요합니다 (다음 슬라이드에서 다룰 내용). 이 슬라이드의 (가상) 예시는 자원 할당 그래프가 어떻게 교착 상태를 명확하게 드러내는지 보여줍니다. 사이클의 형성은 교착 상태의 핵심 시각적 증거이며, 운영체제는 이러한 사이클을 탐지하는 알고리즘을 사용하여 교착 상태를 발견하고 해결할 수 있습니다. 사이클이 있지만 교착 상태가 아닌 그래프 (Graph With A Cycle But No Deadlock) 원문 (Original Text): Graph With A Cycle But No Deadlock (이 슬라이드는 제목만 있고 실제 그래프 이미지가 없으므로, 해당 조건을 만족하는 예시를 구성하여 설명합니다.) 번역 (Translation): 사이클(주기)은 있지만 교착 상태는 아닌 그래프 매우 자세한 설명 (Detailed Explanation): 이 슬라이드는 자원 할당 그래프(RAG)에서 사이클(cycle)이 존재하더라도 항상 교착 상태(deadlock)를 의미하는 것은 아니라는 중요한 점을 강조합니다. 🔄❓ 이는 특히 하나 이상의 자원 유형이 여러 개의 인스턴스(multiple instances)를 가지고 있을 때 발생할 수 있는 상황입니다. 사이클이 있지만 교착 상태가 아닌 예시 시나리오: 다음과 같은 시스템 상태를 가정합니다. 프로세스: P1​,P2​,P3​,P4​ 자원 유형: R1: 인스턴스 1개 R2: 인스턴스 2개 현재 상태: 프로세스 P1​은 자원 R1​의 인스턴스를 요청(requesting)하고 있으며, 이 R1​ 인스턴스는 현재 프로세스 P2​에 의해 보유(held)되어 있습니다. (P1​→R1​←P2​) 프로세스 P2​는 자원 R1​의 인스턴스를 보유하고 있으며, 자원 R2​의 인스턴스 하나를 요청하고 있습니다. (P2​→R2​) 프로세스 P3​는 자원 R2​의 인스턴스 하나를 요청하고 있으며, 이 R2​ 인스턴스는 현재 프로세스 P4​에 의해 보유되어 있습니다. (P3​→R2​←P4​) 자원 R2​에는 총 2개의 인스턴스가 있으며, 그중 하나는 P4​가 보유하고 있고, 나머지 하나는 가용(available) 상태이거나, 또는 P2​에게 할당될 수 있는 상태라고 가정합니다. (여기서는 P2​와 P3​가 R2​를 요청하는 상황을 좀 더 명확히 하기 위해 P2​가 R2​의 한 인스턴스를 점유하고, P3​가 R2​의 다른 인스턴스를 점유하고, P1​이 P2​가 점유한 R2​를 기다리고, P2​는 R1​을 기다리는 사이클을 만들어보겠습니다. 그리고 R2​의 다른 인스턴스를 P4​가 점유하고, P4​는 다른 자원을 기다리지 않는다고 가정하겠습니다.) 더 명확한 시나리오 (사이클 존재, 교착 아님): P1​이 R1​의 인스턴스 하나를 점유하고, R2​의 인스턴스 하나를 요청. (R1​→P1​→R2​) P2​가 R2​의 인스턴스 하나를 점유하고 (다른 인스턴스), R1​의 인스턴스 하나를 요청. (R2​→P2​→R1​) R2​에는 총 2개의 인스턴스가 있습니다. 하나는 P2​가 점유. 다른 하나는 P3​가 점유하고 있고, P3​는 다른 요청이 없음. 자원 할당 그래프 표현 (위 시나리오 기반): 코드 스니펫 graph TD P1((P1)) P2((P2)) P3((P3)) subgraph R1 [R1 (1 instance)] r1_i1(.) end subgraph R2 [R2 (2 instances)] r2_i1(.) r2_i2(.) end r1_i1 --> P1 // P1 holds R1 P1 --> R2 // P1 requests R2 (aiming for r2_i1 held by P2) r2_i1 --> P2 // P2 holds one instance of R2 P2 --> R1 // P2 requests R1 (held by P1) r2_i2 --> P3 // P3 holds the other instance of R2 (and requests nothing else) 그래프 분석: 사이클(Cycle)의 존재: 위 그래프에는 P1​과 P2​ 사이에 명확한 사이클이 존재합니다: P1​requests​R2​(인스턴스 r2_i1을 통해)held by​P2​requests​R1​held by​P1​ 이 사이클은 P1​,P2​,R1​,R2​(의 한 인스턴스)를 포함합니다. 교착 상태 여부 판단: 사이클이 존재함에도 불구하고, 이 시스템은 교착 상태가 아닐 수 있습니다. 왜냐하면 R2​에는 여러 인스턴스가 있기 때문입니다. 현재 P1​은 P2​가 점유하고 있는 R2​의 특정 인스턴스(r2_i1)를 기다리고 있고, P2​는 P1​이 점유하고 있는 R1​을 기다리고 있어 P1​,P2​ 사이에는 교착 관계가 형성되어 있습니다. 하지만, P3​는 R2​의 다른 인스턴스(r2i2)를 점유하고 있으며, 다른 자원을 요청하고 있지 않습니다. P3​는 자신의 작업을 계속 진행하다가 언젠가는 R2​의 인스턴스(r2i2)를 방출(release)할 것입니다. 만약 P1​이 P2​가 가진 r2i1 대신, R2​의 어떤 인스턴스든 사용할 수 있다면, P3​가 r2i2를 방출했을 때 P1​이 이 인스턴스를 할당받을 수 있습니다. P1​이 R2​를 할당받아 작업을 완료하면, P1​은 R1​을 방출합니다. 그러면 P2​가 R1​을 할당받아 작업을 완료하고 R2​의 인스턴스(r2_i1)를 방출합니다. 이렇게 되면 시스템의 모든 프로세스가 결국 작업을 완료할 수 있게 됩니다. 더 간단하고 명확한 예시 (교과서적 예시): P1​→R1​ (요청), R1​은 인스턴스 2개 (i1​,i2​), i1​은 P2​에게 할당. P2​→R2​ (요청), R2​는 인스턴스 1개, R2​는 P1​에게 할당. P3​는 R1​의 i2​를 할당받음. P3​는 R2​를 요청하지 않음. 코드 스니펫 graph TD P1((P1)) P2((P2)) P3((P3)) subgraph R1 [R1 (2 instances)] r1_i1(.) r1_i2(.) end subgraph R2 [R2 (1 instance)] r2_i1(.) end P1 --> R1 // P1 requests R1 r1_i1 --> P2 // P2 holds an instance of R1 P2 --> R2 // P2 requests R2 r2_i1 --> P1 // P1 holds R2 r1_i2 --> P3 // P3 holds another instance of R1 사이클: P1​→R1​(인스턴스 r1_i1을 통해)←P2​→R2​←P1​. 교착 아님: P3​가 작업을 마치고 R1​의 인스턴스 r1_i2를 방출하면, P1​이 이 인스턴스를 사용할 수 있습니다. (만약 P1​이 R1​의 특정 인스턴스가 아닌 아무 인스턴스나 기다리고 있다면). P1​이 R1​을 얻어 작업을 마치면 R2​를 방출하고, 그러면 P2​가 R2​를 얻어 작업을 마칠 수 있습니다. 핵심: 자원 할당 그래프에 사이클이 존재하더라도, 그 사이클에 포함된 자원 유형이 여러 인스턴스를 가지고 있고, 그 중 일부 인스턴스가 사이클 외부의 프로세스에 할당되어 있거나 가용 상태여서, 사이클 내의 프로세스 중 하나가 결국 필요한 자원을 얻을 수 있는 경로가 존재한다면 교착 상태가 아닐 수 있습니다. 이러한 상황 때문에, 자원 유형별로 여러 인스턴스가 있는 시스템에서는 사이클의 존재만으로는 교착 상태를 확정할 수 없습니다. 교착 상태 탐지 알고리즘은 단순히 사이클을 찾는 것 이상으로, 가용 자원과 프로세스의 요청을 고려하여 실제로 진행 불가능한 상황인지를 판단해야 합니다. 이 슬라이드는 교착 상태의 조건과 자원 할당 그래프의 해석에 있어 미묘하지만 중요한 차이를 이해시키는 데 도움을 줍니다. 단일 인스턴스 자원의 경우 사이클 = 교착이지만, 다중 인스턴스 자원의 경우 사이클 = 교착일 수 있다는 점을 기억해야 합니다. 기본 사실들 (Basic Facts) 원문 (Original Text): Basic Facts  If graph contains no cycles Þ no deadlock  If graph contains a cycle Þ  if only one instance per resource type, then deadlock  if several instances per resource type, possibility of deadlock R resource type 의 모든 w instance 가 circular 이어야 deadlock 의 조건중 1개인 circular wait 이 발생한다 (원문 마지막 줄은 한국어로 되어 있어, 번역 없이 설명에 포함합니다.) 번역 (Translation): 기본 사실들  그래프에 사이클(cycle)이 없으면 Þ 교착 상태 없음  그래프에 사이클이 있으면 Þ  자원 유형별로 인스턴스가 하나뿐이라면, 교착 상태임  자원 유형별로 여러 인스턴스가 있다면, 교착 상태의 가능성이 있음 (R resource type 의 모든 w instance 가 circular 이어야 deadlock 의 조건중 1개인 circular wait 이 발생한다 - 이 부분은 원문이 한국어이므로 번역보다는 설명으로 풀어냅니다.) 매우 자세한 설명 (Detailed Explanation): 이 슬라이드는 자원 할당 그래프(RAG)와 교착 상태(deadlock) 간의 관계에 대한 핵심적인 사실들을 요약합니다. 📌 이는 교착 상태를 탐지하고 이해하는 데 있어 매우 중요한 기준을 제공합니다. 그래프에 사이클이 없으면 ⇒ 교착 상태 없음 (If graph contains no cycles ⇒ no deadlock) 설명: 자원 할당 그래프에 어떠한 사이클도 존재하지 않는다면, 시스템에는 교착 상태가 절대로 존재하지 않습니다. 이유: 교착 상태의 네 가지 필요조건 중 하나인 환형 대기(Circular Wait) 조건은 그래프 상에서 사이클로 표현됩니다. 사이클이 없다는 것은 환형 대기가 없다는 의미이며, 네 가지 조건 중 하나라도 만족되지 않으면 교착 상태는 발생할 수 없습니다. 의미: 이는 교착 상태가 아님을 판단하는 매우 강력하고 간단한 기준입니다. 그래프를 그려 사이클이 없는 것을 확인하면, 시스템은 안전하다고 말할 수 있습니다 (적어도 현재 스냅샷에서는). 그래프에 사이클이 있으면 ⇒ (If graph contains a cycle ⇒) 그래프에 사이클이 존재할 경우, 상황은 자원 유형별 인스턴스 수에 따라 달라집니다. 2a. 자원 유형별로 인스턴스가 하나뿐이라면, 교착 상태임 (if only one instance per resource type, then deadlock) 설명: 만약 그래프에 사이클이 존재하고, 그 사이클에 포함된 모든 자원 유형이 오직 하나의 인스턴스만을 가지고 있다면, 시스템은 반드시 교착 상태입니다. 이유: 이 경우, 사이클 내의 각 프로세스는 다음 프로세스가 점유하고 있는 (단 하나뿐인) 자원의 인스턴스를 기다리게 됩니다. 빠져나갈 다른 인스턴스가 없으므로, 환형 대기가 직접적으로 교착 상태를 유발합니다. 이전 슬라이드 \"Resource Allocation Graph With A Deadlock\"의 예시가 이 경우에 해당합니다. 예시: P1​→RA​←P2​→RB​←P1​. 여기서 RA​와 RB​가 각각 단일 인스턴스라면, P1​과 P2​는 교착 상태입니다. 2b. 자원 유형별로 여러 인스턴스가 있다면, 교착 상태의 가능성이 있음 (if several instances per resource type, possibility of deadlock) 설명: 만약 그래프에 사이클이 존재하지만, 그 사이클에 관련된 자원 유형 중 하나 이상이 여러 개의 인스턴스를 가지고 있다면, 이는 교착 상태일 가능성을 시사하지만, 반드시 교착 상태인 것은 아닙니다. 이유: 이전 슬라이드 \"Graph With A Cycle But No Deadlock\"에서 보았듯이, 사이클에 참여하지 않는 다른 프로세스가 해당 다중 인스턴스 자원 중 하나를 사용하고 있다가 방출하면, 사이클 내의 프로세스가 그 자원을 할당받아 사이클을 끊고 진행할 수 있는 여지가 있기 때문입니다. 예시: P1​→RA​←P2​→RB​←P1​. 여기서 RA​는 단일 인스턴스지만 RB​가 2개의 인스턴스(i1​,i2​)를 가지고 있고, P1​이 RB​의 i1​을 기다리고 있는데 P2​가 i1​을 점유하고 있다고 가정합시다. 만약 P3​라는 다른 프로세스가 RB​의 i2​를 점유하고 있다가 곧 방출한다면, P1​은 i2​를 할당받아 진행할 수 있으므로 교착 상태가 아닐 수 있습니다. 그러나 P1​이 반드시 P2​가 점유한 i1​만을 기다려야 하고, i2​가 다른 이유로 P1​에게 할당될 수 없다면 교착이 될 수도 있습니다. 즉, 상황에 따라 다릅니다. \"R resource type 의 모든 w instance 가 circular 이어야 deadlock 의 조건중 1개인 circular wait 이 발생한다\" 해석 및 설명: 이 문장은 \"특정 자원 유형 R의 모든 W개의 인스턴스가 (사이클을 형성하는) 환형 대기에 관여되어야만 교착 상태의 조건 중 하나인 환형 대기가 (해당 자원과 관련하여 명확하게) 발생한다\"는 의미로 해석될 수 있습니다. 하지만 이 표현은 다소 혼란을 줄 수 있으며, 일반적인 교착 상태 정의와 약간 다르게 접근하는 것처럼 보입니다. 일반적인 관점과의 비교: 보통 환형 대기(Circular Wait)는 프로세스들의 집합 {P0​,P1​,…,Pn​}이 있어 P0​가 P1​이 가진 자원을, P1​이 P2​가 가진 자원을, ..., Pn​이 P0​가 가진 자원을 기다리는 상황 자체를 의미합니다. 이때, 각 프로세스가 기다리는 자원이 특정 유형 R의 인스턴스일 수 있습니다. 만약 자원 유형 R이 여러 인스턴스(w개)를 가지고 있고, 사이클 내의 어떤 프로세스가 R의 인스턴스를 기다린다고 할 때, R의 모든 w개 인스턴스가 반드시 그 사이클 내의 다른 프로세스들에 의해 점유되어 있고, 해당 인스턴스들이 모두 환형 대기의 일부를 구성해야만 교착 상태가 된다는 의미는 아닙니다. 오히려, 사이클이 존재하고 해당 사이클에 포함된 자원 요청을 만족시킬 수 있는 가용 인스턴스가 없을 때 교착 상태가 됩니다. 더 정확한 이해 (다중 인스턴스 환경에서의 사이클과 교착): 다중 인스턴스 자원 유형 Ri​가 사이클에 포함되어 있다고 가정해 봅시다. 프로세스 Pk​가 Ri​의 인스턴스를 요청합니다 (Pk​→Ri​). Ri​의 모든 인스턴스가 현재 다른 프로세스들 (Pa​,Pb​,…)에 의해 점유되어 있습니다. 만약 이 프로세스들(Pa​,Pb​,…) 중 일부 또는 전부가 다시 Pk​ (또는 사이클 내 다른 프로세스)가 점유한 자원을 기다리고 있다면, 이것이 교착 상태로 이어질 수 있습니다. 핵심은 Pk​가 기다리는 자원 Ri​에 가용 인스턴스가 없고, Ri​의 인스턴스를 점유한 프로세스들이 사이클을 완성하는 방식으로 다른 자원을 기다리고 있을 때 교착 상태가 발생합니다. 원문의 의도 재해석: 원문의 \"R resource type 의 모든 w instance 가 circular 이어야...\" 부분은 아마도, 특정 자원 R의 모든 인스턴스가 사이클에 참여하는 프로세스들에게 할당되어 있고, 이들 프로세스가 서로를 기다리는 상황이 되면 환형 대기가 명백해진다는 점을 강조하려 했을 수 있습니다. 즉, 해당 자원 유형에 더 이상 '탈출구'(가용 인스턴스)가 없음을 나타내는 표현일 수 있습니다. 그러나 교착 상태는 단순히 한 자원 유형의 모든 인스턴스가 사이클에 묶여있어야만 발생하는 것은 아닙니다. 여러 자원 유형이 얽힌 사이클에서 각 자원의 가용성을 종합적으로 봐야 합니다. 요약하면: 사이클 없음 ⟹ 교착 아님 (절대적). 사이클 있음 + 모든 자원 단일 인스턴스 ⟹ 교착 (절대적). 사이클 있음 + 일부/전부 자원 다중 인스턴스 ⟹ 교착 가능성 (상황 분석 필요). 이때는 해당 사이클 내의 요청을 만족시킬 수 있는 가용 자원이 있는지, 또는 사이클 외부의 프로세스가 자원을 방출하여 사이클을 깰 수 있는지를 확인해야 합니다. 이 기본 사실들은 교착 상태 탐지 알고리즘(예: Banker's 알고리즘의 안전성 검사나 실제 교착 탐지 시)의 이론적 기초가 됩니다. 교착 상태 처리 (Dealing with Deadlock) 원문 (Original Text): Dealing with Deadlock  Three general approaches exist for dealing with deadlock.  Prevent deadlock  Avoid deadlock  Detect Deadlock 번역 (Translation): 교착 상태 처리  교착 상태를 처리하는 세 가지 일반적인 접근 방식이 존재함.  교착 상태 예방 (Prevent deadlock)  교착 상태 회피 (Avoid deadlock)  교착 상태 탐지 (Detect Deadlock) 매우 자세한 설명 (Detailed Explanation): 이 슬라이드는 운영체제가 교착 상태(Deadlock)라는 까다로운 문제에 대처하기 위해 사용할 수 있는 세 가지 주요 전략을 소개합니다. 🛡️ 시스템 설계자나 관리자는 시스템의 특성과 요구 사항에 따라 이 중 하나 또는 조합을 선택하여 적용할 수 있습니다. 경우에 따라서는 교착 상태를 무시하는 네 번째 접근 방식도 언급되기도 합니다(특히 교착 발생 빈도가 매우 낮고, 처리 비용이 더 클 경우). 교착 상태 예방 (Prevent Deadlock): 개념: 교착 상태가 발생하기 위한 네 가지 필요조건(상호 배제, 점유와 대기, 비선점, 환형 대기) 중 어느 하나라도 처음부터 성립하지 않도록 시스템을 설계하는 방식입니다. 마치 질병을 예방하기 위해 미리 백신을 맞는 것과 유사합니다. 목표: 교착 상태가 절대로 발생하지 않도록 보장하는 것입니다. 방법: 상호 배제 조건 부정 (예: 스풀링을 통해 공유 가능하게 만듦) 점유와 대기 조건 부정 (예: 프로세스 시작 시 모든 자원 한 번에 요청 또는 자원 요청 전 기존 자원 모두 반납) 비선점 조건 부정 (예: 자원 강제 회수 허용 - 어렵거나 부작용이 클 수 있음) 환형 대기 조건 부정 (예: 자원에 순서를 부여하여 순서대로만 요청하도록 강제) 장점: 교착 상태가 발생할 가능성 자체를 원천적으로 차단합니다. 단점: 각 조건을 부정하기 위한 제약들이 때로는 자원 활용률(resource utilization)을 심각하게 저하시킬 수 있습니다. 예를 들어, 모든 자원을 미리 요청해야 한다면, 당장 사용하지 않을 자원까지 미리 확보해야 하므로 다른 프로세스가 그 자원을 사용하지 못하게 됩니다. 시스템 처리율(throughput)이 감소할 수 있고, 프로세스의 응답 시간이 길어질 수 있습니다. 어떤 조건(예: 상호 배제)은 특정 자원의 본질적인 특성상 제거하기 어려울 수 있습니다. 교착 상태 회피 (Avoid Deadlock): 개념: 프로세스가 자원을 요청할 때, 시스템은 해당 요청을 승인할 경우 장래에 교착 상태를 유발할 가능성이 있는지(unsafe state로 이어지는지)를 미리 검사합니다. 만약 교착 가능성이 있다면 요청을 승인하지 않고 프로세스를 대기시킵니다. 이는 마치 운전 중 위험한 교차로에 진입하기 전에 교통 상황을 확인하고 안전할 때만 진입하는 것과 유사합니다. 목표: 시스템이 항상 안전 상태(safe state)를 유지하도록 하여 교착 상태를 피해가는 것입니다. 안전 상태란 시스템이 각 프로세스에게 자원을 할당할 수 있는 순서(safe sequence)가 존재하여 모든 프로세스가 교착 없이 종료될 수 있는 상태를 의미합니다. 방법: 프로세스는 시작 시 자신이 앞으로 사용할 최대 자원 요구량을 미리 운영체제에 알려야 합니다. 운영체제는 자원 할당 시마다 Banker's 알고리즘과 같은 알고리즘을 사용하여, 할당 후에도 시스템이 안전 상태를 유지할 수 있는지 검사합니다. 장점: 예방보다 덜 엄격한 조건을 적용하므로 자원 활용률이 더 높을 수 있습니다. 교착 상태 발생을 막으면서도 예방보다는 유연합니다. 단점: 프로세스가 필요한 최대 자원량을 미리 알아야 한다는 제약이 있습니다. (실행 중에 필요한 자원이 동적으로 변하는 경우가 많음) 새로운 프로세스가 시스템에 추가되거나 종료될 때마다, 그리고 자원 요청/해제 시마다 안전성 검사를 수행해야 하므로 상당한 오버헤드가 발생할 수 있습니다. 할당할 수 있는 자원의 수가 고정되어야 합니다. 교착 상태 탐지 및 회복 (Detect Deadlock and Recover): 개념: 교착 상태 예방이나 회피를 위한 특별한 조치를 취하지 않고, 시스템이 교착 상태에 빠지는 것을 허용합니다. 대신, 주기적으로 또는 필요시 교착 상태가 발생했는지 검사(탐지)하고, 만약 발생했다면 이를 해결(회복)하는 방식입니다. 이는 마치 병에 걸린 후 진단하고 치료하는 것과 유사합니다. 목표: 교착 상태가 드물게 발생한다고 가정하고, 발생했을 때만 처리하여 평상시의 시스템 성능 저하를 최소화하는 것입니다. 방법: 탐지 (Detection): 자원 할당 그래프를 사용하여 사이클을 찾거나, Banker's 알고리즘과 유사한 방식으로 현재 상태에서 교착이 발생했는지 확인하는 알고리즘을 사용합니다. 회복 (Recovery): 프로세스 종료 (Process termination): 교착 상태에 관련된 프로세스 중 하나 또는 전부를 강제로 종료시킵니다. 어떤 프로세스를 종료할지는 비용(우선순위, 진행 정도 등)을 고려하여 결정합니다. 자원 선점 (Resource preemption): 교착 상태에 있는 프로세스로부터 자원을 강제로 빼앗아 다른 프로세스에게 할당합니다. 이때 희생자 선택, 롤백(rollback), 기아(starvation) 문제 등을 고려해야 합니다. 장점: 예방이나 회피보다 시스템에 가해지는 제약이 적어, 평상시에는 자원 활용률과 시스템 처리율이 높을 수 있습니다. 교착 상태가 자주 발생하지 않는 시스템에 적합합니다. 단점: 교착 상태가 탐지될 때까지 해당 프로세스들은 작업을 진행하지 못합니다. 회복 과정에서 데이터 손실이나 작업 내용의 일부를 잃을 위험이 있습니다 (특히 프로세스 종료나 자원 선점 시). 탐지 알고리즘의 실행 빈도 결정이 중요합니다. 너무 자주 실행하면 오버헤드가 크고, 너무 드물게 실행하면 교착 상태가 오래 지속될 수 있습니다. 추가: 교착 상태 무시 (Ignoring Deadlock): 일부 시스템(예: 개인용 PC의 운영체제)에서는 교착 상태가 매우 드물게 발생하고, 발생하더라도 사용자가 시스템을 재부팅하는 등의 방식으로 해결할 수 있다고 가정하여, 교착 상태를 처리하기 위한 특별한 메커니즘을 두지 않기도 합니다. 이를 \"타조 알고리즘(Ostrich Algorithm)\"이라고도 부릅니다. 이는 교착 상태 처리 비용이 교착으로 인한 손실보다 크다고 판단될 때 선택될 수 있습니다. 어떤 접근 방식을 선택할지는 시스템의 목적, 신뢰성 요구 수준, 성능 요구 사항, 그리고 교착 상태 발생 빈도 및 그로 인한 영향 등을 종합적으로 고려하여 결정해야 합니다. 이후 슬라이드에서는 이 중 '교착 상태 예방'에 대해 더 자세히 다룰 것으로 예상됩니다. 교착 상태 예방 (Deadlock Prevention) 원문 (Original Text): Deadlock Prevention 7.13 Silberschatz, Galvin and Gagne ©2009Operating System Concepts – 8th Edition 13 Idea: invalidate one of the deadlock conditions 1. Invalidate Mutual exclusion condition 2. Invalidate Hold-and-wait condition 3. Invalidate No preemption condition 4. Invalidate Circular wait condition 번역 (Translation): 교착 상태 예방 7.13 Silberschatz, Galvin and Gagne ©2009 운영체제 개념 – 제8판 13 아이디어: 교착 상태 조건 중 하나를 무효화시킴 1. 상호 배제 조건 무효화 2. 점유와 대기 조건 무효화 3. 비선점 조건 무효화 4. 환형 대기 조건 무효화 매우 자세한 설명 (Detailed Explanation): 이 슬라이드는 교착 상태 처리 전략 중 하나인 교착 상태 예방(Deadlock Prevention)의 핵심 아이디어와 접근 방법을 소개합니다. 💡 교착 상태 예방은 교착 상태가 발생하기 위한 네 가지 필수 조건(상호 배제, 점유와 대기, 비선점, 환형 대기) 중 적어도 하나를 시스템 설계 단계에서부터 원천적으로 제거함으로써 교착 상태 발생 가능성을 완전히 없애는 방법입니다. 핵심 아이디어 (Idea: invalidate one of the deadlock conditions): 교착 상태는 네 가지 조건이 모두 동시에 만족될 때만 발생합니다. 따라서 이 네 가지 조건 중 단 하나라도 만족되지 않도록 시스템을 구성하면 교착 상태는 이론적으로 발생할 수 없습니다. 이는 마치 특정 질병의 발병 원인 중 하나를 제거하여 질병 발생 자체를 막는 것과 유사한 접근 방식입니다. 운영체제 설계자는 다음 네 가지 조건 각각에 대해 이를 어떻게 무효화(invalidate)하거나 약화시킬 수 있는지 고려하여 예방 전략을 수립합니다. 상호 배제 조건 무효화 (Invalidate Mutual exclusion condition): 목표: 자원이 본질적으로 공유 불가능(non-sharable)하지 않다면, 즉 여러 프로세스가 동시에 접근해도 문제가 없다면 상호 배제를 적용하지 않는 것입니다. 방법: 자원의 공유화: 가능한 많은 자원을 공유 가능하게 만듭니다. 예를 들어, 읽기 전용(read-only) 파일은 여러 프로세스가 동시에 읽을 수 있도록 허용하여 상호 배제를 피할 수 있습니다. 스풀링(Spooling): 프린터와 같이 본질적으로 한 번에 하나의 프로세스만 사용해야 하는 자원의 경우, 직접적인 자원 접근 대신 스풀링 기법을 사용합니다. 프로세스는 출력을 프린터에 직접 보내는 대신, 디스크의 특별한 스풀 디렉토리에 출력 내용을 파일로 저장합니다. 그러면 프린터 데몬(daemon)이라는 시스템 프로세스만이 이 스풀 디렉토리에 접근하여 순차적으로 파일들을 실제 프린터로 전송합니다. 이렇게 하면 여러 사용자 프로세스 입장에서는 프린터를 동시에 사용하는 것처럼 보이지만, 실제 물리적 프린터 자원은 프린터 데몬에 의해서만 배타적으로 사용되므로 사용자 프로세스 간의 교착 상태는 발생하지 않습니다 (프린터 데몬과 다른 시스템 자원 간의 교착은 별개 문제). 한계: 모든 자원이 공유 가능하거나 스풀링 가능한 것은 아닙니다. 예를 들어, 특정 데이터 구조를 수정하는 연산은 반드시 상호 배제가 필요합니다. 따라서 상호 배제 조건을 완전히 제거하는 것은 현실적으로 어렵거나 불가능한 경우가 많습니다. 점유와 대기 조건 무효화 (Invalidate Hold-and-wait condition): 목표: 프로세스가 하나의 자원을 점유한 상태에서 다른 자원을 기다리지 못하도록 하는 것입니다. 방법: 방법 A (일괄 요청): 프로세스가 실행을 시작하기 전에 필요한 모든 자원을 한꺼번에 요청하도록 강제합니다. 만약 요청한 모든 자원을 할당받을 수 있을 때만 작업을 시작하고, 그렇지 않으면 모든 자원을 할당받을 수 있을 때까지 대기합니다. 일단 자원을 할당받으면, 다른 자원을 추가로 요청하기 전까지는 이 자원들을 점유하고 사용할 수 있습니다. 방법 B (자원 반납 후 요청): 프로세스가 새로운 자원을 요청하기 전에 현재 점유하고 있는 모든 자원을 일단 반납(release)하도록 합니다. 그리고 필요한 모든 자원(기존에 사용하던 자원 포함)을 다시 한꺼번에 요청합니다. 한계: 프로세스가 실행 초기에 앞으로 필요한 모든 자원을 예측하기 어려울 수 있습니다. 자원을 미리 할당받아 두면, 당장 사용하지 않는 자원들이 오랫동안 묶여 있게 되어 자원 활용률이 매우 낮아집니다 (low resource utilization). 다른 많은 프로세스들이 소수의 프로세스 때문에 자원을 사용하지 못하고 기아 상태(starvation)에 빠질 수 있습니다. 방법 B의 경우, 자원을 반납했다가 다시 요청하는 과정에서 비효율이 발생할 수 있고, 이전에 사용하던 자원을 다시 할당받지 못할 수도 있습니다. 비선점 조건 무효화 (Invalidate No preemption condition): 목표: 이미 할당된 자원을 점유 중인 프로세스로부터 강제로 빼앗아(선점하여) 다른 프로세스에게 할당할 수 있도록 허용하는 것입니다. 방법: 방법 A: 어떤 프로세스가 자원을 요청했을 때 즉시 할당받을 수 없다면(즉, 대기해야 한다면), 그 프로세스가 현재 점유하고 있는 모든 자원을 강제로 반납시킵니다. 이 선점된 자원들은 필요한 다른 프로세스에게 할당될 수 있습니다. 해당 프로세스는 자신이 요청한 새로운 자원과 이전에 점유했다가 선점당한 자원들을 모두 얻을 수 있을 때 다시 작업을 시작합니다. 방법 B: 높은 우선순위의 프로세스가 낮은 우선순위의 프로세스가 점유한 자원을 필요로 할 때, 낮은 우선순위 프로세스로부터 자원을 빼앗아 높은 우선순위 프로세스에게 할당합니다. 한계: 모든 자원에 대해 선점을 적용하기는 어렵습니다. 예를 들어, 프린터로 인쇄 중인 작업을 중간에 선점하면 처음부터 다시 인쇄해야 하거나 데이터가 손상될 수 있습니다. 상태를 쉽게 저장하고 복원할 수 있는 자원(예: CPU, 메모리)에 대해서는 비교적 적용하기 쉽습니다. 선점과 복구 과정에 대한 비용과 복잡성이 클 수 있습니다. 작업의 일관성을 유지하기 어려울 수 있습니다. 환형 대기 조건 무효화 (Invalidate Circular wait condition): 목표: 프로세스들이 원형으로 자원을 기다리는 상황 자체를 방지하는 것입니다. 방법: 자원 순서화 (Resource ordering/hierarchy): 모든 자원 유형에 고유한 번호(순서)를 할당합니다. 그리고 모든 프로세스는 반드시 이 번호의 오름차순(또는 내림차순)으로만 자원을 요청하도록 강제합니다. 예를 들어, 자원 Ri​를 점유한 프로세스가 자원 Rj​를 요청하려면 반드시 j>i (오름차순의 경우)여야 합니다. 이렇게 하면 P1​이 Ri​를 점유하고 Rj​ (j>i)를 기다리고, P2​가 Rj​를 점유하고 Rk​ (k>j)를 기다리는 식으로는 진행될 수 있지만, Pn​이 Rx​를 점유하고 P1​이 점유한 Ry​ (yF(Ri​) 이어야 합니다. 만약 프로세스가 이미 번호가 큰 자원 Rk​를 점유하고 있는데, 그보다 번호가 작은 자원 Rl​ (F(Rl​)F(Ra​) 입니다. 만약 P2​가 Rb​를 점유하고 Rc​를 기다린다면, F(Rc​)>F(Rb​) 입니다. 이런 식으로 요청 간선은 항상 자원 번호가 증가하는 방향으로만 형성됩니다. P1​→Rb​(요청, F(Rb​)>F(Ra​) where Ra​는 P1​이 점유) P2​→Rc​(요청, F(Rc​)>F(Rb​) where Rb​는 P2​가 점유) ... Pn​→Rz​(요청, F(Rz​)>F(Ry​) where Ry​는 Pn​이 점유) 만약 Pn​이 다시 P1​이 점유한 Ra​를 요청하여 사이클을 만들려면, F(Ra​)>F(Ry​) 여야 합니다. 그러나 이미 자원 요청은 번호가 증가하는 순서로만 이루어졌으므로, F(Ra​)가 안전 순서라면, P1​이 현재 가용 자원으로 작업을 마칠 수 있고, P1​이 끝나고 자원을 반납하면 그 자원을 이용하여 P2​가 작업을 마칠 수 있으며, 또 P2​가 끝나고 자원을 반납하면 그 자원으로 P3​가 작업을 마칠 수 있다는 의미입니다. \"Unsafe state is a state that is not safe\" (불안전 상태는 안전하지 않은 상태임): 불안전 상태는 위에서 정의한 안전 순서가 존재하지 않는 상태입니다. 주의할 점: 불안전 상태가 반드시 교착 상태를 의미하는 것은 아닙니다. 불안전 상태는 교착 상태로 이어질 가능성이 있는 상태를 의미합니다. 프로세스들이 실제로 최대 요구량까지 자원을 요청하지 않거나, 특정 순서로 자원을 요청하면 교착을 피할 수도 있습니다. 하지만 회피 알고리즘은 이러한 \"가능성\"을 회피하기 위해 불안전 상태로의 진입 자체를 막습니다. 결론: 자원 할당 거부 전략, 특히 은행가 알고리즘은 교착 상태 회피를 위한 정교한 방법론을 제공합니다. 이는 시스템이 항상 안전 상태를 유지하도록 보장함으로써 교착 상태를 방지합니다. 그러나 이를 위해서는 각 프로세스의 최대 자원 요구량에 대한 사전 정보가 필요하고, 자원 요청 시마다 안전성 검사를 수행하는 데 따른 계산 오버헤드가 발생할 수 있다는 점을 고려해야 합니다. 다음 슬라이드에서는 이 \"안전 상태\"에 대해 더 자세히 정의할 것입니다. 안전 상태 (Safe State) original text Safe State When a process requests an available resource, system must decide if immediate allocation leaves the system in a safe state System is in safe state if there exists a sequence of ALL the processes in the systems such that for each Pi, the resources that Pi can still request can be satisfied by currently available resources + resources held by all the Pj, with j 이 존재하여, 각 Pi에 대해 Pi가 여전히 요청할 수 있는 자원들이 현재 가용한 자원과 모든 Pj (j of ALL the processes in the systems such that for each Pi​, the resources that Pi​ can still request can be satisfied by currently available resources + resources held by all the Pj​, with j)대로 나열했을 때, 이 순서대로 각 프로세스가 자신의 작업을 완료할 수 있음을 보장하는 것입니다. 이 순서에서 각 프로세스 Pi​는 다음 조건을 만족해야 합니다: Pi​가 작업을 완료하기 위해 앞으로 더 요청할 수 있는 자원(needs)의 양이, (현재시스템에즉시사용가능한자원) + (순서상 Pi​보다 앞에 있는 모든 프로세스들 Pj​ (즉, j를 가정해 봅시다. P3: 1개 필요. 현재 가용 자원 3개 중 1개를 할당 가능. P3 실행 후 종료, 자원 1개 반납 (가용: 3개). P1: 1개 더 필요. P3 종료 후 가용 자원은 3개. P1에게 1개 할당 가능. P1 실행 후 종료, 총 2개 반납 (가용: 3−1+2=4개가 아니라, (원래 가용 3) + (P3반납1) - (P1추가1) + (P1최종반납2) = 3 + 1 -1 + 2 = 5. (원래 시스템 총 자원이 5개였다고 가정시). 현재 예시에서는, (시스템 가용 3) + (P3가 사용했던 자원 1개) - (P1이 추가로 필요한 자원 1개) = 3개. P1이 종료하면 (P1이 원래 가졌던 1개 + 추가로 받은 1개) = 2개를 반납. 시스템 가용 자원은 3+2=5 가 아닌, (시스템 가용 3) + (P1이 보유했던 자원 1개) = 4개가 됨. 더 정확히: 초기 가용: 3. (P1이 1개, P2가 1개 점유 중이라 가정, 총 자원은 5개) P3 (1개 필요): 가용 3개 중 1개 할당 (가용 2됨). P3 종료 후 1개 반납 (가용 3됨). P1 (1개 더 필요): 가용 3개 중 1개 할당 (가용 2됨). P1 종료 후 총 2개 반납 (가용 4됨). P2 (2개 더 필요): 가용 4개 중 2개 할당 (가용 2됨). P2 종료 후 총 3개 반납 (가용 5됨). 모든 프로세스가 완료 가능하므로 이 순서는 안전 순서이며, 현재 상태는 안전 상태입니다. 이처럼 안전 상태는 시스템이 교착 상태에 빠지지 않고 모든 프로세스를 성공적으로 완료시킬 수 있는 \"탈출 경로\"가 존재함을 의미합니다. 기본 사실들 (Basic Facts) original text Basic Facts If a system is in safe state Þ no deadlocks If a system is in unsafe state Þ possibility of deadlock Avoidance Þ ensure that a system will never enter an unsafe state. korea 기본 사실들 시스템이 안전 상태(safe state)에 있다면 Þ 교착 상태 없음 시스템이 불안전 상태(unsafe state)에 있다면 Þ 교착 상태의 가능성 있음 회피(Avoidance) Þ 시스템이 절대로 불안전 상태로 진입하지 않도록 보장함. description 이 슬라이드는 안전 상태(Safe State), 불안전 상태(Unsafe State), 그리고 교착 상태(Deadlock) 간의 관계 및 교착 상태 회피(Avoidance) 전략의 목표를 명확하게 요약합니다. 📜 시스템이 안전 상태에 있다면 ⇒ 교착 상태 없음 (If a system is in safe state ⇒ no deadlocks): 의미: 이것은 매우 강력한 보장입니다. 시스템이 현재 안전 상태에 있다면, 즉 모든 프로세스를 교착 없이 완료시킬 수 있는 최소한 하나의 실행 순서(safe sequence)가 존재한다면, 현재 시점에서 시스템에는 교착 상태가 절대 존재하지 않으며, 또한 현재 상태에서 앞으로 교착 상태가 발생하지 않고 모든 프로세스가 결국 종료될 수 있음을 보장합니다 (프로세스들이 선언한 최대 요구량 내에서 자원을 요청하고, 시스템이 회피 알고리즘을 따른다는 가정 하에). 이유: 안전 순서의 정의 자체가 각 프로세스가 필요한 자원을 (비록 기다릴 수는 있지만) 결국에는 얻어서 작업을 완료하고 자원을 반납할 수 있음을 의미하기 때문입니다. 이 연쇄적인 완료 과정은 교착 상태의 특징인 순환 대기를 방지합니다. 시스템이 불안전 상태에 있다면 ⇒ 교착 상태의 가능성 있음 (If a system is in unsafe state ⇒ possibility of deadlock): 의미: 시스템이 불안전 상태에 있다는 것은 안전 순서가 존재하지 않음을 의미합니다. 중요한 점은, 불안전 상태가 반드시 교착 상태를 의미하는 것은 아니라는 것입니다. 불안전 상태는 단지 교착 상태로 이어질 \"가능성\" 또는 \"위험\" 이 있는 상태를 나타냅니다. 이유: 불안전 상태에서는 운영체제가 프로세스들의 자원 요청을 조심스럽게 관리하지 않으면 교착 상태가 발생할 수 있습니다. 하지만, 불안전 상태에 있더라도 프로세스들이 실제로 자신들이 선언한 최대 자원량까지 요청하지 않거나, 또는 우연히 교착을 피하는 순서로 자원을 요청하고 해제하면 교착 상태가 발생하지 않을 수도 있습니다. 예를 들어, 한 프로세스가 최대치보다 적은 자원만 사용하고 일찍 종료하여 다른 프로세스에게 자원을 넘겨줄 수도 있습니다. 예시: 은행에 돈이 거의 바닥났지만(불안전 상태), 대출자들이 동시에 돈을 인출하러 오지 않거나 일부만 인출하면 은행이 파산하지 않을 수 있는 것과 유사합니다. 하지만 모든 대출자가 동시에 최대 금액을 인출하려 한다면 파산(교착)할 수 있습니다. 회피(Avoidance) ⇒ 시스템이 절대로 불안전 상태로 진입하지 않도록 보장함 (Avoidance ⇒ ensure that a system will never enter an unsafe state.): 목표: 교착 상태 회피 알고리즘(예: 은행가 알고리즘)의 핵심 목표는, 시스템이 애초에 불안전 상태로 진입하는 것을 막는 것입니다. 작동 방식: 프로세스가 자원을 요청할 때마다, 회피 알고리즘은 만약 이 요청을 승인했을 경우 시스템이 불안전 상태로 변하게 될지를 미리 검사합니다. 만약 불안전 상태로의 전이가 예상되면, 해당 자원 요청은 거부되고 프로세스는 대기합니다. 오직 요청을 승인해도 시스템이 계속 안전 상태를 유지할 경우에만 자원이 할당됩니다. 결과: 이 전략을 통해 시스템은 항상 안전 상태에 머무르게 되며, 따라서 (위의 1번 사실에 의해) 교착 상태는 발생하지 않게 됩니다. 요약: 안전 상태: 교착 없음 (보장됨) 불안전 상태: 교착 가능성 있음 (반드시 교착은 아님) 교착 상태 회피의 역할: 시스템을 항상 안전 상태 영역 내에 있도록 유지하여, 불안전 상태로의 진입 자체를 차단함으로써 교착 상태를 예방적으로 피하는 것입니다. 이러한 기본 사실들은 교착 상태 회피 기법의 이론적 근거를 제공하며, 왜 회피 알고리즘이 안전 상태를 유지하려고 하는지를 설명해줍니다. 다음 슬라이드에서는 이러한 상태들 간의 관계를 시각적으로 표현할 가능성이 있습니다. 안전, 불안전, 교착 상태 (Safe, Unsafe, Deadlock State) original text Safe, Unsafe, Deadlock State (이 슬라이드는 일반적으로 세 가지 상태 간의 관계를 보여주는 다이어그램이나 그림을 포함하지만, 텍스트만 제공되었습니다. 설명을 위해 일반적인 개념도를 가정하여 설명합니다.) korea 안전, 불안전, 교착 상태 (이 슬라이드는 일반적으로 세 가지 상태 간의 관계를 보여주는 다이어그램이나 그림을 포함하지만, 텍스트만 제공되었습니다. 설명을 위해 일반적인 개념도를 가정하여 설명합니다.) description 이 슬라이드의 제목 \"Safe, Unsafe, Deadlock State\"는 시스템이 처할 수 있는 세 가지 중요한 상태와 그들 사이의 관계를 시각적으로 설명하려는 의도로 보입니다. 실제 이미지는 제공되지 않았지만, 일반적으로 이러한 상태들은 다음과 같은 포함 관계로 표현됩니다. 벤 다이어그램 형태를 생각할 수 있습니다. ⭕ 상태 간의 개념적 관계: 전체 상태 공간 (All Possible System States): 시스템이 취할 수 있는 모든 자원 할당 상태들의 집합입니다. 안전 상태 (Safe States): 전체 상태 공간 내의 한 부분집합입니다. 이 상태에서는 교착 상태가 발생하지 않음을 보장하는 실행 순서(safe sequence)가 존재합니다. 교착 상태 회피 알고리즘은 시스템이 항상 이 안전 상태 영역 내에 머무르도록 하는 것을 목표로 합니다. [안전 상태 영역] 불안전 상태 (Unsafe States): 안전 상태가 아닌 모든 상태를 의미합니다. 즉, 안전 순서가 존재하지 않는 상태들입니다. [불안전 상태 영역 = 전체 상태 영역 - 안전 상태 영역] 중요: 불안전 상태가 곧 교착 상태를 의미하지는 않습니다. 불안전 상태는 교착 상태로 이어질 \"가능성\"이 있는 상태입니다. 교착 상태 (Deadlock States): 불안전 상태 영역 내의 일부분입니다. 실제로 하나 이상의 프로세스가 더 이상 진행할 수 없는 순환 대기에 빠진 상태입니다. [교착 상태 영역 ⊂ 불안전 상태 영역] 시각적 표현 (가상 다이어그램 설명): 일반적으로 다음과 같은 형태로 표현될 수 있습니다: 코드 스니펫 graph TD subgraph 전체 시스템 상태 공간 direction LR subgraph 안전 상태 (Safe States) A[ ] end subgraph 불안전 상태 (Unsafe States) direction LR B_non_deadlock[불안전하지만 교착은 아님] subgraph 교착 상태 (Deadlock States) C[ ] end end end style A fill:#9f9,stroke:#333,stroke-width:2px style C fill:#f99,stroke:#333,stroke-width:2px style B_non_deadlock fill:#ff9,stroke:#333,stroke-width:2px %% 설명: %% 녹색 영역(A): 안전 상태. 시스템이 여기에 머무르면 교착 없음. %% 노란색 영역(B_non_deadlock) + 빨간색 영역(C): 불안전 상태. %% 빨간색 영역(C): 실제 교착 상태. 불안전 상태의 일부임. %% 교착 상태 회피 알고리즘은 시스템이 녹색 영역을 벗어나 노란색/빨간색 영역으로 진입하는 것을 방지함. 다이어그램 해석: 안전 상태에서 불안전 상태로의 전이: 교착 상태 회피 알고리즘은 자원 할당 요청을 승인했을 때 시스템이 안전 상태에서 불안전 상태로 넘어가는 것을 감지하고, 이러한 전이를 막습니다. 즉, 시스템의 운영 지점을 항상 안전 상태 영역 내에 유지하려고 합니다. 불안전 상태에서 교착 상태로의 전이: 시스템이 일단 불안전 상태로 진입했다고 해서 반드시 교착 상태가 되는 것은 아닙니다. 프로세스들이 자원을 요청하는 패턴에 따라 교착 상태로 빠질 수도 있고, 운 좋게 빠져나올 수도 있습니다. 하지만 회피 알고리즘은 이러한 \"운\"에 맡기지 않고 불안전 상태 자체를 피합니다. 교착 상태는 불안전 상태의 부분집합: 모든 교착 상태는 불안전 상태입니다 (교착 상태에서는 안전 순서가 존재할 수 없으므로). 그러나 모든 불안전 상태가 교착 상태인 것은 아닙니다. 교착 상태 회피의 역할: 교착 상태 회피(Avoidance) 전략은 자원 할당 결정 시, 해당 결정이 시스템을 안전 상태 영역에서 불안전 상태 영역으로 이동시키는지 여부를 확인합니다. 만약 그렇다면, 그 할당을 거부하여 시스템을 계속 안전 상태에 머무르게 합니다. 이렇게 함으로써, 시스템이 교착 상태(불안전 상태 영역 내의 특정 부분)에 도달하는 것을 원천적으로 방지합니다. 이러한 상태 간의 관계를 이해하는 것은 교착 상태 회피 알고리즘이 왜 그렇게 설계되었고 어떻게 작동하는지를 파악하는 데 매우 중요합니다. 시스템은 \"위험한\" 불안전 상태로의 진입을 피함으로써 \"실제 재앙\"인 교착 상태를 막는 것입니다. 회피 알고리즘 (Avoidance algorithms) original text Avoidance algorithms Single instance of a resource type Use a resource-allocation graph Multiple instances of a resource type Use the banker’s algorithm resource type 당 2개의 인스턴스가 있을때 뱅커스 알로리즘을 사용한다 resource type 당 2개의 인스턴스가 있을때 뱅커스 알로리즘을 사용한다 korea 회피 알고리즘 자원 유형당 단일 인스턴스인 경우 자원 할당 그래프를 사용함 자원 유형당 다중 인스턴스인 경우 은행가 알고리즘을 사용함 자원 유형 당 2개의 인스턴스가 있을 때 은행가 알고리즘을 사용한다 (원문 반복) 자원 유형 당 2개의 인스턴스가 있을 때 은행가 알고리즘을 사용한다 (원문 반복) description 이 슬라이드는 교착 상태 회피(Deadlock Avoidance)를 위해 사용되는 구체적인 알고리즘들을 시스템의 자원 특성에 따라 구분하여 제시합니다. 특히 자원 유형별 인스턴스 수에 따라 다른 접근법이 사용됨을 강조합니다. 🛠️ 자원 유형당 단일 인스턴스인 경우 (Single instance of a resource type): 알고리즘: 자원 할당 그래프 (Resource-Allocation Graph) 사용 이전에 교착 상태 탐지(Detection)에서 자원 할당 그래프를 사용하여 사이클을 찾는 방법을 논의한 바 있습니다. 교착 상태 회피에서도 유사한 원리를 사용하지만, 목적이 다릅니다. 작동 방식 (회피에서의 활용): 프로세스가 자원을 요청하면, 시스템은 이 요청을 승인한다고 가정하고 자원 할당 그래프에 해당 요청 간선(Pi​→Rj​)을 추가해 봅니다. 또한, 프로세스가 앞으로 요청할 수 있는 모든 가능한 자원들을 나타내는 요구 간선(claim edge)이라는 개념을 도입할 수 있습니다. 요구 간선은 점선으로 표시되며, 프로세스가 미래에 해당 자원을 요청할 수도 있음을 나타냅니다. (요청 간선은 실제 요청, 할당 간선은 실제 할당) 요청을 승인하여 요청 간선이 할당 간선(Rj​→Pi​)으로 바뀌었을 때, 만약 이 그래프에서 사이클(cycle)이 형성될 가능성이 있다면 해당 요청은 교착 상태를 유발할 수 있는 것으로 간주되어 거부됩니다. 단일 인스턴스 환경에서는 사이클이 곧 교착 상태를 의미하기 때문입니다. 정확히는, 요청을 승인했을 때 생기는 할당 간선과 기존의 요구 간선들을 고려하여 사이클이 발생하는지를 검사합니다. 만약 사이클이 발생하면, 해당 요청은 시스템을 불안전 상태로 만들 수 있으므로 거부됩니다. 장점: 개념적으로 비교적 간단합니다. 단점: 자원 유형별 인스턴스가 하나뿐인 제한된 환경에서만 효과적입니다. 자원 유형당 다중 인스턴스인 경우 (Multiple instances of a resource type): 알고리즘: 은행가 알고리즘 (Banker's Algorithm) 사용 자원 유형별로 여러 개의 동일한 인스턴스가 존재하는 경우, 자원 할당 그래프에서의 단순한 사이클 탐지만으로는 교착 상태를 정확히 판단하기 어렵습니다 (사이클이 있어도 교착 상태가 아닐 수 있음). 따라서 더 정교한 알고리즘인 은행가 알고리즘이 사용됩니다. 은행가 알고리즘의 핵심 요소: 최대 요구량 (Max): 각 프로세스가 각 자원 유형에 대해 최대로 필요로 하는 인스턴스 수. 현재 할당량 (Allocation): 현재 각 프로세스에게 할당된 각 자원 유형의 인스턴스 수. 필요량 (Need): 각 프로세스가 작업을 완료하기 위해 앞으로 더 필요한 각 자원 유형의 인스턴스 수 (Need = Max - Allocation). 가용량 (Available): 현재 시스템에서 즉시 사용 가능한 각 자원 유형의 인스턴스 수. 작동 방식 (안전성 검사 - Safety Algorithm): 프로세스가 자원을 요청하면, 일단 요청량이 Need 벡터 이내인지, 그리고 Available 벡터 이내인지를 확인합니다. 만약 그렇다면, 가상으로 자원을 할당해 봅니다 (Available 감소, Allocation 증가, Need 감소). 이 새로운 상태에서 안전성 알고리즘을 실행하여 시스템이 여전히 안전 상태인지 확인합니다. 안전성 알고리즘은 현재 가용 자원과 각 프로세스의 Need를 비교하여, 모든 프로세스를 완료시킬 수 있는 안전 순서(safe sequence)를 찾으려고 시도합니다. (방법 요약) 현재 가용 자원으로 Need를 만족시킬 수 있는 프로세스 Pi​를 찾습니다. 있다면, Pi​가 작업을 마치고 자신이 가진 모든 자원을 반납한다고 가정하고 가용 자원을 업데이트합니다. 이 과정을 반복하여 모든 프로세스가 포함되는 순서를 찾으면 안전 상태입니다. 안전 순서가 발견되면 실제 자원 할당이 이루어지고, 발견되지 않으면 (불안전 상태) 가상 할당을 취소하고 프로세스는 대기합니다. \"resource type 당 2개의 인스턴스가 있을때 뱅커스 알로리즘을 사용한다\" (원문 반복): 이 문장이 두 번 반복된 것은 \"자원 유형당 다중 인스턴스\"의 예시로서 최소 2개 이상의 인스턴스가 있을 때부터 은행가 알고리즘이 적용될 수 있음을 강조하는 것으로 보입니다. 즉, 인스턴스가 하나일 때는 자원 할당 그래프를 통한 사이클 검출 방식이 더 적합하고, 인스턴스가 2개 이상인 다중 인스턴스 환경에서는 은행가 알고리즘과 같은 보다 일반적인 방법이 필요하다는 의미입니다. 물론 은행가 알고리즘은 이론적으로 단일 인스턴스 환경에도 적용할 수 있지만, 다중 인스턴스 환경에서 그 필요성과 효과가 더욱 부각됩니다. 결론: 교착 상태 회피 알고리즘은 시스템 내 자원의 특성(특히 인스턴스 수)에 따라 선택됩니다. 단일 인스턴스/자원 유형: 자원 할당 그래프 기반 사이클 검사 (미래의 잠재적 사이클 형성 방지) 다중 인스턴스/자원 유형: 은행가 알고리즘 (안전 순서 존재 여부 검사를 통해 안전 상태 유지) 은행가 알고리즘은 더 일반적이고 강력하지만, 각 프로세스의 최대 자원 요구량 정보가 필요하고 매 자원 요청 시 안전성 검사로 인한 오버헤드가 발생한다는 점을 항상 염두에 두어야 합니다. Resource-Allocation Graph Scheme original text Claim edge Pi​→Rj​ indicated that process Pi​ may request resource Rj​; represented by a dashed line Claim edge converts to request edge when a process requests a resource Request edge converted to an assignment edge when the resource is allocated to the process When a resource is released by a process, assignment edge reconverts to a claim edge Resources must be claimed a priori in the system korea 클레임 간선 (Claim edge) Pi​→Rj: 프로세스 Pi​가 자원 Rj​를 요청할 수 있음을 나타내며, 점선으로 표현됩니다. 클레임 간선의 변환: 프로세스가 자원을 요청하면 클레임 간선은 요청 간선 (request edge)으로 변환됩니다. 요청 간선의 변환: 자원이 프로세스에 할당되면 요청 간선은 할당 간선 (assignment edge)으로 변환됩니다. 할당 간선의 재변환: 프로세스가 자원을 해제하면 할당 간선은 다시 클레임 간선으로 변환됩니다. 자원의 사전 클레임: 시스템에서 자원은 사전에 클레임되어야 합니다. description 자원 할당 그래프(Resource-Allocation Graph)는 운영체제에서 프로세스와 자원 간의 관계를 시각적으로 표현하여 교착 상태(deadlock)를 탐지하고 방지하는 데 사용되는 도구입니다. 이 슬라이드에서는 자원 할당 그래프의 주요 구성 요소인 간선(edge)의 종류와 그 변환 규칙을 설명합니다. 첫째, 클레임 간선(Pi​→Rj​)은 프로세스 Pi​가 미래에 자원 Rj​를 요청할 수 있음을 나타냅니다. 이 간선은 점선으로 표현되며, 프로세스가 작업을 시작하기 전에 자신이 필요로 할 수 있는 모든 자원을 미리 선언(claim)해야 한다는 \"사전 클레임(a priori claim)\" 원칙을 반영합니다. 예를 들어, 어떤 프로그램이 실행되기 전에 \"저는 프린터와 스캐너가 필요할 수 있습니다\"라고 미리 알리는 것과 같습니다. 이는 시스템이 자원 할당 결정을 내릴 때 미래의 자원 요구 사항을 고려할 수 있도록 돕습니다. 둘째, 프로세스 Pi​가 실제로 자원 Rj​를 요청할 때, 점선으로 표시되었던 클레임 간선은 요청 간선(request edge)으로 변환됩니다. 요청 간선은 실선으로 표현될 수 있으며, 현재 프로세스가 해당 자원을 적극적으로 기다리고 있음을 나타냅니다. 예를 들어, \"프린터를 요청합니다!\"라고 외치는 것과 같은 상황입니다. 셋째, 요청된 자원 Rj​가 프로세스 Pi​에 성공적으로 할당되면 요청 간선은 할당 간선(assignment edge)으로 다시 변환됩니다. 할당 간선은 자원 인스턴스에서 프로세스 Pi​로 향하는 화살표로 표현되며, 프로세스가 해당 자원을 현재 소유하고 있음을 의미합니다. \"프린터가 저에게 할당되었습니다!\"라는 상황입니다. 넷째, 프로세스 Pi​가 자원 Rj​의 사용을 마친 후 이를 해제(release)하면, 할당 간선은 다시 클레임 간선(claim edge)으로 되돌아갑니다. 이는 프로세스가 자원을 더 이상 소유하고 있지 않지만, 미래에 다시 요청할 가능성이 있음을 나타냅니다. \"프린터 사용을 마쳤습니다. 나중에 다시 사용할 수도 있어요.\"와 같은 의미입니다. 마지막으로 \"Resources must be claimed a priori in the system\"이라는 문구는 이 자원 할당 그래프 스키마의 중요한 전제 조건을 강조합니다. 모든 프로세스는 실행을 시작하기 전에 자신이 필요로 할 수 있는 최대 자원 요구량을 미리 시스템에 알려야 합니다. 이는 시스템이 잠재적인 교착 상태를 미리 예측하고 방지하기 위한 정보를 얻는 데 필수적입니다. 이 사전 클레임 정보는 주로 은행원 알고리즘(Banker's Algorithm)과 같은 교착 상태 회피(deadlock avoidance) 알고리즘에서 활용되어, 자원 할당 요청이 들어왔을 때 시스템이 안전한 상태를 유지할 수 있는지 판단하는 데 사용됩니다. 이 모든 간선 변환 과정은 시스템이 프로세스와 자원 간의 복잡한 상호작용을 추적하고, 잠재적인 교착 상태를 식별하며, 이를 방지하기 위한 전략을 적용하는 데 중요한 기반을 제공합니다. Resource-Allocation Graph Visual original text Resource-Allocation Graph korea 자원-할당 그래프 description 이 슬라이드는 단순히 \"Resource-Allocation Graph\"라는 제목만을 포함하고 있으며, 실제 그래프의 시각적인 예시를 기대하게 만듭니다. 앞선 슬라이드에서 설명된 클레임 간선, 요청 간선, 할당 간선의 개념들이 실제 그래프 상에서 어떻게 표현되는지를 보여주는 그림이 뒤따라야 합니다. 일반적인 자원 할당 그래프는 다음과 같은 요소들을 포함합니다: 프로세스 노드 (Process Nodes): 원 또는 타원으로 표시되며, P1​,P2​,…와 같이 프로세스를 나타냅니다. 자원 유형 노드 (Resource Type Nodes): 사각형으로 표시되며, R1​,R2​,…와 같이 특정 유형의 자원(예: 프린터, 스캐너, CPU, 메모리 블록 등)을 나타냅니다. 자원 인스턴스 (Resource Instances): 자원 유형 노드 내부에 점 또는 작은 원으로 표시되며, 해당 자원 유형에 속하는 개별 자원 단위를 나타냅니다. 예를 들어, 3개의 프린터가 있다면 R1​ 노드 안에 3개의 점이 있을 수 있습니다. 간선(Edges)의 종류와 의미는 다음과 같습니다: 요청 간선 (Request Edge): 프로세스 노드에서 자원 유형 노드로 향하는 화살표(Pi​→Rj​). 프로세스 Pi​가 자원 Rj​의 인스턴스 하나를 요청하고 있음을 나타냅니다. 이 간선은 프로세스가 자원을 얻기 위해 기다리고 있음을 의미합니다. 할당 간선 (Assignment Edge): 자원 유형 노드 내의 특정 자원 인스턴스에서 프로세스 노드로 향하는 화살표(Rj​⋅→Pi​). 자원 Rj​의 한 인스턴스가 프로세스 Pi​에 할당되어 있음을 나타냅니다. 이는 프로세스 Pi​가 현재 해당 자원을 사용 중임을 의미합니다. 클레임 간선 (Claim Edge): 프로세스 노드에서 자원 유형 노드로 향하는 점선 화살표(Pi​⇢Rj​). 프로세스 Pi​가 미래에 자원 Rj​를 요청할 수 있음을 나타냅니다. 이는 자원 할당 그래프 알고리즘에서 교착 상태를 회피하기 위한 사전 정보로 사용됩니다. 이 간선은 프로세스가 실행되기 전에 자신이 필요로 할 수 있는 최대 자원을 미리 선언하는 개념과 관련이 있습니다. 자원 할당 그래프는 이러한 노드와 간선들의 관계를 통해 시스템의 현재 자원 할당 상태와 잠재적인 요청 상태를 보여줍니다. 그래프에서 사이클(cycle)이 발견될 경우, 이는 교착 상태(deadlock)의 가능성(potential deadlock) 또는 실제 교착 상태(actual deadlock)를 나타낼 수 있습니다. 사이클이 존재한다고 해서 항상 교착 상태인 것은 아니지만, 교착 상태가 발생하기 위한 필요 조건 중 하나입니다. 특히, 자원 유형 노드에 여러 인스턴스가 있는 경우에는 사이클이 있더라도 교착 상태가 아닐 수 있습니다. 하지만 모든 자원 유형에 인스턴스가 하나뿐인 경우, 사이클은 곧 교착 상태를 의미합니다. 다음 슬라이드들에서 이러한 개념들이 더 구체적인 예시와 함께 설명될 것입니다. Unsafe State In Resource-Allocation Graph original text Unsafe State In Resource-Allocation Graph korea 자원-할당 그래프에서의 불안전 상태 description 이 슬라이드 또한 \"Unsafe State In Resource-Allocation Graph\"라는 제목만을 포함하고 있어, 자원 할당 그래프에서 불안전 상태(unsafe state)가 어떻게 시각적으로 나타나는지 또는 어떤 조건을 만족할 때 불안전 상태로 간주되는지에 대한 구체적인 예시나 설명을 기대하게 합니다. 안전 상태(safe state)와 불안전 상태(unsafe state)의 개념은 주로 은행원 알고리즘(Banker's Algorithm)과 같은 교착 상태 회피 알고리즘에서 중요하게 다루어집니다. 시스템이 안전 상태에 있다는 것은 모든 프로세스에 대해, 프로세스가 요청한 자원을 모두 할당하여 프로세스가 종료될 수 있는 안전 순서열(safe sequence)이 존재한다는 것을 의미합니다. 반면, 불안전 상태는 그러한 안전 순서열을 찾을 수 없는 상태를 말합니다. 불안전 상태가 반드시 교착 상태를 의미하는 것은 아니지만, 교착 상태가 발생할 가능성이 있는 상태를 의미합니다. 즉, 불안전 상태는 교착 상태로 이어질 수 있는 잠재적인 위험을 내포하고 있습니다. 자원 할당 그래프의 관점에서 불안전 상태를 이해하려면, 단순히 사이클의 존재 여부뿐만 아니라 자원의 가용성(availability)과 프로세스의 최대 요구량(maximum claim)을 함께 고려해야 합니다. 불안전 상태의 특징 (자원 할당 그래프와 관련하여): 사이클의 존재 가능성: 자원 할당 그래프에서 사이클이 존재하면 교착 상태가 발생할 수 있습니다. 특히, 모든 자원 유형에 하나의 인스턴스만 있는 경우, 사이클은 교착 상태를 의미합니다. 그러나 여러 인스턴스가 있는 경우, 사이클이 있더라도 시스템이 안전할 수 있습니다 (즉, 사이클이 교착 상태를 의미하지 않을 수도 있습니다). 안전 순서열의 부재: 시스템이 불안전 상태라는 것은 현재 가용한 자원만으로는 더 이상 어떤 프로세스도 자신의 작업을 완료할 수 있도록 충분한 자원을 할당할 수 있는 순서열을 찾을 수 없음을 의미합니다. 즉, 현재 상태에서 어떤 프로세스가 자원을 요청하더라도, 그 요청을 들어주면 시스템이 교착 상태에 빠질 위험이 커진다는 것입니다. 교착 상태의 잠재적 위험: 불안전 상태는 교착 상태가 발생할 수 있는 \"경고 신호\"로 볼 수 있습니다. 시스템은 불안전 상태에 진입하는 것을 피하려고 노력하며, 이를 위해 자원 할당 요청이 들어올 때마다 시스템이 안전 상태를 유지할 수 있는지 여부를 확인합니다. 만약 요청을 승인하면 불안전 상태가 될 가능성이 있다면, 해당 요청을 보류시킵니다. 자원 할당 그래프에서 구체적인 불안전 상태의 예시를 보여준다면, 다음과 같은 상황을 묘사할 수 있습니다: 프로세스들이 자원을 요청하고 있으나, 가용 자원이 부족하여 어떤 프로세스도 당장 작업을 완료할 수 없는 상황. 그래프에 사이클이 존재하며, 이 사이클에 포함된 프로세스들이 모두 서로가 쥐고 있는 자원을 기다리는 상황. 결론적으로, 이 슬라이드는 자원 할당 그래프를 통해 시스템의 안전성을 분석하는 맥락에서 \"불안전 상태\"라는 핵심 개념을 소개하며, 이는 곧 교착 상태를 회피하기 위한 알고리즘의 필요성을 제기하는 중요한 전환점이 됩니다. 다음 슬라이드에서는 이러한 불안전 상태를 방지하기 위한 구체적인 알고리즘인 \"Resource-Allocation Graph Algorithm\"이 소개될 것으로 예상됩니다. Resource-Allocation Graph Algorithm original text Suppose that process Pi​ requests a resource Rj​ The request can be granted only if converting the request edge to an assignment edge does not result in the formation of a cycle in the resource allocation graph korea 프로세스 Pi​가 자원 Rj​를 요청한다고 가정합니다. 요청은 요청 간선(request edge)을 할당 간선(assignment edge)으로 변환했을 때, 자원 할당 그래프에서 사이클(cycle)이 형성되지 않는 경우에만 승인될 수 있습니다. description 이 슬라이드는 자원 할당 그래프 알고리즘(Resource-Allocation Graph Algorithm)의 핵심 원칙을 설명합니다. 이 알고리즘은 교착 상태 회피(deadlock avoidance) 전략 중 하나로, 자원 요청이 들어올 때마다 시스템이 안전한 상태를 유지할 수 있는지 여부를 동적으로 검사합니다. 특히, 이 알고리즘은 자원 유형당 인스턴스가 하나만 있는 경우에 효과적으로 적용될 수 있습니다. 알고리즘의 작동 방식은 다음과 같습니다: 자원 요청 상황: 먼저, 특정 프로세스 Pi​가 자원 Rj​를 요청했다고 가정합니다. 이는 자원 할당 그래프에서 프로세스 Pi​에서 자원 Rj​로 향하는 요청 간선(Pi​→Rj​)이 존재함을 의미합니다. 할당 시뮬레이션 및 사이클 검사: 시스템은 이 요청을 즉시 승인하는 대신, 만약 자원 Rj​가 Pi​에 할당된다면 어떤 일이 일어날지 \"가상으로\" 시뮬레이션해 봅니다. 이 시뮬레이션은 요청 간선(Pi​→Rj​)을 할당 간선(Rj​→Pi​)으로 변환하는 것으로 나타납니다. 즉, 프로세스 Pi​가 자원 Rj​를 얻게 되는 상황을 가정하는 것입니다. 승인 조건: 이 가상 할당 후에 자원 할당 그래프에 새로운 사이클이 형성되지 않는다면, 해당 자원 요청은 안전하게 승인될 수 있습니다. 사이클이 형성되지 않는다는 것은 시스템이 여전히 안전 상태에 있거나, 적어도 교착 상태로 이어질 직접적인 경로가 없음을 의미합니다. 요청 거부 또는 지연: 만약 가상 할당 후에 그래프에 사이클이 형성된다면, 해당 자원 요청은 승인되지 않거나 (즉시 거부되거나) 지연됩니다. 이는 요청을 승인할 경우 교착 상태가 발생할 가능성이 매우 높다는 것을 의미하기 때문입니다. 프로세스 Pi​는 자원 Rj​가 해제될 때까지 기다려야 하거나, 시스템 관리자가 개입하여 교착 상태를 해결해야 할 수도 있습니다. 이 알고리즘의 중요성: 교착 상태 회피: 이 알고리즘은 교착 상태가 발생하기 전에 잠재적인 위험을 감지하고 회피하는 데 중점을 둡니다. 이는 교착 상태가 일단 발생하면 이를 탐지하고 복구하는 것보다 훨씬 효율적인 방법입니다. 사이클 탐지: 그래프에서 사이클을 탐지하는 것은 깊이 우선 탐색(DFS)이나 너비 우선 탐색(BFS)과 같은 표준 그래프 알고리즘을 사용하여 효율적으로 수행할 수 있습니다. 제한 사항: 이 알고리즘은 주로 자원 유형당 단일 인스턴스만 존재하는 시스템에 적합합니다. 만약 자원 유형에 여러 인스턴스가 있다면, 단순히 사이클의 존재만으로는 교착 상태를 단정할 수 없으며, 은행원 알고리즘(Banker's Algorithm)과 같은 더 복잡한 접근 방식이 필요합니다. 이는 은행원 알고리즘이 시스템의 가용 자원, 최대 요구량, 할당된 자원 등을 종합적으로 고려하여 안전 순서열의 존재 여부를 판단하기 때문입니다. 결론적으로, 자원 할당 그래프 알고리즘은 간단하면서도 효과적인 교착 상태 회피 메커니즘을 제공하지만, 그 적용 범위는 자원 유형당 인스턴스 수에 따라 제한될 수 있습니다. Potential Deadlock original text Potential Deadlock I need quad A and B I need quad B and C I need quad C and B I need quad D and A korea 잠재적 교착 상태 나는 A와 B 사분면이 필요하다 나는 B와 C 사분면이 필요하다 나는 C와 B 사분면이 필요하다 나는 D와 A 사분면이 필요하다 description 이 슬라이드는 잠재적 교착 상태(Potential Deadlock)의 개념을 추상적인 예시를 통해 설명합니다. \"quad A and B\", \"quad B and C\" 등의 표현은 어떤 자원 그룹 또는 영역을 의미하는 것으로 보입니다. 이 예시에서 각 문장은 특정 주체가 두 가지 이상의 자원(이 경우, \"quad A\", \"quad B\", \"quad C\", \"quad D\"로 명명된 사분면)을 동시에 필요로 한다는 것을 나타냅니다. 이러한 상황은 다음과 같은 방식으로 잠재적 교착 상태를 유발할 수 있습니다: 자원 요청의 상호 의존성: 각 주체(여기서는 \"I\"로 표현된 어떤 주체들)는 두 개의 사분면을 필요로 합니다. 만약 이들이 요청하는 사분면 중 일부가 이미 다른 주체에 의해 점유되어 있다면, 자원을 얻기 위해 기다려야 합니다. \"나는 A와 B 사분면이 필요하다\" (주체 1) \"나는 B와 C 사분면이 필요하다\" (주체 2) \"나는 C와 B 사분면이 필요하다\" (주체 3 - 주체 2와 유사하지만 요청 순서나 주체가 다를 수 있음) \"나는 D와 A 사분면이 필요하다\" (주체 4) 순환 대기 조건 형성: 교착 상태가 발생하려면 네 가지 필요 조건(상호 배제, 점유 및 대기, 비선점, 순환 대기)이 충족되어야 합니다. 이 예시에서는 순환 대기(circular wait) 조건이 형성될 가능성이 높습니다. 만약 주체 1이 A를 가지고 B를 기다리고, 주체 2가 B를 가지고 C를 기다리고, 주체 3이 C를 가지고 B를 기다리고 (또는 어떤 주체가 C를 가지고 D를 기다리고), 주체 4가 D를 가지고 A를 기다리는 상황이 발생한다면, 주체 1 (A 보유, B 대기) -> 주체 2 (B 보유, C 대기) -> 주체 3 (C 보유, D 대기) -> 주체 4 (D 보유, A 대기) -> 주체 1 이러한 순환적인 대기 상황은 아무도 자원을 해제하지 않고 서로가 필요한 자원을 기다리는 교착 상태로 이어질 수 있습니다. 자원의 제한성: 여기서 \"quad\"는 제한된 수의 인스턴스를 가진 자원을 의미할 가능성이 높습니다. 예를 들어, 각 사분면이 단 하나의 주체만 사용할 수 있는 자원이라면 상호 배제 조건이 충족됩니다. 이 슬라이드는 실제 시스템의 복잡한 자원 할당 시나리오를 단순화하여 보여주며, 여러 프로세스(여기서는 \"I\"로 칭해진 주체들)가 서로 다른 자원을 동시에 요청할 때 어떻게 잠재적인 교착 상태가 발생할 수 있는지 직관적으로 이해하도록 돕습니다. 즉, 현재 당장 교착 상태가 발생한 것은 아니지만, 잘못된 자원 할당 순서나 불운한 타이밍으로 인해 쉽게 교착 상태에 빠질 수 있는 위험한 상태를 의미합니다. 다음 슬라이드에서는 이러한 잠재적 교착 상태가 실제로 어떻게 \"Actual Deadlock\"으로 발전하는지를 보여줄 수 있습니다. Resource Allocation Diagram (첫 번째) original text Resource Allocation Diagram korea 자원 할당 다이어그램 description 이 슬라이드는 \"Resource Allocation Diagram\"이라는 제목만을 포함하고 있어, 앞서 \"Potential Deadlock\" 슬라이드에서 제시된 추상적인 시나리오(\"I need quad A and B\" 등)를 구체적인 자원 할당 그래프(Resource Allocation Graph) 또는 이와 유사한 다이어그램으로 시각화한 내용을 담고 있을 것으로 예상됩니다. 이 다이어그램은 프로세스와 자원 간의 관계를 그래픽으로 표현하여 교착 상태의 발생 가능성을 분석하는 데 사용됩니다. 일반적인 자원 할당 다이어그램에는 다음과 같은 요소들이 포함될 수 있습니다: 프로세스 (Processes): 시스템에서 실행 중인 작업을 나타내며, 보통 원형 노드로 표현됩니다. 자원 유형 (Resource Types): 시스템에서 사용 가능한 자원의 종류를 나타내며, 보통 사각형 노드로 표현됩니다. 자원 인스턴스 (Resource Instances): 각 자원 유형 내에 있는 개별 자원 단위를 나타내며, 자원 유형 노드 내의 점 또는 작은 원으로 표시됩니다. 요청 간선 (Request Edges): 프로세스에서 자원 유형으로 향하는 화살표(예: Pi​→Rj​). 프로세스 Pi​가 자원 Rj​의 인스턴스를 요청하고 있음을 나타냅니다. 할당 간선 (Assignment Edges): 자원 인스턴스에서 프로세스로 향하는 화살표(예: Rj​⋅→Pi​). 자원 Rj​의 인스턴스가 프로세스 Pi​에 할당되어 있음을 나타냅니다. 앞선 \"Potential Deadlock\" 슬라이드에서 언급된 \"quad A\", \"quad B\", \"quad C\", \"quad D\"는 자원 유형이 될 수 있고, 각 \"I\"는 프로세스가 될 수 있습니다. 이 다이어그램에서는 각 프로세스가 어떤 \"quad\" 자원을 현재 보유하고 있고 어떤 \"quad\" 자원을 기다리고 있는지 명확하게 보여줄 것입니다. 예상되는 시각적 표현의 특징: 여러 프로세스 노드와 여러 자원 유형 노드가 존재할 것입니다. 프로세스에서 자원으로 향하는 요청 간선(예: P1​→Quad B)과 자원 인스턴스에서 프로세스로 향하는 할당 간선(예: Quad A→P1​)이 나타날 것입니다. 아마도 이전 슬라이드에서 언급된 \"I need quad A and B\"와 같은 문장이 실제 다이어그램에서는 P1​이 Quad A를 보유하고 P1​→Quad B 요청 간선을 가지는 형태로 표현될 수 있습니다. 잠재적 교착 상태를 보여주기 위해, 다이어그램 내에 사이클(cycle)이 형성되어 있을 가능성이 높습니다. 이 사이클은 프로세스들이 서로가 보유하고 있는 자원을 기다리는 순환 대기(circular wait) 상태를 시각적으로 나타낼 것입니다. 그러나 이 사이클이 반드시 실제 교착 상태를 의미하지는 않습니다. 잠재적이라는 것은 아직은 교착 상태가 아니지만, 조금만 더 자원 할당이 잘못되면 바로 교착 상태로 이어질 수 있다는 경고를 의미합니다. 이 슬라이드는 추상적인 개념을 구체적인 예시로 연결하여 교착 상태의 원인과 메커니즘을 더 쉽게 이해하도록 돕는 역할을 합니다. Resource Allocation Diagram (두 번째) original text Resource Allocation Diagram korea 자원 할당 다이어그램 description 이 슬라이드 또한 \"Resource Allocation Diagram\"이라는 제목만을 가지고 있지만, 앞선 동일한 제목의 슬라이드 다음에 위치한다는 점에서, 아마도 첫 번째 다이어그램에서 보여준 잠재적 교착 상태(Potential Deadlock)가 실제 교착 상태(Actual Deadlock)로 진행되는 과정 또는 다른 교착 상태 시나리오를 시각적으로 보여주는 내용을 담고 있을 것으로 예상됩니다. 첫 번째 \"Resource Allocation Diagram\"이 잠재적인 위험을 내포한 상태를 보여주었다면, 이 두 번째 다이어그램은 그 위험이 현실화된 모습을 묘사할 수 있습니다. 즉, 실제 교착 상태(Actual Deadlock)는 시스템의 모든 교착 상태 필요 조건(상호 배제, 점유 및 대기, 비선점, 순환 대기)이 충족되어, 더 이상 어떤 프로세스도 진행할 수 없는 상태를 의미합니다. 이 다이어그램에서 예상되는 시각적 특징: 명확한 사이클 형성: 실제 교착 상태를 나타내기 위해 자원 할당 그래프에 명확하고 고립된 사이클(cycle)이 형성되어 있을 것입니다. 이 사이클은 관련된 프로세스들과 자원들 간의 순환 대기 관계를 직접적으로 보여줄 것입니다. 각 프로세스가 자원을 점유하고 있으면서 동시에 다른 프로세스가 점유하고 있는 자원을 요청하는, 끊을 수 없는 의존성 고리가 시각적으로 표현될 것입니다. 모든 관련 자원의 점유: 사이클 내의 모든 자원이 어떤 프로세스에 의해 점유되어 있고, 해당 자원을 요청하는 프로세스들이 모두 대기 상태에 있음을 보여줄 것입니다. 즉, 자원 인스턴스에서 프로세스로 향하는 할당 간선과 프로세스에서 자원 유형으로 향하는 요청 간선이 서로를 막고 있는 형태가 됩니다. 진행 불가능: 다이어그램을 통해 어떤 프로세스도 더 이상 자원을 얻거나 작업을 완료할 수 없는 멈춘 상태임을 암시할 수 있습니다. 예를 들어, 가용한 자원이 전혀 없거나, 가용한 자원이 있더라도 사이클에 묶인 프로세스들에게는 의미 없는 상황을 나타낼 수 있습니다. \"Cars in Intersection, again\"과의 연관성: 바로 다음 슬라이드에서 \"Cars in Intersection, again\"이라는 제목이 나오는 것을 보면, 이 다이어그램은 교차로에서 차량들이 서로의 경로를 막아 움직이지 못하는 상황과 매우 유사하게 시각화될 수 있습니다. 각 차량은 프로세스, 교차로의 특정 구역이나 방향은 자원으로 비유될 수 있습니다. 결론적으로, 이 두 번째 \"Resource Allocation Diagram\"은 교착 상태의 비극적인 결과, 즉 시스템의 일부 또는 전체가 멈춰버리는 상황을 시각적으로 명확하게 보여줌으로써, 교착 상태 회피 및 탐지 알고리즘의 중요성을 다시 한번 강조하는 역할을 할 것입니다. Actual Deadlock original text Actual Deadlock HALT until B is free HALT until C is free HALT until D is free HALT until A is free korea 실제 교착 상태 B가 해제될 때까지 멈춤 C가 해제될 때까지 멈춤 D가 해제될 때까지 멈춤 A가 해제될 때까지 멈춤 description 이 슬라이드는 실제 교착 상태(Actual Deadlock)가 발생했을 때의 상황을 명확하고 직관적인 메시지로 보여줍니다. 각 문장은 어떤 주체(이전 슬라이드의 \"I\" 또는 \"Cars in Intersection\"의 차량)가 특정 자원(여기서는 \"B\", \"C\", \"D\", \"A\")이 해제될 때까지 무기한 대기하고 있음을 나타냅니다. 이 메시지들이 의미하는 바: 순환 대기(Circular Wait)의 완성: 이 네 줄의 메시지는 전형적인 순환 대기 상황을 완벽하게 묘사합니다. 어떤 주체는 B가 해제되기를 기다립니다. (예: P1​이 RB​를 기다림) 다른 주체는 C가 해제되기를 기다립니다. (예: P2​가 RC​를 기다림) 또 다른 주체는 D가 해제되기를 기다립니다. (예: P3​이 RD​를 기다림) 마지막 주체는 A가 해제되기를 기다립니다. (예: P4​가 RA​를 기다림) 여기서 핵심은, 만약 B를 쥐고 있는 주체가 A를 기다리고, C를 쥐고 있는 주체가 B를 기다리고, D를 쥐고 있는 주체가 C를 기다리고, A를 쥐고 있는 주체가 D를 기다리는 상황이라면, 이들은 서로가 필요한 자원을 쥐고 있기 때문에 아무도 자원을 해제할 수 없게 됩니다. 주체 1 (A를 가지고 B 대기) -> 주체 2 (B를 가지고 C 대기) -> 주체 3 (C를 가지고 D 대기) -> 주체 4 (D를 가지고 A 대기) -> 주체 1 이러한 고리는 끊어질 수 없으며, 모든 주체는 영원히 \"HALT\" 상태에 머무르게 됩니다. 교착 상태의 4가지 조건 충족: 이 상황은 교착 상태의 네 가지 필수 조건이 모두 충족되었음을 의미합니다: 상호 배제 (Mutual Exclusion): 각 자원(A, B, C, D)은 한 번에 한 주체만 사용할 수 있습니다. 점유 및 대기 (Hold and Wait): 각 주체는 이미 자원(예: P1​은 A를)을 점유하고 있으면서 다른 자원(예: P1​은 B를)을 기다리고 있습니다. 비선점 (No Preemption): 이미 할당된 자원은 해당 주체가 자발적으로 해제하기 전까지는 강제로 뺏을 수 없습니다. 순환 대기 (Circular Wait): 대기하고 있는 주체들의 집합이 순환 고리를 형성합니다. 시스템의 멈춤: 이 메시지들은 해당 자원들을 기다리는 프로세스들이 더 이상 진행할 수 없음을 명확히 보여줍니다. 이는 시스템의 효율성을 저하시키고, 경우에 따라서는 시스템 전체를 마비시킬 수도 있습니다. 이 슬라이드는 교착 상태의 가장 중요한 특징인 \"순환 대기\"를 간단하면서도 강력하게 표현하며, 이는 교착 상태가 시스템에 미치는 치명적인 영향을 강조합니다. 바로 다음 슬라이드에서 \"Cars in Intersection, again\"이라는 제목이 나오는 것을 보면, 이 \"HALT\" 메시지들이 실제 교차로에서 차량들이 꼼짝없이 멈춰 있는 상황에 직접적으로 비유될 것임을 예상할 수 있습니다. 이러한 직관적인 비유는 복잡한 교착 상태 개념을 쉽게 이해하도록 돕습니다. Cars in Intersection, again original text Cars in Intersection, again korea 다시, 교차로의 자동차들 description 이 슬라이드는 \"Cars in Intersection, again\"이라는 제목만을 포함하고 있습니다. 이 제목은 이전에 언급된 \"Potential Deadlock\"과 \"Actual Deadlock\" 슬라이드에서 추상적으로 설명된 개념을 현실 세계의 구체적인 비유로 재강조하거나 확장하려는 의도를 나타냅니다. \"다시\"라는 표현은 이 비유가 이전에 (혹은 다른 문맥에서) 한 번 이상 사용되었음을 암시하며, 교착 상태를 설명하는 데 매우 흔하고 효과적인 예시임을 시사합니다. 교차로의 자동차 비유가 교착 상태를 설명하는 방식: 자원 (Resources): 교차로의 특정 영역(예: 각 차량이 진입해야 할 교차로 내의 공간), 또는 교차로를 안전하게 통과하기 위한 '길' 자체가 자원으로 비유될 수 있습니다. 각각의 자원은 한 번에 하나의 차량(프로세스)만 점유할 수 있습니다 (상호 배제). 프로세스 (Processes): 교차로를 통과하려는 각 자동차가 프로세스에 해당합니다. 각 자동차는 목적지에 도달하기 위해 여러 자원(교차로의 여러 부분)을 순차적으로 또는 동시에 필요로 합니다. 점유 및 대기 (Hold and Wait): 각 자동차는 이미 교차로의 일부(자원)를 점유하고 있으면서(예: 교차로 진입 후 멈춰 섬) 동시에 다른 자동차가 점유하고 있는 교차로의 다른 부분(다른 자원)이 비워지기를 기다립니다. 비선점 (No Preemption): 일단 교차로에 진입한 자동차는 다른 자동차에 의해 강제로 밀려나거나 움직여질 수 없습니다. 오직 운전자(프로세스)가 자발적으로 차를 움직여야만 자원이 해제됩니다. 순환 대기 (Circular Wait): 가장 중요한 교착 상태의 조건입니다. 여러 자동차가 교차로 한가운데에서 서로의 길을 막고, 각 자동차가 다른 자동차가 비워주기를 기다리는 상황이 발생할 수 있습니다. 예를 들어: 차 A는 차 B가 움직이기를 기다리고, 차 B는 차 C가 움직이기를 기다리고, 차 C는 차 D가 움직이기를 기다리고, 차 D는 차 A가 움직이기를 기다리는 경우. 이러한 순환적인 대기 상태는 아무도 움직일 수 없게 만들고, 모든 차량이 교차로에서 꼼짝없이 멈춰 서게 됩니다. 이것이 바로 \"Actual Deadlock\" 슬라이드에서 \"HALT until X is free\"라고 했던 상황과 정확히 일치합니다. 이 비유는 교착 상태의 복잡한 기술적 개념을 일상생활의 친숙한 시나리오로 풀어내어, 비전공자도 쉽게 교착 상태가 왜 발생하며 어떤 결과를 초래하는지 이해할 수 있도록 돕습니다. 특히, \"다시\"라는 표현은 이 비유가 교착 상태의 개념을 설명하는 데 있어 매우 강력하고 효과적인 도구임을 강조합니다. 이 슬라이드 다음에 교차로에서 실제로 교착 상태에 빠진 차량들의 그림이나 애니메이션이 나올 가능성이 높습니다. Banker’s Algorithm original text Multiple instances Each process must a priori claim maximum use When a process requests a resource it may have to wait When a process gets all its resources it must return them in a finite amount of time korea 다중 인스턴스 (Multiple instances): 이 알고리즘은 자원 유형당 여러 인스턴스가 존재하는 환경에서 작동합니다. 사전 최대 클레임 (a priori claim maximum use): 각 프로세스는 실행 전에 자신이 필요로 할 수 있는 최대 자원 요구량을 미리 선언해야 합니다. 자원 요청 시 대기 가능성: 프로세스가 자원을 요청할 때, 자원이 당장 사용 가능하더라도 대기해야 할 수 있습니다. 유한 시간 내 자원 반환: 프로세스가 모든 자원을 할당받아 작업을 완료하면, 유한한 시간 내에 할당받은 모든 자원을 반환해야 합니다. description 은행원 알고리즘(Banker's Algorithm)은 교착 상태 회피(deadlock avoidance)를 위한 대표적인 알고리즘입니다. 이 알고리즘은 시스템이 항상 안전 상태(safe state)를 유지하도록 보장하면서 자원을 할당합니다. 이 슬라이드는 은행원 알고리즘의 네 가지 핵심 특징과 전제 조건을 설명합니다. 다중 인스턴스 (Multiple instances): 이 알고리즘의 가장 큰 특징 중 하나는 각 자원 유형에 여러 개의 인스턴스(instance)가 존재할 수 있는 환경에서 작동한다는 점입니다. 예를 들어, 시스템에 여러 대의 프린터(R1)나 여러 블록의 메모리(R2)가 있을 수 있습니다. 이는 앞서 설명된 자원 할당 그래프 알고리즘(Resource-Allocation Graph Algorithm)이 주로 단일 인스턴스 자원에 적합했던 것과 대조됩니다. 다중 인스턴스 환경에서는 단순히 사이클의 존재만으로 교착 상태를 단정할 수 없기 때문에, 더 정교한 안전성 검사 메커니즘이 필요하며, 은행원 알고리즘이 이를 제공합니다. 사전 최대 클레임 (Each process must a priori claim maximum use): 모든 프로세스는 실행을 시작하기 전에 자신이 작업 완료를 위해 필요로 할 수 있는 각 자원 유형의 최대 개수를 미리 시스템에 선언해야 합니다. 이는 \"a priori claim\" 즉, '사전 클레임'의 원칙입니다. 예를 들어, 프로세스 P1​은 \"저는 최대 3개의 프린터와 2GB의 메모리가 필요할 수 있습니다\"라고 미리 알려야 합니다. 이 정보는 은행원 알고리즘이 자원 할당 요청을 평가하고 시스템의 안전성을 판단하는 데 필수적인 기반이 됩니다. 자원 요청 시 대기 가능성 (When a process requests a resource it may have to wait): 프로세스가 자원을 요청했을 때, 해당 자원이 현재 가용하더라도 시스템이 해당 요청을 즉시 승인하지 않고 프로세스를 대기(wait)시킬 수 있습니다. 이는 시스템의 현재 상태에서 자원 할당을 시뮬레이션했을 때, 만약 그 요청을 승인하면 시스템이 불안전 상태(unsafe state)로 진입할 수 있다고 판단될 경우 발생합니다. 즉, 교착 상태를 회피하기 위해 일시적인 대기가 필요할 수 있다는 의미입니다. 이 점이 은행원 알고리즘이 \"교착 상태 회피\" 알고리즘으로 분류되는 이유입니다. 유한 시간 내 자원 반환 (When a process gets all its resources it must return them in a finite amount of time): 프로세스가 필요한 모든 자원을 성공적으로 할당받아 작업을 완료했다면, 그 프로세스는 유한한 시간 내에 자신이 점유하고 있던 모든 자원을 시스템에 반환해야 합니다. 이 조건은 프로세스가 자원을 무기한으로 점유하지 않도록 보장하여, 다른 프로세스들이 해당 자원을 사용할 수 있게 함으로써 시스템의 자원 활용도를 높이고 교착 상태 발생 가능성을 줄이는 데 기여합니다. 이는 교착 상태의 비선점(No Preemption) 조건과는 관련이 없고, 프로세스의 정상적인 종료 및 자원 해제에 대한 요구사항입니다. 은행원 알고리즘은 이러한 전제 조건들을 바탕으로 자원 할당 요청이 있을 때마다 안전성 알고리즘(Safety Algorithm)을 실행하여 시스템이 안전한 상태를 유지할 수 있는지 검사합니다. 만약 안전한 상태를 유지할 수 있다면 요청을 승인하고, 그렇지 않다면 요청을 지연시키는 방식으로 교착 상태를 회피합니다. Data Structures for the Banker’s Algorithm original text Available: Vector of length m. If available [j]=k, there are k instances of resource type Rj​ available Max: n×m matrix. If Max [i,j]=k, then process Pi​ may request at most k instances of resource type Rj​ Allocation: n×m matrix. If Allocation$[i,j] = k$ then Pi​ is currently allocated k instances of Rj​ Need: n×m matrix. If Need$[i,j] = k$, then Pi​ may need k more instances of Rj​ to complete its task Need [i,j]=Max[i,j]–Allocation[i,j] korea Available (가용 자원 벡터): 길이 m의 벡터. Available[j] = k 는 자원 유형 Rj​의 인스턴스가 k개 현재 사용 가능하다는 것을 의미합니다. Max (최대 요구량 행렬): n×m 크기의 행렬. Max[i,j] = k 는 프로세스 Pi​가 자원 유형 Rj​의 인스턴스를 최대 k개까지 요청할 수 있음을 의미합니다. Allocation (할당량 행렬): n×m 크기의 행렬. Allocation[i,j] = k 는 프로세스 Pi​가 현재 자원 유형 Rj​의 인스턴스를 k개 할당받았음을 의미합니다. Need (남은 필요량 행렬): n×m 크기의 행렬. Need[i,j] = k 는 프로세스 Pi​가 작업을 완료하기 위해 자원 유형 Rj​의 인스턴스가 k개 더 필요함을 의미합니다. Need[i,j] = Max[i,j] – Allocation[i,j] 로 계산됩니다. description 이 슬라이드는 은행원 알고리즘(Banker's Algorithm)이 시스템의 자원 할당 상태를 관리하고 안전성을 판단하기 위해 사용하는 핵심 데이터 구조들을 설명합니다. 여기서 n은 시스템 내의 총 프로세스 수(예: P0​,P1​,…,Pn−1​)를, m은 총 자원 유형 수(예: R0​,R1​,…,Rm−1​)를 나타냅니다. 각 데이터 구조의 상세 내용은 다음과 같습니다: Available (가용 자원 벡터): 정의: 시스템에 현재 사용 가능한 각 자원 유형의 인스턴스 수를 나타내는 벡터입니다. 길이: 자원 유형의 수와 동일한 m의 길이를 가집니다. 의미: Available[j] = k 는 자원 유형 Rj​의 인스턴스 k개가 현재 어떤 프로세스에도 할당되지 않고 자유롭게 사용 가능하다는 것을 의미합니다. 예를 들어, Available[0] = 3 이라면, 시스템에 프린터 3대가 현재 가용하다는 뜻입니다. 이 벡터는 자원 할당 및 해제에 따라 실시간으로 업데이트됩니다. Max (최대 요구량 행렬): 정의: 각 프로세스가 각 자원 유형에 대해 최대로 요청할 수 있는 인스턴스 수를 정의하는 행렬입니다. 크기: n×m 크기를 가집니다. (n은 프로세스 수, m은 자원 유형 수) 의미: Max[i,j] = k 는 프로세스 Pi​가 자신의 작업을 완료하기 위해 자원 유형 Rj​의 인스턴스를 최대 k개까지 필요로 할 수 있다고 사전에 선언했음을 의미합니다. 이 값은 프로세스가 시작될 때 결정되며, 이후에는 변경되지 않습니다. 이는 은행원 알고리즘의 핵심 전제 조건인 \"사전 최대 클레임\"을 반영합니다. Allocation (할당량 행렬): 정의: 각 프로세스에 현재 할당되어 있는 각 자원 유형의 인스턴스 수를 나타내는 행렬입니다. 크기: n×m 크기를 가집니다. 의미: Allocation[i,j] = k 는 프로세스 Pi​가 현재 자원 유형 Rj​의 인스턴스 k개를 사용하고 있음을 의미합니다. 이 행렬은 자원이 할당되거나 해제될 때마다 업데이트됩니다. Need (남은 필요량 행렬): 정의: 각 프로세스가 자신의 작업을 완료하기 위해 각 자원 유형에 대해 추가로 필요한 인스턴스 수를 나타내는 행렬입니다. 크기: n×m 크기를 가집니다. 계산: Need[i,j] = Max[i,j] – Allocation[i,j] 로 계산됩니다. 즉, 프로세스 Pi​가 Rj​ 자원에 대해 최대로 필요하다고 클레임했던 양( Max[i,j] )에서 현재 할당받은 양( Allocation[i,j] )을 뺀 값이 앞으로 더 필요할 수 있는 자원의 양( Need[i,j] )이 됩니다. 의미: Need[i,j] = k 는 프로세스 Pi​가 작업을 마치기 위해 자원 Rj​를 k개 더 필요로 한다는 의미입니다. 이 행렬은 안전성 알고리즘에서 프로세스가 자원 요청을 완료할 수 있는지 여부를 판단하는 데 결정적으로 사용됩니다. 이 네 가지 데이터 구조는 은행원 알고리즘이 시스템의 자원 상태를 완벽하게 파악하고, 들어오는 자원 요청에 대해 안전성을 검증하는 데 필요한 모든 정보를 제공합니다. 다음 슬라이드에서 설명될 안전성 알고리즘(Safety Algorithm)은 이러한 데이터 구조를 활용하여 시스템이 안전 상태에 있는지 여부를 판단하게 됩니다. Safety Algorithm original text Let Work and Finish be vectors of length m and n, respectively. Initialize: Work = Available Finish [i] = false for i=0,1,…,n−1 Find an i such that both: (a) Finish [i] = false (b) Need$_i \\le$ Work If no such i exists, go to step 4 Work = Work + Allocation$_i$ Finish$[i]$ = true go to step 2 If Finish [i] == true for all i, then the system is in a safe state korea 길이 m의 Work 벡터와 길이 n의 Finish 벡터를 선언합니다. Work 를 현재 가용한 자원 상태인 Available 로 초기화합니다. 모든 프로세스 i (0,1,…,n−1)에 대해 Finish[i] 를 false 로 초기화합니다. 다음 두 가지 조건을 모두 만족하는 프로세스 i를 찾습니다: (a) Finish[i] 가 false (즉, 아직 완료되지 않은 프로세스) (b) Need$_i \\le$ Work (즉, 프로세스 Pi​가 추가로 필요로 하는 자원량이 현재 가용한 Work 벡터의 자원량보다 작거나 같음) 이러한 i가 존재하지 않으면 4단계로 이동합니다. 프로세스 Pi​가 완료되었다고 가정하고, Pi​가 할당받았던 자원( Allocation$_i$ )을 Work 에 반환합니다. Work = Work + Allocation$_i$ Finish[i] = true 2단계로 돌아갑니다. 모든 프로세스 i에 대해 Finish[i] 가 true 이면 (즉, 모든 프로세스를 완료시킬 수 있는 안전 순서열을 찾았다면), 시스템은 안전 상태(safe state)에 있습니다. description 안전성 알고리즘(Safety Algorithm)은 은행원 알고리즘의 핵심 부분으로, 현재 시스템의 자원 할당 상태가 안전 상태(safe state)인지 여부를 판단하는 데 사용됩니다. 안전 상태는 모든 프로세스를 교착 상태 없이 완료시킬 수 있는 안전 순서열(safe sequence)이 존재하는 상태를 의미합니다. 이 알고리즘은 가상의 자원 할당 및 반환 과정을 시뮬레이션하여 안전 순서열의 존재 여부를 확인합니다. 알고리즘의 단계별 설명은 다음과 같습니다: 초기화: Work 벡터는 현재 시스템에서 사용 가능한 자원의 양을 추적합니다. 처음에는 Available 벡터(현재 시스템의 실제 가용 자원)의 값으로 초기화됩니다. 이는 \"지금 당장 시스템에 있는 자원이 이만큼 있다\"는 의미입니다. Finish 벡터는 각 프로세스가 완료될 수 있는지 여부를 추적하는 불리언(boolean) 배열입니다. 모든 프로세스는 처음에 false 로 설정됩니다. 이는 \"아직 어떤 프로세스도 완료시킬 수 있다고 확신하지 못한다\"는 의미입니다. n 은 프로세스 수, m 은 자원 유형 수입니다. 안전 순서열 탐색: 이 단계는 아직 완료되지 않은 프로세스($ Finish[i] = false $) 중에서 현재 Work 벡터(가용 자원)만으로도 자신의 남은 필요량( Need$_i$ )을 모두 충족시킬 수 있는 프로세스 Pi​를 찾습니다. Need$_i \\le$ Work 라는 조건은 벡터 비교를 의미합니다. 즉, Pi​가 필요로 하는 각 자원 유형의 양이 현재 Work 에 있는 해당 자원 유형의 양보다 작거나 같아야 합니다. 이러한 프로세스 Pi​를 찾는다는 것은, Pi​에게 자원을 할당하여 Pi​가 작업을 완료할 수 있도록 하는 것이 가능하다고 가정하는 것입니다. 만약 이러한 프로세스를 더 이상 찾을 수 없다면, 현재 Work 로 완료시킬 수 있는 프로세스가 없다는 뜻이므로 4단계로 넘어갑니다. 가상 자원 반환: 2단계에서 조건을 만족하는 프로세스 Pi​를 찾았다면, 이 프로세스 Pi​가 자신의 작업을 완료했다고 가정하고, Pi​가 현재 점유하고 있던 모든 자원( Allocation$_i$ )을 시스템에 반환했다고 시뮬레이션합니다. Work = Work + Allocation$_i$ 는 Pi​가 반환한 자원을 Work 벡터에 더하여, 이제 시스템에 더 많은 자원이 가용하게 되었음을 나타냅니다. Finish[i] = true 로 설정하여 Pi​가 완료되었음을 표시합니다. 이제 Work 벡터가 업데이트되었으므로, 2단계로 돌아가서 업데이트된 Work 를 가지고 다른 프로세스들을 완료시킬 수 있는지 다시 시도합니다. 안전 상태 판단: 모든 프로세스 i에 대해 Finish[i] 가 true 로 설정되었다면, 이는 시스템에 있는 모든 프로세스가 (어떤 특정 순서열에 따라) 자신의 작업을 완료하고 자원을 해제할 수 있음을 의미합니다. 즉, 안전 순서열이 존재하며, 시스템은 안전 상태에 있습니다. 만약 모든 프로세스를 완료시키지 못하고 2단계에서 더 이상 조건을 만족하는 프로세스를 찾을 수 없어서 4단계로 넘어왔는데, 여전히 Finish[i] 가 false 인 프로세스가 남아있다면, 시스템은 불안전 상태(unsafe state)에 있는 것입니다. 이 알고리즘은 시스템의 자원 할당 결정 시 매번 수행되어, 잠재적인 교착 상태를 미리 감지하고 회피하는 데 중요한 역할을 합니다. Resource-Request Algorithm for Process Pi​ original text Request = request vector for process Pi​. If Request$_i [j] = k$ then process Pi​ wants k instances of resource type Rj​ If Request$i \\le$ Need$i$ go to step 2. Otherwise, raise error condition, since process has exceeded its maximum claim If Request$_i \\le$ Available, go to step 3. Otherwise Pi​ must wait, since resources are not available Pretend to allocate requested resources to Pi​ by modifying the state as follows: Available = Available – Request$i$; Allocation$i$ = Allocation$i$ + Request$i$; Need$i$ = Need$i$ – Request$_i$; If safe ⟹ the resources are allocated to Pi​ If unsafe ⟹Pi​ must wait, and the old resource-allocation state is restored korea Request 는 프로세스 Pi​의 요청 벡터입니다. 만약 Request$_i$[j] = k 라면, 프로세스 Pi​가 자원 유형 Rj​의 인스턴스를 k개 요청한다는 것을 의미합니다. 만약 Request$_i \\le$ Need$_i$ 이면 2단계로 이동합니다. 그렇지 않으면, 프로세스가 자신의 최대 클레임을 초과했으므로 오류 조건을 발생시킵니다. 만약 Request$_i \\le$ Available 이면 3단계로 이동합니다. 그렇지 않으면, 자원이 가용하지 않으므로 Pi​는 대기해야 합니다. 요청된 자원을 Pi​에 할당하는 것을 가정하고 상태를 다음과 같이 수정합니다: Available = Available – Request$_i$; Allocation$_i$ = Allocation$_i$ + Request$_i$; Need$_i$ = Need$_i$ – Request$_i$; 만약 안전성 알고리즘( Safety Algorithm )을 실행한 결과 시스템이 safe 상태이면: 자원이 Pi​에 실제로 할당됩니다. 만약 안전성 알고리즘을 실행한 결과 시스템이 unsafe 상태이면: Pi​는 대기해야 하며, 이전 자원 할당 상태가 복원됩니다. description 자원 요청 알고리즘(Resource-Request Algorithm)은 프로세스 Pi​가 자원 Rj​의 인스턴스를 요청했을 때, 은행원 알고리즘이 이 요청을 어떻게 처리할지 결정하는 절차를 정의합니다. 이 알고리즘의 목표는 교착 상태를 회피하면서 자원 요청을 최대한 효율적으로 처리하는 것입니다. 알고리즘의 단계별 설명은 다음과 같습니다: 최대 클레임 초과 검사: 프로세스 Pi​가 요청한 자원 양( Request$_i$ )이 이전에 선언했던 최대 필요량( Need$_i$ )을 초과하는지 확인합니다. Request$_i \\le$ Need$_i$ 는 Pi​가 현재 요청한 자원의 각 유형별 개수가 Pi​가 자신의 작업을 완료하기 위해 추가로 필요하다고 명시한 양을 넘어서는 안 된다는 것을 의미합니다. 만약 Request$_i >$ Need$_i$ 라면, 이는 프로세스가 원래 약속했던 최대 요구량을 넘어선 부당한 요청이므로, 시스템은 이를 오류로 처리하고 요청을 거부합니다. 가용 자원 부족 검사: 현재 시스템에 가용한 자원( Available )이 프로세스 Pi​의 요청량( Request$_i$ )을 충족시킬 수 있는지 확인합니다. Request$_i \\le$ Available 는 현재 시스템에 요청된 모든 자원 유형에 대해 충분한 인스턴스가 사용 가능한지를 확인합니다. 만약 Request$_i >$ Available 라면, 현재 가용한 자원이 부족하므로 Pi​는 해당 자원이 해제될 때까지 대기해야 합니다. 이 경우, 요청은 즉시 거부되는 것이 아니라 보류 상태가 됩니다. 가상 할당 및 안전성 검사: 앞선 두 단계를 통과하면, 시스템은 요청된 자원을 Pi​에 실제로 할당하기 전에, 만약 할당했을 경우 시스템이 안전 상태를 유지할 수 있는지 시뮬레이션합니다. 상태 임시 수정: 시스템은 다음과 같이 데이터 구조를 임시로 업데이트하여 자원 할당이 이루어진 것처럼 가정합니다: Available = Available – Request$_i$; : 요청된 자원만큼 가용 자원이 줄어듭니다. Allocation$_i$ = Allocation$_i$ + Request$_i$; : Pi​에게 할당된 자원량이 늘어납니다. Need$_i$ = Need$_i$ – Request$_i$; : Pi​가 앞으로 더 필요로 할 자원량이 줄어듭니다. 안전성 알고리즘 실행: 이 임시로 수정된 상태에서 안전성 알고리즘(Safety Algorithm)을 실행하여 시스템이 여전히 안전 상태인지 판단합니다. 결정: If safe : 안전성 알고리즘의 결과 시스템이 안전 상태로 유지된다면, 요청된 자원이 Pi​에 실제로 할당됩니다. 임시 수정된 상태가 실제 시스템 상태가 됩니다. If unsafe : 안전성 알고리즘의 결과 시스템이 불안전 상태로 진입하게 된다면, 해당 요청은 거부되고 Pi​는 대기해야 합니다. 시스템의 자원 할당 상태는 이전 상태로 복원됩니다. 즉, 시뮬레이션했던 모든 변경 사항이 되돌려집니다. 이 알고리즘은 교착 상태를 능동적으로 회피하는 핵심 메커니즘입니다. 자원 요청이 들어올 때마다 엄격한 검사를 통해 시스템의 안전성을 보장하고, 잠재적인 위험을 감지하여 자원 할당 결정을 내립니다. 이는 시스템의 복잡성과 오버헤드를 증가시키지만, 교착 상태로 인한 심각한 문제를 방지하는 데 필수적입니다. Example of Banker’s Algorithm original text 5 processes P0​ through P4​; 3 resource types: A (10 instances), B (5 instances), and C (7 instances) Snapshot at time T0: Allocation Max Available A B C A B C A B C P0 0 1 0 7 5 3 3 3 2 P1 2 0 0 3 2 2 P2 3 0 2 9 0 2 P3 2 1 1 2 2 2 P4 0 0 2 4 3 3 korea 5개의 프로세스: P0​부터 P4​까지. 3가지 자원 유형: A: 10개 인스턴스 B: 5개 인스턴스 C: 7개 인스턴스 시간 T0​에서의 스냅샷: Allocation Max Available A B C A B C A B C P0 0 1 0 7 5 3 3 3 2 P1 2 0 0 3 2 2 P2 3 0 2 9 0 2 P3 2 1 1 2 2 2 P4 0 0 2 4 3 3 description 이 슬라이드는 은행원 알고리즘(Banker's Algorithm)의 실제 적용을 보여주기 위한 구체적인 예시를 제시합니다. 시스템의 초기 상태, 즉 시간 T0​에서의 자원 할당 스냅샷을 보여주며, 이를 통해 다음 단계에서 안전성 검사를 수행할 수 있는 기반 데이터를 제공합니다. 예시의 구성 요소: 프로세스 수 (n): 시스템에는 총 5개의 프로세스, 즉 P0​,P1​,P2​,P3​,P4​가 존재합니다. 자원 유형 수 (m) 및 총 인스턴스 수: 시스템에는 3가지 유형의 자원, 즉 A, B, C가 존재하며, 각 유형별 총 인스턴스 수는 다음과 같습니다: 자원 A: 총 10개 인스턴스 자원 B: 총 5개 인스턴스 자원 C: 총 7개 인스턴스 이 총 인스턴스 수는 이후 Available 벡터와 Allocation 행렬의 합산을 검증하는 데 사용될 수 있습니다. 시간 T0​에서의 스냅샷: 시스템의 현재 자원 할당 상태를 보여주는 세 가지 주요 데이터 구조의 초기 값입니다. Allocation (할당량 행렬): 각 프로세스가 현재 각 자원 유형으로부터 얼마나 많은 인스턴스를 할당받고 있는지를 나타냅니다. P0: A 0개, B 1개, C 0개 할당 P1: A 2개, B 0개, C 0개 할당 P2: A 3개, B 0개, C 2개 할당 P3: A 2개, B 1개, C 1개 할당 P4: A 0개, B 0개, C 2개 할당 총 할당된 자원: (A: 0+2+3+2+0=7), (B: 1+0+0+1+0=2), (C: 0+0+2+1+2=5) Max (최대 요구량 행렬): 각 프로세스가 작업을 완료하기 위해 각 자원 유형으로부터 최대로 필요하다고 사전에 클레임한 인스턴스 수를 나타냅니다. P0: A 7개, B 5개, C 3개까지 필요할 수 있음 P1: A 3개, B 2개, C 2개까지 필요할 수 있음 P2: A 9개, B 0개, C 2개까지 필요할 수 있음 P3: A 2개, B 2개, C 2개까지 필요할 수 있음 P4: A 4개, B 3개, C 3개까지 필요할 수 있음 Available (가용 자원 벡터): 현재 시스템에 남아 있는 각 자원 유형의 가용 인스턴스 수입니다. A: 3개, B: 3개, C: 2개 이 값은 (Total Instances - Sum of allocated instances for each type) 으로 검증될 수 있습니다. A: 10−7=3 (일치) B: 5−2=3 (일치) C: 7−5=2 (일치) 따라서 주어진 Available 값은 현재 할당 상태와 총 자원 인스턴스 수에 부합합니다. 이 초기 스냅샷은 다음 슬라이드에서 Need (남은 필요량 행렬)을 계산하고, 이어서 안전성 알고리즘(Safety Algorithm)을 실행하여 시스템의 현재 상태가 안전한지 아닌지를 판단하는 데 사용될 것입니다. 이 예시는 은행원 알고리즘의 작동 방식을 단계별로 이해하는 데 매우 중요한 출발점입니다. Example (Cont.) original text The content of the matrix Need is defined to be Max – Allocation Need A B C P0 7 4 3 P1 1 2 2 P2 6 0 0 P3 0 1 1 P4 4 3 1 The system is in a safe state since the sequence satisfies safety criteria korea Need (남은 필요량) 행렬의 내용은 Max – Allocation 으로 정의됩니다. Need A B C P0 7 4 3 P1 1 2 2 P2 6 0 0 P3 0 1 1 P4 4 3 1 시스템은 < P1, P3, P4, P2, P0 > 순서열이 안전성 조건을 만족하므로 안전 상태(safe state)에 있습니다. description 이 슬라이드는 은행원 알고리즘 예시의 연속으로, 이전 슬라이드에서 주어진 Max 와 Allocation 행렬을 바탕으로 Need 행렬을 계산하고, 계산된 Need 행렬과 Available 벡터를 사용하여 안전성 알고리즘(Safety Algorithm)을 실행한 결과를 보여줍니다. Need (남은 필요량) 행렬 계산: Need[i,j] = Max[i,j] – Allocation[i,j] 공식에 따라 각 프로세스의 Need 값을 계산합니다. P0: Max[0]=(7,5,3) , Allocation[0]=(0,1,0) ⟹ Need[0]=(7-0, 5-1, 3-0) = (7,4,3) P1: Max[1]=(3,2,2) , Allocation[1]=(2,0,0) ⟹ Need[1]=(3-2, 2-0, 2-0) = (1,2,2) P2: Max[2]=(9,0,2) , Allocation[2]=(3,0,2) ⟹ Need[2]=(9-3, 0-0, 2-2) = (6,0,0) P3: Max[3]=(2,2,2) , Allocation[3]=(2,1,1) ⟹ Need[3]=(2-2, 2-1, 2-1) = (0,1,1) P4: Max[4]=(4,3,3) , Allocation[4]=(0,0,2) ⟹ Need[4]=(4-0, 3-0, 3-2) = (4,3,1) 계산된 Need 행렬은 슬라이드에 제시된 내용과 정확히 일치합니다. 안전성 알고리즘 실행 및 안전 순서열 찾기: 이제 Available = (3,3,2)와 계산된 Need 행렬, 그리고 Allocation 행렬을 사용하여 안전성 알고리즘을 실행합니다. 초기 상태: Work = (3,3,2) , Finish = [F,F,F,F,F] 1단계: P1 을 찾습니다. Need[1] = (1,2,2) 입니다. Need[1] <= Work ? (1,2,2) <= (3,3,2) ⟹ True (모든 요소가 작거나 같음) P1 을 실행시킬 수 있습니다. Work = Work + Allocation[1] = (3,3,2) + (2,0,0) = (5,3,2) Finish[1] = True 안전 순서열: < P1 > 2단계: 다음 프로세스를 찾습니다. P3 을 찾습니다. Need[3] = (0,1,1) 입니다. Need[3] <= Work ? (0,1,1) <= (5,3,2) ⟹ True P3 을 실행시킬 수 있습니다. Work = Work + Allocation[3] = (5,3,2) + (2,1,1) = (7,4,3) Finish[3] = True 안전 순서열: < P1, P3 > 3단계: 다음 프로세스를 찾습니다. P4 를 찾습니다. Need[4] = (4,3,1) 입니다. Need[4] <= Work ? (4,3,1) <= (7,4,3) ⟹ True P4 를 실행시킬 수 있습니다. Work = Work + Allocation[4] = (7,4,3) + (0,0,2) = (7,4,5) Finish[4] = True 안전 순서열: < P1, P3, P4 > 4단계: 다음 프로세스를 찾습니다. P2 를 찾습니다. Need[2] = (6,0,0) 입니다. Need[2] <= Work ? (6,0,0) <= (7,4,5) ⟹ True P2 를 실행시킬 수 있습니다. Work = Work + Allocation[2] = (7,4,5) + (3,0,2) = (10,4,7) Finish[2] = True 안전 순서열: < P1, P3, P4, P2 > 5단계: 마지막 프로세스를 찾습니다. P0 를 찾습니다. Need[0] = (7,4,3) 입니다. Need[0] <= Work ? (7,4,3) <= (10,4,7) ⟹ True P0 를 실행시킬 수 있습니다. Work = Work + Allocation[0] = (10,4,7) + (0,1,0) = (10,5,7) Finish[0] = True 안전 순서열: < P1, P3, P4, P2, P0 > 최종 결과: 모든 Finish 값이 True 가 되었으므로, 시스템은 안전 상태에 있으며, 찾은 < P1, P3, P4, P2, P0 > 는 유효한 안전 순서열입니다. 이 슬라이드는 은행원 알고리즘이 시스템의 현재 상태가 안전한지 여부를 어떻게 결정하는지 명확하게 보여주는 핵심 예시입니다. 안전 순서열을 찾음으로써 시스템이 교착 상태에 빠지지 않고 모든 프로세스를 완료시킬 수 있음을 증명합니다. Example: P1 Request (1,0,2) original text Check that Request ≤ Available (that is, (1,0,2)≤(3,3,2) ⟹ true Allocation Need Available A B C A B C A B C P0 0 1 0 7 4 3 2 3 0 P1 3 0 2 0 2 0 P2 3 0 2 6 0 0 P3 2 1 1 0 1 1 P4 0 0 2 4 3 1 Executing safety algorithm shows that sequence satisfies safety requirement Can request for (3,3,0) by P4 be granted? Can request for (0,2,0) by P0 be granted? korea 요청 검사: 요청량( Request = (1,0,2) )이 현재 가용 자원( Available = (3,3,2) )보다 작거나 같은지 확인합니다. ( (1,0,2) \\le (3,3,2) ⟹ 참) 임시 할당 후 상태: Allocation Need Available A B C A B C A B C P0 0 1 0 7 4 3 2 3 0 P1 3 0 2 0 2 0 P2 3 0 2 6 0 0 P3 2 1 1 0 1 1 P4 0 0 2 4 3 1 안전성 알고리즘 실행: 안전성 알고리즘을 실행한 결과, 순서열 < P1, P3, P4, P0, P2 > 가 안전성 조건을 만족합니다. 추가 질문: P4​의 요청 (3,3,0)은 승인될 수 있는가? P0​의 요청 (0,2,0)은 승인될 수 있는가? description 이 슬라이드는 은행원 알고리즘의 자원 요청 알고리즘(Resource-Request Algorithm)이 실제 어떻게 작동하는지 보여주는 예시입니다. 특히, 프로세스 P1​이 자원 (1,0,2)를 요청했을 때의 과정을 상세히 설명하고 있습니다. P1​의 요청 분석: 프로세스 P1​이 자원 유형 A 1개, B 0개, C 2개, 즉 Request[1] = (1,0,2) 를 요청했습니다. 1단계: Request vs Need 검사: 이전 슬라이드에서 Need[1] 은 (1,2,2) 였습니다. Request[1] = (1,0,2) 이고 Need[1] = (1,2,2) 이므로, (1,0,2) \\le (1,2,2) 입니다. 이 조건은 참입니다. (요청량이 최대 필요량을 초과하지 않음) 2단계: Request vs Available 검사: 초기 Available 은 (3,3,2) 였습니다. Request[1] = (1,0,2) 이고 Available = (3,3,2) 이므로, (1,0,2) \\le (3,3,2) 입니다. 이 조건은 참입니다. (요청을 처리할 가용 자원이 충분함) 가상 할당 및 상태 업데이트: 두 가지 검사를 통과했으므로, 이제 시스템은 자원을 P1​에 할당했다고 가정하고 상태를 업데이트합니다. 기존 상태: Available = (3,3,2) Allocation[1] = (2,0,0) Need[1] = (1,2,2) 임시 수정: Available = Available – Request[1] = (3,3,2) – (1,0,2) = (2,3,0) Allocation[1] = Allocation[1] + Request[1] = (2,0,0) + (1,0,2) = (3,0,2) Need[1] = Need[1] – Request[1] = (1,2,2) – (1,0,2) = (0,2,0) 슬라이드에 제시된 임시 할당 후의 Available , Allocation[1] , Need[1] 값이 위 계산 결과와 일치합니다. 다른 프로세스들의 Allocation 과 Need 는 P1​의 요청에 의해 변경되지 않으므로, 이전 상태 그대로 유지됩니다. 안전성 알고리즘 실행: 이 임시로 수정된 상태에서 (새로운 Available 값 (2,3,0) 을 가지고) 안전성 알고리즘을 다시 실행합니다. 슬라이드는 < P1, P3, P4, P0, P2 > 라는 안전 순서열을 찾았다고 명시합니다. 이는 이 임시 상태가 여전히 안전함을 의미합니다. 안전 순서열 검증 (간략히): Work = (2,3,0) P1 ( Need[1]=(0,2,0) \\le Work=(2,3,0) ): Work = Work + Allocation[1] = (2,3,0) + (3,0,2) = (5,3,2) P3 ( Need[3]=(0,1,1) \\le Work=(5,3,2) ): Work = Work + Allocation[3] = (5,3,2) + (2,1,1) = (7,4,3) P4 ( Need[4]=(4,3,1) \\le Work=(7,4,3) ): Work = Work + Allocation[4] = (7,4,3) + (0,0,2) = (7,4,5) P0 ( Need[0]=(7,4,3) \\le Work=(7,4,5) ): Work = Work + Allocation[0] = (7,4,5) + (0,1,0) = (7,5,5) P2 ( Need[2]=(6,0,0) \\le Work=(7,5,5) ): Work = Work + Allocation[2] = (7,5,5) + (3,0,2) = (10,5,7) 모든 프로세스가 완료될 수 있으므로, 시스템은 안전 상태입니다. 결론: P1​의 요청 (1,0,2) 는 시스템을 안전 상태에 유지하므로 승인됩니다. 임시 변경된 상태가 실제 시스템의 새로운 자원 할당 상태가 됩니다. 추가 질문: 슬라이드는 두 가지 추가 질문을 던집니다. 이는 독자가 은행원 알고리즘을 추가로 연습하고 이해도를 높이도록 유도합니다. 이 질문들에 대한 답을 찾으려면 위와 동일한 자원 요청 알고리즘 단계를 각 요청에 대해 적용해야 합니다. P4​의 요청 (3,3,0) 승인 여부: Need[4] = (4,3,1) . Request[4] = (3,3,0) . (3,3,0) \\le (4,3,1) (참). P1​ 요청 승인 후 Available = (2,3,0) 입니다. Request[4] = (3,3,0) . (3,3,0) \\le (2,3,0) (거짓, A 자원이 부족). 결론: P4​의 요청 (3,3,0)은 현재 Available 자원 부족으로 거부됩니다 (또는 대기해야 합니다). P0​의 요청 (0,2,0) 승인 여부: Need[0] = (7,4,3) . Request[0] = (0,2,0) . (0,2,0) \\le (7,4,3) (참). P1​ 요청 승인 후 Available = (2,3,0) 입니다. Request[0] = (0,2,0) . (0,2,0) \\le (2,3,0) (참). 가상 할당: Available = (2,3,0) - (0,2,0) = (2,1,0) Allocation[0] = (0,1,0) + (0,2,0) = (0,3,0) Need[0] = (7,4,3) - (0,2,0) = (7,2,3) 안전성 알고리즘 실행 (새로운 Available = (2,1,0) 사용): Work = (2,1,0) P1 ( Need[1]=(0,2,0) ): (0,2,0) \\not\\le (2,1,0) (B 자원 부족). P3 ( Need[3]=(0,1,1) ): (0,1,1) \\le (2,1,0) (참). Work = (2,1,0) + Allocation[3]=(2,1,1) = (4,2,1) 이제 Work = (4,2,1) 로 다른 프로세스를 시도. P1 ( Need[1]=(0,2,0) \\le Work=(4,2,1) ): Work = (4,2,1) + Allocation[1]=(3,0,2) = (7,2,3) P4 ( Need[4]=(4,3,1) ): (4,3,1) \\not\\le (7,2,3) (B 자원 부족). ... 계속 시도해도 모든 프로세스를 완료시킬 수 있는 안전 순서열을 찾기 어렵습니다. 특히 P0와 P2의 Need 값이 커서 현재 Work로는 충족시키기 어렵습니다. 따라서, P0 의 요청 (0,2,0) 은 시스템을 불안전 상태로 만들 가능성이 높으므로 거부될 것입니다. 이 예시는 은행원 알고리즘이 어떻게 자원 요청을 동적으로 평가하고, 시스템의 안전성을 유지하면서 자원 할당을 결정하는지 잘 보여줍니다. Memory Management 메모리 관리 (Memory Management) 메모리 관리는 운영체제의 핵심 기능 중 하나로, 한정된 물리적 메모리(RAM)를 여러 프로세스들이 효율적이고 안전하게 공유할 수 있도록 관리하는 기법입니다. 각 프로세스가 필요로 하는 메모리 공간을 할당하고, 사용이 끝나면 회수하며, 다른 프로세스의 공간을 침범하지 않도록 보호하는 역할을 수행합니다. Types of Memory Management (메모리 관리 유형) Fixed Partitioning (고정 분할) Dynamic Partitioning (동적 분할) Paging (페이징) Segmentation (세그먼테이션) Segmentation with Paging (페이징을 이용한 세그먼테이션) 이 슬라이드는 앞으로 다룰 다양한 메모리 관리 기법의 종류를 나열하고 있습니다. 각 기법은 메모리를 어떻게 나누고, 프로세스에 어떻게 할당하며, 어떤 장단점을 가지는지에 따라 구분됩니다. 고정 분할: 메모리를 미리 여러 개의 고정된 크기의 구획(파티션)으로 나누어 사용하는 방식입니다. 동적 분할: 프로세스가 요청하는 크기에 맞춰 메모리 구획을 동적으로 할당하는 방식입니다. 페이징: 프로세스의 논리 주소 공간을 동일한 크기의 '페이지'로, 물리 메모리를 동일한 크기의 '프레임'으로 나누어 관리하는 방식입니다. 세그먼테이션: 프로세스의 논리 주소 공간을 의미 단위인 '세그먼트'(예: 코드, 데이터, 스택)로 나누어 관리하는 방식입니다. 페이징을 이용한 세그먼테이션: 세그먼테이션의 장점과 페이징의 장점을 결합한 방식으로, 세그먼트를 다시 페이지 단위로 나누어 관리합니다. Fixed Partitioning (고정 분할) Equal-size partitions (동일 크기 분할) Any process whose size is less than or equal to the partition size can be loaded into an available partition (파티션 크기보다 작거나 같은 크기의 모든 프로세스는 가용한 파티션에 적재될 수 있습니다) 고정 분할 방식 중 '동일 크기 분할'에 대해 설명합니다. 동일 크기 분할: 시스템이 시작될 때 전체 메모리를 동일한 크기를 가진 여러 개의 파티션으로 미리 나누어 놓습니다. 예를 들어 1000KB 메모리를 100KB 크기의 파티션 10개로 나눌 수 있습니다. 프로세스 적재 조건: 실행하려는 프로세스의 크기가 이 미리 정해진 파티션의 크기보다 작거나 같아야 합니다. 만약 프로세스 크기가 50KB라면 100KB 파티션에 들어갈 수 있지만, 프로세스 크기가 150KB라면 100KB 파티션에는 들어갈 수 없습니다. 가용 파티션: 비어있는 파티션 중 아무 곳에나 프로세스를 적재할 수 있습니다. 이 슬라이드는 동일 크기 고정 분할 방식에서 메모리가 할당된 모습을 시각적으로 보여줍니다. 메모리가 여러 개의 동일한 크기의 칸(파티션)으로 나누어져 있고, 각 파티션에 프로세스 P1, P2, P3가 적재되어 있는 상황을 가정할 수 있습니다. 각 프로세스(P1, P2, P3)는 각각의 파티션 크기보다 작거나 같기 때문에 해당 파티션에 적재될 수 있었습니다. Problems (문제점) Large process can’t fit (큰 프로세스는 적재될 수 없습니다) Small process wastes memory (작은 프로세스는 메모리를 낭비합니다)→ Internal fragmentation (내부 단편화) 동일 크기 고정 분할 방식의 문제점을 설명합니다. 큰 프로세스 적재 불가: 만약 시스템의 파티션 크기가 모두 100KB로 고정되어 있는데, 120KB 크기의 프로세스 P4가 실행을 요청하면, 이 프로세스는 어떤 파티션에도 들어갈 수 없어 실행될 수 없습니다. (그림에서 P4 옆의 X 표시) 작은 프로세스로 인한 메모리 낭비 (내부 단편화): 만약 파티션 크기가 100KB인데, 30KB 크기의 프로세스가 이 파티션에 적재된다면, 나머지 70KB(100KB - 30KB)는 해당 프로세스에 의해 사용되지 않지만, 이미 이 파티션은 할당된 것으로 간주되어 다른 프로세스가 사용할 수 없습니다. 이렇게 할당된 파티션 내부에 사용되지 않고 낭비되는 공간을 내부 단편화(Internal Fragmentation)라고 합니다. (그림에서 마지막 두 파티션의 X 표시) 예를 들어 P1이 80KB, P2가 90KB, P3가 70KB이고 파티션 크기가 100KB라면, P1에서는 20KB, P2에서는 10KB, P3에서는 30KB의 내부 단편화가 발생합니다. Varied-Size Fixed Partitioning (가변 크기 고정 분할) 고정 분할 방식의 또 다른 형태로 '가변 크기 고정 분할'을 소개합니다. (슬라이드에는 상세 설명이 없지만, 일반적인 개념을 설명합니다.) 가변 크기 고정 분할 (Unequal-size fixed partitioning): 시스템이 시작될 때 전체 메모리를 서로 다른 크기를 가진 여러 개의 파티션으로 미리 나누어 놓습니다. 예를 들어 1000KB 메모리를 50KB, 100KB, 150KB, 300KB, 400KB 크기의 파티션들로 나눌 수 있습니다. 장점: 동일 크기 분할 방식에 비해 다양한 크기의 프로세스들을 좀 더 유연하게 수용할 수 있습니다. 큰 프로세스는 큰 파티션에, 작은 프로세스는 작은 파티션에 할당함으로써 내부 단편화를 줄이려는 시도입니다. 단점: 여전히 고정된 분할이므로, 아무리 작은 프로세스라도 가장 작은 파티션보다 크면 적재될 수 없을 수도 있고, 큰 파티션에 작은 프로세스가 들어가면 내부 단편화는 여전히 발생합니다. 파티션의 크기와 개수를 미리 정해야 하므로, 시스템 관리자가 향후 실행될 프로세스들의 크기 분포를 예측해야 하는 부담이 있습니다. 프로세스를 어떤 파티션에 할당할 것인지 결정하는 정책이 필요합니다 (예: 각 프로세스를 수용할 수 있는 가장 작은 파티션에 할당). Problems with Fixed Partitions (고정 분할의 문제점) The number of active processes is limited by the system (to the pre-determined number of partitions) (활성 프로세스의 수는 시스템에 의해 (미리 결정된 파티션 수만큼으로) 제한됩니다) A large number of very small process will not use space efficiently (매우 작은 프로세스가 다수 존재할 경우 공간을 효율적으로 사용하지 못합니다) Solutions? (해결책은?) 고정 분할 방식(동일 크기든 가변 크기든)의 일반적인 문제점을 다시 한번 요약하고 해결책을 묻고 있습니다. 활성 프로세스 수 제한: 고정 분할 방식에서는 파티션의 수가 정해져 있기 때문에, 동시에 실행될 수 있는 프로세스의 최대 개수가 이 파티션 수로 제한됩니다. 예를 들어 파티션이 5개라면, 아무리 작은 프로세스들이라도 최대 5개까지만 동시에 메모리에 적재될 수 있습니다. 메모리 공간이 충분히 남아 있더라도 (예: 각 파티션의 내부 단편화로 인해) 더 이상 새로운 프로세스를 받아들일 수 없는 상황이 발생할 수 있습니다. 작은 프로세스들의 비효율적 공간 사용: 특히 동일 크기 고정 분할에서, 파티션 크기보다 훨씬 작은 프로세스들이 많이 실행되면, 각 파티션마다 큰 내부 단편화가 발생하여 전체적인 메모리 사용 효율이 크게 떨어집니다. 가변 크기 고정 분할도 이 문제를 완전히 해결하지는 못합니다. 해결책?: 이러한 고정 분할의 한계를 극복하기 위한 방법으로 다음에 나올 동적 분할, 페이징, 세그먼테이션 등의 기법들이 제시됩니다. Dynamic Partitioning (동적 분할) Partitions are of variable length and number (파티션의 길이와 수가 가변적입니다) Process is allocated as much as required (프로세스는 필요한 만큼의 메모리를 할당받습니다) OS decides which free block to allocate (운영체제가 어느 가용 블록을 할당할지 결정합니다) 동적 분할 방식은 고정 분할의 단점을 해결하기 위해 등장했습니다. 가변 길이 및 수의 파티션: 동적 분할에서는 미리 파티션을 나누어 놓지 않습니다. 메모리는 초기에 하나의 큰 가용 블록(free block 또는 hole)으로 간주됩니다. 프로세스가 도착하면, 운영체제는 이 가용 블록 중에서 해당 프로세스를 수용할 수 있는 공간을 찾아 정확히 필요한 만큼의 크기로 파티션을 만들어 할당합니다. 따라서 파티션의 크기와 개수는 실행 중인 프로세스들의 요구에 따라 동적으로 변합니다. 필요한 만큼 할당: 프로세스가 70KB를 필요로 하면 정확히 70KB를 할당합니다. 이로 인해 고정 분할에서 발생했던 내부 단편화 문제는 발생하지 않습니다. (이론적으로는 그렇지만, 실제로는 할당 단위 등의 이유로 아주 작은 내부 단편화가 발생할 수도 있으나, 고정 분할에 비하면 무시할 만한 수준입니다.) OS의 할당 결정: 여러 개의 가용 블록이 존재할 수 있습니다 (프로세스가 종료되어 메모리를 반납하면 해당 공간이 가용 블록이 됨). 새로운 프로세스에 어떤 가용 블록을 할당할지 결정하는 정책(알고리즘)이 필요합니다. Allocation Strategy (할당 전략) How to satisfy a request of size n from a list of free holes? (크기 n의 요청을 가용 홀(hole) 리스트로부터 어떻게 만족시킬 것인가?) First-fit (최초 적합): Allocate the first hole that is big enough (충분히 큰 첫 번째 가용 홀을 할당) Best-fit (최적 적합): Allocate the smallest hole that is big enough; must search entire list, unless ordered by size (충분히 큰 가장 작은 가용 홀을 할당; 크기 순으로 정렬되어 있지 않다면 전체 리스트를 탐색해야 함) Produces the smallest leftover hole (가장 작은 남은 홀을 생성) Worst-fit (최악 적합): Allocate the largest hole; must also search entire list (가장 큰 가용 홀을 할당; 역시 전체 리스트를 탐색해야 함) Produces the largest leftover hole (가장 큰 남은 홀을 생성) First-fit and best-fit better than worst-fit in terms of speed and storage utilization (최초 적합과 최적 적합이 속도와 저장 공간 활용 측면에서 최악 적합보다 우수합니다) 동적 분할에서 여러 개의 가용 블록(홀) 중 어느 것을 선택하여 프로세스에 할당할지를 결정하는 다양한 전략(알고리즘)입니다. 최초 적합 (First-fit): 가용 메모리 블록 리스트를 처음부터 순서대로 탐색하여, 프로세스를 수용할 수 있는 충분한 크기의 첫 번째 블록을 할당합니다. 장점: 검색 시간이 빠를 수 있습니다 (운이 좋으면 리스트 앞부분에서 바로 찾음). 단점: 리스트 앞부분에 큰 가용 블록들이 집중되어 있을 경우, 작은 요청들이 이 큰 블록들을 잘게 쪼개어 나중에 큰 요청을 처리하지 못할 수 있습니다. 최적 적합 (Best-fit): 가용 메모리 블록 리스트 전체를 탐색하여, 프로세스를 수용할 수 있는 블록들 중에서 크기가 가장 작은 블록 (즉, 요청 크기와 가장 비슷한 크기의 블록)을 할당합니다. 장점: 요청 크기에 딱 맞는 블록을 할당하므로, 할당 후 남는 조각(leftover hole)의 크기를 최소화합니다. 이렇게 하면 아주 작은, 쓸모없는 조각들이 생기는 것을 방지하여 메모리 활용률을 높일 수 있다고 기대합니다. 단점: 항상 전체 리스트를 검색해야 하므로 검색 시간이 오래 걸릴 수 있습니다. 또한, 매우 작은 조각들이 많이 생겨서 오히려 단편화를 심화시킬 수도 있습니다. 최악 적합 (Worst-fit): 가용 메모리 블록 리스트 전체를 탐색하여, 프로세스를 수용할 수 있는 블록들 중에서 크기가 가장 큰 블록을 할당합니다. 장점: 큰 블록을 할당하고 남은 조각도 상대적으로 크기 때문에, 이 남은 조각이 다른 중간 크기의 프로세스를 수용할 가능성을 높이려는 의도입니다. 단점: 항상 전체 리스트를 검색해야 합니다. 큰 블록을 계속 사용하면 큰 가용 공간이 빨리 소진되어, 나중에 매우 큰 프로세스가 도착했을 때 할당하지 못할 수 있습니다. 결론: 일반적으로 시뮬레이션 결과에 따르면, 최초 적합과 최적 적합이 최악 적합보다 평균적인 검색 시간이나 메모리 단편화 관리 측면에서 더 나은 성능을 보인다고 알려져 있습니다. 최초 적합은 구현이 간단하고 평균적으로 성능도 괜찮아서 많이 사용됩니다. 최적 적합은 더 많은 검색 시간을 소요하지만, 때로는 더 나은 메모리 활용을 보일 수도 있습니다. 이 슬라이드는 가상의 메모리 상태를 보여주고, 새로운 프로세스 P8(크기가 명시되진 않음)이 도착했을 때 각 할당 전략이 어떤 가용 공간을 선택할지를 개념적으로 나타냅니다. 현재 메모리에는 운영체제(OS)와 기존 프로세스들(process 6, 3, 5, 7)이 적재되어 있고, 그 사이사이에 여러 개의 가용 공간(hole)들이 존재합니다. First-fit: P8의 요청 크기를 만족하는 첫 번째 가용 공간을 할당합니다 (위에서부터 탐색 가정). Best-fit: P8의 요청 크기를 만족하면서 가장 크기가 비슷한 (즉, 남는 공간이 가장 적은) 가용 공간을 할당합니다. Worst-fit: P8의 요청 크기를 만족하면서 가장 크기가 큰 가용 공간을 할당합니다. 실제 어떤 공간이 선택될지는 P8의 크기와 각 가용 공간의 정확한 크기에 따라 달라집니다. 이 그림은 각 전략의 선택 기준이 다름을 보여줍니다. (Diagram Series Showing Process Allocation and Deallocation leading to External Fragmentation) OS process 1 process 2 process 3 OS process 1 process 3 (process 2 deallocated) OS process 1 process 3 (hole where process 2 was) OS process 1 process 4 process 3 (process 4 allocated in the hole) process 4 process 5 OS process 1 process 3 process 5 (process 4 deallocated) OS process 3 process 5 (process 1 deallocated) OS process 6 process 3 process 5 (process 6 allocated) OS process 6 process 3 process 5 process 7 (process 7 allocated) External Fragmentation! (외부 단편화!) \\[설명\\] 이 슬라이드 시리즈는 동적 분할 방식에서 프로세스들이 메모리에 할당되고 해제되는 과정을 시간 순서대로 보여주면서, 결국 외부 단편화(External Fragmentation)가 발생하는 상황을 설명합니다. 초기 상태: OS, P1, P2, P3가 순서대로 메모리에 적재되어 있습니다. P2 종료: P2가 사용하던 메모리 공간이 반납되어 가용 공간(hole)이 됩니다. (OS, P1, [hole1], P3) (hole1 표시) P4 할당: P4가 도착하여 hole1에 적재됩니다. (OS, P1, P4, P3) P5 할당: P5가 P3 다음에 적재됩니다. (OS, P1, P4, P3, P5) (그림에서는 P4, P5가 연달아 표시되어 있는데, 이는 P3 다음에 P5가 할당되고, 그 후 P4가 종료된 상황으로 해석하거나, 혹은 P4가 종료된 후 P5가 다른 곳에 할당된 상황으로 볼 수 있습니다. 슬라이드 순서상 P4 종료 후 P5가 다른 hole에 할당된 것으로 보는게 맞겠습니다. 하지만 그림 연결이 조금 불명확합니다. 중요한 것은 프로세스들이 할당되고 해제됨에 따라 hole들이 생긴다는 것입니다.) 정정 및 명확화: 슬라이드의 그림 흐름을 따라가면, OS, P1, P4, P3 (이전 상태에서 P2 자리에 P4 할당) 그림에서 \"process 4 process 5\"라고만 되어 있어, P3 이후에 P5가 할당되었다고 가정하겠습니다. 즉, (OS, P1, P4, P3, P5). 다음 그림 \"OS process 1 process 3 process 5\"는 P4가 종료된 상태를 나타냅니다. 즉, (OS, P1, [hole2], P3, P5) . P1 종료: P1이 사용하던 공간이 반납되어 가용 공간이 됩니다. (OS, [hole3], [hole2], P3, P5) P6 할당: P6가 도착하여 가장 앞쪽 가용 공간인 hole3에 적재됩니다 (First-fit 가정). (OS, P6, [hole2], P3, P5) P7 할당: P7이 도착하여 P5 다음의 가용 공간에 적재됩니다. (OS, P6, [hole2], P3, P5, P7) 외부 단편화 발생: 이제 메모리 상태는 (OS, P6, [hole2], P3, P5, P7) 입니다. 여기서 [hole2] 는 P4가 사용했던 공간으로, 여전히 가용 상태입니다. 만약 새로운 프로세스 P8이 도착했는데, P8의 크기가 [hole2] 보다는 크지만, [hole2] 와 메모리 끝 어딘가에 있을지 모르는 다른 작은 가용 공간들의 합보다는 작거나 같은 상황을 생각해봅시다. 즉, 전체 가용 메모리 공간의 합은 P8을 수용하기에 충분하지만, 이 가용 공간들이 연속적이지 않고 작은 조각들로 흩어져 있어서 P8을 할당할 수 없는 상태가 발생합니다. 이것이 바로 외부 단편화입니다. 메모리가 프로세스들의 외부에 잘게 조각나 있다는 의미입니다. Dynamic Partitioning Example (동적 분할 예제) External Fragmentation (외부 단편화) Memory external to all processes is fragmented (모든 프로세스의 외부에 있는 메모리가 단편화됨) Compaction (압축 또는 집약) OS moves processes so that they are contiguous (운영체제가 프로세스들을 이동시켜 연속적으로 만듦) Time consuming and wastes CPU time (시간이 많이 소모되고 CPU 시간을 낭비함) \\[설명\\] 외부 단편화 재정의: 사용 중인 프로세스들 사이에 끼어 있는 작은 가용 메모리 공간들로 인해, 총 가용 메모리 양은 충분함에도 불구하고 특정 크기의 프로세스를 할당할 수 없는 현상입니다. 압축 (Compaction): 외부 단편화 문제를 해결하기 위한 방법 중 하나입니다. 운영체제가 메모리에 흩어져 있는 여러 프로세스들을 한쪽으로 이동시켜 연속된 공간을 만들고, 흩어져 있던 가용 공간들을 하나의 큰 가용 블록으로 합칩니다. 예: (P1, hole1, P2, hole2, P3) -> (P1, P2, P3, hole_combined) 문제점: 시간 소모: 메모리 내용을 복사하고 이동시키는 작업은 매우 많은 시간이 소요됩니다. 이 동안 시스템은 다른 작업을 거의 수행할 수 없습니다. CPU 시간 낭비: 압축 작업 자체가 CPU 자원을 사용합니다. 어떤 프로세스를 옮길 것인가?: 모든 프로세스를 옮겨야 합니다. 언제 압축을 수행할 것인가?: 너무 자주 하면 성능 저하, 너무 안 하면 단편화 심화. 실행 중인 프로세스 이동의 어려움: 프로세스가 실행 중에 메모리 주소가 바뀌면 프로그램 내의 주소 참조가 모두 엉망이 됩니다. 따라서 압축은 프로세스의 주소 바인딩이 실행 시간 바인딩(execution time binding)일 때, 즉 MMU와 같은 하드웨어 지원으로 논리 주소와 물리 주소가 동적으로 변환될 때만 가능합니다. (이후 슬라이드에서 주소 바인딩 설명됨) Fragmentation (단편화) External Fragmentation – total free memory is enough for new process, but it is not contiguous (외부 단편화 – 새 프로세스를 수용하기에 총 가용 메모리는 충분하지만, 연속적이지 않음) Internal Fragmentation – allocated memory to a process but never used (내부 단편화 – 프로세스에 할당되었지만 사용되지 않는 메모리) Fixed partitioning has only internal frag. (고정 분할은 내부 단편화만 가짐) Dynamic partitioning has only external frag. (동적 분할은 외부 단편화만 가짐) First fit has 50-percent rule (최초 적합은 50% 규칙을 가짐) 31KB 내부 단편화) 외부 단편화: 버디 시스템도 여전히 외부 단편화가 발생할 수 있습니다. 예를 들어, 크기가 다른 가용 블록들이 서로 인접해 있지 않거나, 버디 관계가 아닌 인접한 가용 블록들은 합쳐지지 않기 때문입니다. 하지만 일반적인 동적 분할보다는 관리하기 용이합니다. Example of Buddy System (버디 시스템 예제) (Diagram illustrating splitting and merging of blocks of sizes like 1M, 512K, 256K, 128K, 64K) \\[설명\\] 이 슬라이드는 버디 시스템의 동작을 시각적으로 보여주는 예제입니다. 초기 상태: 예를 들어 1MB의 전체 메모리 블록이 있다고 가정합니다. 할당 예시: 프로세스 A (100KB) 요청: 1MB -> 512KB + 512KB. 512KB -> 256KB + 256KB. 256KB -> 128KB + 128KB. A에 128KB 할당. 프로세스 B (200KB) 요청: 남은 128KB 옆의 256KB를 가져와 할당 (만약 256KB가 없다면 다른 512KB를 다시 분할). 프로세스 C (70KB) 요청: 남은 128KB에서 분할. 128KB -> 64KB + 64KB. C에 64KB 할당 (내부 단편화 발생). 또는 70KB는 128KB 블록에 할당될 수 있습니다. 해제 및 합병 예시: 프로세스 A (128KB) 해제: A가 사용하던 128KB 블록의 버디(다른 128KB)가 가용 상태인지 확인. 가용 상태라면 합병하여 256KB 블록 생성. 이 256KB 블록의 버디(다른 256KB)도 가용 상태라면 다시 합병하여 512KB 생성. 이런 식으로 가능한 최대 크기로 합병을 시도합니다. 그림은 이러한 분할과 합병 과정을 단계별로 보여주며, 특정 크기의 블록들이 어떻게 생성되고 사라지는지를 나타냅니다. Tree Representation of Buddy System (버디 시스템의 트리 표현) (Diagram showing a binary tree where leaf nodes represent smallest allocatable blocks and parent nodes represent merged blocks) \\[설명\\] 버디 시스템은 이진 트리 형태로 표현하고 관리하기 용이합니다. 루트 노드는 전체 메모리 블록을 나타냅니다. 각 노드가 분할되면 두 개의 자식 노드가 생성되며, 이 두 자식 노드는 서로 버디 관계입니다. 리프 노드는 현재 할당 가능한 가장 작은 단위의 블록이거나, 이미 할당된 블록들을 나타낼 수 있습니다. 트리를 사용하면 특정 크기의 블록을 찾거나, 해제 시 버디를 찾고 합병하는 과정을 효율적으로 수행할 수 있습니다. 예를 들어, 어떤 블록의 주소와 크기를 알면, 그 버디 블록의 주소를 간단한 비트 연산(XOR)으로 계산할 수 있습니다 (블록 크기가 2k이고, 블록 시작 주소가 X일 때, X의 k번째 비트를 반전시키면 버디의 주소를 얻을 수 있는 경우가 많음, 구현에 따라 다름). 이 슬라이드의 그림은 이러한 트리 구조를 시각화하여, 어떤 블록들이 할당되었고 (예: 음영 처리), 어떤 블록들이 가용한지 (예: 빈 노드), 그리고 어떤 블록들이 더 큰 블록으로 합쳐질 수 있는지를 보여줍니다. Background (배경 지식) Program must be brought (from disk) into memory and placed within a process for it to be run (프로그램이 실행되려면 (디스크에서) 메모리로 가져와 프로세스 내에 배치되어야 합니다) Main memory and registers are only storage CPU can access directly (주 기억장치(메인 메모리)와 레지스터만이 CPU가 직접 접근할 수 있는 저장 공간입니다) Memory unit only sees a stream of addresses + read requests, or address + data and write requests (메모리 장치는 주소 + 읽기 요청 스트림 또는 주소 + 데이터 및 쓰기 요청 스트림만을 봅니다) Register access in one CPU clock (or less) (레지스터 접근은 CPU 클럭 1회 (또는 그 이하) 소요) Main memory can take many cycles (주 기억장치 접근은 많은 사이클이 소요될 수 있습니다) Cache sits between main memory and CPU registers (캐시는 주 기억장치와 CPU 레지스터 사이에 위치합니다) Protection of memory required to ensure correct operation (올바른 작동을 보장하기 위해 메모리 보호가 필요합니다) \\[설명\\] 메모리 관리를 이해하는 데 필요한 기본적인 컴퓨터 시스템 구조 및 동작 원리를 설명합니다. 프로그램 실행 과정: 프로그램(실행 파일 형태)은 평소에는 보조 기억장치(예: 하드 디스크, SSD)에 저장되어 있습니다. 이를 실행하려면, 프로그램 코드가 주 기억장치(RAM)로 적재(load)되어야 합니다. 이 적재된 프로그램의 실행 단위를 프로세스라고 합니다. CPU의 직접 접근: CPU는 매우 빠르지만, 오직 레지스터와 주 기억장치(메인 메모리)에 있는 데이터와 명령어에만 직접 접근할 수 있습니다. 디스크에 있는 내용은 직접 실행하거나 처리할 수 없습니다. 메모리 장치의 역할: 메모리 컨트롤러를 포함한 메모리 유닛은 CPU로부터 주소와 함께 읽기/쓰기 요청을 받아 처리합니다. CPU가 \"0x1000번지에서 데이터를 읽어와라\" 또는 \"0x2000번지에 이 데이터를 써라\"와 같은 형태로 요청합니다. 접근 속도 차이: 레지스터: CPU 내부에 있어 접근 속도가 가장 빠릅니다 (CPU 클럭과 동기화, 1클럭 이내). 주 기억장치 (RAM): 레지스터보다 훨씬 느립니다. CPU가 RAM에 접근하려면 수십~수백 CPU 클럭 사이클이 소요될 수 있습니다. 캐시 메모리: 이러한 속도 차이를 완화하기 위해 CPU와 주 기억장치 사이에 고속의 캐시 메모리를 둡니다. 자주 사용될 것으로 예상되는 데이터나 명령어를 미리 캐시에 가져다 놓으면, CPU는 대부분의 경우 느린 주 기억장치 대신 빠른 캐시에 접근하여 성능을 향상시킬 수 있습니다 (캐시 히트). 메모리 보호: 여러 프로세스가 동시에 메모리에 적재되어 실행되는 다중 프로그래밍 환경에서는, 한 프로세스가 다른 프로세스의 메모리 영역이나 운영체제의 메모리 영역을 침범하여 데이터를 손상시키거나 시스템 전체를 불안정하게 만드는 것을 방지해야 합니다. 이를 위해 메모리 보호 기법이 필수적입니다. Base and Limit Registers (기준 레지스터와 한계 레지스터) A pair of base and limit registers define the logical address space (한 쌍의 기준 레지스터와 한계 레지스터가 논리 주소 공간을 정의합니다) \\[설명\\] 메모리 보호를 위한 간단하면서도 효과적인 하드웨어 기법 중 하나로 기준 레지스터와 한계 레지스터를 사용합니다. 논리 주소 공간(Logical Address Space): 프로세스 입장에서 바라보는 메모리 주소의 범위입니다. 보통 0번지부터 시작하는 연속적인 주소 공간으로 인식합니다. 기준 레지스터 (Base Register): 어떤 프로세스가 메모리에 적재될 때, 그 프로세스가 적재된 물리 메모리의 시작 주소를 저장합니다. '재배치 레지스터(Relocation Register)'라고도 불립니다. 한계 레지스터 (Limit Register): 해당 프로세스의 논리 주소 공간의 크기(즉, 프로그램의 길이)를 저장합니다. 프로세스가 접근할 수 있는 최대 논리 주소값을 나타냅니다. 프로세스가 어떤 논리 주소 L에 접근하려고 할 때, 하드웨어는 다음을 검사합니다: 0 logical address -> [check: address >= base? and address < base + limit?] -> yes -> physical address -> memory; no -> trap to OS) base : 접근 가능한 최소 물리 주소 limit : 접근 가능한 논리 주소의 최대값. base + limit (슬라이드 그림에는 base + limit으로 되어있지만, 더 정확히는 논리주소 Compiler -> Object module -> Linker -> Load module -> Loader -> In-memory process with dynamic linking/loading) \\[설명\\] 사용자 프로그램이 작성되어 실행되기까지 거치는 여러 단계를 보여주는 다이어그램입니다. 각 단계에서 주소와 관련된 변환 및 바인딩 작업이 이루어집니다. Source Program (소스 프로그램): 프로그래머가 작성한 고급 언어 또는 어셈블리어 코드. 심볼릭 주소(변수명, 함수명 등)를 사용합니다. Compiler (컴파일러) 또는 Assembler (어셈블러): 소스 프로그램을 기계가 이해할 수 있는 오브젝트 모듈(object module)로 번역합니다. 심볼릭 주소를 재배치 가능 주소(상대 주소)로 변환합니다. 외부 참조(다른 모듈에 정의된 함수나 변수)에 대한 정보를 포함합니다. Object Module (오브젝트 모듈): 컴파일된 결과물. 재배치 정보, 심볼 테이블 등을 포함합니다. Linker (링커): 여러 개의 오브젝트 모듈과 필요한 라이브러리 모듈들을 결합하여 하나의 실행 가능한 로드 모듈(load module) 또는 실행 파일(executable file)을 만듭니다. 모듈 간의 외부 참조를 해결하여 주소를 연결합니다. 주소들을 하나의 통합된 (논리적) 주소 공간 내의 주소로 조정합니다. Load Module (로드 모듈) / Executable File (실행 파일): 디스크에 저장된, 실행 준비가 된 프로그램입니다. Loader (로더): 사용자가 프로그램을 실행시키면, 로더가 로드 모듈을 메모리에 적재합니다. 적재 시간 바인딩을 사용한다면, 이 때 재배치 가능 주소를 물리 주소로 변환합니다. 실행 시간 바인딩을 사용한다면, 논리 주소 형태를 유지한 채로 적재하고, MMU가 실행 중에 주소 변환을 담당합니다. In-memory process (메모리 내의 프로세스): 메모리에 적재되어 실행 중인 프로그램. Dynamic Linking (동적 연결): 라이브러리 루틴과의 연결이 실행 시점까지 지연되는 경우. 필요할 때 해당 라이브러리가 메모리에 로드되고 연결됩니다. (다음 슬라이드에서 설명) Dynamic Loading (동적 적재): 프로그램의 특정 루틴이 호출될 때까지 디스크에 남아 있다가, 호출되는 순간에 메모리에 적재되는 경우. (다음 슬라이드에서 설명) 이 과정을 통해 프로그램의 추상적인 주소 표현이 점차 구체적인 물리 메모리 주소로 연결됩니다. Logical vs. Physical Address Space (논리적 주소 공간 대 물리적 주소 공간) The concept of a logical address space that is bound to a separate physical address space is central to proper memory management (별도의 물리적 주소 공간에 바인딩되는 논리적 주소 공간의 개념은 적절한 메모리 관리의 핵심입니다) Logical address – generated by the CPU; also referred to as virtual address (논리 주소 – CPU에 의해 생성됨; 가상 주소라고도 함) Physical address – address seen by the memory unit (물리 주소 – 메모리 장치에 의해 보여지는 주소) Logical and physical addresses are the same in compile-time and load-time address-binding schemes; logical (virtual) and physical addresses differ in execution-time address-binding scheme (논리 주소와 물리 주소는 컴파일 시간 및 적재 시간 주소 바인딩 방식에서는 동일함; 논리(가상) 주소와 물리 주소는 실행 시간 주소 바인딩 방식에서 다름) Logical address space is the set of all logical addresses generated by a program (논리 주소 공간은 프로그램에 의해 생성된 모든 논리 주소의 집합입니다) Physical address space is the set of all physical addresses generated by a program (물리 주소 공간은 프로그램에 의해 생성된 모든 물리 주소의 집합입니다) \\[설명\\] 논리 주소와 물리 주소의 개념은 현대 메모리 관리의 근간을 이룹니다. 핵심 개념: 프로세스는 자신만의 독립적인 논리 주소 공간을 가지고, 이 논리 주소 공간이 실제 물리 메모리의 특정 위치(물리 주소 공간)에 매핑(바인딩)됩니다. 논리 주소 (Logical Address): CPU가 생성하는 주소입니다. 프로세스 입장에서 바라보는 주소이며, 보통 0번지부터 시작하는 연속적인 공간으로 인식됩니다. 가상 주소 (Virtual Address)와 거의 동의어로 사용됩니다. (엄밀히는 페이징이나 세그먼테이션 같은 가상 메모리 기법에서 사용되는 논리 주소를 가상 주소라고 부르는 경향이 있습니다.) 물리 주소 (Physical Address): 실제 메모리 하드웨어(RAM) 상의 주소입니다. 메모리 관리 장치(MMU)를 거쳐 최종적으로 메모리 버스에 실리는 주소입니다. 주소 바인딩 시점에 따른 관계: 컴파일 시간 및 적재 시간 바인딩: 프로그램이 생성하는 주소가 곧 물리 주소입니다. 즉, 논리 주소 = 물리 주소. 이 경우, 프로그램은 자신이 메모리의 어느 위치에 있는지 알아야 하거나, 한번 정해진 위치에서 이동할 수 없습니다. 실행 시간 바인딩: CPU가 생성하는 논리 주소와 메모리가 보는 물리 주소가 다릅니다. MMU가 이 둘 사이의 변환을 담당합니다. 이를 통해 프로세스는 물리 메모리의 어느 위치에든 적재될 수 있고, 심지어 실행 중에 이동될 수도 있습니다. 주소 공간: 논리 주소 공간 (Logical Address Space): 한 프로그램이 만들어낼 수 있는 모든 논리 주소들의 집합입니다. 예를 들어, 32비트 시스템에서 각 프로세스는 232 바이트 (4GB) 크기의 논리 주소 공간을 가질 수 있습니다 (이론적으로). 물리 주소 공간 (Physical Address Space): 시스템에 장착된 실제 물리 메모리의 주소 범위에 해당하는 모든 물리 주소들의 집합입니다. 예를 들어, 1GB RAM이 장착된 시스템의 물리 주소 공간 크기는 1GB입니다. 실행 시간 바인딩을 통해 논리 주소 공간과 물리 주소 공간을 분리함으로써, 프로그래머는 실제 물리 메모리 크기나 다른 프로세스의 존재에 대해 크게 신경 쓰지 않고 프로그램을 작성할 수 있게 되며, 운영체제는 메모리를 보다 유연하고 효율적으로 관리할 수 있습니다. Memory-Management Unit (MMU) (메모리 관리 장치) Hardware device that at run time maps virtual to physical address (실행 시간에 가상 주소를 물리 주소로 매핑하는 하드웨어 장치) Many methods possible, covered in the rest of this chapter (다양한 방법이 가능하며, 이 장의 나머지 부분에서 다룸) To start, consider simple scheme where the value in the relocation register is added to every address generated by a user process at the time it is sent to memory (우선, 사용자 프로세스에 의해 생성된 모든 주소가 메모리로 보내질 때 재배치 레지스터의 값이 더해지는 간단한 방식을 고려해 봅시다) Base register now called relocation register (기준 레지스터가 이제 재배치 레지스터로 불림) MS-DOS on Intel 80x86 used 4 relocation registers (인텔 80x86 기반 MS-DOS는 4개의 재배치 레지스터를 사용했음) The user program deals with logical addresses; it never sees the real physical addresses (사용자 프로그램은 논리 주소를 다루며, 실제 물리 주소를 절대로 보지 못합니다) Execution-time binding occurs when reference is made to location in memory (실행 시간 바인딩은 메모리 위치에 대한 참조가 이루어질 때 발생합니다) Logical address bound to physical addresses (논리 주소가 물리 주소에 바인딩됨) \\[설명\\] MMU는 실행 시간 주소 바인딩을 가능하게 하는 핵심 하드웨어입니다. MMU의 역할: CPU와 메모리 사이에 위치하며, CPU가 생성한 논리(가상) 주소를 물리 주소로 변환하는 역할을 수행합니다. 이 변환은 프로그램 실행 중에, 즉 런타임에 이루어집니다. 다양한 변환 방식: MMU는 단순한 재배치 레지스터 방식부터 페이징 테이블, 세그먼트 테이블을 사용하는 복잡한 방식까지 다양한 주소 변환 메커니즘을 지원할 수 있습니다. 이 장의 뒷부분에서 페이징과 세그먼테이션을 다룰 때 더 자세히 나옵니다. 간단한 MMU 방식 (재배치 레지스터 사용): 가장 기본적인 MMU 기능은 재배치 레지스터(Relocation Register, 이전의 Base Register와 유사한 개념)를 사용하는 것입니다. CPU가 생성한 모든 논리 주소에 재배치 레지스터의 값을 더하여 물리 주소를 얻습니다. Physical Address = Logical Address + Relocation_Register_Value 이때 한계 레지스터(Limit Register)도 함께 사용하여 메모리 보호 기능을 제공합니다. 즉, Logical Address < Limit_Register_Value 인지 검사합니다. 예시: MS-DOS는 인텔 80x86 CPU의 세그먼트 레지스터(CS, DS, SS, ES - 일종의 재배치 레지스터)를 사용하여 메모리를 관리했습니다. 각 세그먼트 레지스터는 64KB 세그먼트의 시작 주소를 가리켰습니다. 사용자 프로그램과 논리 주소: 사용자 프로그램은 항상 논리 주소만을 사용합니다. 프로그래머나 컴파일러는 프로그램이 실제 물리 메모리의 어느 위치에 적재될지 알 필요가 없습니다. 프로그램은 자신이 0번지부터 시작하는 연속적인 메모리 공간을 사용하는 것처럼 동작합니다. 실행 시간 바인딩의 실제: CPU가 메모리를 참조하는 명령어(예: LOAD , STORE )를 실행할 때마다, 해당 명령어에 포함된 논리 주소가 MMU에 의해 물리 주소로 즉시 변환됩니다. 이것이 바로 실행 시간 바인딩이 실제로 일어나는 순간입니다. MMU 덕분에 운영체제는 프로세스를 메모리의 어느 위치에든 적재할 수 있고, 필요에 따라 이동시킬 수도 있으며, 각 프로세스에게 격리된 주소 공간을 제공하여 시스템의 안정성과 효율성을 높일 수 있습니다. Dynamic relocation using a relocation register (재배치 레지스터를 사용한 동적 재배치) (Diagram showing: CPU -> logical address -> MMU [Relocation register +] -> physical address -> Memory) \\[설명\\] 이 다이어그램은 재배치 레지스터(MMU의 일부)를 사용한 가장 기본적인 동적 주소 변환 과정을 보여줍니다. CPU: 논리 주소(logical address)를 생성합니다. 이 주소는 보통 0부터 시작하는 상대적인 주소입니다. (예: 100) MMU (Memory Management Unit): CPU로부터 논리 주소를 받습니다. MMU 내의 재배치 레지스터(relocation register)에는 해당 프로세스가 적재된 물리 메모리의 시작 주소가 저장되어 있습니다. (예: 재배치 레지스터 값 = 14000) 논리 주소에 재배치 레지스터 값을 더합니다. (예: 100 + 14000 = 14100) (그림에는 생략되었지만, 이 과정에서 한계 레지스터를 이용한 유효 범위 검사도 함께 이루어집니다.) Physical Address (물리 주소): 변환된 결과가 물리 주소입니다. (예: 14100) Memory (메모리): 이 물리 주소를 사용하여 실제 메모리 위치에 접근합니다. 이러한 동적 재배치 덕분에, 같은 프로그램이라도 메모리의 다른 위치에 적재될 때마다 재배치 레지스터 값만 변경해주면 되므로, 코드를 수정할 필요 없이 유연하게 메모리를 사용할 수 있습니다. Dynamic Loading (동적 적재) Routine is not loaded until it is called (루틴이 호출될 때까지 적재되지 않습니다) Better memory-space utilization; unused routine is never loaded (더 나은 메모리 공간 활용; 사용되지 않는 루틴은 절대로 적재되지 않음) All routines kept on disk in relocatable load format (모든 루틴은 재배치 가능 적재 포맷으로 디스크에 유지됨) Useful when large amounts of code are needed to handle infrequently occurring cases (드물게 발생하는 경우를 처리하기 위해 많은 양의 코드가 필요할 때 유용함) No special support from the operating system is required (운영체제의 특별한 지원이 필요하지 않음) Implemented through program design (프로그램 설계를 통해 구현됨) OS can help by providing libraries to implement dynamic loading (OS는 동적 적재를 구현하기 위한 라이브러리를 제공하여 도움을 줄 수 있음) \\[설명\\] 동적 적재는 메모리 효율성을 높이기 위한 기법으로, 프로그램의 모든 부분을 실행 시작 시점에 메모리에 올리는 대신, 특정 루틴(함수 또는 코드 모듈)이 실제로 호출되는 시점에 메모리에 적재하는 방식입니다. 지연된 적재: 프로그램이 시작될 때는 핵심적인 부분만 메모리에 적재하고, 나머지 루틴들은 디스크에 재배치 가능한 형태로 대기합니다. 어떤 루틴이 호출되면, 그제서야 해당 루틴을 디스크에서 메모리로 가져와 연결하고 실행합니다. 메모리 효율성 향상: 프로그램 내에는 많은 기능들이 있지만, 실제 실행 시에는 그중 일부만 사용되는 경우가 많습니다 (예: 복잡한 소프트웨어의 특정 고급 기능, 오류 처리 루틴 등). 동적 적재를 사용하면, 한 번도 호출되지 않는 루틴은 메모리를 전혀 차지하지 않으므로 메모리 공간을 절약할 수 있습니다. 특히 전체 프로그램 크기가 매우 클 때 유용합니다. 적재 형태: 디스크에 있는 루틴들은 재배치 가능한 형태로 저장되어 있어, 메모리의 어느 위치에 적재되더라도 주소 문제가 발생하지 않도록 준비되어 있습니다. 유용성: 프로그램의 크기는 크지만 특정 기능(예: 도움말 기능, 특정 오류 복구 루틴, 드문 사용자 옵션 처리)이 자주 사용되지 않을 때 효과적입니다. 이렇게 하면 프로그램 시작 시간도 빨라질 수 있습니다. 구현: 원칙적으로 동적 적재는 프로그래머가 직접 프로그램 로직 내에서 구현할 수 있습니다. 예를 들어, 루틴 호출 시 해당 루틴이 메모리에 있는지 확인하고, 없으면 로드하는 코드를 작성할 수 있습니다. 운영체제는 이러한 동적 적재 기능을 쉽게 구현할 수 있도록 시스템 라이브러리(예: dlopen() , dlsym() 같은 함수를 제공하는 라이브러리)를 제공하여 프로그래머를 도울 수 있습니다. 동적 적재는 특히 메모리가 제한적인 환경이나 매우 큰 애플리케이션에서 유용하며, 프로그램의 시작 속도를 개선하고 전반적인 시스템 성능을 향상시키는 데 기여할 수 있습니다. Dynamic Linking (동적 연결) Static linking – system libraries and program code combined by the loader into the binary program image (정적 연결 – 시스템 라이브러리와 프로그램 코드가 로더에 의해 바이너리 프로그램 이미지로 결합됨) Dynamic linking –linking postponed until execution time (동적 연결 – 연결이 실행 시간까지 연기됨) Small piece of code, stub, used to locate the appropriate memory-resident library routine (스텁이라는 작은 코드 조각이 메모리에 상주하는 적절한 라이브러리 루틴을 찾는 데 사용됨) Stub replaces itself with the address of the routine, and executes the routine (스텁은 자신을 루틴의 주소로 대체하고, 해당 루틴을 실행함) Operating system checks if routine is in processes’ memory address (운영체제는 루틴이 프로세스의 메모리 주소 공간에 있는지 확인합니다) If not in address space, add to address space (주소 공간에 없다면, 주소 공간에 추가함) Dynamic linking is particularly useful for libraries (동적 연결은 특히 라이브러리에 유용합니다) System also known as shared libraries (이 시스템은 공유 라이브러리라고도 알려져 있음) \\[설명\\] 동적 연결은 라이브러리 함수들과 같은 외부 코드와의 연결(linking)을 프로그램 실행 시간까지 미루는 기법입니다. 정적 연결 (Static Linking): 전통적인 방식에서는 프로그램이 컴파일되고 링크될 때, 프로그램 코드뿐만 아니라 프로그램이 사용하는 모든 라이브러리 루틴(예: printf 함수)의 코드까지 실행 파일 안에 포함시킵니다. 단점: 실행 파일의 크기가 커집니다. 여러 프로그램이 동일한 라이브러리 루틴을 사용하더라도, 각 프로그램의 실행 파일마다 해당 루틴의 복사본이 포함되므로 디스크 공간이 낭비됩니다. 또한 메모리에 여러 복사본이 동시에 올라올 수 있어 메모리도 낭비됩니다. 라이브러리가 업데이트되면, 해당 라이브러리를 사용하는 모든 프로그램을 다시 링크하고 재배포해야 합니다. 동적 연결 (Dynamic Linking): 라이브러리 루틴과의 실제 연결은 프로그램이 실행될 때(또는 해당 루틴이 처음 호출될 때) 이루어집니다. 스텁 (Stub): 프로그램 코드 내에는 실제 라이브러리 루틴 대신, 해당 루틴을 찾는 방법을 아는 작은 코드 조각인 '스텁'이 포함됩니다. 동작 과정: 프로그램이 동적으로 연결된 라이브러리 루틴을 처음 호출하면, 스텁이 실행됩니다. 스텁은 운영체제에게 해당 라이브러리 루틴이 이미 메모리에 있는지 확인하도록 요청합니다. 만약 라이브러리 루틴이 메모리에 있다면 (다른 프로세스가 이미 로드했을 수 있음), 스텁은 그 루틴의 메모리 주소를 얻어와서, 다음부터는 스텁을 거치지 않고 바로 그 주소로 점프하도록 자신의 코드를 수정하거나 포인터를 설정합니다. 그리고 해당 루틴을 실행합니다. 만약 라이브러리 루틴이 메모리에 없다면, 운영체제는 디스크에서 해당 라이브러리를 찾아 메모리에 적재하고, 그 주소를 스텁에게 알려줍니다. 이후 과정은 3번과 동일합니다. 장점 (특히 라이브러리에 유용): 메모리 절약: 라이브러리 코드가 메모리에 한 번만 적재되고, 이를 필요로 하는 모든 프로세스들이 이 한 벌의 코드를 공유합니다. 이를 공유 라이브러리(Shared Libraries)라고 합니다 (예: Windows의 DLL, Linux의 .so 파일). 디스크 공간 절약: 실행 파일 자체에는 라이브러리 코드가 포함되지 않으므로 파일 크기가 작아집니다. 쉬운 업데이트: 라이브러리가 업데이트되면, 해당 라이브러리 파일만 교체하면 됩니다. 이 라이브러리를 사용하는 프로그램들은 다시 컴파일하거나 링크할 필요 없이 새로운 버전의 라이브러리를 자동으로 사용하게 됩니다 (인터페이스가 호환되는 한). 운영체제의 역할: 동적 연결에서는 운영체제가 라이브러리 관리, 메모리 적재, 주소 매핑 등에 관여하여 프로세스들이 공유 라이브러리를 안전하고 효율적으로 사용할 수 있도록 지원합니다. 동적 적재와 동적 연결은 종종 함께 사용되어 시스템 자원의 효율성을 극대화합니다. Swapping (스와핑) A process can be swapped temporarily out of memory to a backing store, and then brought back into memory for continued execution (프로세스는 일시적으로 메모리에서 백킹 스토어로 스왑 아웃되었다가, 계속 실행하기 위해 다시 메모리로 스왑 인될 수 있습니다) Total physical memory space of processes can exceed physical memory (프로세스들의 총 물리 메모리 공간 요구량이 실제 물리 메모리를 초과할 수 있게 함) Backing store – fast disk large enough to accommodate copies of all memory images for all users; must provide direct access to these memory images (백킹 스토어 – 모든 사용자의 모든 메모리 이미지 복사본을 수용할 만큼 충분히 큰 고속 디스크; 이러한 메모리 이미지에 대한 직접 접근을 제공해야 함) Roll out, roll in – swapping variant used for priority-based scheduling algorithms; lower-priority process is swapped out so higher-priority process can be loaded and executed (롤 아웃, 롤 인 – 우선순위 기반 스케줄링 알고리즘에 사용되는 스와핑 변형; 우선순위가 낮은 프로세스가 스왑 아웃되고 우선순위가 높은 프로세스가 적재되어 실행될 수 있도록 함) Major part of swap time is transfer time; total transfer time is directly proportional to the amount of memory swapped (스왑 시간의 주요 부분은 전송 시간임; 총 전송 시간은 스왑되는 메모리 양에 정비례함) System maintains a ready queue of ready-to-run processes which have memory images on disk (시스템은 디스크에 메모리 이미지를 가진, 실행 준비가 된 프로세스들의 준비 큐를 유지함) Does the swapped out process need to swap back in to same physical addresses? (스왑 아웃된 프로세스가 동일한 물리 주소로 다시 스왑 인되어야 하는가?) Depends on address binding method (주소 바인딩 방법에 따라 다름) Plus consider pending I/O to / from process memory space (또한 프로세스 메모리 공간으로/부터의 보류 중인 입출력을 고려해야 함) Modified versions of swapping are found on many systems (i.e., UNIX, Linux, and Windows) (수정된 버전의 스와핑이 많은 시스템(예: UNIX, Linux, Windows)에서 발견됨) Swapping normally disabled (스와핑은 일반적으로 비활성화되어 있음) Started if more than threshold amount of memory allocated (임계치 이상의 메모리가 할당되면 시작됨) Disabled again once memory demand reduced below threshold (메모리 요구량이 임계치 아래로 줄어들면 다시 비활성화됨) \\[설명\\] 스와핑은 물리 메모리보다 더 많은 프로세스를 시스템에서 실행할 수 있도록 하는 고전적인 메모리 관리 기법입니다. 가상 메모리의 초기 형태로 볼 수 있습니다. 기본 개념: 메모리가 부족할 경우, 현재 메모리에 있는 프로세스 중 일부(또는 전체)를 일시적으로 디스크의 특별한 공간인 백킹 스토어(backing store)로 내보내고(swap out), 필요할 때 다시 메모리로 가져와(swap in) 실행을 계속합니다. 이를 통해 물리 메모리의 크기보다 더 큰 총량의 프로세스들이 시스템에 존재할 수 있게 됩니다. 백킹 스토어: 스왑된 프로세스들의 메모리 이미지를 저장하는 공간으로, 충분히 크고 빨라야 합니다 (보통 하드 디스크나 SSD의 특정 파티션 또는 파일). 메모리 이미지에 빠르게 직접 접근(direct access)할 수 있어야 효율적입니다. 롤 아웃, 롤 인 (Roll out, roll in): 스와핑의 한 형태로, 주로 우선순위 기반 스케줄링에서 사용됩니다. 실행 준비가 된 더 높은 우선순위의 프로세스를 위해 현재 실행 중이거나 메모리에 있는 낮은 우선순위의 프로세스를 백킹 스토어로 내보내는(롤 아웃) 방식입니다. 스왑 시간: 스와핑은 디스크 입출력을 동반하므로 시간이 많이 걸립니다. 주된 시간은 메모리 내용을 디스크로 옮기거나 디스크에서 메모리로 옮기는 전송 시간(transfer time)이며, 이는 스왑되는 메모리 양에 비례합니다. 디스크 접근 시간(탐색 시간, 회전 지연 시간)도 추가됩니다. 준비 큐: 스왑 아웃된 프로세스들은 디스크에 메모리 이미지를 가진 채로 준비 큐(ready queue)에서 자신의 차례를 기다릴 수 있습니다. CPU를 할당받기 전에 메모리로 스왑 인되어야 합니다. 재적재 주소 문제: 스왑 아웃되었다가 다시 스왑 인될 때, 반드시 원래의 물리 주소로 돌아와야 할까요? 이는 주소 바인딩 방식에 따라 다릅니다. 컴파일 시간 또는 적재 시간 바인딩: 원래의 물리 주소로 돌아와야 합니다. 그렇지 않으면 주소 참조가 모두 틀어집니다. 실행 시간 바인딩 (MMU 사용): 아무 물리 주소 위치로나 스왑 인될 수 있습니다. MMU가 논리 주소를 새로운 물리 주소로 올바르게 변환해 주기 때문입니다. 현대 시스템은 대부분 이 방식을 사용합니다. 보류 중인 입출력 (Pending I/O): 스왑하려는 프로세스가 현재 입출력 작업을 수행 중이라면 문제가 복잡해집니다. 예를 들어, DMA(Direct Memory Access)를 통해 디스크 컨트롤러가 프로세스의 메모리 영역에 직접 데이터를 쓰고 있는데 이 프로세스가 스왑 아웃되면 데이터가 엉뚱한 곳에 써지거나 유실될 수 있습니다. 해결책으로는, 입출력 작업이 완료될 때까지 해당 프로세스의 스와핑을 금지합니다. 입출력을 항상 운영체제 커널 버퍼를 통해 수행하고, 스왑 시에는 이 커널 버퍼만 주의하면 되도록 합니다. 현대 시스템에서의 스와핑: 초기 운영체제에서는 프로세스 전체를 스왑 인/아웃하는 방식을 많이 사용했지만, 현대의 Windows, Linux, UNIX 같은 시스템에서는 이보다 더 발전된 형태의 페이징 기반 가상 메모리 시스템을 사용합니다. 이러한 시스템에서도 여전히 '스와핑'이라는 용어가 사용되는데, 이는 주로 페이지 단위로 메모리와 디스크 간에 데이터가 이동하는 것을 의미합니다(페이지 아웃, 페이지 인). 시스템은 보통 메모리가 매우 부족해지는 특정 임계점(threshold)에 도달할 때까지는 스와핑(페이지 아웃)을 적극적으로 하지 않다가, 임계점을 넘어서면 스와핑을 시작하여 메모리 압박을 해소하려고 시도합니다. 메모리 여유가 다시 생기면 스와핑 빈도를 줄입니다. 스와핑은 다중 프로그래밍의 정도를 높이고 메모리를 유연하게 사용하는 데 기여하지만, 디스크 I/O로 인한 성능 저하가 크다는 단점이 있습니다. Schematic View of Swapping (스와핑의 개략도) (Diagram showing: Operating System in Main memory, User Space in Main memory with Process P1. Backing store (disk) with Process P2. Arrows indicating P1 can be swapped out to disk, and P2 can be swapped in to main memory.) \\[설명\\] 이 그림은 스와핑의 기본 개념을 간단하게 보여줍니다. 주 기억장치 (Main memory): 운영체제(OS)가 상주하고 있고, 사용자 공간(User Space)에는 현재 실행 중이거나 실행 대기 중인 프로세스 P1이 적재되어 있습니다. 백킹 스토어 (Backing store, 보통 디스크): 다른 프로세스 P2의 메모리 이미지가 저장되어 있습니다. P2는 현재 메모리에 없지만 실행될 수 있는 상태이거나, 이전에 메모리에서 스왑 아웃된 상태일 수 있습니다. 스왑 아웃 (Swap out): 화살표는 주 기억장치에 있는 프로세스 P1이 백킹 스토어로 이동될 수 있음(스왑 아웃)을 나타냅니다. 이는 P1이 잠시 실행을 멈추거나, 더 높은 우선순위의 프로세스에게 메모리를 양보해야 할 때 발생할 수 있습니다. 스왑 인 (Swap in): 다른 화살표는 백킹 스토어에 있는 프로세스 P2가 주 기억장치로 이동될 수 있음(스왑 인)을 나타냅니다. 이는 P2가 실행될 차례가 되었거나, P1이 스왑 아웃되어 생긴 빈 공간을 활용하기 위함일 수 있습니다. 이러한 스왑 인/아웃 과정을 통해 제한된 주 기억장치를 여러 프로세스가 시간적으로 공유하여 사용할 수 있게 됩니다. Context Switch Time including Swapping (스와핑을 포함한 문맥 교환 시간) If next processes to be put on CPU is not in memory, need to swap out a process and swap in target process (CPU에 다음에 놓일 프로세스가 메모리에 없다면, 한 프로세스를 스왑 아웃하고 대상 프로세스를 스왑 인해야 합니다) Context switch time can then be very high (그러면 문맥 교환 시간이 매우 길어질 수 있습니다) 100MB process swapping to hard disk with transfer rate of 50MB/sec (50MB/초의 전송률을 가진 하드 디스크로 100MB 프로세스 스와핑) Plus disk latency of 8 ms (디스크 지연 시간 8ms 추가) Swap out time of 2008 ms (스왑 아웃 시간 2008ms) Plus swap in of same sized process (동일 크기 프로세스의 스왑 인 추가) Total context switch swapping component time of 4016ms (> 4 seconds) (총 문맥 교환 스와핑 요소 시간 4016ms (> 4초)) Can reduce if reduce size of memory swapped – by knowing how much memory really being used (실제로 사용되는 메모리 양을 앎으로써 스왑되는 메모리 크기를 줄이면 감소 가능) System calls to inform OS of memory use via request memory and release memory (메모리 요청 및 해제 시스템 호출을 통해 OS에 메모리 사용량을 알림) \\[설명\\] 스와핑이 문맥 교환(context switch) 시간에 미치는 영향을 설명합니다. 문맥 교환은 CPU가 한 프로세스에서 다른 프로세스로 실행을 전환하는 과정입니다. 스와핑과 문맥 교환: 만약 다음으로 실행될 프로세스가 현재 메모리에 없고 백킹 스토어에 있다면, 문맥 교환의 일부로 스와핑 작업이 필요하게 됩니다. 즉, 현재 메모리에 있는 어떤 프로세스를 디스크로 스왑 아웃하고 (공간 확보를 위해), 디스크에 있는 다음 실행 프로세스를 메모리로 스왑 인해야 합니다. 매우 높은 문맥 교환 비용: 스와핑은 디스크 I/O를 포함하므로 매우 느립니다. 따라서 스와핑이 필요한 문맥 교환은 그렇지 않은 경우보다 훨씬 더 많은 시간이 소요됩니다. 예시 계산: 프로세스 크기: 100MB 디스크 전송률: 50MB/초 디스크 지연 시간 (latency - 탐색 시간 + 회전 지연 시간 등): 8ms 스왑 아웃 시간: 전송 시간 = 100MB / 50MB/초 = 2초 = 2000ms 총 스왑 아웃 시간 = 전송 시간 + 지연 시간 = 2000ms + 8ms = 2008ms 스왑 인 시간: 동일 크기 프로세스를 스왑 인하는 데도 유사한 시간(2008ms)이 소요됩니다. 총 스와핑 관련 문맥 교환 시간: 스왑 아웃 시간 + 스왑 인 시간 = 2008ms + 2008ms = 4016ms, 즉 4초가 넘습니다. 이는 일반적인 CPU 작업에 비해 엄청나게 긴 시간입니다. CPU는 이 시간 동안 거의 아무 일도 못하고 기다리게 됩니다. 스왑 크기 줄이기: 스왑되는 데이터의 양을 줄이면 스왑 시간을 단축할 수 있습니다. 프로세스가 할당받은 전체 메모리 영역이 아니라, 그중에서도 실제로 사용하고 있는 부분만 스왑 대상으로 삼으면 됩니다. (예: 프로그램 코드 중에서도 자주 안 쓰이는 부분, 데이터 중에서도 현재 접근하지 않는 부분 등) 운영체제는 프로세스가 실제로 어느 정도의 메모리를 사용하는지 파악할 필요가 있습니다. 이를 위해 프로세스가 request_memory() (메모리 더 필요) 또는 release_memory() (이 메모리 더 이상 안 씀) 같은 시스템 호출을 통해 운영체제에 메모리 사용 현황을 알려줄 수 있습니다. (현대 가상 메모리 시스템에서는 페이지 단위로 사용 여부를 추적하여 더 정교하게 관리합니다.) 결론적으로, 프로세스 전체를 스왑하는 방식은 비용이 매우 크므로, 현대 시스템에서는 필요한 부분(페이지)만 스왑하는 페이징 기법을 주로 사용하며, 스와핑 발생 자체를 최소화하려고 노력합니다. Contiguous Allocation (연속 할당) Main memory usually into two partitions: (주 기억장치는 대개 두 개의 파티션으로 나뉨:) Resident operating system, usually held in low memory with interrupt vector (상주 운영체제, 보통 인터럽트 벡터와 함께 낮은 메모리 주소에 위치) User processes then held in high memory (사용자 프로세스들은 그 후 높은 메모리 주소에 위치) Each process contained in single contiguous section of memory (각 프로세스는 메모리의 단일 연속된 구역에 포함됨) Relocation registers used to protect user processes from each other, and from changing operating-system code and data (재배치 레지스터는 사용자 프로세스들을 서로로부터, 그리고 운영체제 코드 및 데이터 변경으로부터 보호하는 데 사용됨) Base register contains value of smallest physical address (기준 레지스터는 가장 작은 물리 주소값을 가짐) Limit register contains range of logical addresses – each logical address must be less than the limit register (한계 레지스터는 논리 주소의 범위를 가짐 – 각 논리 주소는 한계 레지스터보다 작아야 함) MMU maps logical address dynamically (MMU가 논리 주소를 동적으로 매핑함) Can then allow actions such as kernel code being transient and kernel changing size (그러면 커널 코드의 일부가 일시적이 되거나 커널 크기가 변경되는 등의 동작을 허용할 수 있음) \\[설명\\] 연속 할당은 가장 단순한 메모리 할당 방식 중 하나로, 각 프로세스가 메모리의 한 군데에 연속적인 공간을 할당받는 것을 의미합니다. 고정 분할과 동적 분할 모두 이 범주에 속합니다. 메모리 구조: 주 기억장치는 일반적으로 두 부분으로 나뉩니다. 운영체제 영역: 주로 낮은 주소 영역(low memory)을 차지하며, 시스템 부팅 및 운영에 필수적인 커널 코드, 데이터, 인터럽트 벡터 테이블 등이 위치합니다. 이 부분은 사용자 프로세스로부터 보호됩니다. 사용자 프로세스 영역: 운영체제 영역 다음의 높은 주소 영역(high memory)에 사용자 프로세스들이 적재됩니다. 단일 연속 구역: 각 사용자 프로세스는 메모리 내에서 물리적으로 연속된 하나의 덩어리(block)로 적재됩니다. 예를 들어, 100KB 크기의 프로세스는 100KB의 연속된 메모리 공간을 차지하며, 여러 조각으로 나뉘어 저장되지 않습니다. 보호 및 재배치를 위한 레지스터: 재배치 레지스터 (Relocation Register, 또는 기준 레지스터 Base Register): 프로세스가 적재된 물리 메모리의 시작 주소를 저장합니다. CPU가 생성하는 논리 주소에 이 값을 더하여 물리 주소를 얻습니다. 이를 통해 프로세스를 메모리의 어느 위치에든 적재(재배치)할 수 있게 됩니다. 한계 레지스터 (Limit Register): 프로세스의 논리 주소 공간의 크기(길이)를 저장합니다. CPU가 생성하는 논리 주소는 이 한계 레지스터 값보다 작아야 합니다. 이를 통해 프로세스가 자신에게 할당된 메모리 영역을 벗어나 다른 프로세스나 운영체제의 영역을 침범하는 것을 방지합니다. MMU의 역할: 이러한 재배치 및 한계 검사는 MMU(Memory Management Unit)라는 하드웨어에 의해 실행 시간에 동적으로 이루어집니다. 커널 유연성: 이러한 하드웨어 지원(MMU, 재배치/한계 레지스터)은 운영체제 커널 자체의 관리에도 유연성을 제공합니다. 예를 들어, 커널의 일부 모듈을 필요에 따라 메모리에 적재하거나 내릴 수 있게 하고(transient kernel code), 시스템 실행 중에 커널이 사용하는 메모리 크기를 동적으로 변경하는 것도 가능하게 합니다. 연속 할당은 구현이 비교적 간단하지만, 앞서 논의된 고정 분할의 내부 단편화 문제나 동적 분할의 외부 단편화 문제를 야기할 수 있습니다. 이러한 문제를 해결하기 위해 비연속 할당 방식인 페이징과 세그먼테이션이 등장하게 됩니다. Hardware Support for Relocation and Limit Registers (재배치 및 한계 레지스터를 위한 하드웨어 지원) (Diagram very similar to 8.22: CPU -> logical address -> [check: logical address < limit register?] -> yes -> [physical address = logical address + relocation register] -> memory; no -> trap to OS) \\[설명\\] 이 그림은 재배치(Relocation) 레지스터와 한계(Limit) 레지스터를 사용한 하드웨어 기반 주소 변환 및 보호 메커니즘을 다시 한번 보여줍니다. 슬라이드 8.22의 그림과 거의 동일한 개념입니다. CPU가 논리 주소(Logical Address) 생성: 프로세스 내에서 사용되는 주소로, 보통 0부터 시작합니다. 한계 검사 (Limit Check): 생성된 논리 주소가 한계 레지스터(Limit Register)에 저장된 값보다 작은지 비교합니다 ( Logical Address < Limit Register Value ). 한계 레지스터는 해당 프로세스의 크기를 나타내므로, 이 검사는 논리 주소가 프로세스에게 할당된 주소 범위를 벗어나는지 확인하는 것입니다. 만약 논리 주소가 한계 레지스터 값보다 크거나 같다면 (No 경로), 이는 유효하지 않은 접근이므로 트랩(trap)이 발생하여 운영체제에게 제어권이 넘어갑니다. (예: \"segmentation fault\" 또는 \"access violation\" 오류) 물리 주소 계산 (Physical Address Calculation): 논리 주소가 유효 범위 내에 있다면 (Yes 경로), 이 논리 주소에 재배치 레지스터(Relocation Register, 또는 Base Register)의 값을 더하여 물리 주소(Physical Address)를 계산합니다. Physical Address = Logical Address + Relocation Register Value 재배치 레지스터에는 해당 프로세스가 물리 메모리에 적재된 시작 주소가 들어있습니다. 메모리 접근: 계산된 물리 주소를 사용하여 실제 메모리에 접근합니다. 이러한 하드웨어 메커니즘은 모든 메모리 접근 시 자동으로 수행되며, 각 프로세스가 자신만의 격리된 메모리 공간을 안전하게 사용하도록 보장하고, 프로그램의 재배치를 용이하게 합니다. 이는 실행 시간 주소 바인딩의 핵심 요소입니다. Contiguous Allocation (Cont.) (연속 할당 (계속)) Multiple-partition allocation (다중 파티션 할당) Degree of multiprogramming limited by number of partitions (다중 프로그래밍의 정도는 파티션 수에 의해 제한됨) Hole – block of available memory; holes of various size are scattered throughout memory (홀 – 사용 가능한 메모리 블록; 다양한 크기의 홀들이 메모리 전체에 흩어져 있음) When a process arrives, it is allocated memory from a hole large enough to accommodate it (프로세스가 도착하면, 그것을 수용할 만큼 충분히 큰 홀로부터 메모리를 할당받음) Process exiting frees its partition, adjacent free partitions combined (프로세스가 종료되면 그 파티션을 반환하고, 인접한 가용 파티션들은 합쳐짐) Operating system maintains information about: (운영체제는 다음에 대한 정보를 유지함:) a) allocated partitions (할당된 파티션들) b) free partitions (hole) (가용 파티션들 (홀)) (Diagram showing OS, process 5, process 8, process 2 initially. Then process 8 exits, leaving a hole. Then process 9 arrives and is allocated. Then process 5 exits. Then process 10 arrives and is allocated in the hole left by process 5 or another hole.) \\[설명\\] 이 슬라이드는 연속 할당 방식 중에서도 주로 동적 분할(Dynamic Partitioning)에서의 다중 파티션 할당 상황을 설명하고 있습니다. (고정 분할도 다중 파티션이지만, 여기서 설명하는 'hole'의 생성과 관리, 인접 홀 병합 등은 동적 분할의 특징에 더 가깝습니다.) 다중 파티션 할당: 메모리가 여러 개의 파티션으로 나뉘어 여러 프로세스를 동시에 적재하는 방식입니다. 다중 프로그래밍 정도 제한: (고정 분할의 경우) 파티션의 개수가 동시에 실행될 수 있는 프로세스의 최대 수를 결정합니다. 동적 분할의 경우에도 사용 가능한 메모리 총량과 외부 단편화 정도에 따라 제한됩니다. 홀 (Hole): 메모리 내에서 현재 사용되지 않고 비어 있는 가용 메모리 블록을 '홀'이라고 부릅니다. 동적 분할에서는 프로세스들이 할당되고 해제됨에 따라 다양한 크기의 홀들이 메모리 전체에 흩어져 나타납니다. 프로세스 도착 시 할당: 새로운 프로세스가 시스템에 도착하면, 운영체제는 현재 있는 홀들 중에서 해당 프로세스를 수용할 수 있을 만큼 충분히 큰 홀을 찾아 메모리를 할당합니다. (이때 First-fit, Best-fit, Worst-fit 등의 할당 전략이 사용됩니다.) 프로세스 종료 시 반환 및 병합: 프로세스가 실행을 마치고 종료되면, 그 프로세스가 사용하던 메모리 공간은 다시 가용 상태(홀)가 됩니다. 만약 새로 생긴 이 홀이 기존의 다른 홀(들)과 물리적으로 인접해 있다면, 이들은 하나의 더 큰 홀로 합병(merge 또는 coalesce)됩니다. 이는 작은 홀들이 너무 많이 생기는 것을 방지하여 외부 단편화를 줄이는 데 도움이 됩니다. 운영체제의 정보 관리: 운영체제는 메모리 관리를 위해 현재 어떤 부분들이 이미 할당된 파티션인지, 그리고 어떤 부분들이 가용 상태인 홀인지에 대한 정보를 리스트나 테이블 형태로 계속 추적하고 유지해야 합니다. (예: 각 블록의 시작 주소, 크기, 상태(할당/가용) 등) 다이어그램 설명: 그림은 시간의 흐름에 따른 메모리 상태 변화를 보여줍니다. 초기: OS, P5, P8, P2가 메모리에 적재되어 있습니다. P8 종료: P8이 있던 자리가 홀로 변합니다. (OS, P5, [hole1], P2) P9 도착 및 할당: P9이 hole1 또는 다른 적절한 홀에 할당됩니다 (그림에서는 P8 자리에 P9이 들어감). (OS, P5, P9, P2) P5 종료: P5가 있던 자리가 홀로 변합니다. ([hole2], P9, P2) (OS는 생략된 듯) P10 도착 및 할당: P10이 P5가 남긴 hole2 또는 다른 홀에 할당됩니다. (P10, P9, P2) 이러한 과정이 반복되면서 메모리에는 사용 중인 프로세스들과 다양한 크기의 홀들이 섞여 존재하게 되며, 외부 단편화가 발생할 가능성이 커집니다. Dynamic Storage-Allocation Problem (동적 저장 공간 할당 문제) How to satisfy a request of size n from a list of free holes? (크기 n의 요청을 가용 홀 리스트로부터 어떻게 만족시킬 것인가?) First-fit: Allocate the first hole that is big enough (충분히 큰 첫 번째 가용 홀을 할당) Best-fit: Allocate the smallest hole that is big enough; must search entire list, unless ordered by size (충분히 큰 가장 작은 가용 홀을 할당; 크기 순으로 정렬되어 있지 않다면 전체 리스트를 탐색해야 함) Produces the smallest leftover hole (가장 작은 남은 홀을 생성) Worst-fit: Allocate the largest hole; must also search entire list (가장 큰 가용 홀을 할당; 역시 전체 리스트를 탐색해야 함) Produces the largest leftover hole (가장 큰 남은 홀을 생성) First-fit and best-fit better than worst-fit in terms of speed and storage utilization (최초 적합과 최적 적합이 속도와 저장 공간 활용 측면에서 최악 적합보다 우수합니다) \\[설명\\] 이 슬라이드는 슬라이드 8.12에서 설명했던 동적 분할에서의 할당 전략(First-fit, Best-fit, Worst-fit)을 다시 한번 반복하고 있습니다. 이는 동적 저장 공간 할당 문제의 핵심적인 부분이기 때문입니다. 문제 정의: 현재 메모리에는 여러 개의 가용 공간(홀)들이 흩어져 있습니다. 새로운 프로세스가 크기 n 의 메모리를 요청할 때, 이 가용 홀들 중 어느 것을 선택하여 할당할 것인가 하는 문제입니다. First-fit (최초 적합): 홀 리스트를 순서대로 검색하여 n 보다 크거나 같은 첫 번째 홀을 선택합니다. 장점: 검색 시간이 비교적 빠를 수 있습니다 (평균적으로 리스트의 절반만 검색). 구현이 간단합니다. 단점: 큰 홀들이 앞부분에 있을 경우 작은 요청에 의해 잘게 나뉘어, 나중에 큰 요청을 처리하지 못할 수 있습니다. Best-fit (최적 적합): 홀 리스트 전체를 검색하여 n 보다 크거나 같으면서 그 크기 차이가 가장 작은 홀(즉, hole_size - n 이 최소가 되는 홀)을 선택합니다. 장점: 요청 크기에 가장 근접한 홀을 사용하므로, 할당 후 남는 조각(leftover hole)의 크기가 최소화됩니다. 이는 매우 작은, 쓸모없는 홀들이 생기는 것을 줄여 메모리 활용률을 높일 수 있다는 기대를 줍니다. 단점: 항상 전체 홀 리스트를 검색해야 하므로 검색 시간이 오래 걸립니다 (리스트가 크기순으로 정렬되어 있다면 개선 가능). 또한, 매우 작은 홀들이 많이 생성되어 오히려 단편화를 악화시킬 수도 있습니다. Worst-fit (최악 적합): 홀 리스트 전체를 검색하여 n 보다 크거나 같은 홀 중에서 가장 큰 홀을 선택합니다. 장점 (이론적): 큰 홀을 사용하고 남은 조각도 상대적으로 크기 때문에, 그 남은 조각이 다른 중간 크기의 프로세스를 수용할 가능성을 높이려는 의도입니다. 단점: 항상 전체 홀 리스트를 검색해야 합니다. 큰 가용 공간이 빨리 소모되어, 정작 매우 큰 프로세스가 필요할 때 할당 가능한 큰 홀이 남아있지 않을 수 있습니다. 실험 결과, 일반적으로 성능이 좋지 않은 것으로 알려져 있습니다. 일반적인 평가: 시뮬레이션 결과와 실제 구현 경험에 따르면, 최초 적합(First-fit)과 최적 적합(Best-fit)이 최악 적합(Worst-fit)보다 평균적인 검색 속도나 메모리 단편화 관리(storage utilization) 측면에서 더 나은 성능을 보입니다. 최초 적합은 구현의 단순성과 괜찮은 평균 성능 때문에 널리 사용됩니다. 최적 적합은 검색 비용이 더 들지만 때때로 메모리 활용도를 약간 더 높일 수 있습니다. Fragmentation (단편화) External Fragmentation – total memory space exists to satisfy a request, but it is not contiguous (외부 단편화 – 요청을 만족시키기에 충분한 총 메모리 공간이 존재하지만, 연속적이지 않음) Internal Fragmentation – allocated memory may be slightly larger than requested memory; this size difference is memory internal to a partition, but not being used (내부 단편화 – 할당된 메모리가 요청된 메모리보다 약간 클 수 있음; 이 크기 차이는 파티션 내부의 메모리이지만 사용되지 않음) First fit analysis reveals that given N blocks allocated, 0.5 N blocks lost to fragmentation (최초 적합 분석에 따르면, N개의 블록이 할당되었을 때 0.5N 개의 블록이 단편화로 손실됨) 1/3 may be unusable -> 50-percent rule (1/3이 사용 불가능할 수 있음 → 50% 규칙) \\[설명\\] 이 슬라이드는 슬라이드 8.16의 내용을 다시 한번 강조하며, 단편화의 종류와 그 영향을 설명합니다. 외부 단편화 (External Fragmentation): 발생 원인: 동적 할당 방식에서 프로세스들이 메모리에 할당되고 해제되는 과정이 반복되면서, 사용 중인 메모리 블록들 사이에 작은 크기의 가용 공간(홀)들이 흩어져 생기는 현상입니다. 문제점: 이 작은 홀들의 크기를 모두 합하면 새로운 프로세스를 수용하기에 충분한 공간이 될 수 있지만, 각각의 홀은 너무 작아서 실제로 할당할 수 없는 상태입니다. 즉, 메모리가 '외부적으로' 잘게 조각나 있는 것입니다. 내부 단편화 (Internal Fragmentation): 발생 원인: 고정 분할 방식이나, 할당 단위가 정해져 있는 경우 (예: 페이징, 버디 시스템) 발생합니다. 프로세스에 할당된 메모리 블록(파티션 또는 페이지)의 크기가 실제 프로세스가 필요로 하는 크기보다 약간 클 때, 그 차이만큼의 공간이 해당 블록 '내부에서' 사용되지 않고 낭비되는 현상입니다. 예: 100KB 크기의 고정 파티션에 80KB 프로세스를 할당하면 20KB의 내부 단편화 발생. 4KB 페이지에 1KB의 데이터만 저장하면 3KB의 내부 단편화 발생. 최초 적합(First-fit)과 50% 규칙 (50-percent Rule): 이론적, 실험적 분석에 따르면, 최초 적합 할당 전략을 오랜 시간 사용했을 때, 평균적으로 할당된 블록 N개당 약 0.5N개의 추가적인 가용 블록(단편화된 홀)이 발생한다는 경험적 규칙입니다. 이는 곧 전체 메모리 중 약 1/3 정도가 외부 단편화로 인해 사용 불가능한 상태가 될 수 있음을 의미합니다 (메모리 사용률이 약 2/3). 이는 매우 일반화된 규칙이며, 실제 상황에서는 작업 부하(workload)의 특성에 따라 달라질 수 있습니다. 단편화는 메모리 관리 시스템의 효율성을 저해하는 주요 요인이므로, 이를 줄이거나 해결하기 위한 다양한 기법(압축, 페이징, 세그먼테이션 등)이 연구되고 사용됩니다. Fragmentation (Cont.) (단편화 (계속)) Reduce external fragmentation by compaction (압축을 통해 외부 단편화를 줄임) Shuffle memory contents to place all free memory together in one large block (모든 가용 메모리를 하나의 큰 블록으로 모으기 위해 메모리 내용을 재배치함) Compaction is possible only if relocation is dynamic, and is done at execution time (압축은 재배치가 동적이고 실행 시간에 수행될 경우에만 가능함) I/O problem (입출력 문제) → Latch job in memory while it is involved in I/O (입출력에 관여하는 동안 작업을 메모리에 고정시킴) → Do I/O only into OS buffers (입출력을 OS 버퍼로만 수행함) Now consider that backing store has same fragmentation problems (이제 백킹 스토어도 동일한 단편화 문제를 가짐을 고려하십시오) \\[설명\\] 외부 단편화 문제에 대한 해결책 중 하나인 압축(Compaction)과 그와 관련된 문제점, 그리고 백킹 스토어의 단편화 문제를 언급합니다. 압축 (Compaction)을 통한 외부 단편화 감소: 방법: 메모리 압축은 운영체제가 현재 메모리에 흩어져 있는 모든 사용 중인 프로세스들을 한쪽으로 이동시켜 연속적으로 배치하고, 그 결과로 흩어져 있던 작은 가용 공간(홀)들을 하나의 큰 연속된 가용 공간으로 통합하는 작업입니다. 예: [P1] [hole1] [P2] [hole2] [P3] 상태를 [P1] [P2] [P3] [merged_hole] 상태로 만듭니다. 조건: 압축을 하려면 프로세스의 메모리 주소가 실행 중에 변경될 수 있어야 합니다. 이는 주소 바인딩이 실행 시간(execution time)에 동적으로 이루어질 때만 가능합니다. 즉, MMU와 같은 하드웨어 지원으로 논리 주소와 물리 주소가 분리되어 관리되어야 합니다. 컴파일 시간이나 적재 시간 바인딩에서는 압축이 불가능하거나 매우 어렵습니다. 압축의 입출력 문제 (I/O Problem): 프로세스가 메모리 내의 특정 버퍼를 사용하여 디스크 등과 입출력(I/O) 작업을 수행 중일 때, 그 프로세스를 이동시키면 문제가 발생합니다. 입출력 장치(예: DMA 컨트롤러)는 이전의 물리 주소로 계속 데이터를 전송하거나 읽으려고 할 것이기 때문입니다. 해결 방안: 입출력 중인 작업 메모리 고정 (Latch job in memory): 특정 프로세스가 입출력 작업을 완료할 때까지는 해당 프로세스를 메모리에서 이동시키지 않고 고정(pinning)합니다. 이 프로세스는 압축 대상에서 제외됩니다. OS 버퍼를 통한 입출력: 모든 입출력 연산은 사용자 프로세스 공간이 아닌, 운영체제가 관리하는 고정된 버퍼(OS buffers)를 통해서만 이루어지도록 합니다. 데이터는 먼저 OS 버퍼로 전송된 후, 사용자 프로세스 공간으로 복사되거나 그 반대로 이루어집니다. 이렇게 하면 사용자 프로세스가 이동하더라도 OS 버퍼의 주소는 변하지 않으므로 입출력에 문제가 없습니다. 다만, 데이터 복사로 인한 오버헤드가 추가됩니다. 백킹 스토어의 단편화 (Fragmentation on Backing Store): 스와핑이나 페이징에 사용되는 백킹 스토어(주로 디스크)도 메모리와 유사하게 단편화 문제를 겪을 수 있습니다. 프로세스나 페이지들이 디스크에 저장되었다가 삭제되는 과정이 반복되면, 디스크 상에도 사용 중인 공간과 빈 공간(hole)들이 흩어져 존재하게 됩니다. 이로 인해 큰 프로세스 이미지나 연속된 페이지들을 저장할 공간을 찾기 어려워지거나, 파일 시스템의 성능이 저하될 수 있습니다. 디스크 조각 모음(defragmentation)과 같은 유틸리티가 이러한 백킹 스토어의 단편화를 줄이는 데 사용될 수 있습니다. 압축은 외부 단편화를 해결하는 효과적인 방법일 수 있지만, 상당한 시스템 오버헤드(시간 소모, CPU 사용)를 유발하므로 자주 수행하기는 어렵습니다. 따라서 압축의 필요성을 줄이는 다른 메모리 관리 기법(페이징 등)이 더 선호됩니다. 페이징 (Paging) 페이징은 운영체제의 메모리 관리 기법 중 하나로, 프로세스를 일정한 크기의 작은 조각인 페이지(page)로 나누고, 물리 메모리 역시 동일한 크기의 프레임(frame)으로 나누어, 각 페이지를 비어있는 어떤 프레임에도 불연속적으로(non-contiguously) 할당할 수 있도록 하는 방식입니다. 이를 통해 이전의 연속 할당 방식(고정 분할, 동적 분할)에서 발생했던 외부 단편화 문제를 해결하고 메모리 사용의 유연성과 효율성을 높이는 것을 목표로 합니다. Goal (목표) No external fragmentation problem (외부 단편화 문제 없음) Efficient memory sharing (효율적인 메모리 공유) Flexible memory use (유연한 메모리 사용) Idea (아이디어) Divide a process into multiple fragments (프로세스를 다수의 조각으로 분할) Allocation each fragment anywhere (각 조각을 어느 곳에나 할당) Maintain where the fragments are (조각들이 어디에 있는지 유지/관리) \\[한글 번역 및 상세 설명\\] 목표 외부 단편화 문제 해결: 페이징의 가장 중요한 목표 중 하나입니다. 동적 분할 방식에서는 프로세스들이 메모리에 할당되고 해제되면서 사용 가능한 메모리 공간들이 작은 조각(hole)들로 흩어져, 총 가용 공간은 충분함에도 불구하고 큰 프로세스를 할당할 수 없는 외부 단편화가 발생했습니다. 페이징은 프로세스를 작은 페이지 단위로 나누고, 이 페이지들을 물리 메모리의 비어있는 어떤 프레임에든 배치할 수 있으므로, 가용 프레임이 N개 있다면 정확히 N개의 페이지를 수용할 수 있습니다. 즉, 메모리 공간이 조각나서 낭비되는 외부 단편화가 발생하지 않습니다. 👍 효율적인 메모리 공유: 동일한 프로그램을 여러 프로세스가 실행할 때, 프로그램의 코드 부분(텍스트 세그먼트)은 변하지 않으므로 공유될 수 있습니다. 페이징 환경에서는 이 공유 코드 페이지들을 각 프로세스의 논리 주소 공간에 매핑하되, 물리 메모리에는 단 하나의 복사본만 올려놓고 여러 프로세스가 해당 프레임을 공유하도록 할 수 있습니다. 이는 메모리 사용량을 크게 절약합니다. 예를 들어, 텍스트 편집기 프로그램을 10개의 프로세스가 동시에 사용한다면, 코드 부분은 메모리에 단 한 벌만 존재하고 모든 프로세스가 이를 공유하게 됩니다. 각 프로세스는 자신만의 데이터 페이지만 별도로 가집니다. 유연한 메모리 사용: 프로세스를 실행하기 위해 필요한 메모리 공간이 물리적으로 연속될 필요가 없습니다. 덕분에 메모리의 빈 공간을 효율적으로 활용할 수 있으며, 프로세스 크기가 물리 메모리보다 큰 경우에도 가상 메모리 기법과 결합하여 프로그램을 실행할 수 있는 기반을 제공합니다. (가상 메모리는 이 슬라이드 범위 밖이지만, 페이징은 가상 메모리 구현의 핵심 기술입니다.) 아이디어 프로세스를 다수의 조각으로 분할: 페이징의 핵심 아이디어는 프로세스의 논리 주소 공간을 페이지(page)라고 하는 고정된 크기(예: 4KB, 8KB)의 여러 조각으로 나누는 것입니다. 각 조각을 어느 곳에나 할당: 이렇게 나누어진 각 페이지는 물리 메모리의 프레임(frame)이라고 불리는, 페이지와 동일한 크기의 빈 공간 중 어느 곳에든 위치할 수 있습니다. 중요한 것은 이 프레임들이 서로 연속적일 필요가 없다는 점입니다. 예를 들어, 한 프로세스의 페이지 0은 프레임 5에, 페이지 1은 프레임 12에, 페이지 2는 프레임 2에 저장될 수 있습니다. 조각들이 어디에 있는지 유지/관리: 각 페이지가 물리 메모리의 어느 프레임에 저장되어 있는지를 운영체제가 정확히 알고 있어야 합니다. 이 매핑 정보를 저장하고 관리하는 자료구조가 바로 페이지 테이블(page table)입니다. 각 프로세스는 자신만의 페이지 테이블을 가집니다. Paging (페이징) Partition physical memory into equal size frames (물리 메모리를 동일한 크기의 프레임으로 분할) \\[한글 번역 및 상세 설명\\] 페이징 시스템에서 물리 메모리(RAM)는 프레임(frame)이라고 불리는 여러 개의 고정된 크기의 블록으로 나뉩니다. 물리 메모리 분할: 시스템에 설치된 전체 물리 메모리가 대상입니다. 예를 들어 1GB의 RAM이 있다면, 이 1GB 전체가 프레임들로 나누어집니다. 동일 크기: 모든 프레임의 크기는 같습니다. 이 크기는 페이지의 크기와 정확히 일치하며, 보통 2의 거듭제곱(예: 512바이트, 1KB, 4KB, 2MB 등)으로 정해집니다. 4KB가 현대 시스템에서 흔히 사용되는 크기입니다. 프레임의 역할: 프레임은 프로세스의 페이지가 실제로 저장되는 물리적인 공간 단위입니다. 운영체제는 어떤 프레임이 사용 중이고 어떤 프레임이 비어있는지(가용 프레임 리스트)를 관리합니다. Partition physical memory into equal size frames (물리 메모리를 동일한 크기의 프레임으로 분할) Divide logical memory into same-size pages (논리 메모리를 동일한 크기의 페이지로 분할) \\[한글 번역 및 상세 설명\\] 물리 메모리가 프레임으로 나뉘는 것처럼, 각 프로세스의 논리 주소 공간(logical memory) 역시 페이지(page)라고 하는 블록으로 나뉩니다. 논리 메모리 분할: 프로세스가 실행될 때 가지는 가상적인 주소 공간입니다. 예를 들어 32비트 시스템에서 각 프로세스는 최대 4GB의 논리 주소 공간을 가질 수 있습니다. 이 논리 주소 공간이 페이지들로 나누어집니다. 동일 크기: 페이지의 크기는 프레임의 크기와 정확히 같습니다. 이는 페이지와 프레임 간의 1:1 매핑을 단순화합니다. 만약 프레임 크기가 4KB라면, 페이지 크기도 4KB입니다. 페이지의 역할: 페이지는 프로세스가 사용하는 논리적인 메모리 단위입니다. CPU가 생성하는 주소(논리 주소)는 특정 페이지의 특정 위치를 가리키게 됩니다. Partition physical memory into equal size frames (물리 메모리를 동일한 크기의 프레임으로 분할) Divide logical memory into same-size pages (논리 메모리를 동일한 크기의 페이지로 분할) Each page can go to any free frame (각 페이지는 어떤 가용 프레임으로든 갈 수 있음) \\[한글 번역 및 상세 설명\\] 이것이 페이징의 핵심적인 유연성을 보여주는 부분입니다. 비연속적 할당: 한 프로세스를 구성하는 여러 페이지들은 물리 메모리 상에서 연속적으로 위치할 필요가 없습니다. 예를 들어, 프로세스 A의 페이지 0은 프레임 10에, 페이지 1은 프레임 3에, 페이지 2는 프레임 25에 저장될 수 있습니다. 가용 프레임 활용: 운영체제는 비어있는 프레임(free frame)의 목록을 유지하고 있다가, 새로운 페이지를 메모리에 적재해야 할 때 이 목록에서 임의의 가용 프레임을 선택하여 할당합니다. 외부 단편화 해결: 이 방식 덕분에, 물리 메모리에 흩어져 있는 작은 빈 공간(프레임 단위)들도 효과적으로 사용할 수 있게 되어 외부 단편화 문제가 발생하지 않습니다. Partition physical memory into equal size frames (물리 메모리를 동일한 크기의 프레임으로 분할) Divide logical memory into same-size pages (논리 메모리를 동일한 크기의 페이지로 분할) Each page can go to any free frame (각 페이지는 어떤 가용 프레임으로든 갈 수 있음) OS knows the mapping (운영체제는 매핑을 알고 있음) page table (페이지 테이블) \\[한글 번역 및 상세 설명\\] 프로세스의 페이지들이 물리 메모리의 프레임들에 흩어져 저장될 수 있기 때문에, 운영체제는 각 논리 페이지가 어느 물리 프레임에 저장되어 있는지를 정확히 추적해야 합니다. 매핑 정보: 논리적인 페이지 번호(page number)와 이 페이지가 실제 저장된 물리적인 프레임 번호(frame number) 간의 대응 관계를 매핑(mapping)이라고 합니다. 페이지 테이블 (Page Table): 이 매핑 정보를 저장하는 자료구조가 바로 페이지 테이블입니다. 각 프로세스는 자신만의 페이지 테이블을 가집니다. 페이지 테이블은 일반적으로 배열 형태로 구현되며, 배열의 인덱스는 페이지 번호에 해당하고, 해당 인덱스에 저장된 값은 그 페이지가 적재된 프레임 번호입니다. 예를 들어, 페이지 테이블[i] = j 라면, i번 페이지는 j번 프레임에 있다는 의미입니다. 페이지 테이블에는 프레임 번호 외에도 페이지의 유효/무효 비트(valid/invalid bit), 접근 권한 비트(read/write/execute), 수정 여부 비트(dirty bit) 등 다양한 부가 정보가 포함될 수 있습니다. CPU가 논리 주소를 참조할 때, 이 페이지 테이블을 사용하여 해당 논리 주소가 속한 페이지가 어느 프레임에 있는지 알아내고, 이를 통해 최종 물리 주소를 계산합니다. Physical address space of a process can be noncontiguous; process is allocated physical memory whenever the latter is available (프로세스의 물리 주소 공간은 비연속적일 수 있음; 프로세스는 물리 메모리가 가용할 때마다 할당받음) Divide physical memory into fixed-sized blocks called frames (물리 메모리를 프레임이라고 불리는 고정 크기 블록으로 분할) Size is power of 2, between 512 bytes and 16 Mbytes (크기는 2의 거듭제곱이며, 512 바이트에서 16 메가바이트 사이임) Divide logical memory into blocks of same size called pages (논리 메모리를 페이지라고 불리는 동일 크기의 블록으로 분할) Keep track of all free frames (모든 가용 프레임을 추적함) To run a program of size N pages, need to find N free frames and load program (크기가 N 페이지인 프로그램을 실행하려면, N개의 가용 프레임을 찾아 프로그램을 적재해야 함) Set up a page table to translate logical to physical addresses (논리 주소를 물리 주소로 변환하기 위해 페이지 테이블을 설정함) Backing store likewise split into pages (백킹 스토어(보조 기억 장치)도 마찬가지로 페이지 단위로 분할됨) Still have Internal fragmentation (여전히 내부 단편화는 존재함) \\[한글 번역 및 상세 설명\\] 이 슬라이드는 페이징의 주요 특징과 동작 방식을 요약합니다. 프로세스의 비연속적 물리 주소 공간: 페이징을 사용하면 프로세스가 사용하는 물리 메모리 영역이 한 곳에 모여 있을 필요가 없습니다. 페이지 단위로 흩어져 있는 여러 프레임에 분산되어 저장될 수 있습니다. 이는 메모리가 가용할 때마다(즉, 빈 프레임이 발견될 때마다) 그 프레임에 페이지를 할당할 수 있기 때문입니다. 프레임 (Frames): 물리 메모리는 프레임이라는 고정된 크기의 블록으로 나뉩니다. 크기: 프레임(및 페이지) 크기는 일반적으로 하드웨어적인 이유(주소 계산의 용이성)로 2의 거듭제곱으로 정해집니다. 예를 들어 29=512 바이트, 210=1KB, 212=4KB, 220=1MB, 최대 224=16MB (또는 그 이상)까지 다양할 수 있습니다. 현대 시스템에서는 4KB나 2MB(Huge Page)가 주로 사용됩니다. 페이지 (Pages): 각 프로세스의 논리 주소 공간도 프레임과 동일한 크기의 페이지라는 블록으로 나뉩니다. 가용 프레임 관리: 운영체제는 현재 사용 중이지 않은, 즉 비어있는 모든 프레임의 목록(free-frame list)을 유지하고 관리해야 합니다. 프로세스가 새로운 페이지를 필요로 할 때 이 목록에서 프레임을 할당해줍니다. 프로그램 실행 과정: 크기가 N개의 페이지로 구성된 프로그램을 실행하려면, 운영체제는 먼저 N개의 가용 프레임을 찾아야 합니다. 이 N개의 프레임은 물리 메모리 어디에 있든 상관없습니다 (연속될 필요 없음). 그런 다음 프로그램의 각 페이지를 이 프레임들에 적재(load)합니다. 페이지 테이블 설정: 프로그램 페이지들이 프레임에 적재된 후, 운영체제는 해당 프로세스를 위한 페이지 테이블을 설정합니다. 이 페이지 테이블에는 각 논리 페이지 번호가 어느 물리 프레임 번호에 매핑되는지에 대한 정보가 기록됩니다. 이 테이블은 CPU가 논리 주소를 물리 주소로 변환하는 데 사용됩니다. 백킹 스토어의 페이지화: 페이징은 가상 메모리 시스템과 긴밀하게 연관됩니다. 가상 메모리에서는 현재 사용되지 않는 페이지들을 디스크의 특정 영역인 백킹 스토어(backing store 또는 swap space)로 내보낼 수 있습니다(페이지 아웃). 이 백킹 스토어 역시 메모리와 마찬가지로 페이지 크기의 단위로 관리되어, 메모리 페이지를 그대로 디스크에 저장하거나 읽어올 수 있도록 합니다. 내부 단편화 (Internal Fragmentation) 문제 잔존: 😥 페이징은 외부 단편화는 해결하지만, 내부 단편화는 여전히 발생할 수 있습니다. 프로세스의 크기가 페이지 크기의 정확한 배수가 아닐 경우, 마지막 페이지는 완전히 채워지지 않고 일부 공간이 남게 됩니다. 이 남는 공간이 바로 내부 단편화입니다. 예를 들어 페이지 크기가 4KB인데 프로세스의 마지막 부분이 1KB라면, 해당 페이지에는 3KB의 사용되지 않는 공간(내부 단편화)이 발생합니다. 평균적으로 프로세스당 마지막 페이지에서 (페이지 크기 / 2) 만큼의 내부 단편화가 발생할 수 있다고 봅니다. Paging: Logical Addresses (페이징: 논리 주소) • 16-bit address, page size 1K=210, first 6 bit=page #, last 10bit = offset (16비트 주소, 페이지 크기 1KB=210, 처음 6비트=페이지 번호, 마지막 10비트=오프셋) \\[한글 번역 및 상세 설명\\] 페이징 시스템에서 CPU가 생성하는 논리 주소는 두 부분으로 나뉘어 해석됩니다: 페이지 번호(page number, p)와 페이지 오프셋(page offset, d). 예시 설명: 16-bit logical address (16비트 논리 주소): CPU가 생성할 수 있는 전체 논리 주소의 길이가 16비트라는 의미입니다. 이는 216=65,536 바이트 (64KB)의 논리 주소 공간을 나타냅니다. Page size 1K = 210 bytes (페이지 크기 1KB): 하나의 페이지(그리고 프레임)의 크기가 1KB, 즉 1024바이트라는 의미입니다. 1KB=210 바이트이므로, 페이지 내의 특정 바이트를 가리키기 위해서는 10비트가 필요합니다. 이것이 오프셋의 비트 수가 됩니다. Last 10 bits = offset (마지막 10비트는 오프셋): 논리 주소의 하위 10비트는 페이지 내에서의 상대적인 위치(변위)를 나타내는 오프셋(d)으로 사용됩니다. 오프셋 값의 범위는 0부터 210−1 (즉, 0부터 1023)까지입니다. First 6 bits = page # (처음 6비트는 페이지 번호): 전체 논리 주소 16비트 중 오프셋으로 사용된 10비트를 제외한 나머지 상위 비트들이 페이지 번호(p)로 사용됩니다. 즉, 16−10=6 비트가 페이지 번호가 됩니다. 이 6비트로는 26=64개의 서로 다른 페이지(페이지 0부터 페이지 63까지)를 구분할 수 있습니다. 일반화: 만약 논리 주소의 전체 길이가 m 비트이고, 페이지 크기가 2n 바이트라면, 하위 n 비트는 페이지 오프셋(d)이 됩니다. 상위 m-n 비트는 페이지 번호(p)가 됩니다. 이러한 논리 주소의 구조는 하드웨어(MMU)에 의해 해석되어 물리 주소로 변환됩니다. 페이지 번호(p)는 페이지 테이블의 인덱스로 사용되어 해당 페이지가 저장된 프레임 번호(f)를 찾는 데 사용되고, 오프셋(d)은 그 프레임 내에서의 상대 위치를 나타내므로 물리 주소 계산 시 그대로 사용됩니다. Paging: Logical to Physical Address (페이징: 논리 주소에서 물리 주소로) (Diagram showing logical address (page p, offset d) being translated. 'p' goes into page table, which outputs frame 'f'. 'f' is combined with 'd' to form physical address (frame f, offset d) which accesses physical memory.) page table (페이지 테이블) logical address (논리 주소) [ p | d ] physical address (물리 주소) [ f | d ] physical memory (물리 메모리) frame number (프레임 번호) \\[한글 번역 및 상세 설명\\] 이 다이어그램은 페이징 시스템에서 논리 주소가 물리 주소로 변환되는 핵심 과정을 보여줍니다. ⚙️ CPU가 논리 주소 생성: CPU는 실행할 명령어에 따라 논리 주소를 생성합니다. 이 논리 주소는 내부적으로 두 부분으로 구성됩니다. 페이지 번호 (p, page number): 논리 주소의 상위 비트들로, 어떤 페이지를 참조하는지를 나타냅니다. 오프셋 (d, offset): 논리 주소의 하위 비트들로, 해당 페이지 내에서 얼마나 떨어져 있는 위치(바이트 단위)를 참조하는지를 나타냅니다. 페이지 테이블 참조: 논리 주소에서 추출된 페이지 번호(p)는 페이지 테이블(page table)의 인덱스로 사용됩니다. 페이지 테이블의 각 항목(entry)에는 해당 논리 페이지가 실제로 저장되어 있는 물리 메모리의 프레임 번호(f, frame number)가 들어있습니다. (그리고 유효 비트, 보호 비트 등의 추가 정보도 포함될 수 있습니다.) 즉, 프레임 번호 (f) = 페이지 테이블 [페이지 번호 (p)] 와 같은 연산이 수행됩니다. 물리 주소 형성: 페이지 테이블로부터 얻은 프레임 번호(f)와 원래 논리 주소에 있던 오프셋(d)을 결합하여 최종적인 물리 주소를 만듭니다. 물리 주소의 구조는 [프레임 번호 (f) | 오프셋 (d)] 가 됩니다. 오프셋(d)은 변환 과정에서 변경되지 않고 그대로 사용됩니다. 이는 오프셋이 페이지(또는 프레임) 내에서의 상대적인 위치를 의미하기 때문입니다. 페이지가 어떤 프레임에 적재되든 그 페이지 내부의 구조는 동일합니다. 물리 메모리 접근: 이렇게 형성된 물리 주소를 사용하여 실제 물리 메모리에 접근하여 원하는 데이터나 명령어를 가져오거나 저장합니다. 이 모든 주소 변환 과정은 하드웨어인 MMU(Memory Management Unit)에 의해 매우 빠르게 수행됩니다. 이 과정 덕분에 프로그래머나 컴파일러는 실제 물리 메모리 구조를 신경 쓸 필요 없이 논리 주소 공간만을 대상으로 작업할 수 있습니다. Address Translation Scheme (주소 변환 방식) Address generated by CPU is divided into: (CPU에 의해 생성된 주소는 다음으로 나뉩니다:) Page number (p) (페이지 번호 (p)) → index into a page table = (page #, frame #) (페이지 테이블의 인덱스 = (페이지 번호, 프레임 번호)) Page offset (d) (페이지 오프셋 (d)) → offset within the page (frame) (페이지(프레임) 내의 오프셋) Given m bits logical address, page size 2n (m 비트 논리 주소이고, 페이지 크기가 2n일 때) → last n bit = offset = 0∼2n−1 (마지막 n 비트 = 오프셋 = 0∼2n−1) → first m-n bit = page number = 0∼2m−n−1 (처음 m-n 비트 = 페이지 번호 = 0∼2m−n−1) page table translates: page no → frame no (M-n bits) (페이지 테이블 변환: 페이지 번호 → 프레임 번호 (M-n 비트)) → M≥m (단, M은 물리 주소 비트 수, M이 m보다 크거나 같을 수 있음) \\[한글 번역 및 상세 설명\\] 페이징에서의 주소 변환 메커니즘을 더 자세히 설명합니다. CPU 생성 주소의 구성 요소: 페이지 번호 (p): 논리 주소에서 이 부분이 추출되어 페이지 테이블을 조회하는 데 사용됩니다. 페이지 테이블은 각 논리 페이지 번호에 해당하는 물리 프레임 번호를 저장하고 있습니다. (슬라이드에서는 (page #, frame #) 쌍으로 표현했는데, 정확히는 page #를 인덱스로 사용하여 frame #를 얻는다는 의미입니다.) 페이지 오프셋 (d): 해당 페이지(그리고 그 페이지가 적재된 프레임) 내에서의 정확한 바이트 위치를 나타냅니다. 이 값은 주소 변환 과정에서 변경되지 않습니다. 주소 비트 할당 (m비트 논리 주소, 페이지 크기 2n 바이트 가정): 오프셋 (offset): 페이지 크기가 2n 바이트이므로, 페이지 내의 모든 바이트를 고유하게 식별하려면 n 개의 비트가 필요합니다. 따라서 논리 주소의 하위 n 비트가 오프셋으로 사용됩니다. 오프셋 값의 범위는 0부터 2n−1까지입니다. 페이지 번호 (page number): 논리 주소의 총 m 비트 중 오프셋으로 사용된 n 비트를 제외한 나머지 m-n 비트가 페이지 번호로 사용됩니다. 이 페이지 번호는 0부터 2m−n−1까지의 값을 가질 수 있으며, 이는 해당 프로세스가 가질 수 있는 총 페이지의 수를 나타냅니다. 페이지 테이블의 변환 역할: 페이지 테이블은 논리적인 페이지 번호(p)를 물리적인 프레임 번호(f)로 변환(매핑)하는 역할을 합니다. 만약 물리 주소의 전체 길이가 M 비트이고, 오프셋 부분이 n 비트라면, 프레임 번호를 나타내는 데 사용되는 비트 수는 M-n 비트가 됩니다. 이 M-n 비트로 2M−n개의 서로 다른 프레임을 구분할 수 있습니다. M≥m에 대한 노트: 슬라이드에는 M≥m이라고 되어 있지만, 이는 항상 참은 아닙니다. M: 물리 주소 공간의 비트 수 (예: 실제 RAM 크기에 따라 결정) m: 논리 주소 공간의 비트 수 (예: CPU 아키텍처에 따라 프로세스가 가질 수 있는 최대 가상 주소 공간 크기, 예: 32비트 CPU는 m=32) 물리 메모리(RAM)가 논리 주소 공간보다 작을 수도 있습니다 (예: 32비트 프로세스가 1GB RAM 시스템에서 실행). 반대로, 64비트 CPU의 논리 주소 공간은 현재 시스템들의 물리 RAM보다 훨씬 큽니다. 여기서 M-n 은 프레임 번호를 표현하는 데 필요한 비트 수를 의미하고, 페이지 테이블에 저장되는 프레임 번호의 실제 크기를 나타냅니다. 다이어그램 요약: 논리 주소: [페이지 번호 p (m-n 비트)] [오프셋 d (n 비트)] 페이지 테이블: p 를 입력으로 받아 f (프레임 번호)를 출력. 물리 주소: [프레임 번호 f (M-n 비트)] [오프셋 d (n 비트)] 이 변환은 페이징 시스템의 핵심이며, MMU에 의해 하드웨어적으로 처리되어야 효율적인 실행이 가능합니다. Paging Hardware (페이징 하드웨어) (Diagram showing CPU generating a logical address (p, d). 'p' is used as an index into a Page table. The Page table base register (PTBR) points to the start of the page table in physical memory. The entry at PTBR + p gives the frame number 'f'. 'f' is concatenated with 'd' to form the physical address, which then accesses physical memory.) CPU [ p | d ] -> Page table [ PTBR + p gives f ] -> Physical memory [ f | d ] Logical Address -> Physical Address Page table base register (PTBR) / Page table length register (PTLR) \\[한글 번역 및 상세 설명\\] 이 슬라이드의 다이어그램은 페이징에서 주소 변환을 지원하는 데 필요한 하드웨어 요소, 특히 페이지 테이블이 메모리에 저장되는 방식과 접근 과정을 보여줍니다. CPU의 논리 주소 생성: CPU는 논리 주소를 생성하며, 이는 페이지 번호(p)와 오프셋(d)으로 나뉩니다. 페이지 테이블 위치: 각 프로세스는 자신만의 페이지 테이블을 가지고 있으며, 이 페이지 테이블은 물리 메모리에 저장됩니다. 운영체제는 현재 실행 중인 프로세스의 페이지 테이블이 물리 메모리의 어디에 시작하는지를 알아야 합니다. 이 시작 주소는 특별한 CPU 레지스터인 페이지 테이블 기준 레지스터(Page Table Base Register, PTBR)에 저장됩니다. (때로는 Page Table Pointer 라고도 불립니다.) 페이지 테이블 길이 레지스터(Page Table Length Register, PTLR)도 사용될 수 있습니다. 이는 페이지 테이블의 크기(즉, 유효한 페이지 번호의 범위)를 나타내어, 프로세스가 자신의 페이지 테이블 범위를 벗어나는 잘못된 페이지 번호로 접근하는 것을 방지합니다 (메모리 보호). p >= PTLR 이면 오류입니다. 페이지 테이블 항목(PTE) 접근: CPU가 생성한 논리 주소의 페이지 번호(p)와 PTBR 값을 사용하여, 해당 페이지에 대한 정보(페이지 테이블 항목, Page Table Entry, PTE)가 저장된 물리 메모리 주소를 계산합니다. 만약 각 PTE의 크기가 entry_size 바이트라면, 원하는 PTE의 주소는 PTBR + (p * entry_size) 가 됩니다. (다이어그램에서는 단순하게 PTBR + p 로 표현했는데, 이는 p 가 PTE의 오프셋 인덱스로 사용되고, PTBR이 이미 올바른 시작점을 가리킨다고 가정한 것입니다. 실제로는 PTE 크기를 고려해야 합니다.) 이 주소에서 PTE를 읽어옵니다. PTE에는 해당 페이지가 적재된 프레임 번호(f)가 들어있습니다. 물리 주소 형성 및 접근: 읽어온 프레임 번호(f)와 원래의 오프셋(d)을 결합하여 최종 물리 주소를 만듭니다. ( [f | d] ) 이 물리 주소를 사용하여 실제 데이터나 명령어가 있는 물리 메모리 위치에 접근합니다. 중요한 점: 페이지 테이블의 메모리 상주: 페이지 테이블 자체가 주 메모리에 있다는 것은, 모든 논리 주소 참조가 잠재적으로 두 번의 메모리 접근을 필요로 한다는 것을 의미합니다. 첫 번째 접근: 페이지 테이블에서 프레임 번호를 가져오기 위한 접근. 두 번째 접근: 실제 데이터나 명령어를 가져오기 위한 접근. 성능 문제: 두 번의 메모리 접근은 시스템 성능을 크게 저하시킬 수 있습니다. 이를 해결하기 위해 대부분의 시스템은 TLB(Translation Lookaside Buffer) 또는 주소 변환 버퍼라고 불리는 고속의 하드웨어 캐시를 사용합니다. TLB에는 최근에 사용된 (페이지 번호, 프레임 번호) 매핑 정보가 저장되어 있어, 페이지 테이블을 매번 메모리에서 읽어오는 대신 TLB에서 빠르게 찾을 수 있도록 합니다. (TLB는 이 슬라이드 세트에는 명시적으로 나오지 않았지만, 페이징 하드웨어의 매우 중요한 부분입니다.) PTBR과 PTLR(또는 이와 유사한 메커니즘)은 문맥 교환(context switch) 시 운영체제에 의해 현재 실행될 프로세스의 값으로 변경되어야 합니다. 그래야 각 프로세스가 자신만의 페이지 테이블을 올바르게 참조할 수 있습니다. Paging Example (페이징 예제) (Diagram with Logical memory (pages 0-3, data a-p), Page table (mapping pages to frames), and Physical memory (frames 0-7, showing where data a-p is stored based on page table, and some frames are free).) 이 슬라이드는 앞서 설정된 값(m=4, n=2, 32바이트 물리 메모리)을 바탕으로 페이징 시스템의 구체적인 메모리 할당 상태를 보여줍니다. 🗺️ 논리 주소 및 프로세스 공간: 4-bit logical address (m=4): 논리 주소는 4비트로 표현됩니다. 16-byte process space: 24=16 바이트의 논리 주소 공간을 가집니다. 2-bit page no (m-n=2), 0~3: 논리 주소의 상위 2비트는 페이지 번호(p)를 나타내며, 값의 범위는 00(0)부터 11(3)까지 총 4개의 페이지입니다. 2-bit offset (n=2), 4-byte pages: 논리 주소의 하위 2비트는 오프셋(d)을 나타내며, 페이지(프레임) 크기는 22=4 바이트입니다. 물리 주소 및 메모리: 5-bit physical address: 물리 주소는 5비트로 표현됩니다. 32-byte memory: 25=32 바이트의 물리 메모리 공간을 가집니다. 물리 메모리는 4바이트 크기의 프레임 8개(32바이트 / 4바이트/프레임 = 8 프레임)로 나누어집니다. 프레임 번호는 3비트(000부터 111까지)로 표현됩니다. 다이어그램 해석: Logical Memory (논리 메모리): 프로세스가 \"보는\" 메모리 모습입니다. 0번지부터 15번지(0000부터 1111까지)까지 16바이트의 연속된 공간으로 보입니다. 이 공간은 4개의 페이지(Page 0, Page 1, Page 2, Page 3)로 나뉘어 있고, 각 페이지는 4바이트의 데이터(임의로 a부터 p까지 문자로 표현)를 담고 있습니다. Page 0 (논리 주소 0000~0011): 데이터 a, b, c, d Page 1 (논리 주소 0100~0111): 데이터 e, f, g, h Page 2 (논리 주소 1000~1011): 데이터 i, j, k, l Page 3 (논리 주소 1100~1111): 데이터 m, n, o, p Page Table (페이지 테이블): 이 프로세스의 각 논리 페이지가 어느 물리 프레임에 저장되어 있는지를 보여주는 매핑 테이블입니다. (페이지 번호 프레임 번호) 형태로 표시되어 있습니다. 페이지 0 (00)은 프레임 5 (101)에 매핑됩니다. 페이지 1 (01)은 프레임 6 (110)에 매핑됩니다. 페이지 2 (10)은 프레임 1 (001)에 매핑됩니다. 페이지 3 (11)은 프레임 2 (010)에 매핑됩니다. Physical Memory (물리 메모리): 실제 RAM의 모습입니다. 8개의 프레임(frame-0부터 frame-7까지)으로 구성되어 있습니다. 각 프레임의 시작 물리 주소도 표시되어 있습니다 (예: frame-1은 00100부터 시작). 페이지 테이블의 매핑 정보에 따라 각 페이지의 데이터가 실제 물리 프레임에 저장된 모습을 보여줍니다. 프레임 0 (00000): 비어 있음 (free) 프레임 1 (00100): 페이지 2의 데이터 (i, j, k, l) 저장 프레임 2 (01000): 페이지 3의 데이터 (m, n, o, p) 저장 프레임 3 (01100): 비어 있음 (free) 프레임 4 (10000): 비어 있음 (free) 프레임 5 (10100): 페이지 0의 데이터 (a, b, c, d) 저장 프레임 6 (11000): 페이지 1의 데이터 (e, f, g, h) 저장 프레임 7 (11100): 비어 있음 (free) 중요한 것은 페이지 0, 1, 2, 3이 물리 메모리에서 (프레임 5, 6, 1, 2 순서로) 비연속적으로 흩어져 저장되어 있다는 점입니다. 이 예제를 통해 프로세스의 논리적인 연속성이 페이지 테이블을 통해 물리적인 비연속성으로 어떻게 변환되는지 명확히 알 수 있습니다. Fragmentation in Paging (페이징에서의 단편화) Internal fragmentation (내부 단편화) Page size = 2,048 bytes (페이지 크기 = 2,048 바이트) Process size = 72,766 bytes (프로세스 크기 = 72,766 바이트) 35 pages + 1,086 bytes (35 페이지 + 1,086 바이트) Internal fragmentation = 2,048 - 1,086 = 962 bytes (내부 단편화 = 2,048 - 1,086 = 962 바이트) Frame size & fragmentation (프레임 크기와 단편화) Internal fragmentation = 1byte ~ (frame size – 1) (내부 단편화 = 1바이트 ~ (프레임 크기 – 1)) Average fragmentation = 1 / 2 frame size (평균 단편화 = 프레임 크기의 1/2) Small frame size better? (작은 프레임 크기가 더 좋은가?) Small frame size → Small internal fragmentation (작은 프레임 크기 → 작은 내부 단편화) → Large page table (→ 큰 페이지 테이블) Large frame size → Small page table (큰 프레임 크기 → 작은 페이지 테이블) → More internal fragmentation (→ 더 많은 내부 단편화) \\[한글 번역 및 상세 설명\\] 페이징 시스템은 외부 단편화 문제를 해결하지만, 내부 단편화(Internal Fragmentation) 문제는 여전히 안고 있습니다. 내부 단편화 발생 원리: 프로세스는 다양한 크기를 가질 수 있지만, 페이징 시스템에서는 메모리가 고정된 크기의 페이지(프레임) 단위로 할당됩니다. 만약 프로세스의 전체 크기가 페이지 크기의 정확한 배수가 아니라면, 프로세스의 마지막 페이지는 해당 페이지를 다 채우지 못하고 일부 공간만 사용하게 됩니다. 이때, 마지막 페이지에서 사용되지 않고 남는 공간이 바로 내부 단편화입니다. 이 공간은 해당 프로세스에 할당은 되었지만, 실제로는 사용되지 않아 낭비되는 부분입니다. 예시: 페이지(프레임) 크기 = 2,048 바이트 (2KB) 프로세스 크기 = 72,766 바이트 필요한 페이지 수 계산: 프로세스가 완전히 채울 수 있는 페이지 수 = 72,766÷2,048=35 (몫) 하고 1,086 바이트가 남습니다 (나머지). 즉, 이 프로세스는 35개의 페이지를 꽉 채우고, 추가로 1,086 바이트의 데이터를 더 저장해야 합니다. 이 남은 1,086 바이트를 저장하기 위해 어쩔 수 없이 한 개의 페이지(36번째 페이지)가 더 할당됩니다. 내부 단편화 계산: 36번째 페이지의 크기는 2,048 바이트이지만, 이 중 1,086 바이트만 사용됩니다. 따라서 사용되지 않고 낭비되는 공간 (내부 단편화) = 2,048 바이트−1,086 바이트=962 바이트. 이 프로세스 하나에서만 962바이트의 메모리가 내부 단편화로 인해 낭비되는 것입니다. 프레임(페이지) 크기와 단편화의 관계: 내부 단편화의 범위: 어떤 프로세스에 대해 발생하는 내부 단편화의 크기는 최소 0바이트 (프로세스 크기가 페이지 크기의 정확한 배수일 경우)부터 최대 (프레임 크기 - 1) 바이트까지 가능합니다. (슬라이드의 1byte ~ (frame size – 1) 표현은, 프로세스가 최소 1바이트의 데이터를 마지막 페이지에 가져야 단편화가 frame_size - 1 이 된다는 의미로, 0인 경우를 제외하고 본다면 타당합니다.) 평균 내부 단편화: 다양한 크기의 많은 프로세스들이 시스템에서 실행된다고 가정할 때, 평균적으로 각 프로세스당 약 프레임(페이지) 크기의 절반 (1/2)만큼의 내부 단편화가 발생한다고 알려져 있습니다. 이는 통계적인 추정치입니다. 적절한 프레임(페이지) 크기 결정의 딜레마 (Small frame size better?): 작은 프레임(페이지) 크기를 사용할 경우: 👍 장점: 내부 단편화의 크기가 줄어듭니다. 프로세스 크기가 페이지 크기의 배수에서 조금 벗어나더라도 낭비되는 공간이 작아집니다. 👎 단점: 페이지 테이블의 크기가 매우 커집니다. 예를 들어, 같은 크기의 프로세스라도 페이지 크기가 절반이 되면 필요한 페이지 수는 두 배가 되고, 따라서 페이지 테이블의 항목 수도 두 배가 됩니다. 큰 페이지 테이블은 그 자체로 메모리를 많이 차지하고, 페이지 테이블 검색(특히 TLB miss 시)에 더 많은 시간이 소요될 수 있습니다. 또한 디스크 I/O 시 더 많은 페이지 전송이 필요할 수 있습니다. 큰 프레임(페이지) 크기를 사용할 경우: 👍 장점: 페이지 테이블의 크기가 작아집니다. 관리해야 할 페이지 수가 줄어들기 때문입니다. 이는 페이지 테이블로 인한 메모리 오버헤드와 검색 시간을 줄일 수 있습니다. 디스크 I/O 효율도 높아질 수 있습니다 (한 번에 많은 데이터를 전송). 👎 단점: 내부 단편화가 커질 가능성이 높습니다. 프로세스의 마지막 부분이 큰 프레임의 작은 일부만 사용할 경우 낭비되는 공간이 많아집니다. 결국, 페이지(프레임) 크기는 시스템 설계 시 내부 단편화, 페이지 테이블 크기, 디스크 I/O 효율성, TLB 성능 등 다양한 요소를 고려하여 신중하게 결정해야 하는 트레이드오프(trade-off) 관계에 있습니다. 현대 시스템은 이러한 단점을 보완하기 위해 여러 크기의 페이지(예: 4KB 표준 페이지 + 2MB/1GB Huge Page)를 함께 지원하기도 합니다. Quiz (퀴즈) Consider a simple paging system with the following parameters: $2^{32}$ bytes of physical memory; page size of $2^{10}$ bytes; $2^{16}$ pages of logical address space. (다음 매개변수를 가진 간단한 페이징 시스템을 고려하십시오: 물리 메모리 $2^{32}$ 바이트; 페이지 크기 $2^{10}$ 바이트; 논리 주소 공간 $2^{16}$ 페이지.) How many bits are in a logical address? (논리 주소는 몇 비트입니까?) How many bytes in a frame? (프레임의 바이트 수는 얼마입니까?) How many bits in the physical address specify the frame? (물리 주소에서 프레임을 지정하는 비트 수는 몇 개입니까?) How many entries in the page table? (페이지 테이블의 항목 수는 몇 개입니까?) \\[한글 번역 및 상세 설명 및 풀이\\] 페이징 시스템의 매개변수를 이해하고 계산하는 퀴즈입니다. 🤓 주어진 매개변수 분석: 물리 메모리 크기 = $2^{32}$ 바이트 (이는 4GB에 해당합니다.) 페이지 크기 = $2^{10}$ 바이트 (이는 1KB 또는 1024 바이트에 해당합니다.) 논리 주소 공간의 페이지 수 = $2^{16}$ 페이지 (이는 65,536개의 페이지를 의미합니다.) 풀이: How many bits are in a logical address? (논리 주소는 몇 비트입니까?) 논리 주소는 페이지 번호(p) 부분과 오프셋(d) 부분으로 구성됩니다. 오프셋(d) 비트 수: 페이지 크기가 210 바이트이므로, 페이지 내의 특정 바이트를 가리키기 위한 오프셋은 10비트가 필요합니다. (오프셋은 0부터 210−1까지의 값을 가짐) 즉, n = 10 비트. 페이지 번호(p) 비트 수: 논리 주소 공간에 216개의 페이지가 있다고 주어졌습니다. 216개의 서로 다른 페이지를 구분하기 위해서는 16비트가 필요합니다. (페이지 번호는 0부터 216−1까지의 값을 가짐) 즉, m-n = 16 비트. 총 논리 주소 비트 수 (m): 논리 주소 비트 수 = (페이지 번호 비트 수) + (오프셋 비트 수) m=(m−n)+n=16+10=26 비트. 답: 26 비트 How many bytes in a frame? (프레임의 바이트 수는 얼마입니까?) 페이징 시스템에서 프레임의 크기는 항상 페이지의 크기와 동일합니다. 주어진 페이지 크기 = $2^{10}$ 바이트. 답: $2^{10}$ 바이트 (또는 1024 바이트) How many bits in the physical address specify the frame? (물리 주소에서 프레임을 지정하는 비트 수는 몇 개입니까?) 물리 주소도 프레임 번호(f) 부분과 오프셋(d) 부분으로 구성됩니다. 물리 메모리의 총 프레임 수 계산: 물리 메모리 크기 = $2^{32}$ 바이트. 프레임 크기 = $2^{10}$ 바이트. 총 프레임 수 = (물리 메모리 크기) / (프레임 크기) = $2^{32}\\div2^{10}$=$2^{32 - 10}$=$2^{22}$ 개의 프레임. 프레임 번호(f) 비트 수: $2^{22}$개의 서로 다른 프레임을 구분하기 위해서는 22비트가 필요합니다. (프레임 번호는 0부터 $2^{22}$−1까지의 값을 가짐) 답: 22 비트 (참고: 전체 물리 주소 비트 수는 프레임 번호 비트(22) + 오프셋 비트(10) = 32비트이며, 이는 주어진 물리 메모리 크기 232 바이트와 일치합니다.) How many entries in the page table? (페이지 테이블의 항목 수는 몇 개입니까?) 페이지 테이블은 프로세스의 각 논리 페이지에 대한 매핑 정보를 담고 있습니다. 따라서 페이지 테이블의 항목(entry) 수는 해당 프로세스가 가질 수 있는 총 논리 페이지의 수와 같습니다. 주어진 논리 주소 공간의 페이지 수 = $2^{16}$ 페이지. 답: $2^{16}$ 개 (또는 65,536 개) Free Frames (가용 프레임) Before allocation (할당 전) After allocation (할당 후) (Diagram showing a list of free frames, and then a process's pages (page 0, 1, 2, 3) being allocated to some of these frames, and the page table being updated. The free frame list also gets updated.) \\[한글 번역 및 상세 설명\\] 이 슬라이드의 다이어그램은 페이징 시스템에서 프로세스에 메모리가 할당되기 전과 후의 가용 프레임(free frame) 상태 변화를 시각적으로 보여줍니다. 가용 프레임 (Free Frames): 물리 메모리는 일정한 크기의 프레임들로 나누어지며, 이 중 어떤 프로세스에게도 아직 할당되지 않은, 즉 비어있는 프레임들을 '가용 프레임'이라고 합니다. 운영체제는 이러한 가용 프레임들의 목록을 관리합니다 (이를 가용 프레임 리스트(free-frame list)라고 부릅니다). Before allocation (할당 전): 다이어그램의 왼쪽 부분은 새로운 프로세스(예: new_process )가 메모리를 할당받기 전의 상태를 나타냅니다. 물리 메모리에는 여러 개의 프레임들이 있으며, 이 중 일부는 이미 다른 프로세스나 운영체제에 의해 사용 중일 수 있고, 나머지 프레임들은 가용 상태입니다. 가용 프레임 리스트에는 현재 사용 가능한 프레임들의 번호가 들어있습니다. (예: 14, 13, 18, 20, 15번 프레임 등이 가용 상태) After allocation (할당 후): 다이어그램의 오른쪽 부분은 new_process 에게 메모리가 할당된 후의 상태를 보여줍니다. new_process 는 여러 개의 페이지(예: page 0, page 1, page 2, page 3)로 구성되어 있습니다. 운영체제는 가용 프레임 리스트에서 new_process 의 페이지 수만큼 프레임을 가져와 각 페이지에 할당합니다. 예를 들어: page 0 은 가용 프레임 중 하나(예: 14번 프레임)에 할당됩니다. page 1 은 그 다음 가용 프레임(예: 13번 프레임)에 할당됩니다. 이런 식으로 page 2 는 18번, page 3 은 20번 프레임에 할당될 수 있습니다. 페이지 테이블 업데이트: new_process 의 페이지 테이블에는 이러한 매핑 정보가 기록됩니다. 즉, page_table[0] = 14 , page_table[1] = 13 , page_table[2] = 18 , page_table[3] = 20 과 같이 설정됩니다. 가용 프레임 리스트 업데이트: 할당된 프레임들(14, 13, 18, 20)은 더 이상 가용 상태가 아니므로 가용 프레임 리스트에서 제거됩니다. 따라서 가용 프레임 리스트는 (예: 15번 프레임부터 시작하는 리스트)로 업데이트됩니다. 이 과정은 페이징 시스템이 어떻게 외부 단편화 없이 필요한 만큼의 프레임을 (비록 물리적으로 흩어져 있을지라도) 프로세스에게 할당하는지를 보여줍니다. 운영체제는 항상 가용 프레임 목록을 정확하게 유지하여 효율적인 메모리 할당 및 회수를 보장해야 합니다. Implementation of Page Table (페이지 테이블 구현) Page table is kept in main memory (페이지 테이블은 주 메모리에 유지됨) Page-table base register (PTBR) points to the page table (페이지 테이블 기준 레지스터(PTBR)는 페이지 테이블을 가리킴) Page-table length register (PTLR) indicates size of the page table (페이지 테이블 길이 레지스터(PTLR)는 페이지 테이블의 크기를 나타냄) In this scheme every data/instruction access requires two memory accesses (이 방식에서는 모든 데이터/명령어 접근이 두 번의 메모리 접근을 필요로 함) One for the page table and one for the data / instruction (하나는 페이지 테이블을 위해, 다른 하나는 데이터/명령어를 위해) The two memory access problem can be solved by the use of a special fast-lookup hardware cache called associative memory or translation look-aside buffers (TLBs) (두 번의 메모리 접근 문제는 연관 메모리 또는 TLB(Translation Look-aside Buffer)라고 불리는 특별한 고속 검색 하드웨어 캐시를 사용하여 해결될 수 있음) Some TLBs store address-space identifiers (ASIDs) in each TLB entry – uniquely identifies each process to provide address-space protection for that process (일부 TLB는 각 TLB 항목에 주소 공간 식별자(ASID)를 저장함 – 각 프로세스를 고유하게 식별하여 해당 프로세스에 대한 주소 공간 보호를 제공함) Otherwise need to flush at every context switch (그렇지 않으면 모든 문맥 교환 시에 플러시(비우기)해야 함) TLBs typically small (64 to 1,024 entries) (TLB는 일반적으로 작음 (64개에서 1,024개 항목)) \\[한글 번역 및 상세 설명\\] 페이지 테이블을 실제로 시스템에서 어떻게 구현하고 관리하는지에 대한 중요한 사항들을 설명합니다. 📜 페이지 테이블의 주 메모리 저장: 각 프로세스는 자신만의 페이지 테이블을 가집니다. 이 페이지 테이블은 프로세스의 논리 주소 공간이 클수록 커질 수 있습니다 (예: 32비트 주소 공간, 4KB 페이지 크기 → 220개의 항목, 각 항목 4바이트 시 4MB). 이렇게 큰 자료구조를 CPU 내의 한정된 레지스터에 모두 저장하는 것은 불가능하므로, 페이지 테이블은 주 메모리(main memory, RAM)에 저장됩니다. 페이지 테이블 접근을 위한 레지스터: PTBR (Page-Table Base Register, 페이지 테이블 기준 레지스터): CPU 내의 특별한 레지스터로, 현재 실행 중인 프로세스의 페이지 테이블이 주 메모리에서 시작하는 물리 주소를 가지고 있습니다. CPU가 논리 주소를 물리 주소로 변환해야 할 때, 이 PTBR을 참조하여 메모리에서 해당 프로세스의 페이지 테이블을 찾습니다. PTLR (Page-Table Length Register, 페이지 테이블 길이 레지스터): 이 레지스터는 페이지 테이블의 크기(즉, 페이지 테이블이 가진 항목의 수 또는 유효한 페이지 번호의 최대치)를 저장합니다. 이는 프로세스가 자신의 논리 주소 공간 범위를 벗어나는 페이지 번호(예: 페이지 테이블의 크기를 넘어서는 인덱스)로 메모리에 접근하려는 시도를 막아 메모리 보호 기능을 수행합니다. CPU가 생성한 페이지 번호 p 가 PTLR 값보다 크거나 같으면 주소 오류 트랩이 발생합니다. 두 번의 메모리 접근 문제 (Two Memory Access Problem): 💔 페이지 테이블이 주 메모리에 있기 때문에, CPU가 하나의 데이터나 명령어를 메모리에서 가져오려고 할 때마다 최소 두 번의 메모리 접근이 필요하게 됩니다. 첫 번째 접근: 페이지 테이블 자체에 접근하여, 논리 페이지 번호에 해당하는 페이지 테이블 항목(PTE)을 읽어와 물리 프레임 번호를 알아냅니다. 두 번째 접근: 이렇게 얻은 물리 프레임 번호와 오프셋을 결합한 최종 물리 주소를 사용하여, 실제 원하는 데이터나 명령어를 메모리에서 읽어옵니다. 메모리 접근은 CPU 내부 연산에 비해 매우 느린 작업이므로, 매번 두 번씩 접근하는 것은 시스템 성능에 심각한 저하를 초래합니다. TLB (Translation Look-aside Buffer)를 이용한 해결책: 🚀 이 두 번의 메모리 접근 문제를 해결(완화)하기 위해 대부분의 현대 CPU는 TLB라는 특별한 하드웨어 캐시를 사용합니다. TLB는 연관 메모리(associative memory)로 구현되며, 최근에 사용된 (페이지 번호, 프레임 번호) 매핑 정보를 매우 빠르게 검색할 수 있도록 설계된 작은 고속 캐시입니다. CPU가 논리 주소를 생성하면, 먼저 TLB에서 해당 페이지 번호에 대한 매핑 정보가 있는지 병렬적으로(associatively) 검색합니다 (TLB 조회). TLB 히트(Hit): 만약 TLB에 해당 정보가 있으면(캐시 히트), 프레임 번호를 즉시 얻어 물리 주소를 형성하고 메모리에 접근합니다. 이 경우, 페이지 테이블을 위한 주 메모리 접근이 생략되므로 한 번의 메모리 접근만 필요하게 됩니다(실제 데이터/명령어 접근). TLB 미스(Miss): 만약 TLB에 정보가 없으면(캐시 미스), 어쩔 수 없이 PTBR을 사용하여 주 메모리의 페이지 테이블에 접근하여 프레임 번호를 가져옵니다 (두 번의 메모리 접근 발생). 그리고 이렇게 얻은 (페이지 번호, 프레임 번호) 매핑 정보는 다음 사용을 위해 TLB에 새로 저장됩니다 (이때 TLB가 꽉 차 있다면 기존 항목 중 하나를 교체해야 함 - LRU 등의 교체 정책 사용). ASID (Address-Space Identifiers, 주소 공간 식별자): TLB는 여러 프로세스가 공유하는 하드웨어 자원입니다. 문맥 교환이 일어나 다른 프로세스가 실행되면, 이전 프로세스의 TLB 항목들이 새 프로세스에게는 유효하지 않을 수 있습니다 (같은 논리 페이지 번호라도 다른 물리 프레임을 가리킬 수 있기 때문). 이 문제를 해결하기 위해 일부 TLB는 각 항목에 ASID를 저장합니다. ASID는 각 프로세스를 고유하게 식별하는 번호입니다. TLB 조회 시 페이지 번호뿐만 아니라 현재 실행 중인 프로세스의 ASID도 함께 사용하여, 정확히 해당 프로세스의 매핑 정보만 찾도록 합니다. ASID를 사용하면, 문맥 교환 시 TLB 전체를 비울(flush) 필요가 없어집니다. 각 항목이 어떤 프로세스에 속하는지 구분할 수 있기 때문입니다. ASID가 없다면, 문맥 교환마다 TLB를 플러시해야 하므로 TLB 효율이 떨어집니다 (새 프로세스는 처음부터 TLB 미스를 많이 겪게 됨). TLB 크기: TLB는 고속의 연관 메모리로 만들어지기 때문에 가격이 비싸고 전력 소모도 있습니다. 따라서 그 크기는 제한적입니다. 일반적으로 64개에서 1,024개 정도의 항목을 가집니다. (현대 CPU는 더 많은 항목을 가질 수도 있고, 계층적 TLB 구조를 사용하기도 합니다.) 크기는 작지만, 프로그램 실행의 지역성(locality of reference - 한 번 참조된 메모리 영역은 곧 다시 참조될 가능성이 높고, 그 주변 영역도 참조될 가능성이 높다는 성질) 때문에 TLB 히트율(hit ratio)은 매우 높게(예: 98% 이상) 유지될 수 있어 성능 향상에 크게 기여합니다. Memory Access with Paging (페이징에서의 메모리 접근) With paging, every data/instruction access requires (페이징 사용 시, 모든 데이터/명령어 접근은 다음을 필요로 함) 2 memory accesses (2번의 메모리 접근) One for the page table and one for the data / instruction (하나는 페이지 테이블을 위해, 다른 하나는 데이터/명령어를 위해) \\[한글 번역 및 상세 설명\\] 페이징 시스템에서 TLB가 없을 경우 발생하는 \"두 번의 메모리 접근\" 문제를 시각적으로 명확하게 보여주는 슬라이드입니다. 💔➡️💔 문제점 반복 강조: 페이징을 사용할 때, 페이지 테이블이 주 메모리에 있기 때문에, CPU가 어떤 데이터나 명령어를 실제 메모리에서 가져오기까지 총 두 번의 주 메모리 접근이 필요하다는 점을 다시 한번 강조합니다. 다이어그램 설명: CPU가 논리 주소 생성: CPU가 특정 논리 주소(Page X 내의 어떤 오프셋)를 참조하려고 합니다. 접근 1: 페이지 테이블 조회 (get address - 정확히는 get frame number): CPU는 먼저 이 논리 주소의 페이지 번호 부분(X)을 사용하여, 현재 프로세스의 페이지 테이블에서 해당 페이지 X에 대한 항목(PTE)을 찾아야 합니다. PTBR 레지스터가 가리키는 주 메모리 위치에서 페이지 테이블이 시작되므로, 이 위치로부터 페이지 X의 PTE를 읽어옵니다. 이것이 첫 번째 메모리 접근입니다. 이 접근을 통해 페이지 X가 저장된 물리 프레임 번호를 얻습니다. 접근 2: 실제 데이터/명령어 조회 (get data/instruction): 첫 번째 접근에서 얻은 프레임 번호와 원래 논리 주소의 오프셋 부분을 결합하여 최종 물리 주소를 계산합니다. 이 계산된 물리 주소를 사용하여 주 메모리에 다시 한번 접근하여, 실제 원하는 데이터나 명령어를 가져옵니다. 이것이 두 번째 메모리 접근입니다. 성능 저하: 메모리 접근은 상대적으로 느린 작업입니다. 만약 메모리 접근에 100 나노초(ns)가 걸린다고 가정하면, TLB가 없는 순수 페이징 시스템에서는 단일 논리 주소 참조에 200ns가 소요됩니다 (페이지 테이블 접근 100ns + 실제 데이터 접근 100ns). 이는 메모리 접근 속도를 실질적으로 절반으로 떨어뜨리는 것과 같아서 시스템 전체 성능에 큰 부담을 줍니다. 이러한 심각한 성능 문제를 해결하기 위해 다음 슬라이드에서 설명할 TLB(Translation Look-aside Buffer)가 필수적으로 사용됩니다. Memory Access with Paging (페이징에서의 메모리 접근) Solution: translation look-aside buffer (해결책: TLB - 주소 변환 색인 버퍼) a special fast-lookup hardware cache (특별한 고속 검색 하드웨어 캐시) associative memory (연관 메모리) address-space identifiers (ASIDs) (주소 공간 식별자) distinguish between entries of different processes (다른 프로세스들의 항목들을 구별함) Otherwise need to flush at every context switch (그렇지 않으면 모든 문맥 교환 시에 플러시해야 함) TLBs typically small (64 to 1,024 entries) (TLB는 일반적으로 작음 (64에서 1,024 항목)) Operation (동작 방식) Works like a cache (캐시처럼 동작함) Replacement policies must be considered (교체 정책이 고려되어야 함) Some entries can be wired down for permanent fast access (일부 항목은 영구적인 빠른 접근을 위해 고정될(wired down) 수 있음) \\[한글 번역 및 상세 설명\\] 두 번의 메모리 접근 문제를 해결하기 위한 핵심 요소인 TLB(Translation Look-aside Buffer)에 대해 더 자세히 설명합니다. 💡 TLB 정의 및 특징: 특별한 고속 검색 하드웨어 캐시: TLB는 MMU(Memory Management Unit) 내에 위치하거나 CPU 코어에 매우 가까이 위치하는 작고 매우 빠른 하드웨어 캐시입니다. 주 목적은 최근에 사용된 (논리 페이지 번호, 물리 프레임 번호) 매핑 정보를 저장하여, 페이지 테이블을 위한 느린 주 메모리 접근을 피하는 것입니다. 연관 메모리 (Associative Memory): TLB는 종종 연관 메모리(또는 내용 주소 지정 가능 메모리, CAM - Content Addressable Memory)로 구현됩니다. 연관 메모리는 일반 메모리처럼 주소를 주면 내용을 찾는 것이 아니라, 내용(여기서는 페이지 번호)을 주면 그 내용과 일치하는 모든 항목을 병렬적으로 동시에 검색하여 해당 항목(여기서는 프레임 번호)을 매우 빠르게 찾아낼 수 있습니다. ASIDs (Address-Space Identifiers, 주소 공간 식별자): 다중 프로그래밍 환경에서는 여러 프로세스가 동시에 시스템에 존재하며, CPU는 이들 사이를 빠르게 전환(문맥 교환)합니다. 각 프로세스는 자신만의 페이지 테이블을 가지고 있으므로, 논리 페이지 번호 10이 프로세스 A에서는 프레임 100을 의미하고, 프로세스 B에서는 프레임 200을 의미할 수 있습니다. TLB에 단순히 (페이지 번호, 프레임 번호) 쌍만 저장하면, 문맥 교환 시 이전 프로세스의 TLB 항목이 새 프로세스에게 잘못된 정보를 줄 수 있습니다. 이를 방지하기 위해 ASID가 사용됩니다. ASID는 각 프로세스에 할당되는 고유한 식별자입니다. TLB 항목에는 (ASID, 페이지 번호, 프레임 번호, 기타 정보) 형태로 저장됩니다. TLB를 검색할 때는 현재 실행 중인 프로세스의 ASID와 찾고자 하는 페이지 번호를 함께 사용하여, 정확히 현재 프로세스에 해당하는 항목만 찾습니다. ASID의 장점: 문맥 교환이 발생해도 TLB 전체를 비울(flush) 필요가 없습니다. ASID를 통해 서로 다른 프로세스의 TLB 항목들이 공존할 수 있기 때문입니다. ASID가 없다면, 문맥 교환 시마다 TLB를 비워야 하므로, 새 프로세스가 실행될 때마다 TLB 미스가 자주 발생하여 성능이 저하됩니다. TLB 크기: 연관 메모리는 일반 SRAM보다 복잡하고 비싸기 때문에 TLB의 크기는 제한적입니다. 일반적으로 수십에서 수천 개(예: 64~1024개, 현대 CPU는 더 많을 수 있음)의 항목을 가집니다. L1 TLB, L2 TLB 등 계층적인 구조를 갖기도 합니다. TLB 동작 방식: 캐시처럼 동작: 일반적인 데이터 캐시나 명령어 캐시와 유사하게 동작합니다. TLB 히트(Hit): CPU가 논리 주소를 생성하면, 먼저 TLB에서 해당 페이지 번호(+ASID)를 찾습니다. 정보가 있으면(히트), 프레임 번호를 즉시 얻어 주소 변환을 완료합니다. (매우 빠름) TLB 미스(Miss): 정보가 없으면(미스), 주 메모리의 페이지 테이블에 접근하여 프레임 번호를 가져와야 합니다. (느림) 그리고 이 새로운 매핑 정보는 TLB에 저장됩니다. 교체 정책 (Replacement Policies): TLB 미스가 발생하여 새로운 항목을 TLB에 넣어야 하는데 TLB가 꽉 차 있다면, 기존 항목 중 하나를 제거(evict)해야 합니다. 이때 어떤 항목을 제거할지를 결정하는 정책이 필요합니다. 일반적인 캐시 교체 알고리즘인 LRU(Least Recently Used), FIFO(First-In First-Out), 또는 랜덤 방식 등이 사용될 수 있습니다. 고정 항목 (Wired Down Entries): 일부 TLB 항목은 교체 대상에서 제외되어 영구적으로 TLB에 상주하도록 \"고정(wire down)\"될 수 있습니다. 이는 운영체제 커널의 매우 중요한 페이지나, 성능에 민감한 애플리케이션의 핵심 페이지 등에 사용되어 항상 빠른 접근을 보장하기 위함입니다. TLB는 페이징 시스템의 성능을 실용적인 수준으로 끌어올리는 데 결정적인 역할을 합니다. 높은 TLB 히트율을 유지하는 것이 페이징 시스템 성능의 핵심입니다. Associative Memory (연관 메모리) Associative memory – parallel search (연관 메모리 – 병렬 검색) Address translation (p, d) (주소 변환 (p, d)) If p is in associative register, get frame # out (만약 p가 연관 레지스터에 있으면, 프레임 번호를 가져옴) Otherwise get frame # from page table in memory (그렇지 않으면 메모리의 페이지 테이블에서 프레임 번호를 가져옴) \\[한글 번역 및 상세 설명\\] TLB의 핵심 구현 기술인 연관 메모리(Associative Memory)와 이를 이용한 주소 변환 과정을 설명합니다. 연관 메모리 – 병렬 검색: 연관 메모리는 일반적인 RAM(주소로 데이터 접근)과 달리, 내용(content)을 기반으로 데이터를 검색합니다. 그래서 CAM(Content Addressable Memory)이라고도 불립니다. 병렬 검색(Parallel Search): 연관 메모리의 가장 큰 특징은 저장된 모든 항목에 대해 검색 키(여기서는 페이지 번호 p )를 동시에, 병렬적으로 비교한다는 것입니다. 이 덕분에 검색 속도가 매우 빠릅니다. 일반 메모리에서 특정 내용을 찾으려면 순차적으로 비교해야 하지만, 연관 메모리는 하드웨어적으로 모든 비교를 한 번에 수행합니다. TLB는 이러한 연관 메모리로 구성되어 (페이지 번호, 프레임 번호) 쌍들을 저장합니다. 주소 변환 (p, d) 과정: CPU가 논리 주소 (p, d) 를 생성합니다. p 는 페이지 번호, d 는 오프셋입니다. 페이지 번호 p 가 TLB(연관 레지스터들로 구성됨)에 검색 키로 제시됩니다. If p is in associative register, get frame # out (TLB 히트): TLB는 p 와 일치하는 페이지 번호를 가진 항목이 있는지 모든 연관 레지스터를 병렬적으로 검사합니다. 일치하는 항목이 발견되면(TLB 히트), 해당 항목에 저장된 프레임 번호(frame #)를 즉시 출력합니다. 이 프레임 번호와 원래의 오프셋 d 를 결합하여 물리 주소를 완성합니다. (매우 빠름) Otherwise get frame # from page table in memory (TLB 미스): p 와 일치하는 항목이 TLB에 없으면(TLB 미스), 시스템은 주 메모리에 있는 페이지 테이블에 접근하여 해당 페이지 p 에 대한 프레임 번호를 찾아야 합니다. (느림) 페이지 테이블에서 프레임 번호를 찾은 후, 이 (페이지 번호 p , 프레임 번호 f ) 쌍은 다음번 빠른 조회를 위해 TLB에 새로 적재됩니다. (이때 TLB 교체 정책이 적용될 수 있습니다.) 다이어그램 설명: 왼쪽에는 (Page #, Frame #) 쌍을 저장하는 연관 레지스터들(TLB의 각 항목)이 나열되어 있습니다. CPU로부터 논리 주소의 페이지 번호 p 부분이 입력됩니다. p 는 TLB의 모든 'Page #' 열의 값들과 동시에 비교됩니다. 만약 일치하는 p 가 있다면 (화살표로 표시된 히트 상황), 해당 'Frame #' 열의 값이 출력되어 물리 주소 형성에 사용됩니다. 연관 메모리를 사용함으로써 TLB는 페이지 테이블 참조의 대부분을 매우 빠르게 처리하여, 페이징으로 인한 두 번의 메모리 접근 문제를 효과적으로 완화시킵니다. Paging With TLB (TLB를 사용한 페이징) \\[한글 번역 및 상세 설명\\] TLB를 포함한 전체 페이징 주소 변환 과정을 보여주는 종합적인 다이어그램입니다. 🌟 CPU가 논리 주소 (p, d) 생성: CPU는 페이지 번호 p 와 오프셋 d 로 구성된 논리 주소를 생성합니다. TLB 조회 (Cache Access - 시간 e 소요): MMU는 먼저 페이지 번호 p 를 사용하여 TLB를 검색합니다. 이 TLB 접근에는 e 만큼의 시간이 걸립니다. ( e 는 주 메모리 접근 시간 M 보다 훨씬 작습니다.) TLB 히트 (TLB Hit) 시: 만약 TLB에서 p 와 일치하는 항목이 발견되면 (TLB 히트), 해당 항목으로부터 프레임 번호 f 를 즉시 얻습니다. 물리 주소 (f, d) 가 형성됩니다. 이 물리 주소를 사용하여 주 메모리에 접근하여 실제 데이터나 명령어를 가져옵니다. 이 메모리 접근에는 M 만큼의 시간이 걸립니다. 총 소요 시간 (TLB 히트 시) = e + M TLB 미스 (TLB Miss) 시: 만약 TLB에서 p 와 일치하는 항목이 없으면 (TLB 미스), 다음 단계로 진행합니다. 페이지 테이블 조회 (Memory Access - 시간 M 소요): 시스템은 주 메모리에 있는 페이지 테이블에 접근하여 페이지 p 에 대한 페이지 테이블 항목(PTE)을 찾아야 합니다. 이 접근에 M 만큼의 시간이 걸립니다. PTE로부터 프레임 번호 f 를 얻습니다. 물리 주소 형성 및 실제 데이터 접근 (Memory Access - 시간 M 소요): 얻은 프레임 번호 f 와 오프셋 d 를 결합하여 물리 주소 (f, d) 를 형성합니다. 이 물리 주소를 사용하여 주 메모리에 접근하여 실제 데이터나 명령어를 가져옵니다. 이 접근에도 M 만큼의 시간이 걸립니다. TLB 업데이트: 페이지 테이블에서 가져온 새로운 ( p , f ) 매핑 정보는 다음번 조회를 위해 TLB에 저장됩니다. (만약 TLB가 꽉 찼다면, 기존 항목 중 하나가 교체 정책에 따라 제거됩니다.) 총 소요 시간 (TLB 미스 시) = e (TLB 조회) + M (페이지 테이블 접근) + M (실제 데이터 접근) = e + 2M 시간 변수: M : 주 메모리 접근 시간 (상대적으로 느림) e : TLB (캐시) 접근 시간 (매우 빠름, e << M ) 이 다이어그램은 TLB가 어떻게 \"빠른 경로(fast path)\" (TLB 히트 시)와 \"느린 경로(slow path)\" (TLB 미스 시)를 제공하여 평균적인 메모리 접근 시간을 크게 단축시키는지를 잘 보여줍니다. TLB 히트율이 높을수록 시스템 성능은 향상됩니다. Paging With TLB-hit (TLB 히트 시 페이징) 634x339 (Diagram focusing on TLB hit path: CPU -> Logical Address (p,d) -> TLB (hit) -> Frame f -> Physical Address (f,d) -> Memory) 1 cache access (1번의 캐시 접근) 1 memory access (1번의 메모리 접근) Access time = e + M (접근 시간 = e + M) \\[한글 번역 및 상세 설명\\] 이 슬라이드는 TLB 히트(TLB Hit)가 발생했을 때의 메모리 접근 과정을 집중적으로 보여줍니다. 이것이 페이징 시스템에서 가장 바람직하고 빠른 경로입니다. 과정 요약 (TLB 히트 시): CPU가 논리 주소 (p,d) 생성. TLB 조회: 페이지 번호 p 를 사용하여 TLB를 검색합니다. (시간 e 소요) 히트!: p 에 대한 (페이지 번호, 프레임 번호) 매핑 정보가 TLB에 존재합니다. TLB로부터 프레임 번호 f 를 즉시 얻습니다. 물리 주소 형성: (f,d) 로 물리 주소를 만듭니다. 데이터 접근: 형성된 물리 주소로 주 메모리에 접근하여 원하는 데이터나 명령어를 가져옵니다. (시간 M 소요) 소요 시간 및 접근 횟수: 1 cache access (1번의 캐시 접근): TLB를 조회하는 데 한 번의 캐시(TLB) 접근이 필요합니다. 1 memory access (1번의 메모리 접근): 실제 데이터나 명령어를 주 메모리에서 가져오는 데 한 번의 메모리 접근이 필요합니다. (페이지 테이블을 위한 추가적인 메모리 접근은 발생하지 않습니다.) Access time (총 접근 시간) = e (TLB 접근 시간) + M (메모리 접근 시간) TLB 히트 시에는 페이지 테이블을 직접 참조할 필요가 없으므로, \"두 번의 메모리 접근\" 문제가 발생하지 않고, 마치 TLB가 없는 시스템에서 직접 메모리에 한 번 접근하는 것과 유사한 속도(e는 M에 비해 매우 작으므로 e+M ≈ M)를 낼 수 있게 됩니다. 따라서 TLB 히트율을 높이는 것이 페이징 시스템의 성능에 매우 중요합니다. Paging With TLB-miss (TLB 미스 시 페이징) (Diagram focusing on TLB miss path: CPU -> Logical Address (p,d) -> TLB (miss) -> Page Table (in memory, access 1) -> Frame f -> Physical Address (f,d) -> Memory (access 2 for data). The (p,f) pair is then written to TLB.) 1 cache access (1번의 캐시 접근 - 미스) 2 memory accesses (2번의 메모리 접근 - 페이지 테이블 + 실제 데이터) Access time = e + 2M (접근 시간 = e + 2M) \\[한글 번역 및 상세 설명\\] 이 슬라이드는 TLB 미스(TLB Miss)가 발생했을 때의 메모리 접근 과정을 보여줍니다. 이는 TLB 히트 시보다 더 많은 시간이 소요되는 경로입니다. 과정 요약 (TLB 미스 시): CPU가 논리 주소 (p,d) 생성. TLB 조회: 페이지 번호 p 를 사용하여 TLB를 검색합니다. (시간 e 소요) 미스!: p 에 대한 매핑 정보가 TLB에 존재하지 않습니다. 페이지 테이블 조회 (첫 번째 메모리 접근): 주 메모리에 있는 페이지 테이블에 접근하여, 페이지 번호 p 에 해당하는 페이지 테이블 항목(PTE)을 읽어옵니다. 이 과정에서 프레임 번호 f 를 얻습니다. (시간 M 소요) 물리 주소 형성: (f,d) 로 물리 주소를 만듭니다. 데이터 접근 (두 번째 메모리 접근): 형성된 물리 주소로 주 메모리에 접근하여 원하는 데이터나 명령어를 가져옵니다. (시간 M 소요) TLB 업데이트: 페이지 테이블에서 가져온 새로운 매핑 정보 ( p 와 그에 해당하는 f )를 다음번 빠른 조회를 위해 TLB에 저장합니다. 만약 TLB가 가득 찼다면, 기존 항목 중 하나가 교체 정책에 따라 제거되고 새 항목이 들어갑니다. 소요 시간 및 접근 횟수: 1 cache access (1번의 캐시 접근): TLB를 조회했으나 실패(미스)한 접근입니다. 2 memory accesses (2번의 메모리 접근): 페이지 테이블에서 프레임 번호를 가져오기 위한 접근. 실제 데이터나 명령어를 주 메모리에서 가져오기 위한 접근. Access time (총 접근 시간) = e (TLB 접근 시간) + M (페이지 테이블 접근 시간) + M (실제 데이터 접근 시간) = e + 2M TLB 미스가 발생하면, TLB가 없을 때와 마찬가지로 두 번의 주 메모리 접근이 필요하게 되어 성능 저하가 발생합니다. 그러나 프로그램 실행의 지역성(locality) 덕분에 한 번 TLB에 적재된 정보는 짧은 시간 내에 다시 사용될 확률이 높아, 전체적으로는 TLB 히트가 미스보다 훨씬 자주 발생합니다. 이로 인해 평균적인 메모리 접근 시간은 크게 향상됩니다. Effective Access Time (유효 접근 시간) Associative Lookup = e time unit (연관 메모리 조회 = e 시간 단위) Can be α = 98%, e = 20ns for TLB search, 140ns for memory access (더 느린 메모리지만 더 나은 히트율을 가정 → α = 98%, TLB 검색에 e = 20ns, 메모리 접근에 140ns) EAT = 0.98 x 160 + 0.02 x 300 = 162.8ns \\[한글 번역 및 상세 설명\\] TLB를 사용하는 페이징 시스템의 평균적인 메모리 접근 성능을 나타내는 유효 접근 시간(Effective Access Time, EAT)을 계산하는 방법을 설명합니다. ⏱️ 용어 정의: Associative Lookup (연관 메모리 조회 시간) = e : TLB에서 페이지 번호를 검색하는 데 걸리는 시간입니다. 이 시간 e 는 주 메모리 접근 시간 M 에 비해 매우 작습니다 (예: M 의 10% 미만). Hit ratio (히트율) = α (알파): CPU가 생성한 논리 주소의 페이지 번호가 TLB에서 발견될 확률(비율)입니다. 예를 들어 α = 0.80 은 80%의 경우 TLB에서 원하는 정보를 찾는다는 의미입니다. 히트율은 TLB의 크기, 프로그램의 메모리 접근 패턴(지역성), 교체 정책 등에 의해 영향을 받습니다. (TLB 미스율은 1-α 가 됩니다.) 유효 접근 시간 (EAT) 계산 원리: EAT는 TLB 히트 시의 접근 시간과 TLB 미스 시의 접근 시간을 각각의 발생 확률(히트율, 미스율)로 가중 평균하여 계산합니다. TLB 히트 시 접근 시간 = e + M (TLB 조회 시간 + 실제 데이터 메모리 접근 시간) TLB 미스 시 접근 시간 = e + M + M = e + 2M (TLB 조회 시간 + 페이지 테이블 메모리 접근 시간 + 실제 데이터 메모리 접근 시간) 따라서, EAT = α * (e + M) + (1 - α) * (e + 2M) 슬라이드의 EAT 공식에 대한 부연 설명: 슬라이드에 제시된 EAT = (1 + e) α + (2 + e)(1 – α) 와 EAT = 2 + e – α 공식은, 주 메모리 접근 시간 M 을 1 시간 단위로 정규화하고, e 를 M 에 대한 상대적인 비율로 표현했을 때 유도될 수 있는 형태입니다. 만약 M=1 이고, e 가 e_ratio = e_actual / M 이라면, EATratio​=α(eratio​+1)+(1−α)(eratio​+2) =αeratio​+α+eratio​+2−αeratio​−2α =eratio​+2−α 이것이 슬라이드의 2 + e – α 형태와 일치합니다. (여기서 e 는 eratio​를 의미) 그러나 슬라이드의 예제 계산에서는 e 와 M 에 실제 시간 단위(ns)를 사용하고 있으므로, EAT = α * (e + M) + (1 - α) * (e + 2M) 공식을 직접 사용하는 것이 더 명확합니다. 슬라이드의 계산 과정은 이 정확한 공식을 따르고 있습니다. 예제 계산 1: 가정: 히트율 α = 80% (0.80) , TLB 검색 시간 e = 20ns , 주 메모리 접근 시간 M = 100ns . TLB 히트 시 접근 시간 = 20ns+100ns=120ns. TLB 미스 시 접근 시간 = 20ns+100ns+100ns=220ns. EAT = 0.80×(120ns)+(1−0.80)×(220ns) =0.80×120ns+0.20×220ns =96ns+44ns=140ns. TLB가 없을 때의 접근 시간(200ns)에 비해 상당히 개선되었음을 알 수 있습니다. (140ns는 200ns 대비 30% 성능 향상) 예제 계산 2 (더 느린 메모리, 더 높은 히트율): 가정: 히트율 α = 98% (0.98) , TLB 검색 시간 e = 20ns , 주 메모리 접근 시간 M = 140ns . TLB 히트 시 접근 시간 = 20ns+140ns=160ns. TLB 미스 시 접근 시간 = 20ns+140ns+140ns=300ns. EAT = 0.98×(160ns)+(1−0.98)×(300ns) =0.98×160ns+0.02×300ns =156.8ns+6ns=162.8ns. 이 예는 히트율이 매우 높더라도( α=0.98 ), 주 메모리 자체가 느리면( M=140ns ) EAT가 이전 예( M=100ns , α=0.80 일 때 140ns)보다 나빠질 수 있음을 보여줍니다. 또한, 히트율이 EAT에 미치는 영향이 매우 크다는 것을 알 수 있습니다. 미스율이 단 2%임에도 불구하고, 미스 시의 페널티(300ns)가 크기 때문에 EAT에 영향을 줍니다. 결론: TLB는 페이징 시스템의 성능에 필수적이며, EAT는 TLB 히트율( α ), TLB 접근 시간( e ), 주 메모리 접근 시간( M )에 의해 결정됩니다. 높은 히트율을 유지하는 것이 EAT를 낮추는 데 가장 중요합니다. Memory Protection (메모리 보호) Memory protection implemented by associating protection bit with each frame to indicate if read-only or read-write access is allowed (메모리 보호는 각 프레임에 보호 비트를 연관시켜 읽기 전용 또는 읽기/쓰기 접근이 허용되는지를 나타냄으로써 구현됨) Can also add more bits to indicate page execute-only, and so on (페이지 실행 전용 등을 나타내기 위해 더 많은 비트를 추가할 수도 있음) Valid-invalid bit attached to each entry in the page table: (페이지 테이블의 각 항목에 유효-무효 비트가 첨부됨:) “valid” indicates that the associated page is in the process’ logical address space, and is thus a legal page (\"유효\"는 연관된 페이지가 프로세스의 논리 주소 공간 내에 있으며, 따라서 합법적인 페이지임을 나타냄) “invalid” indicates that the page is not in the process’ logical address space (\"무효\"는 해당 페이지가 프로세스의 논리 주소 공간 내에 없음을 나타냄) Or use PTLR (또는 PTLR을 사용함) Any violations result in a trap to the kernel (어떤 위반이라도 커널로의 트랩을 발생시킴) \\[한글 번역 및 상세 설명\\] 페이징 시스템에서 메모리 보호는 매우 중요한 기능입니다. 각 프로세스가 자신에게 할당된 메모리 영역에만 접근하고, 허용된 방식으로만 접근하도록 보장해야 합니다. 🛡️ 프레임(또는 페이지 테이블 항목) 단위의 보호 비트: 메모리 보호는 주로 페이지 테이블의 각 항목(PTE, Page Table Entry)에 여러 종류의 보호 비트(protection bits)를 추가하여 구현됩니다. (슬라이드에서는 \"각 프레임에\"라고 했지만, 실제로는 PTE에 저장되어 해당 프레임에 적용됩니다.) 읽기/쓰기 권한 (Read-Only or Read-Write): 가장 기본적인 보호 비트는 해당 페이지(프레임)에 대한 접근 권한을 나타냅니다. 읽기 전용 (Read-Only, RO): 이 비트가 설정된 페이지는 내용을 읽을 수만 있고, 쓰려고 시도하면 하드웨어 트랩(예: 보호 오류, segmentation fault)이 발생합니다. 프로그램의 코드 부분(텍스트 세그먼트)은 보통 읽기 전용으로 설정됩니다. 읽기/쓰기 (Read-Write, RW): 이 페이지는 내용을 읽고 쓰는 것이 모두 허용됩니다. 데이터 세그먼트나 스택 세그먼트의 페이지들은 보통 읽기/쓰기 권한을 가집니다. 추가적인 보호 비트: 실행 전용 (Execute-Only, XO) 또는 실행 금지 (No-Execute, NX / Execute-Disable, XD): 페이지의 내용을 CPU 명령어로 실행할 수 있는지 여부를 제어합니다. 코드 페이지는 실행 가능해야 하지만, 데이터 페이지나 스택 페이지는 악의적인 코드 실행(예: 버퍼 오버플로우 공격)을 막기 위해 실행 금지(NX 또는 XD 비트 설정)로 설정하는 것이 보안상 중요합니다. 기타 커널 모드/사용자 모드 접근 권한 비트 등 더 세분화된 보호 기능이 있을 수 있습니다. 유효-무효 비트 (Valid-Invalid Bit): 페이지 테이블의 각 항목(PTE)에는 유효(valid) 비트 또는 무효(invalid) 비트 (보통 하나의 비트로 표현되어 0이면 무효, 1이면 유효)가 포함됩니다. \"valid\" (유효): 이 비트가 설정되어 있으면, 해당 논리 페이지는 현재 프로세스의 합법적인 논리 주소 공간에 속하며, 또한 실제로 물리 메모리(프레임)에 적재되어 사용 가능한 상태임을 의미합니다. 페이지 테이블 항목에 있는 프레임 번호가 유효하다는 뜻입니다. \"invalid\" (무효): 해당 논리 페이지가 프로세스의 합법적인 주소 공간에 아예 속하지 않음을 의미할 수 있습니다. 예를 들어, 프로세스가 4개의 페이지만 사용하는데 페이지 번호 5에 접근하려 할 때, 페이지 5에 대한 PTE의 유효 비트는 'invalid'로 설정되어 있을 것입니다. 또는, 해당 페이지가 합법적인 주소 공간에는 속하지만 현재 물리 메모리에 없고 디스크(백킹 스토어)에 내려가 있는 상태(swapped out 또는 paged out)임을 의미할 수도 있습니다. 이 경우, 접근 시 페이지 폴트(page fault)라는 트랩이 발생하고, 운영체제는 해당 페이지를 디스크에서 메모리로 가져오는 작업을 수행합니다. PTLR (Page-Table Length Register)과의 관계: PTLR은 프로세스가 가질 수 있는 페이지 번호의 최대 범위를 제한합니다. 만약 CPU가 생성한 페이지 번호 p 가 PTLR 값보다 크거나 같다면, 이는 명백히 잘못된 접근이므로 PTLR에 의해 1차적으로 걸러집니다. p 가 PTLR 범위 내에 있더라도, 해당 p 에 대한 PTE의 유효 비트가 'invalid'로 설정되어 있다면, 그 페이지는 접근할 수 없습니다. 즉, PTLR은 페이지 테이블 전체의 크기를 제한하고, 유효-무효 비트는 각 개별 페이지의 유효성을 나타냅니다. 위반 시 커널 트랩: 프로세스가 허용되지 않은 방식(예: 읽기 전용 페이지에 쓰기 시도)으로 메모리에 접근하거나, 유효하지 않은 페이지(예: PTLR 범위 초과 또는 유효 비트가 'invalid'인 페이지)에 접근하려고 하면, 하드웨어(MMU)는 이를 감지하고 즉시 트랩(trap)을 발생시켜 운영체제 커널에게 제어권을 넘깁니다. 커널은 이 트랩의 원인을 분석하여, 만약 단순한 페이지 폴트(페이지가 디스크에 있는 경우)라면 해당 페이지를 메모리로 가져오는 등의 처리를 하고 프로세스를 재개시키지만, 진짜 메모리 보호 위반(예: 권한 없는 접근, 존재하지 않는 주소 접근)이라면 \"Segmentation Fault\", \"Access Violation\", \"General Protection Fault\" 등의 오류를 발생시키고 해당 프로세스를 강제 종료시킵니다. 이러한 메모리 보호 메커니즘은 각 프로세스가 자신만의 격리된 환경에서 안전하게 실행될 수 있도록 보장하며, 시스템 전체의 안정성과 보안을 유지하는 데 핵심적인 역할을 합니다. Valid (v) or Invalid (i) Bit In A Page Table (페이지 테이블 내의 유효(v) 또는 무효(i) 비트) \\[한글 번역 및 상세 설명\\] 이 다이어그램은 페이지 테이블 내의 유효-무효(valid-invalid) 비트가 어떻게 사용되어 메모리 접근을 제어하는지를 보여줍니다. 📄✅❌ 시나리오: 한 프로세스가 0부터 1MB까지의 논리 주소 공간을 가지고 있다고 가정합니다. 이 논리 주소 공간은 여러 개의 페이지(Page 0, Page 1, ...)로 나뉩니다. 이 프로세스를 위한 페이지 테이블이 존재하며, 각 항목은 (프레임 번호, 유효/무효 비트) 쌍으로 구성됩니다. 다이어그램 해석: 논리 메모리 (Logical memory): 프로세스가 인식하는 주소 공간입니다. 페이지 테이블 (Page Table): 항목 0 (Page 0): 프레임 2에 매핑되어 있고, 유효 비트가 'v' (valid)입니다. 이는 논리 페이지 0이 합법적이며 현재 물리 프레임 2에 적재되어 있음을 의미합니다. 이 페이지에 대한 접근은 허용됩니다. 항목 1 (Page 1): 프레임 3에 매핑, 'v'. 합법적이고 프레임 3에 적재됨. 항목 2 (Page 2): 프레임 4를 가리키지만 유효 비트가 'i' (invalid)입니다. 이것은 두 가지 경우를 의미할 수 있습니다: 페이지가 디스크에 있음 (페이지 폴트 상황): 논리 페이지 2는 이 프로세스의 합법적인 부분이지만, 현재 물리 메모리에 없고 디스크(백킹 스토어)에 스왑 아웃(또는 페이지 아웃)되어 있을 수 있습니다. 이 페이지에 접근하려고 하면 페이지 폴트(page fault)라는 트랩이 발생합니다. 운영체제는 이 트랩을 처리하여 페이지 2를 디스크에서 비어있는 프레임(예: 프레임 4 또는 다른 프레임)으로 가져온 후, 페이지 테이블 항목을 (새 프레임 번호, 'v')로 갱신하고, 중단되었던 명령어를 다시 실행시킵니다. 페이지가 합법적이지 않음 (세그멘테이션 폴트 상황): 논리 페이지 2가 이 프로세스가 사용하도록 허가된 논리 주소 공간의 일부가 아닐 수도 있습니다. (예: 프로세스가 실제로 페이지 0, 1, 3, 5만 사용하고 페이지 2, 4는 사용하지 않는 경우). 이 경우, 접근 시도 시 메모리 보호 위반으로 간주되어 트랩(예: 세그멘테이션 폴트)이 발생하고 프로세스가 종료될 수 있습니다. (다이어그램에서 프레임 4는 물리 메모리에서 \"unused or for other process\"로 표시되어 있으므로, 페이지 폴트 상황보다는 후자의 경우, 즉 페이지 2가 이 프로세스에게 할당되지 않은 논리 영역임을 암시할 수 있습니다. 또는, 프레임 4가 현재 비어있어서 페이지 2가 디스크에서 로드될 수 있는 대상 프레임일 수도 있습니다.) 항목 3 (Page 3): 프레임 7에 매핑, 'v'. 합법적이고 프레임 7에 적재됨. 항목 4 (Page 4): 프레임 8을 가리키지만 'i'. 항목 2와 유사한 상황입니다. 항목 5 (Page 5): 프레임 9에 매핑, 'v'. 합법적이고 프레임 9에 적재됨. 물리 메모리 (Physical Memory): 페이지 테이블에서 'v'로 표시된 페이지들이 실제 프레임에 어떻게 적재되어 있는지를 보여줍니다. 프레임 2에는 페이지 0이, 프레임 3에는 페이지 1이, 프레임 7에는 페이지 3이, 프레임 9에는 페이지 5가 들어있습니다. 프레임 4와 프레임 8은 현재 비어 있거나 다른 프로세스가 사용 중일 수 있습니다. ('i'로 표시된 페이지들이 가리키는 프레임 번호는 페이지 폴트 시 재활용될 수 있는 후보이거나, 단순히 이전 정보일 수 있습니다.) 유효-무효 비트의 역할 요약: 메모리 보호: 프로세스가 자신의 논리 주소 공간을 벗어나는 페이지에 접근하는 것을 방지합니다. 가상 메모리 지원: 페이지가 현재 물리 메모리에 있는지(valid), 아니면 디스크에 있는지(invalid, 페이지 폴트 유발)를 나타내는 데 사용됩니다. 이를 통해 실제 물리 메모리 크기보다 더 큰 논리 주소 공간을 사용할 수 있게 됩니다. 유효-무효 비트는 PTLR과 함께, 그리고 다른 보호 비트들(읽기/쓰기/실행)과 함께 강력한 메모리 보호 및 관리 체계를 구성합니다. Shared Pages (공유 페이지) Shared code (공유 코드) One copy of read-only (reentrant) code shared among processes (i.e., text editors, compilers, window systems) (읽기 전용 (재진입 가능) 코드의 한 복사본이 여러 프로세스간에 공유됨 (예: 텍스트 편집기, 컴파일러, 윈도우 시스템)) Similar to multiple threads sharing the same process space (여러 스레드가 동일한 프로세스 공간을 공유하는 것과 유사함) Also useful for interprocess communication if sharing of read-write pages is allowed (읽기/쓰기 페이지의 공유가 허용된다면 프로세스 간 통신에도 유용함) Private code and data (개인 코드 및 데이터) Each process keeps a separate copy of the code and data (각 프로세스는 코드와 데이터의 개별적인 복사본을 유지함) The pages for the private code and data can appear anywhere in the logical address space (개인 코드와 데이터를 위한 페이지들은 논리 주소 공간의 어느 곳에나 나타날 수 있음) \\[한글 번역 및 상세 설명\\] 페이징 시스템의 중요한 장점 중 하나는 메모리를 효율적으로 공유할 수 있다는 것입니다. 🤝 공유 코드 (Shared Code): 개념: 여러 프로세스가 동일한 프로그램 코드를 실행할 때 (예: 여러 사용자가 동시에 같은 텍스트 편집기나 웹 브라우저를 실행하는 경우), 해당 프로그램의 코드 부분은 모든 프로세스에 의해 공유될 수 있습니다. 조건: 공유되는 코드는 반드시 읽기 전용(read-only)이어야 하며, 실행 중에 스스로를 수정하지 않는 재진입 가능(reentrant) 코드여야 합니다. 재진입 코드는 여러 프로세스(또는 스레드)가 동시에 실행해도 각자의 상태(예: 레지스터 값, 스택 데이터)에만 영향을 미치고 코드 자체는 변경되지 않아 안전하게 공유될 수 있습니다. 구현: 공유 코드의 페이지들은 물리 메모리에 단 한 벌만 적재됩니다. 그리고 이 코드를 사용하는 각 프로세스의 페이지 테이블은 해당 논리 페이지들을 이 동일한 물리 프레임들로 매핑합니다. 예시: 텍스트 편집기, 컴파일러, 라이브러리 루틴(예: 표준 C 라이브러리), 윈도우 시스템의 GUI 코드 등은 여러 프로세스에 의해 공유될 수 있는 대표적인 예입니다. 장점: 메모리를 크게 절약할 수 있습니다. 1MB 크기의 공유 코드를 10개의 프로세스가 사용한다면, 공유하지 않을 경우 10MB가 필요하지만 공유하면 1MB만으로 충분합니다. 스레드와의 유사성: 한 프로세스 내의 여러 스레드들이 부모 프로세스의 주소 공간(코드, 데이터, 힙 등)을 공유하는 것과 유사한 개념입니다. 다만, 공유 페이지는 서로 다른 프로세스들 간의 공유라는 점이 다릅니다. 프로세스 간 통신 (Interprocess Communication, IPC) 수단: 만약 읽기/쓰기(read-write)가 가능한 페이지를 여러 프로세스가 공유하도록 허용한다면, 이는 매우 효율적인 프로세스 간 통신 수단이 될 수 있습니다 (이를 공유 메모리(shared memory) 기법이라고 합니다). 한 프로세스가 공유 메모리 페이지에 데이터를 쓰면, 다른 공유하는 프로세스들이 즉시 그 변경된 내용을 볼 수 있습니다. 단, 여러 프로세스가 동시에 공유 메모리에 접근하여 데이터를 수정할 수 있으므로, 데이터 일관성을 유지하기 위한 동기화 메커니즘(예: 세마포어, 뮤텍스)이 반드시 필요합니다. 개인 코드 및 데이터 (Private Code and Data): 일반적으로 각 프로세스는 자신만의 개인적인 코드와 데이터를 가집니다. 개인 데이터: 각 프로세스의 데이터 세그먼트(전역 변수, 정적 변수), 스택 세그먼트(지역 변수, 함수 호출 정보), 힙 세그먼트(동적 할당 메모리) 등은 다른 프로세스와 공유되지 않고 해당 프로세스에게만 고유합니다. 이 페이지들은 각 프로세스마다 별도의 물리 프레임에 할당됩니다. 개인 코드: 코드가 재진입 가능하지 않거나, 프로세스별로 다르게 수정될 수 있는 경우에는 공유되지 않고 각 프로세스가 자신만의 복사본을 가질 수도 있습니다 (흔한 경우는 아님). 이러한 개인 페이지들은 해당 프로세스의 논리 주소 공간 내 어디에든 위치할 수 있으며, 페이지 테이블을 통해 고유한 물리 프레임으로 매핑됩니다. 페이징을 통한 공유는 시스템 자원, 특히 메모리를 매우 효율적으로 사용할 수 있게 해주며, 빠른 프로세스 간 통신 방법도 제공합니다. Shared Pages Example (공유 페이지 예제) Process P1 page table: ed1->F3, data1->F1, ed2->F4, ed3->F6 Process P2 page table: ed1->F3, data2->F7, ed2->F4, ed3->F6 Process P3 page table: ed1->F3, data3->F2, ed2->F4, ed3->F6 Physical Memory: F1(data1), F2(data3), F3(ed1), F4(ed2), F6(ed3), F7(data2) \\[한글 번역 및 상세 설명\\] 이 다이어그램은 세 개의 프로세스(P1, P2, P3)가 공통된 코드 페이지들을 공유하고, 각자 고유한 데이터 페이지를 가지는 상황을 보여주는 훌륭한 예제입니다. 👨‍👨‍👧‍👦+💾 시나리오: 세 개의 프로세스 P1, P2, P3가 동일한 편집기 프로그램(예: vi 또는 emacs )을 사용하고 있다고 가정합니다. 이 편집기 프로그램의 코드는 여러 페이지(예: ed1 , ed2 , ed3 )로 구성되어 있으며, 이 코드는 읽기 전용이고 재진입 가능하다고 가정합니다. 각 프로세스는 편집 작업을 위한 자신만의 데이터(예: P1은 data1 , P2는 data2 , P3는 data3 )를 가지고 있습니다. 다이어그램 해석: 각 프로세스의 페이지 테이블: Process P1의 페이지 테이블: ed1 (편집기 코드 페이지 1) → 물리 프레임 3 (F3) data1 (P1의 개인 데이터 페이지) → 물리 프레임 1 (F1) ed2 (편집기 코드 페이지 2) → 물리 프레임 4 (F4) ed3 (편집기 코드 페이지 3) → 물리 프레임 6 (F6) Process P2의 페이지 테이블: ed1 (편집기 코드 페이지 1) → 물리 프레임 3 (F3) 페이지 테이블)을 순차적으로 접근하여 주소 변환을 수행합니다. 그리고 이렇게 힘들게 알아낸 변환 결과는 다음 사용을 위해 TLB에 새로 저장합니다. 효과: 프로그램은 특정 시간 동안 특정 코드와 데이터 영역에 집중적으로 접근하는 경향이 있습니다. 이를 참조의 지역성(Locality of Reference)이라고 합니다. 이 특성 덕분에 한 번 TLB에 저장된 주소 변환 정보는 다시 사용될 확률이 매우 높습니다. 따라서 TLB 히트율(Hit Ratio)은 보통 99% 이상으로 매우 높게 유지되며, 다중 레벨 페이징의 성능 저하 단점을 대부분 상쇄시켜 줍니다. 결론적으로, 다중 레벨 페이징은 TLB라는 하드웨어의 도움을 받아 메모리 공간 효율과 시간 효율이라는 두 마리 토끼를 모두 잡는 매우 효과적인 가상 메모리 관리 기법입니다. virtual memory Background Original Text Background Virtual memory – separation of user logical memory from physical memory Only part of the program needs to be in memory for execution Logical address space can therefore be much larger than physical address space Virtual memory can be implemented via: Demand paging Demand segmentation Korean Translation 배경 가상 메모리 – 사용자 논리 메모리와 물리 메모리의 분리 실행을 위해 프로그램의 일부만 메모리에 있으면 됨 따라서 논리 주소 공간이 물리 주소 공간보다 훨씬 클 수 있음 가상 메모리는 다음을 통해 구현될 수 있음: 요구 페이징 요구 세그먼테이션 Detailed Explanation 이 슬라이드는 컴퓨터 과학, 특히 운영 체제 분야의 핵심 개념인 가상 메모리(Virtual Memory)의 기본적인 배경과 원리를 소개하고 있습니다. 가상 메모리는 현대 컴퓨팅 환경에서 메모리를 효율적으로 관리하고 다중 프로그래밍을 가능하게 하는 필수적인 기술입니다. 가상 메모리: 논리 메모리와 물리 메모리의 분리 가상 메모리의 가장 근본적인 아이디어는 '사용자 프로그램이 바라보는 메모리(논리 메모리)와 실제 하드웨어 메모리(물리 메모리)를 분리하는 것'입니다. 과거의 컴퓨터 시스템에서는 프로그램이 실행되려면 전체 코드가 물리적인 RAM(Random Access Memory)에 모두 올라가야 했습니다. 이는 여러 가지 제약을 낳았습니다. 첫째, 프로그램의 크기가 물리 메모리의 크기보다 클 수 없었습니다. 512MB의 RAM을 가진 컴퓨터에서는 1GB 크기의 프로그램을 실행할 수 없었습니다. 둘째, 여러 프로그램을 동시에 실행하는 다중 프로그래밍 환경에서 메모리 공간을 나누어 사용하는 것이 매우 비효율적이었습니다. 모든 프로그램의 전체 코드를 메모리에 올려야 했기 때문에 동시에 실행할 수 있는 프로그램의 수가 제한되었습니다. 가상 메모리는 이러한 문제를 해결하기 위해 등장했습니다. 가상 메모리 시스템에서 프로그램은 '논리 주소 공간(Logical Address Space)'이라는 자신만의 독립적인 메모리 공간을 가집니다. 이 공간은 0번지부터 시작하는 연속적인 주소로 구성되어 있으며, 프로그래머는 이 논리 주소 공간만을 고려하여 프로그램을 작성합니다. 예를 들어, 32비트 CPU 환경에서는 각 프로세스(실행 중인 프로그램)가 $2^{32}$바이트, 즉 4GB에 달하는 거대한 논리 주소 공간을 가질 수 있습니다. 중요한 점은 이 논리 주소 공간이 실제 물리 메모리(RAM)와 직접적으로 연결되지 않는다는 것입니다. 대신, 운영 체제와 CPU의 하드웨어 지원 유닛인 MMU(Memory Management Unit, 메모리 관리 장치)가 중간에서 이 둘을 연결하는 다리 역할을 합니다. MMU는 프로그램이 사용하는 논리 주소(가상 주소)를 실제 물리 메모리의 주소로 실시간으로 변환해줍니다. 이 변환 과정 덕분에 프로그램은 실제 메모리가 어떻게 구성되어 있는지, 다른 프로그램과 어떻게 공간을 나누어 쓰고 있는지 전혀 신경 쓸 필요가 없습니다. 즉, 각 프로그램은 4GB의 메모리를 독차지하고 있는 것처럼 착각하게 됩니다. 이러한 '분리'는 두 가지 핵심적인 이점을 가져옵니다. 메모리 보호: 각 프로세스는 자신만의 독립적인 논리 주소 공간을 가지므로, 한 프로세스가 다른 프로세스의 메모리 영역을 침범할 수 없습니다. 운영 체제는 MMU를 통해 각 프로세스가 할당된 주소 범위 내에서만 메모리에 접근하도록 통제할 수 있어 시스템의 안정성이 크게 향상됩니다. 메모리 효율성 증대: 프로그램의 모든 부분이 물리 메모리에 올라올 필요가 없어집니다. 이는 다음 항목에서 더 자세히 설명됩니다. 실행에 필요한 부분만 메모리에 적재 가상 메모리의 또 다른 혁신적인 특징은 '프로그램의 전체가 아닌, 당장 실행에 필요한 부분만 물리 메모리에 올려놓고 실행을 시작할 수 있다'는 점입니다. 대부분의 프로그램은 특정 순간에 코드의 극히 일부만을 실행합니다. 예를 들어, 워드 프로세서 프로그램에는 수많은 기능(글꼴 변경, 표 만들기, 맞춤법 검사 등)이 있지만, 사용자는 한 번에 하나의 기능만을 사용합니다. 또한, 프로그램에는 오류 처리 루틴이나 초기화 코드처럼 한 번만 실행되거나 거의 실행되지 않는 부분들이 많습니다. 가상 메모리 시스템은 이러한 프로그램의 지역성(locality of reference) 원리를 활용합니다. 프로그램 전체를 실행 전에 메모리에 올리는 대신, 실행에 필요한 최소한의 부분(예: 메인 함수 주변 코드)만 먼저 올립니다. 그리고 실행 중에 만약 메모리에 없는 코드나 데이터에 접근하려고 하면, 그 순간에 해당 부분을 디스크(보조 기억 장치)에서 물리 메모리로 가져옵니다. 이처럼 '필요할 때 가져오는' 방식을 요구 페이징(Demand Paging)이라고 합니다. 이 방식은 다음과 같은 장점을 가집니다. 빠른 프로그램 시작: 프로그램 전체를 읽어올 때까지 기다릴 필요가 없으므로 사용자는 프로그램을 더 빨리 시작할 수 있습니다. 메모리 공간 절약: 실제로 사용되는 부분만 메모리를 차지하므로, 한정된 물리 메모리로 더 많은 프로그램을 동시에 실행할 수 있습니다. 이는 시스템의 전반적인 처리율(throughput)과 효율성을 높입니다. 논리 주소 공간 > 물리 주소 공간 위에서 설명한 두 가지 특징, 즉 논리-물리 주소의 분리와 필요한 부분만 메모리에 적재하는 방식 덕분에 '프로그램의 논리 주소 공간이 실제 물리 메모리의 크기보다 훨씬 커질 수 있습니다.' 예를 들어, 4GB의 RAM을 가진 컴퓨터에서도 10GB 크기의 대용량 데이터 분석 프로그램을 실행할 수 있습니다. 이 프로그램은 10GB의 논리 주소 공간을 가집니다. 실행 시점에는 당장 필요한 수십 MB의 코드와 데이터만 4GB RAM의 일부에 올라옵니다. 프로그램이 실행되면서 10GB 논리 주소 공간 내의 다른 데이터에 접근하려고 하면, 운영 체제는 현재 RAM에 있는 데이터 중 당장 사용되지 않는 부분을 디스크로 잠시 내보내고(이를 '스왑 아웃'이라 합니다), 디스크에 있던 새로운 데이터를 그 자리에 읽어옵니다('스왑 인'). 이 과정은 마치 도서관의 서고(디스크)와 내 책상(물리 메모리)의 관계와 같습니다. 서고에는 수만 권의 책(거대한 논리 주소 공간)이 있지만, 내 책상 위에는 당장 읽고 있는 몇 권의 책(물리 메모리에 올라온 부분)만 올려놓습니다. 다른 책이 필요하면, 보고 있던 책을 잠시 서고에 다시 가져다 놓고 새로운 책을 가져오는 것과 같습니다. 이처럼 가상 메모리는 디스크를 RAM의 확장 공간처럼 사용하여 프로그램에게는 거대한 메모리가 있는 것처럼 환상을 제공합니다. 가상 메모리의 구현 방식 슬라이드에서는 가상 메모리를 구현하는 두 가지 주요 기법을 언급합니다. 요구 페이징 (Demand Paging): 가상 메모리를 구현하는 가장 보편적인 방법입니다. 이 방식에서는 논리 주소 공간과 물리 메모리를 모두 '페이지(Page)'와 '프레임(Frame)'이라는 고정된 크기의 블록으로 나눕니다. 논리 주소 공간의 블록을 페이지라 하고, 물리 메모리의 블록을 프레임이라 합니다. 페이지와 프레임의 크기는 보통 동일합니다(예: 4KB). 메모리 관리는 이 페이지 단위로 이루어지며, 필요한 페이지만 디스크에서 메모리의 비어있는 프레임으로 '요구될 때' 가져옵니다. 요구 세그먼테이션 (Demand Segmentation): 세그먼테이션은 메모리를 고정 크기가 아닌, 논리적인 의미를 가지는 가변 크기의 '세그먼트(Segment)' 단위로 나눕니다. 예를 들어, 프로그램은 코드 세그먼트, 데이터 세그먼트, 스택 세그먼트 등으로 나뉠 수 있습니다. 요구 세그먼테이션은 이러한 세그먼트 단위로 메모리 적재 여부를 결정합니다. 즉, 특정 세그먼트가 필요할 때 해당 세그먼트 전체를 메모리로 가져옵니다. 현대의 대부분 운영 체제(Windows, Linux, macOS 등)는 페이징 기법을 기본으로 사용하며, 일부 시스템에서는 페이징과 세그먼테이션을 결합한 하이브리드 방식을 사용하기도 합니다. 그러나 가상 메모리 논의의 중심에는 항상 요구 페이징이 있습니다. Virtual Memory That is Larger Than Physical Memory Original Text Virtual Memory That is Larger Than Physical Memory (Diagram illustrating a large logical memory space mapped to a smaller physical memory, with overflow stored on disk) Korean Translation 물리 메모리보다 큰 가상 메모리 (거대한 논리 메모리 공간이 더 작은 물리 메모리에 매핑되고, 나머지는 디스크에 저장되는 것을 보여주는 다이어그램) Detailed Explanation 이 슬라이드의 다이어그램은 가상 메모리의 핵심 원리인 '논리 주소 공간이 물리 주소 공간보다 클 수 있다'는 개념을 시각적으로 압축하여 보여줍니다. 이 그림은 가상 메모리 시스템의 전체적인 구조와 동작 방식을 이해하는 데 매우 중요합니다. 다이어그램의 구성 요소 분석 다이어그램은 크게 세 부분으로 구성되어 있습니다. 논리 메모리 (Logical Memory): 다이어그램의 왼쪽에 길고 큰 막대로 표현됩니다. 이는 하나의 프로세스가 인식하는 메모리 공간, 즉 논리 주소 공간을 나타냅니다. 이 공간은 0번지부터 시작하여 매우 큰 주소까지 이어지는 연속적인(contiguous) 메모리처럼 보입니다. 프로세스는 자신이 이 거대한 메모리 공간을 독점적으로 사용한다고 생각하며, 모든 메모리 주소 참조는 이 논리 주소 공간을 기준으로 이루어집니다. 다이어그램에서 이 막대가 매우 큰 것은 프로그램이 가질 수 있는 잠재적인 메모리 크기가 매우 크다는 것을 상징합니다. 물리 메모리 (Physical Memory): 다이어그램의 오른쪽에 논리 메모리보다 훨씬 작은 막대로 표현됩니다. 이것이 바로 컴퓨터에 실제로 장착된 하드웨어인 RAM입니다. 이 공간은 한정되어 있으며, 시스템에서 실행되는 모든 프로세스(및 운영 체제 자체)가 공유해서 사용해야 하는 귀중한 자원입니다. 다이어그램에서 논리 메모리보다 작게 그려진 것은 가상 메모리의 핵심 전제, 즉 프로그램이 요구하는 총 메모리(논리 메모리)가 실제 사용 가능한 RAM(물리 메모리)보다 클 수 있다는 점을 강조합니다. 디스크 (Backing Store / Swap Space): 다이어그램에서 원통형 저장 장치로 표현되며, '매핑(mapping)' 화살표의 일부가 이쪽을 향하고 있습니다. 이는 하드 디스크 드라이브(HDD)나 솔리드 스테이트 드라이브(SSD)와 같은 보조 기억 장치를 의미합니다. 가상 메모리 시스템에서 디스크는 두 가지 중요한 역할을 합니다. 백킹 스토어(Backing Store): 실행 파일의 전체 내용(코드, 데이터 등)이 저장되는 공간입니다. 프로세스가 처음 생성될 때, 운영 체제는 이 실행 파일을 참조하여 프로세스의 논리 주소 공간을 설정합니다. 스왑 공간(Swap Space): 물리 메모리가 부족할 때, 당장 사용되지 않는 메모리 페이지/세그먼트를 임시로 내려놓는(swap out) 공간입니다. 나중에 해당 데이터가 다시 필요해지면, 이 스왑 공간에서 다시 물리 메모리로 읽어옵니다(swap in). 본질적으로 디스크를 RAM의 확장 공간처럼 사용하는 것입니다. 메모리 매핑 (Memory Mapping): 논리 메모리와 물리 메모리, 그리고 디스크 사이를 연결하는 화살표들이 바로 메모리 매핑 과정을 나타냅니다. 이 매핑은 운영 체제와 MMU(메모리 관리 장치)에 의해 관리되는 페이지 테이블(Page Table)이라는 자료 구조를 통해 이루어집니다. 다이어그램을 보면, 논리 메모리의 특정 부분들(페이지들)만이 물리 메모리의 특정 위치(프레임들)에 연결(매핑)되어 있음을 알 수 있습니다. 논리 메모리의 나머지 부분들은 물리 메모리에 존재하지 않으며, 이들은 디스크에 저장되어 있음을 암시합니다. 다이어그램이 전달하는 핵심 메시지 이 다이어그램은 가상 메모리가 어떻게 '착시 현상'을 만들어내는지를 보여줍니다. 추상화(Abstraction)와 환상(Illusion): 프로세스는 거대하고 연속적인 '논리 메모리'라는 이상적인 환경에서 실행됩니다. 하지만 실제 현실('물리 메모리')는 작고, 여러 조각으로 나뉘어 있으며, 다른 프로세스들과 공유되고 있습니다. 가상 메모리 시스템은 이 복잡한 현실을 감추고 프로세스에게 단순하고 이상적인 메모리 모델을 제공하는 추상화 계층입니다. 마치 우리가 자동차를 운전할 때 엔진 내부의 복잡한 폭발 과정을 신경 쓰지 않고 액셀과 핸들만 조작하는 것과 같습니다. 비연속적인 물리 메모리 할당: 다이어그램에서 논리 메모리의 연속된 영역들이 물리 메모리에서는 서로 떨어진, 비연속적인 위치에 할당될 수 있음을 보여줍니다. 예를 들어, 논리 주소 0-4KB에 해당하는 페이지는 물리 주소 8192번지에, 논리 주소 4-8KB에 해당하는 페이지는 물리 주소 20480번지에 저장될 수 있습니다. 이러한 유연성 덕분에 운영 체제는 물리 메모리의 빈 공간(단편화된 공간 포함)을 효율적으로 활용할 수 있습니다. 디스크를 활용한 공간 확장: 논리 메모리의 모든 내용이 물리 메모리에 있을 필요는 없습니다. 당장 사용되지 않는 부분은 디스크에 남아있습니다. 이는 마치 책상(물리 메모리)이 좁아도 거대한 도서관(디스크)이 뒤에 있기 때문에 수많은 책(논리 메모리)을 다룰 수 있는 것과 같은 원리입니다. 이 메커니즘 덕분에 물리 메모리의 크기는 더 이상 프로그램의 크기를 제약하는 절대적인 한계가 아니게 됩니다. MMU의 역할: 이 모든 과정의 중심에는 MMU가 있습니다. CPU가 \"논리 주소 100번지의 데이터를 가져와라\"라는 명령을 내리면, 이 주소는 MMU로 전달됩니다. MMU는 페이지 테이블을 참조하여 논리 주소 100번지가 현재 물리 메모리의 어떤 주소에 매핑되어 있는지 찾아냅니다. 만약 물리 주소 8292번지에 매핑되어 있다면, MMU는 이 주소를 실제 메모리 버스로 보내 데이터를 가져옵니다. 만약 페이지 테이블에 해당 논리 주소가 물리 메모리에 없다고 표시되어 있다면, MMU는 '페이지 폴트(Page Fault)'라는 예외를 발생시켜 운영 체제에게 도움을 요청합니다. 그러면 운영 체제가 디스크에서 해당 데이터를 물리 메모리로 가져오는 후속 작업을 처리하게 됩니다. 결론적으로, 이 슬라이드는 가상 메모리가 단순한 이론이 아니라, 페이지 테이블을 이용한 주소 변환(address translation)과 디스크를 활용한 공간 확장을 통해 실제로 구현되는 구체적인 시스템임을 명확히 보여줍니다. 이는 현대 운영 체제가 다중 프로그래밍, 대용량 프로그램 실행, 시스템 안정성 확보라는 세 마리 토끼를 모두 잡을 수 있게 해주는 근간 기술입니다. Demand Paging Original Text Demand Paging Bring a page into memory only when it is needed Less I/O needed, no unnecessary I/O Less memory needed Faster response More users Korean Translation 요구 페이징 필요할 때만 페이지를 메모리로 가져옴 더 적은 I/O 필요, 불필요한 I/O 없음 더 적은 메모리 필요 더 빠른 응답 시간 => '순수 페이징(Pure Paging)' 또는 '선행 페이징(Anticipatory Paging)' 보다는 빠름 더 많은 사용자 수용 가능 Detailed Explanation 이 슬라이드는 가상 메모리를 구현하는 가장 대표적인 방법인 요구 페이징(Demand Paging)의 개념과 그로 인해 얻을 수 있는 핵심적인 이점들을 설명합니다. 요구 페이징은 '게으른 스와퍼(Lazy Swapper)'라고도 불리며, 효율적인 메모리 관리를 위한 핵심 전략입니다. 요구 페이징의 핵심 원리: \"필요할 때만 가져온다\" 요구 페이징의 기본 철학은 매우 단순하고 직관적입니다. \"어떤 페이지(page)가 실제로 필요해지기 전까지는 절대 물리 메모리로 가져오지 않는다.\" 여기서 '페이지'란 가상 메모리 시스템에서 메모리를 관리하는 고정된 크기의 기본 단위를 의미합니다(보통 4KB). '필요해지는 순간'이란 CPU가 해당 페이지 내에 있는 명령어(instruction)나 데이터(data)에 접근하려고 시도하는 바로 그 순간을 말합니다. 이와 반대되는 개념은 '순수 페이징(Pure Paging)' 또는 '선행 페이징(Anticipatory Paging)'으로, 프로그램이 시작될 때 프로그램에 속한 모든 페이지를 미리 물리 메모리에 올려놓는 방식입니다. 요구 페이징은 이러한 비효율적인 선행 작업을 완전히 배제합니다. 대신, 프로그램이 시작되면 운영 체제는 해당 프로그램의 실행에 필요한 최소한의 정보(예: 페이지 테이블)만 준비하고, 실제 코드나 데이터 페이지는 하나도 메모리에 올리지 않은 상태에서 실행을 시작합니다. 프로세스가 실행을 시작하고 첫 번째 명령어를 읽으려 할 때, 해당 명령어가 포함된 페이지는 당연히 물리 메모리에 없습니다. 이때 하드웨어(MMU)는 '페이지 폴트(Page Fault)'라는 예외(trap)를 발생시켜 운영 체제에 알립니다. 그러면 운영 체제는 이 요청에 응답하여 디스크(Backing Store)에서 해당 페이지를 찾아서 비어있는 물리 메모리 공간(프레임, frame)으로 가져옵니다. 이 작업이 완료되면, 중단되었던 명령어부터 실행을 재개합니다. 이후에도 프로그램이 실행되면서 메모리에 없는 새로운 페이지에 접근할 때마다 이 '요구 → 페이지 폴트 → 디스크 I/O → 메모리 적재' 과정이 반복됩니다. 이처럼 게으르게, 즉 요청이 있을 때만 수동적으로 페이지를 가져오는 방식은 시스템 전반에 걸쳐 상당한 성능 향상과 자원 효율성을 가져옵니다. 요구 페이징의 장점 (Benefits of Demand Paging) 슬라이드에 나열된 장점들은 요구 페이징의 본질적인 특성에서 비롯됩니다. 더 적은 I/O 필요, 불필요한 I/O 없음 (Less I/O needed, no unnecessary I/O) 컴퓨터 시스템에서 디스크 입출력(I/O)은 CPU 연산이나 메모리 접근에 비해 극도로 느린 작업입니다. 만약 프로그램의 모든 페이지를 실행 전에 메모리로 가져온다면, 수십, 수백 메가바이트의 데이터를 디스크에서 읽어오는 데 상당한 시간이 소요됩니다. 하지만 대부분의 프로그램은 전체 코드 중 극히 일부만 실행하거나, 특정 기능은 전혀 사용하지 않을 수 있습니다. 요구 페이징은 실제로 사용되는 페이지만을 디스크에서 읽어오기 때문에, 불필요한 디스크 I/O를 원천적으로 차단합니다. 이는 시스템의 I/O 부하를 크게 줄여줍니다. 더 적은 메모리 필요 (Less memory needed) 프로그램의 전체가 아닌, 현재 실행에 필요한 부분집합(working set)만이 물리 메모리를 차지합니다. 예를 들어, 100MB 크기의 프로그램이 실행 중에 실제로 10MB의 페이지만을 집중적으로 사용한다면, 물리 메모리에서는 단 10MB만 점유하게 됩니다. 이는 각 프로세스가 차지하는 물리 메모리의 양(footprint)을 크게 줄여줍니다. 한정된 물리 메모리 자원을 훨씬 효율적으로 사용할 수 있게 되는 것입니다. 더 빠른 응답 (Faster response) 사용자 입장에서 체감되는 가장 큰 장점 중 하나입니다. 프로그램을 실행시켰을 때, 시스템은 프로그램 전체를 디스크에서 읽어올 때까지 기다릴 필요가 없습니다. 최소한의 초기화 후 즉시 실행을 시작하고, 필요한 페이지는 실행 과정에서 '투명하게(transparently)' 가져옵니다. 이로 인해 프로그램의 시작 시간(startup time)이 극적으로 단축됩니다. 사용자는 프로그램을 클릭한 후 거의 즉각적으로 프로그램 창이 뜨고 상호작용을 시작할 수 있습니다. 물론, 초기 실행 중에 여러 번의 페이지 폴트가 발생하여 약간의 지연이 있을 수 있지만, 전체를 로딩하는 것보다는 훨씬 빠른 사용자 경험을 제공합니다. 더 많은 사용자 수용 가능 (More users) 이는 앞선 장점들의 종합적인 결과입니다. 각 프로세스가 더 적은 물리 메모리를 사용하므로, 동일한 크기의 물리 메모리에서 더 많은 수의 프로세스를 동시에 실행할 수 있습니다. 이를 '다중 프로그래밍의 정도(degree of multiprogramming)가 높아진다'고 표현합니다. 더 많은 프로세스가 메모리에 동시에 상주할 수 있다는 것은, CPU가 유휴 상태에 빠질 확률이 줄어든다는 것을 의미합니다. 한 프로세스가 디스크 I/O(예: 페이지 폴트 처리)를 기다리는 동안, CPU는 메모리에 있는 다른 프로세스로 전환하여 작업을 계속할 수 있습니다. 결과적으로 시스템의 전반적인 자원 활용률과 처리량이 극대화됩니다. 이는 특히 여러 사용자가 동시에 접속하여 자원을 공유하는 서버 환경에서 매우 중요한 이점입니다. 결론적으로, 요구 페이징은 단순한 기술을 넘어 현대 운영 체제의 성능과 효율성을 정의하는 핵심 패러다임입니다. 자원을 '필요할 때까지 아껴두는' 이 게으른 접근 방식은 I/O, 메모리, CPU 등 시스템의 모든 핵심 자원을 훨씬 효율적으로 사용하게 만들어, 결과적으로 더 빠르고 더 많은 작업을 처리할 수 있는 강력한 컴퓨팅 환경을 구축하는 기반이 됩니다. Valid-Invalid Bit Original Text Valid-Invalid Bit With each page table entry a valid–invalid bit is associated (v => in-memory – memory resident, i => not-in-memory) Example of a page table snapshot: (Diagram showing a page table with entries containing a frame number and a valid-invalid bit) During address translation, if valid–invalid bit in page table entry is i => page fault Korean Translation 유효-무효 비트 각 페이지 테이블 항목에는 유효-무효 비트가 연관되어 있음 (v => 메모리 내에 있음 – 메모리 상주, i => 메모리 내에 없음) 페이지 테이블 스냅샷 예시: (프레임 번호와 유효-무효 비트를 포함하는 페이지 테이블 항목 다이어그램) 주소 변환 중에 페이지 테이블 항목의 유효-무효 비트가 i이면 => 페이지 폴트 발생 Detailed Explanation 이 슬라이드는 요구 페이징을 하드웨어 수준에서 실제로 구현하기 위한 핵심 메커니즘인 유효-무효 비트(Valid-Invalid Bit)에 대해 설명합니다. 이 작은 1비트짜리 플래그는 가상 메모리 시스템이 제대로 동작하는 데 있어 결정적인 역할을 합니다. 페이지 테이블과 주소 변환의 기초 먼저 유효-무효 비트의 역할을 이해하기 위해 페이지 테이블(Page Table)의 기본 기능부터 살펴보겠습니다. 가상 메모리 시스템에서 CPU가 생성하는 모든 주소는 논리 주소(가상 주소)입니다. 이 주소는 실제 메모리 버스로 전달되기 전에 물리 주소로 변환되어야 합니다. 이 변환 작업을 담당하는 것이 하드웨어인 MMU(메메모리 관리 장치)이며, MMU는 페이지 테이블이라는 자료 구조를 참조하여 변환을 수행합니다. 페이지 테이블은 각 프로세스마다 하나씩 존재하며, 프로세스의 논리 주소 공간을 물리 메모리에 매핑하는 정보를 담고 있습니다. 논리 주소는 보통 두 부분으로 나뉩니다. 페이지 번호 (Page Number, p): 논리 주소 공간 내에서 해당 주소가 몇 번째 페이지에 속하는지를 나타냅니다. 오프셋 (Offset, d): 페이지 내에서 상대적인 위치를 나타냅니다. MMU는 CPU로부터 논리 주소를 받으면, 페이지 번호(p)를 인덱스로 사용하여 해당 프로세스의 페이지 테이블에 접근합니다. 페이지 테이블의 p 번째 항목(Entry)에는 해당 논리 페이지가 저장된 물리 메모리의 프레임 번호(Frame Number, f)가 적혀 있습니다. MMU는 이 프레임 번호(f)와 원래의 오프셋(d)을 조합하여 최종적인 물리 주소를 만들어냅니다. 예를 들어, 페이지 크기가 4KB(2&lt;sup>12&lt;/sup> 바이트)이고, CPU가 논리 주소 8195를 요청했다고 가정해봅시다. 논리 주소 8195 = 페이지 번호 2 (8195 / 4096 = 2) + 오프셋 3 (8195 % 4096 = 3) MMU는 페이지 테이블의 2번 인덱스를 찾아봅니다. 만약 2번 인덱스에 프레임 번호 7이 저장되어 있다면, 최종 물리 주소 = 프레임 번호 7 * 4096 + 오프셋 3 = 28672 + 3 = 28675가 됩니다. 유효-무효 비트의 도입 위의 설명은 모든 페이지가 항상 물리 메모리에 존재한다고 가정한 '순수 페이징'의 경우입니다. 하지만 요구 페이징 환경에서는 어떤 페이지는 물리 메모리에 있고, 어떤 페이지는 디스크에 있습니다. MMU는 주소 변환 과정에서 이 상태를 구분할 방법이 필요합니다. 바로 이때 유효-무효 비트가 사용됩니다. 페이지 테이블의 각 항목(Page Table Entry, PTE)은 프레임 번호 외에 추가적인 비트들을 포함하는데, 그중 가장 중요한 것이 바로 유효-무효 비트(또는 존재 비트, present bit)입니다. 이 비트는 단 1비트로, 두 가지 상태를 나타냅니다. 유효 (Valid, 'v' 또는 1로 표시): 이 비트가 '유효'로 설정되어 있다면, 이는 \"해당 논리 페이지가 현재 물리 메모리에 존재하며, 이 페이지 테이블 항목에 있는 프레임 번호는 유효한 물리 프레임 주소다\" 라는 의미입니다. 이 경우 MMU는 정상적으로 프레임 번호를 읽어서 물리 주소 변환을 완료합니다. 무효 (Invalid, 'i' 또는 0으로 표시): 이 비트가 '무효'로 설정되어 있다면, 이는 두 가지 중 하나의 상황을 의미합니다. 페이지가 합법적이지만, 현재 물리 메모리에 없다 (디스크에 있다): 이것이 바로 요구 페이징에서 페이지 폴트를 일으키는 주된 원인입니다. 프로세스가 접근하려는 페이지는 자신의 논리 주소 공간에 속하는 합법적인 페이지이지만, 아직 물리 메모리로 로드되지 않은 상태입니다. 페이지가 불법적이다: 프로세스가 자신의 논리 주소 공간에 할당되지도 않은 주소(예: 4GB 공간에서 5GB에 해당하는 주소)에 접근하려는 경우입니다. 유효-무효 비트와 페이지 폴트 슬라이드의 마지막 줄이 이 메커니즘의 핵심을 요약합니다: \"주소 변환 중에, 만약 유효-무효 비트가 'i'이면 => 페이지 폴트 발생\" CPU가 특정 논리 주소에 대한 접근을 요청하면, MMU의 동작은 다음과 같이 진행됩니다. MMU는 논리 주소에서 페이지 번호(p)를 추출합니다. MMU는 페이지 테이블의 p 번째 항목에 접근합니다. MMU는 해당 항목의 유효-무효 비트를 가장 먼저 확인합니다. If 비트가 'v' (valid)이면: MMU는 같은 항목에 있는 프레임 번호를 사용하여 물리 주소를 계산하고, 메모리 접근을 계속 진행합니다. 이 모든 과정은 하드웨어 내에서 매우 빠르게 처리되며, 운영 체제의 개입이 없습니다. If 비트가 'i' (invalid)이면: MMU는 주소 변환을 즉시 중단합니다. 그리고 CPU에 '페이지 폴트(Page Fault)'라는 이름의 트랩(trap, 하드웨어 인터럽트의 일종)을 발생시킵니다. 이 트랩은 현재 실행 중인 프로세스를 중단시키고, 운영 체제 내에 미리 정의된 페이지 폴트 핸들러(Page Fault Handler) 루틴으로 제어권을 강제로 넘깁니다. 결국 유효-무효 비트는 하드웨어(MMU)와 소프트웨어(운영 체제) 사이의 중요한 소통 채널 역할을 합니다. MMU는 이 비트를 통해 자신이 해결할 수 없는 문제(페이지가 메모리에 없음)를 감지하고, 운영 체제에게 \"이 문제를 해결해달라\"고 요청하는 신호를 보내는 것입니다. 이 신호가 바로 페이지 폴트이며, 이 신호를 받은 운영 체제는 해당 페이지를 디스크에서 메모리로 가져오는 복잡한 작업을 수행하게 됩니다. 이 간단한 1비트의 존재 덕분에, '필요할 때만 페이지를 가져오는' 요구 페이징의 복잡한 로직이 효율적으로 구현될 수 있는 것입니다. Page Table When Some Pages Are Not in Main Memory Original Text Page Table When Some Pages Are Not in Main Memory (Diagram showing logical memory, the page table, and physical memory. Some page table entries point to frames in physical memory, while others are marked invalid, corresponding to pages not loaded from the disk.) Korean Translation 일부 페이지가 주 메모리에 없을 때의 페이지 테이블 (논리 메모리, 페이지 테이블, 물리 메모리를 보여주는 다이어그램. 일부 페이지 테이블 항목은 물리 메모리의 프레임을 가리키고, 다른 항목들은 무효(invalid)로 표시되어 디스크로부터 아직 로드되지 않은 페이지에 해당함을 보여줌.) Detailed Explanation 이 슬라이드의 다이어그램은 앞선 슬라이드에서 설명한 이론적인 개념들, 즉 논리/물리 주소 공간의 분리, 페이지 테이블, 그리고 유효-무효 비트가 실제 시스템에서 어떻게 통합되어 동작하는지를 구체적인 예시를 통해 시각적으로 보여줍니다. 이 그림은 가상 메모리 시스템의 '스냅샷'과 같으며, 특정 시점의 메모리 상태를 명확하게 이해하는 데 큰 도움을 줍니다. 다이어그램 상세 분석 다이어그램은 세 개의 주요 컴포넌트로 구성되어 있습니다. 논리 메모리 (Logical Memory): 프로세스가 보는 가상적인 주소 공간을 나타냅니다. 다이어그램에서는 0부터 7까지 번호가 매겨진 8개의 페이지(Page 0, Page 1, ..., Page 7)로 구성되어 있습니다. 중요한 점은 프로세스 자신에게 이 메모리는 연속적인 하나의 큰 덩어리로 인식된다는 것입니다. 프로세스는 자신의 코드가 Page 0, Page 1, Page 2 순서로 차곡차곡 저장되어 있다고 생각합니다. 각 페이지의 물리적 위치나 존재 여부는 전혀 알지 못합니다. 페이지 테이블 (Page Table): 논리 메모리와 물리 메모리를 연결하는 핵심적인 자료 구조입니다. 논리 페이지 번호를 인덱스로 사용합니다. 각 항목(Entry)은 최소한 두 개의 필드를 가집니다: 유효-무효 비트(valid-invalid bit)와 프레임 번호(frame number). 유효(Valid) 항목의 예시: Page 0: v(유효) 비트가 세팅되어 있고, 프레임 번호 4를 가리킵니다. 이는 논리 페이지 0이 현재 물리 메모리의 4번 프레임에 로드되어 있음을 의미합니다. Page 1: v(유효) 비트가 세팅되어 있고, 프레임 번호 3을 가리킵니다. Page 2: v(유효) 비트가 세팅되어 있고, 프레임 번호 1을 가리킵니다. Page 5: v(유효) 비트가 세팅되어 있고, 프레임 번호 2를 가리킵니다. 무효(Invalid) 항목의 예시: Page 3, 4, 6, 7: i(무효) 비트가 세팅되어 있습니다. 이는 이 논리 페이지들이 현재 물리 메모리에 존재하지 않음을 의미합니다. 이 페이지들의 실제 데이터는 디스크(Backing Store) 어딘가에 저장되어 있을 것입니다. 프레임 번호 필드는 의미가 없거나(null), 해당 페이지의 디스크 주소 정보를 담고 있을 수도 있습니다. 물리 메모리 (Physical Memory): 실제 하드웨어 RAM을 나타냅니다. '프레임(Frame)'이라는 페이지와 동일한 크기의 블록으로 나뉘어 있습니다. 다이어그램을 보면, 물리 메모리의 프레임들이 순서대로 사용되지 않고 있음을 알 수 있습니다. 예를 들어, 논리적으로 연속된 Page 0, 1, 2가 물리적으로는 각각 Frame 4, 3, 1이라는 비연속적이고 흩어져 있는 공간에 할당되었습니다. 또한, 물리 메모리의 일부 프레임(예: Frame 0, 5, 6, 7 등)은 비어 있을 수도 있고, 다른 프로세스의 페이지에 의해 사용되고 있을 수도 있습니다. 다이어그램은 오직 이 특정 프로세스에 할당된 프레임들만 보여주고 있습니다. 다이어그램을 통한 주소 변환 과정 시뮬레이션 이 스냅샷을 기반으로 CPU가 주소를 요청할 때 어떤 일이 일어나는지 따라가 볼 수 있습니다. 시나리오 1: 성공적인 주소 변환 (페이지가 메모리에 있는 경우) CPU가 논리 페이지 2에 속하는 주소(예: C언어 코드의 variable_C 에 접근)를 요청합니다. MMU는 페이지 테이블의 2번 인덱스로 갑니다. 유효-무효 비트가 'v' (valid)인 것을 확인합니다. 같은 항목에 있는 프레임 번호 '1'을 읽습니다. CPU가 요청한 원래 주소의 오프셋(offset)과 프레임 번호 1을 조합하여 최종 물리 주소를 계산합니다. 계산된 물리 주소를 통해 실제 RAM에 접근하여 데이터를 읽거나 씁니다. 이 과정은 매우 빠르며 운영체제의 개입 없이 하드웨어적으로 처리됩니다. 시나리오 2: 페이지 폴트 발생 (페이지가 메모리에 없는 경우) CPU가 논리 페이지 3에 속하는 주소(예: 함수 function_D 를 호출)를 요청합니다. MMU는 페이지 테이블의 3번 인덱스로 갑니다. 유효-무효 비트가 'i' (invalid)인 것을 확인합니다. MMU는 주소 변환을 즉시 멈추고 페이지 폴트 트랩을 발생시켜 운영 체제를 호출합니다. 운영 체제의 페이지 폴트 핸들러가 실행되어, 디스크에서 논리 페이지 3의 데이터를 찾아 물리 메모리의 비어있는 프레임(예: Frame 7)으로 로드합니다. 로드 후, 페이지 테이블의 3번 항목을 업데이트합니다: 유효-무효 비트를 'v'로 바꾸고 프레임 번호를 '7'로 설정합니다. 원래의 명령어를 다시 시작합니다. 이제 MMU는 페이지 3에 대한 주소 변환을 성공적으로 마칠 수 있습니다. 다이어그램의 함의 이 다이어그램은 가상 메모리의 중요한 특징들을 명확히 보여줍니다. 투명성 (Transparency): 프로세스는 페이지 폴트의 존재나 물리 메모리의 복잡한 할당 상태를 전혀 인지하지 못합니다. 모든 것은 MMU와 운영체제에 의해 '투명하게' 처리됩니다. 유연성 (Flexibility): 논리적으로 연속적인 페이지를 물리적으로 흩어진 프레임에 배치할 수 있어, 물리 메모리의 단편화(fragmentation) 문제를 완화하고 메모리 사용 효율을 높입니다. 효율성 (Efficiency): 프로그램의 일부만 메모리에 올려도 실행이 가능하므로, 물리 메모리를 절약하고 더 많은 프로그램을 동시에 실행할 수 있게 합니다. 결론적으로 이 다이어그램은 요구 페이징 시스템의 정적인 구조와 동적인 동작 방식을 한눈에 파악할 수 있게 해주는 훌륭한 시각 자료입니다. 논리적 관점과 물리적 현실 사이의 괴리를 페이지 테이블과 유효-무효 비트가 어떻게 메우는지를 명확하게 보여줍니다. Slide 6 Original Text Page Fault If there is a reference to a page, first reference to that page will trap to operating system: page fault Operating system looks at another table to decide: Invalid reference => abort Just not in memory Get empty frame Swap page into frame via scheduled disk operation Reset tables to indicate page now in memory. Set validation bit = v Restart the instruction that caused the page fault Korean Translation 페이지 폴트 페이지에 대한 참조가 있을 경우, 해당 페이지에 대한 첫 번째 참조는 운영 체제에 트랩을 발생시킴: 페이지 폴트 운영 체제는 다른 테이블을 참조하여 결정: 유효하지 않은 참조 => 중단 단지 메모리에 없을 뿐 빈 프레임을 얻음 스케줄된 디스크 작업을 통해 페이지를 프레임으로 스왑 인 페이지가 이제 메모리에 있음을 나타내도록 테이블을 재설정. 유효 비트를 v로 설정 페이지 폴트를 유발한 명령을 다시 시작 Detailed Explanation 이 슬라이드는 페이지 폴트(Page Fault)가 발생했을 때, 운영 체제가 이를 처리하는 구체적인 절차를 단계별로 상세하게 설명합니다. 페이지 폴트는 오류가 아니라, 요구 페이징 시스템이 동작하기 위한 정상적이고 필수적인 과정입니다. 이는 하드웨어(MMU)가 해결할 수 없는 문제를 소프트웨어(운영 체제)에게 위임하는 정교한 협력 메커니즘입니다. 페이지 폴트의 발생 슬라이드의 첫 줄은 페이지 폴트가 발생하는 근본적인 상황을 설명합니다. 프로세스가 특정 메모리 주소(논리 주소)에 접근하려고 할 때, MMU가 해당 논리 주소가 속한 페이지를 페이지 테이블에서 찾아봅니다. 이때 만약 해당 페이지 테이블 항목(PTE)의 유효-무효 비트가 '무효(invalid)'로 설정되어 있다면, MMU는 하드웨어 트랩(trap)을 발생시킵니다. 이 트랩이 바로 '페이지 폴트'입니다. 이 트랩은 현재 실행 중이던 프로세스의 상태(레지스터 값, 프로그램 카운터 등)를 안전하게 저장하고, 운영 체제 커널 내에 미리 정의된 페이지 폴트 핸들러(Page Fault Handler) 루틴으로 제어권을 넘깁니다. 이제부터 운영 체제의 역할이 시작됩니다. 슬라이드는 이 처리 과정을 5단계로 나누어 설명합니다. 1단계: 참조의 유효성 검사 (Check Validity) 페이지 폴트가 발생했다고 해서 무조건 페이지를 메모리로 가져오는 것은 아닙니다. 운영 체제는 먼저 이 메모리 접근 시도가 합법적인지를 판단해야 합니다. 유효-무효 비트가 '무효'인 이유는 두 가지일 수 있기 때문입니다. 합법적이지만 메모리에 없는 경우 (Just not in memory): 프로세스가 자신의 논리 주소 공간 내에 있는 주소(예: 코드 영역, 데이터 영역, 스택 영역 등)에 접근했지만, 해당 페이지가 아직 디스크에서 물리 메모리로 로드되지 않은 경우입니다. 이것이 일반적인 요구 페이징 시나리오입니다. 불법적인 참조인 경우 (Invalid reference): 프로세스가 자신에게 할당된 논리 주소 공간의 범위를 벗어나는 주소에 접근하려고 시도한 경우입니다. 예를 들어, 4MB의 주소 공간을 할당받은 프로세스가 5MB 위치의 주소를 참조하려는 경우입니다. 이는 명백한 프로그래밍 오류이며, 메모리 보호(memory protection) 위반입니다. 운영 체제는 프로세스 제어 블록(Process Control Block, PCB) 등에 저장된 해당 프로세스의 메모리 할당 정보(예: 코드와 데이터 세그먼트의 시작 주소와 크기)를 참조하여, 폴트가 발생한 주소가 합법적인 범위 내에 있는지 검사합니다. 검사 결과가 '불법'이면: 운영 체제는 더 이상 진행하지 않고 해당 프로세스를 강제 종료(abort)시킵니다. 이는 \"세그멘테이션 폴트(Segmentation Fault)\"와 같은 오류 메시지로 사용자에게 알려집니다. 검사 결과가 '합법'이면: 운영 체제는 다음 단계로 진행하여 페이지를 메모리로 가져올 준비를 합니다. 2단계: 빈 프레임 확보 (Get Empty Frame) 이제 합법적인 페이지를 디스크에서 읽어와 담아둘 물리 메모리 공간, 즉 빈 프레임(empty frame)을 찾아야 합니다. 운영 체제는 현재 사용 가능한 빈 프레임들의 목록(free-frame list)을 관리하고 있습니다. 만약 이 목록에 빈 프레임이 있다면, 간단히 하나를 할당받아 사용하면 됩니다. 만약 빈 프레임이 없다면? 이것이 바로 페이지 교체(Page Replacement)가 필요한 상황입니다. 운영 체제는 현재 메모리에 있는 페이지들 중에서 '희생될 페이지(victim page)'를 하나 선택해야 합니다. 어떤 페이지를 희생시킬지 결정하는 정책을 페이지 교체 알고리즘(예: LRU, FIFO 등)이라고 하며, 이는 시스템 성능에 매우 큰 영향을 미칩니다. 희생될 페이지가 결정되면, 그 페이지가 차지하던 프레임을 비우고 새로운 페이지를 위한 공간으로 사용합니다. 3단계: 페이지 스왑 인 (Swap Page In) 빈 프레임이 확보되면, 운영 체제는 디스크 컨트롤러에게 I/O 요청을 보냅니다. 폴트를 발생시킨 페이지가 디스크의 어느 위치에 저장되어 있는지 찾습니다. (이 정보는 보통 운영 체제가 관리하는 별도의 자료 구조에 있습니다.) 해당 위치의 데이터를 읽어서, 2단계에서 확보한 빈 프레임으로 복사(load)하라고 명령합니다. 이 디스크 I/O 작업은 CPU 속도에 비해 매우 느립니다. 수 밀리초(ms)가 걸릴 수 있습니다. 이 긴 시간 동안 CPU를 놀게 둘 수는 없으므로, 운영 체제는 페이지 폴트를 일으킨 프로세스의 상태를 '대기(waiting)' 상태로 바꾸고, CPU를 다른 실행 가능한 프로세스에게 할당합니다(Context Switching). 이렇게 함으로써 시스템의 전반적인 효율성을 유지합니다. 4단계: 테이블 재설정 (Reset Tables) 디스크 읽기 작업이 완료되었다는 인터럽트를 디스크 컨트롤러로부터 받으면, 운영 체제는 후속 작업을 마무리합니다. 페이지 폴트를 일으켰던 프로세스의 페이지 테이블을 수정합니다. 해당 페이지의 항목으로 가서, 유효-무효 비트를 'i'에서 'v'로 변경합니다. 프레임 번호 필드에, 3단계에서 페이지를 로드한 물리 프레임의 번호를 기록합니다. 이제 이 프로세스는 다시 실행될 준비가 되었으므로, 프로세스의 상태를 '대기'에서 '준비(ready)' 상태로 바꾸고, 준비 큐(ready queue)에 넣습니다. 5단계: 명령어 재시작 (Restart Instruction) 이제 모든 준비가 끝났습니다. CPU 스케줄러에 의해 해당 프로세스가 다시 실행될 차례가 되면, 운영 체제는 이전에 페이지 폴트 때문에 중단되었던 바로 그 명령어를 처음부터 다시 시작시킵니다. 이제 해당 명령어가 다시 실행되면, MMU는 똑같은 논리 주소에 대한 변환을 시도할 것입니다. 하지만 이번에는 페이지 테이블의 유효-무효 비트가 'v'로 설정되어 있고, 올바른 프레임 번호가 기록되어 있으므로, 주소 변환은 성공적으로 완료됩니다. 프로세스는 페이지 폴트가 있었는지조차 모른 채 자연스럽게 실행을 이어나가게 됩니다. 이 5단계의 과정은 가상 메모리 시스템의 심장과도 같은 동적인 절차이며, 운영 체제가 어떻게 하드웨어와 협력하여 사용자에게 '무한한 메모리'라는 환상을 제공하는지를 명확하게 보여줍니다. Steps in Handling a Page Fault Original Text Steps in Handling a Page Fault (A detailed diagram illustrating the 6 steps of handling a page fault, from the initial memory reference to restarting the instruction.) Korean Translation 페이지 폴트 처리 단계 (최초 메모리 참조부터 명령어 재시작까지, 페이지 폴트를 처리하는 6가지 단계를 보여주는 상세한 다이어그램) Detailed Explanation 이 슬라이드의 다이어그램은 바로 앞 슬라이드에서 텍스트로 설명했던 페이지 폴트 처리 과정을 한 장의 그림으로 요약하여 시각화한 것입니다. 각 번호가 붙은 단계는 운영 체제와 하드웨어가 협력하여 메모리에 없는 페이지를 가져오는 일련의 동작을 명확하게 보여줍니다. 이 다이어그램을 단계별로 따라가면 페이지 폴트 처리의 전체 흐름을 직관적으로 이해할 수 있습니다. 다이어그램의 6단계 상세 해설 참조 (Reference) 동작 주체: CPU (프로세스) 설명: 모든 것은 프로세스가 특정 논리 주소(logical address)에 접근을 시도하면서 시작됩니다. 이 주소는 읽으려는 명령어일 수도 있고, 읽거나 쓰려는 데이터일 수도 있습니다. CPU는 이 논리 주소를 생성하여 MMU(메모리 관리 장치)에게 전달합니다. 다이어그램에서는 load M 이라는 명령어로 표현되어, 'M'이라는 주소의 내용을 로드하라는 요청을 나타냅니다. 트랩 (Trap to Operating System) 동작 주체: MMU (하드웨어) 설명: MMU는 전달받은 논리 주소를 물리 주소로 변환하기 위해 프로세스의 페이지 테이블을 참조합니다. 이때 해당 페이지의 유효-무효 비트(valid-invalid bit)가 '무효(invalid)'인 것을 발견합니다. 이는 요청된 페이지가 현재 물리 메모리에 없다는 의미입니다. MMU는 주소 변환을 계속할 수 없으므로, 하드웨어 트랩(trap)을 발생시켜 CPU의 제어권을 강제로 운영 체제(Operating System)에게 넘깁니다. 이 순간이 바로 '페이지 폴트'가 발생한 시점입니다. 페이지가 디스크에 있음을 확인 (Page is on backing store) 동작 주체: 운영 체제 (소프트웨어) 설명: 제어권을 넘겨받은 운영 체제의 페이지 폴트 핸들러가 가장 먼저 하는 일입니다. 앞 슬라이드의 1단계(유효성 검사)에 해당합니다. 운영 체제는 내부 자료 구조를 확인하여 이 메모리 참조가 합법적인지, 즉 프로세스에 할당된 주소 공간 내의 접근인지 확인합니다. 합법적인 참조라면, 해당 페이지가 단지 물리 메모리에 없을 뿐이며, 디스크(backing store)에 존재한다는 것을 확인합니다. 불법적인 참조였다면 여기서 프로세스를 종료시켰을 것입니다. 페이지 인 (Bring in missing page / Page In) 동작 주체: 운영 체제 & 디스크 컨트롤러 설명: 이제 운영 체제는 페이지를 물리 메모리로 가져오는 실제 작업을 시작합니다. a. 빈 프레임 찾기: 운영 체제는 물리 메모리에서 비어있는 프레임을 찾습니다. (만약 없다면, 페이지 교체 알고리즘을 실행하여 희생될 페이지를 골라 해당 프레임을 비웁니다.) b. 디스크 I/O 요청: 디스크에 있는 해당 페이지의 위치를 찾아, 디스크 컨트롤러에게 그 내용을 새로 찾은 빈 프레임으로 읽어오라는 I/O 요청을 보냅니다. c. 컨텍스트 스위칭: 디스크 I/O는 매우 느리기 때문에, 운영 체제는 이 페이지 폴트를 일으킨 프로세스를 '대기' 상태로 만들고, CPU를 다른 '준비' 상태의 프로세스에게 할당합니다. 이 다이어그램에서는 이 복잡한 과정을 '페이지를 가져온다(Bring in missing page)'는 하나의 화살표로 단순화하여 표현했습니다. 테이블 재설정 (Reset page table) 동작 주체: 운영 체제 설명: 디스크 I/O 작업이 완료되면 디스크 컨트롤러가 CPU에 인터럽트를 보내 운영 체제에게 알립니다. 그러면 운영 체제는 마무리 작업을 수행합니다. 프로세스의 페이지 테이블로 가서, 방금 메모리로 가져온 페이지에 해당하는 항목을 수정합니다. 유효-무효 비트를 'i'에서 'v'로 변경합니다. 프레임 번호 필드에 실제 데이터가 로드된 물리 프레임의 주소를 기록합니다. '대기' 상태에 있던 프로세스를 다시 실행할 수 있는 '준비' 상태로 변경하여 준비 큐에 넣습니다. 명령어 재시작 (Restart instruction) 동작 주체: 운영 체제 -> CPU (프로세스) 설명: 이제 모든 것이 준비되었습니다. CPU 스케줄러에 의해 이 프로세스가 다시 실행될 차례가 오면, 제어권이 프로세스에게 돌아갑니다. 중요한 것은, 실행이 중단되었던 지점부터가 아니라, 페이지 폴트를 유발했던 바로 그 명령어를 처음부터 다시 실행한다는 점입니다. 이번에는 load M 명령이 다시 실행되면 MMU는 페이지 테이블을 참조하고, 5단계에서 업데이트된 유효한(valid) 항목을 찾게 됩니다. 따라서 주소 변환은 성공적으로 완료되고, 프로세스는 아무 일도 없었다는 듯이 다음 명령어로 계속 진행하게 됩니다. 이 다이어그램은 페이지 폴트가 단순한 오류가 아니라, 하드웨어와 소프트웨어가 긴밀하게 협력하여 가상 메모리라는 추상화를 구현하는 매우 정교하고 동적인 프로세스임을 명확하게 보여줍니다. 각 단계의 역할과 주체를 이해하는 것은 운영 체제의 메모리 관리 방식을 이해하는 데 필수적입니다. What Happens if There is no Free Frame? Original Text What Happens if There is no Free Frame? Page replacement Find some page in memory, but not really in use, page it out Performance – want an algorithm which will result in minimum number of page faults Korean Translation 만약 빈 프레임이 없다면 어떻게 되는가? 페이지 교체 메모리에 있지만 실제로는 사용되지 않는 페이지를 찾아 페이지 아웃시킴 성능 – 최소한의 페이지 폴트를 유발하는 알고리즘을 원함 Detailed Explanation 이 슬라이드는 요구 페이징 시스템 운영 중에 발생하는 매우 현실적이고 중요한 문제 상황을 제기하고, 그 해결책인 페이지 교체(Page Replacement)의 개념을 소개합니다. 이 문제는 시스템의 성능에 직접적인 영향을 미치기 때문에 매우 신중하게 다루어져야 합니다. 문제 상황: 빈 프레임의 고갈 (No Free Frame) 가상 메모리 시스템은 다중 프로그래밍의 정도를 높여 여러 프로세스를 동시에 메모리에 상주시키는 것을 목표로 합니다. 시스템이 계속 동작하고, 여러 프로세스가 페이지 폴트를 일으키면서 디스크로부터 새로운 페이지들을 물리 메모리로 가져오다 보면, 결국 물리 메모리의 모든 프레임이 가득 차는 순간이 오게 됩니다. 바로 이 시점에서, 또 다른 프로세스가 페이지 폴트를 일으켰다고 가정해봅시다. 운영 체제는 이 새로운 페이지를 가져와야 하지만, 더 이상 넣어둘 빈 공간(free frame)이 없습니다. 이것이 바로 슬라이드가 제기하는 \"만약 빈 프레임이 없다면 어떻게 되는가?\"라는 질문의 핵심 상황입니다. 시스템이 멈추거나 오류를 내서는 안 되므로, 이 상황을 해결할 방법이 필요합니다. 해결책: 페이지 교체 (Page Replacement) 이 문제의 해결책은 페이지 교체입니다. 아이디어는 간단합니다. \"새로운 페이지를 위한 공간을 만들기 위해, 현재 물리 메모리에 있는 페이지 중 하나를 희생양(victim)으로 골라 디스크로 쫓아낸다.\" 이 과정은 다음과 같은 세부 단계로 나뉩니다. 희생 페이지 선택 (Victim Selection): 운영 체제는 현재 물리 메모리에 있는 모든 페이지 중에서 어떤 페이지를 제거할지 결정해야 합니다. 이 선택은 페이지 교체 알고리즘(Page Replacement Algorithm)에 따라 이루어집니다. 어떤 알고리즘을 사용하느냐에 따라 시스템의 전체 성능이 크게 달라질 수 있습니다. 희생 페이지 내보내기 (Paging Out the Victim): 선택된 희생 페이지를 처리하는 과정입니다. 여기서 중요한 고려사항은 '더티 비트(Dirty Bit)' 또는 '수정 비트(Modify Bit)'입니다. 더티 비트(Dirty Bit): 페이지 테이블 항목(PTE)에 포함된 또 다른 하드웨어 비트입니다. 페이지가 물리 메모리로 로드된 이후, 그 내용에 한 번이라도 쓰기(write) 작업이 발생했다면 하드웨어에 의해 이 비트가 1로 설정됩니다. If Dirty Bit is 1 (페이지가 수정됨): 페이지의 내용이 변경되었다는 의미이므로, 이 변경사항을 잃어버리지 않으려면 해당 페이지의 내용을 디스크의 스왑 공간(swap space)에 반드시 기록(write back)해야 합니다. 이 과정을 페이지 아웃(page out) 또는 스왑 아웃(swap out)이라고 합니다. 이 과정은 디스크 쓰기 I/O를 수반하므로 시간이 걸립니다. If Dirty Bit is 0 (페이지가 수정되지 않음): 페이지가 메모리에 올라온 후 한 번도 수정되지 않았다는 의미입니다. 이는 디스크에 있는 원본 내용과 메모리에 있는 내용이 동일하다는 뜻이므로, 굳이 디스크에 다시 쓸 필요가 없습니다. 그냥 메모리에서 덮어쓰면 됩니다. 디스크 I/O를 한 번 절약할 수 있으므로 훨씬 효율적입니다. 새로운 페이지 가져오기 (Paging In the New Page): 희생 페이지가 차지하던 프레임이 비워지면(또는 덮어쓸 준비가 되면), 페이지 폴트를 유발했던 새로운 페이지를 디스크에서 이 프레임으로 읽어옵니다. 이 과정은 페이지 인(page in) 또는 스왑 인(swap in)입니다. 페이지 테이블 업데이트: 이 모든 과정이 끝나면, 관련된 두 페이지의 페이지 테이블 항목을 모두 업데이트해야 합니다. 희생 페이지: 유효-무효 비트를 'v'에서 'i'로 변경합니다. 새로운 페이지: 유효-무효 비트를 'i'에서 'v'로 변경하고, 프레임 번호를 새로 할당된 프레임의 번호로 기록합니다. 성능 목표: 최소한의 페이지 폴트 슬라이드의 마지막 줄은 페이지 교체의 궁극적인 목표를 강조합니다. \"최소한의 페이지 폴트를 유발하는 알고리즘을 원한다.\" 페이지 교체는 필연적으로 디스크 I/O를 동반하며, 이는 시스템 성능에 큰 부담을 줍니다. 만약 페이지 교체 알고리즘이 나빠서, 방금 쫓아낸 페이지가 바로 다음 순간에 다시 필요해지는 상황이 반복된다면 어떻게 될까요? 예를 들어, A 페이지를 쫓아내고 B를 가져왔더니, 바로 B를 사용하는 명령 다음에 A를 사용하는 명령이 있는 경우입니다. 그러면 또다시 페이지 폴트가 발생하고, 이번에는 B나 다른 페이지를 쫓아내고 A를 다시 가져와야 합니다. 이렇게 페이지 교체가 과도하게 일어나 시스템이 실제 작업은 거의 하지 못하고 페이지를 디스크와 메모리 사이에서 옮기느라 모든 시간을 허비하는 현상을 스래싱(Thrashing)이라고 합니다. 스래싱이 발생하면 시스템의 성능은 급격히 저하됩니다. 따라서 좋은 페이지 교체 알고리즘은 \"가까운 미래에 가장 사용될 가능성이 낮은 페이지\"를 희생양으로 선택해야 합니다. 미래를 예측할 수는 없으므로, 다양한 알고리즘들은 과거의 페이지 사용 패턴을 기반으로 미래를 추측합니다. 대표적인 알고리즘은 다음과 같습니다. FIFO (First-In, First-Out): 가장 오래전에 메모리에 들어온 페이지를 교체합니다. 구현은 간단하지만 성능이 좋지 않을 수 있습니다. LRU (Least Recently Used): 가장 오랫동안 사용되지 않은 페이지를 교체합니다. 미래에도 사용되지 않을 가능성이 높다는 지역성 원리에 기반하며, 일반적으로 성능이 우수하지만 구현이 복잡합니다. OPT (Optimal): 앞으로 가장 오랫동안 사용되지 않을 페이지를 교체합니다. 이론적으로 최적의 알고리즘이지만, 미래를 예측해야 하므로 실제 구현은 불가능하고 다른 알고리즘의 성능을 평가하는 기준으로만 사용됩니다. 결론적으로, 페이지 교체는 가상 메모리 시스템이 메모리 과할당(over-allocation) 상태에서도 지속적으로 동작할 수 있게 해주는 필수적인 메커니즘입니다. 하지만 이 과정의 효율성은 어떤 페이지를 교체할지 결정하는 알고리즘의 성능에 크게 좌우되며, 이는 운영 체제 설계의 중요한 연구 분야 중 하나입니다. Need For Page Replacement Original Text Need For Page Replacement (A diagram illustrating the process of page replacement: a victim page is paged out to disk, and the desired new page is paged into the now-free frame.) Korean Translation 페이지 교체의 필요성 (페이지 교체 과정을 보여주는 다이어그램: 희생 페이지가 디스크로 페이지 아웃되고, 원하는 새로운 페이지가 비워진 프레임으로 페이지 인됨.) Detailed Explanation 이 슬라이드의 다이어그램은 앞선 슬라이드에서 설명한 페이지 교체(Page Replacement)의 전체 과정을 시각적으로 명확하게 보여주는 순서도입니다. 이 그림은 페이지 폴트가 발생했지만 물리 메모리에 빈 프레임이 없을 때, 운영 체제가 어떻게 이 위기를 기회로 바꾸어 시스템을 계속 작동시키는지를 단계별로 나타냅니다. 다이어그램의 단계별 상세 분석 이 다이어그램은 페이지 교체의 핵심적인 두 가지 동작, 즉 페이지 아웃(Page Out)과 페이지 인(Page In)을 중심으로 구성되어 있습니다. 전체 과정을 따라가며 각 단계의 의미를 분석해 보겠습니다. 전제 상황: 프로세스가 페이지 j에 접근하려고 시도했으나, 페이지 j는 현재 물리 메모리에 없어 페이지 폴트가 발생했습니다. 동시에, 물리 메모리의 모든 프레임은 이미 다른 페이지들로 가득 차 있는 상태입니다. 희생 페이지 선택 및 페이지 아웃 (Find Victim page 'm' and Page it out) 동작: 다이어그램의 첫 번째 큰 화살표(1, 2)는 '페이지 아웃' 과정을 보여줍니다. 운영 체제의 페이지 교체 알고리즘이 실행되어, 현재 물리 메모리에 있는 페이지 중 페이지 m을 희생양(victim)으로 선택합니다. page out : 선택된 페이지 m은 물리 메모리의 프레임에서 디스크(Backing Store / Swap Space)로 복사됩니다. 이 그림에서 페이지 아웃 화살표가 그려져 있다는 것은, 페이지 m의 더티 비트(dirty bit)가 1이었음을 암시합니다. 즉, 페이지 m은 메모리에 로드된 후 내용이 변경되었기 때문에, 그 변경사항을 디스크에 저장해야만 합니다. 만약 더티 비트가 0이었다면 이 디스크 쓰기 과정은 생략될 수 있습니다. 희생 페이지의 페이지 테이블 수정 (Change page table for page 'm') 동작: 다이어그램의 작은 원(3)은 페이지 테이블의 변경을 나타냅니다. 페이지 m이 더 이상 물리 메모리에 존재하지 않으므로, 이 사실을 페이지 테이블에 반영해야 합니다. invalid : 페이지 m에 해당하는 페이지 테이블 항목(PTE)으로 가서 유효-무효 비트를 'v'(valid)에서 'i'(invalid)로 변경합니다. 이제 시스템은 페이지 m이 물리 메모리에 없다는 것을 알게 됩니다. 프레임 번호 필드는 이제 무의미해집니다. 새로운 페이지의 페이지 인 (Page in desired page 'j') 동작: 다이어그램의 두 번째 큰 화살표(4)는 '페이지 인' 과정을 보여줍니다. 이제 페이지 m이 차지하던 프레임이 비워졌으므로(또는 덮어쓸 수 있게 되었으므로), 원래 페이지 폴트를 유발했던 페이지 j를 디스크에서 이 프레임으로 읽어옵니다. page in : 이 과정은 디스크 읽기 I/O를 수반하며, 완료될 때까지 해당 프로세스는 대기 상태에 머물게 됩니다. 새로운 페이지의 페이지 테이블 수정 (Reset page table for page 'j') 동작: 다이어그램의 마지막 원(5)은 새로운 페이지 j에 대한 페이지 테이블 업데이트를 보여줍니다. valid : 페이지 j가 이제 물리 메모리에 성공적으로 로드되었으므로, 페이지 j에 해당하는 페이지 테이블 항목을 수정합니다. 유효-무효 비트를 'i'에서 'v'로 변경하고, 프레임 번호 필드에 페이지 j가 새로 자리 잡은 프레임의 번호를 기록합니다. 과정 완료 후: 이 모든 과정이 끝나면, 페이지 폴트를 유발했던 원래의 명령어가 재시작됩니다. 이제 CPU가 페이지 j에 접근하면 MMU는 유효한 페이지 테이블 항목을 통해 성공적으로 주소 변환을 수행할 수 있습니다. 프로세스는 아무 일 없었다는 듯이 실행을 계속합니다. 다이어그램이 강조하는 점 자원의 순환: 페이지 교체는 물리 메모리라는 한정된 자원을 '순환'시키는 메커니즘입니다. 오래되거나 덜 중요한 페이지를 내보내고, 새롭고 더 중요한 페이지를 들여옴으로써, 작은 물리 메모리로도 거대한 가상 메모리 공간을 시뮬레이션할 수 있습니다. I/O 비용: 이 다이어그램은 페이지 교체가 두 번의 디스크 I/O(페이지 아웃을 위한 쓰기 1번, 페이지 인을 위한 읽기 1번)를 유발할 수 있음을 명확히 보여줍니다. (물론, 더티 비트가 0이면 쓰기는 생략됩니다.) 이는 페이지 교체가 왜 비용이 많이 드는 작업이며, 왜 좋은 교체 알고리즘을 통해 페이지 폴트 자체를 최소화해야 하는지를 설명해 줍니다. 데이터 일관성: 페이지 테이블을 정확하게 업데이트하는 과정(2, 4단계)은 시스템의 데이터 일관성을 유지하는 데 매우 중요합니다. 만약 이 정보가 잘못되면 시스템은 엉뚱한 메모리 위치에 접근하여 심각한 오류를 일으킬 수 있습니다. 결론적으로, 이 다이어그램은 페이지 교체의 필요성과 그 구체적인 동작 방식을 압축적으로 보여주는 훌륭한 시각 자료입니다. 물리 메모리가 가득 찼을 때 시스템이 어떻게 동적으로 공간을 확보하고, 새로운 요청을 처리하며, 이 과정에서 발생하는 정보(페이지 테이블)를 일관되게 관리하는지를 한눈에 이해할 수 있게 해줍니다. Page Replacement Original Text Page Replacement 637x337 Korean Translation 페이지 교체 Detailed Explanation 이 슬라이드는 운영 체제의 메모리 관리에서 매우 중요한 주제인 페이지 교체(Page Replacement)를 소개하는 제목 슬라이드입니다. 이전 슬라이드들에서 가상 메모리의 개념, 요구 페이징, 페이지 폴트 처리 과정 등을 살펴보았습니다. 페이지 교체는 이러한 과정 중, 특히 페이지 폴트가 발생했으나 물리 메모리에 새로운 페이지를 적재할 빈 프레임(free frame)이 없는 경우에 반드시 필요한 핵심적인 후속 조치입니다. 페이지 교체의 본질과 필요성 가상 메모리 시스템의 주된 이점 중 하나는 물리 메모리의 크기보다 훨씬 큰 프로그램을 실행할 수 있게 하고, 동시에 여러 프로그램을 메모리에 올려 다중 프로그래밍의 정도를 높이는 것입니다. 이는 프로그램의 일부만 메모리에 적재하고, 나머지는 디스크(백킹 스토어)에 보관하다가 필요할 때마다 가져오는 요구 페이징(Demand Paging) 기법을 통해 가능해집니다. 프로세스가 실행 중에 아직 물리 메모리에 없는 페이지에 접근하려고 하면 페이지 폴트(Page Fault)가 발생합니다. 운영 체제는 이 폴트를 처리하기 위해 해당 페이지를 디스크에서 물리 메모리로 가져와야 합니다. 이때, 만약 물리 메모리에 사용 가능한 빈 프레임이 있다면 문제는 간단합니다. 해당 빈 프레임에 페이지를 적재하고 페이지 테이블을 업데이트한 후, 중단된 명령을 재시작하면 됩니다. 그러나 시스템이 오랜 시간 동안 여러 프로세스를 실행하다 보면, 물리 메모리의 모든 프레임이 특정 페이지들로 채워지게 됩니다. 바로 이러한 상황, 즉 물리 메모리가 가득 찬 상태에서 추가적인 페이지 폴트가 발생했을 때, 페이지 교체가 필요하게 됩니다. 새로운 페이지를 위한 공간을 만들기 위해서는 현재 메모리에 있는 페이지 중 하나를 희생양(victim page)으로 선택하여 디스크로 내보내고(이를 페이지 아웃(page out) 또는 스왑 아웃(swap out)이라고 합니다), 그 자리에 새로운 페이지를 가져와야 합니다(이를 페이지 인(page in) 또는 스왑 인(swap in)이라고 합니다). 만약 페이지 교체 메커니즘이 없다면, 물리 메모리가 가득 찼을 때 더 이상 새로운 페이지를 가져올 수 없으므로, 새로운 페이지를 요구하는 프로세스는 실행을 계속할 수 없게 됩니다. 이는 결국 시스템의 다중 프로그래밍 능력을 심각하게 저해하고, 특정 프로세스의 실행이 중단되는 결과를 초래할 수 있습니다. 따라서 페이지 교체는 가상 메모리 시스템이 지속적으로, 그리고 효율적으로 동작하기 위한 필수 불가결한 기능입니다. 페이지 교체의 목표: 페이지 폴트율 최소화 페이지 교체의 궁극적인 목표는 페이지 폴트율(page-fault rate)을 최소화하는 것입니다. 페이지 폴트가 발생하면 다음과 같은 일련의 작업이 수반됩니다. 운영 체제로의 트랩 발생 현재 프로세스의 상태 저장 페이지 폴트 처리 루틴 실행 (참조 유효성 검사 등) (페이지 교체가 필요한 경우) 희생 페이지 선택 희생 페이지가 변경되었다면 디스크에 기록 (페이지 아웃) 새로운 페이지를 디스크에서 읽어오기 (페이지 인) 페이지 테이블 업데이트 프로세스 준비 큐로 이동 프로세스 재시작 이 과정에서 가장 시간이 많이 소요되는 부분은 디스크 입출력(I/O) 작업인 페이지 아웃과 페이지 인입니다. 디스크 접근 시간은 CPU의 연산 속도나 메모리 접근 속도에 비해 수천에서 수만 배 느립니다. 따라서 페이지 폴트가 자주 발생하면 시스템의 전체 성능은 급격히 저하됩니다. CPU는 대부분의 시간을 디스크 I/O가 완료되기를 기다리며 보내게 되고, 실제 유용한 작업은 거의 수행하지 못하는 상태, 즉 스래싱(Thrashing) 상태에 빠질 수 있습니다. 스래싱을 방지하고 시스템 성능을 최적으로 유지하기 위해서는 어떤 페이지를 희생시킬지 결정하는 페이지 교체 알고리즘(Page Replacement Algorithm)의 역할이 매우 중요합니다. 좋은 알고리즘은 \"가까운 미래에 참조될 가능성이 가장 낮은 페이지\"를 교체함으로써, 곧바로 다시 필요해질 페이지를 내보내는 어리석은 선택을 피해야 합니다. 만약 잘못된 선택으로 인해 방금 내보낸 페이지가 즉시 다시 필요해진다면, 또 다른 페이지 폴트가 발생하고 추가적인 디스크 I/O가 수반되어 성능을 악화시키기 때문입니다. 페이지 교체 과정의 복잡성 페이지 교체는 단순히 페이지 하나를 내보내고 다른 하나를 들여오는 간단한 작업처럼 보일 수 있지만, 실제로는 여러 가지 고려 사항이 얽혀 있는 복잡한 과정입니다. 희생 페이지 선택: 어떤 기준으로 희생 페이지를 선택할 것인가? (이후 슬라이드에서 다양한 알고리즘 소개) 더티 비트(Dirty Bit) / 수정 비트(Modify Bit) 처리: 희생 페이지가 메모리에 적재된 후 내용이 변경되었는지 여부를 확인해야 합니다. 변경되었다면(더티 비트=1), 디스크에 그 내용을 기록해야 하지만, 변경되지 않았다면(더티 비트=0) 디스크 쓰기 작업을 생략하여 시간을 절약할 수 있습니다. 프레임 잠금(Frame Locking): 특정 페이지(예: 운영 체제 커널 코드, I/O 버퍼)는 절대로 디스크로 스왑 아웃되면 안 되는 경우가 있습니다. 이러한 페이지는 프레임에 '잠겨(locked)' 있다고 하며, 페이지 교체 알고리즘은 이러한 페이지를 희생양으로 선택해서는 안 됩니다. 알고리즘 구현 복잡성 및 오버헤드: 정교한 알고리즘은 더 나은 선택을 할 수 있지만, 알고리즘 자체를 실행하는 데 드는 시간과 자원(오버헤드)이 클 수 있습니다. 단순한 알고리즘은 오버헤드가 적지만 최적의 선택을 하지 못할 수 있습니다. 따라서 성능과 오버헤드 사이의 균형을 맞추는 것이 중요합니다. 이후 슬라이드에서는 이러한 페이지 교체 과정에서 가장 핵심적인 부분인 \"어떤 페이지를 교체할 것인가?\"라는 질문에 답하기 위한 다양한 페이지 교체 알고리즘들을 살펴볼 것입니다. 각 알고리즘은 서로 다른 철학과 전략을 가지고 있으며, 그에 따른 장단점과 성능 특성을 보입니다. 이 제목 슬라이드는 바로 그 논의의 시작을 알리는 역할을 합니다. Page Replacement Algorithms Original Text Page Replacement Algorithms Page-replacement algorithm Want lowest page-fault rate on both first access and re-access Optimal FIFO (First In First Out) Least Recently Used (LRU) Korean Translation 페이지 교체 알고리즘 페이지 교체 알고리즘 첫 접근과 재접근 모두에서 가장 낮은 페이지 폴트율을 원함 최적 (Optimal) 선입선출 (FIFO) 최근 최소 사용 (LRU) Detailed Explanation 이 슬라이드는 다양한 페이지 교체 알고리즘(Page Replacement Algorithms)을 소개하며, 이들 알고리즘이 추구하는 근본적인 목표를 명시하고 있습니다. 페이지 교체 알고리즘은 가상 메모리 시스템의 성능에 지대한 영향을 미치는 운영 체제의 핵심 구성 요소입니다. 페이지 교체 알고리즘이란? 앞서 설명했듯이, 페이지 폴트가 발생했을 때 물리 메모리에 빈 프레임이 없다면, 운영 체제는 기존에 메모리에 있던 페이지 중 하나를 디스크로 내보내고(페이지 아웃), 그 자리에 필요한 새 페이지를 가져와야(페이지 인) 합니다. 이때, \"어떤 페이지를 희생시킬 것인가?\"를 결정하는 구체적인 규칙 또는 전략을 페이지 교체 알고리즘이라고 합니다. 이 알고리즘의 선택은 시스템의 효율성에 매우 큰 영향을 미칩니다. 잘못된 페이지를 교체하면 곧바로 다시 해당 페이지가 필요해져 또 다른 페이지 폴트를 유발하고, 이는 시스템 성능 저하로 이어지기 때문입니다. 알고리즘의 목표: 최소 페이지 폴트율 (Lowest Page-Fault Rate) 슬라이드는 페이지 교체 알고리즘의 가장 중요한 목표를 \"첫 접근과 재접근 모두에서 가장 낮은 페이지 폴트율을 원함 (Want lowest page-fault rate on both first access and re-access)\"이라고 명시하고 있습니다. 이를 좀 더 자세히 풀어보겠습니다. 페이지 폴트율 (Page-Fault Rate): 프로그램 실행 중 발생하는 페이지 폴트의 빈도를 나타내는 지표입니다. 예를 들어, 1000번의 메모리 참조 중 10번의 페이지 폴트가 발생했다면 폴트율은 1%입니다. 이 비율이 낮을수록 시스템은 디스크 I/O에 소요되는 시간을 줄이고, 실제 연산에 더 많은 시간을 할애할 수 있어 전반적인 성능이 향상됩니다. 첫 접근 (First Access): 어떤 페이지가 프로그램 실행 후 처음으로 참조되는 경우를 의미합니다. 요구 페이징 시스템에서는 페이지가 처음 참조될 때 물리 메모리에 없는 것이 정상이므로, 이때 발생하는 페이지 폴트는 불가피합니다. 이를 필수 페이지 폴트(compulsory page fault) 또는 콜드 스타트 폴트(cold-start fault)라고도 합니다. 페이지 교체 알고리즘은 이러한 첫 접근 폴트 자체를 줄일 수는 없습니다. 재접근 (Re-access): 이미 메모리에 한 번 적재되었던 페이지가 디스크로 스왑 아웃되었다가 다시 참조되는 경우, 또는 메모리에 계속 머물러 있던 페이지가 다시 참조되는 경우를 포괄적으로 의미합니다. 페이지 교체 알고리즘의 성능은 주로 이 재접근 시의 페이지 폴트를 얼마나 잘 줄이느냐에 따라 평가됩니다. 만약 알고리즘이 현명하게 희생 페이지를 선택하여 가까운 미래에 다시 사용될 페이지를 메모리에 잘 유지시킨다면, 재접근 시에는 페이지 폴트 없이 메모리에서 바로 데이터를 가져올 수 있습니다(이를 페이지 히트(page hit)라고 합니다). 반대로, 곧 사용될 페이지를 성급하게 내쫓는다면, 재접근 시 또다시 페이지 폴트가 발생하여 디스크 I/O를 유발합니다. 따라서 페이지 교체 알고리즘의 핵심 과제는, 불가피한 첫 접근 폴트를 제외하고, 한정된 메모리 프레임 내에서 미래에 참조될 가능성이 높은 페이지들을 최대한 오래 유지함으로써 재접근 시의 폴트율을 낮추는 것입니다. 소개되는 주요 알고리즘 슬라이드는 대표적인 페이지 교체 알고리즘 세 가지를 나열하고 있습니다. 이들은 페이지 교체 알고리즘 논의에서 가장 기본적이면서도 중요한 위치를 차지합니다. 최적 알고리즘 (Optimal Algorithm, OPT 또는 MIN): 핵심 아이디어: 앞으로 가장 오랜 기간 동안 사용되지 않을 페이지를 교체합니다. 특징: 이론적으로 가장 낮은 페이지 폴트율을 보장합니다. 즉, 가능한 최상의 성능을 나타냅니다. 그러나 페이지의 미래 참조 시점을 정확히 예측해야 하므로 실제 시스템에서는 구현이 불가능합니다. 주로 다른 알고리즘의 성능을 평가하고 비교하기 위한 기준점(benchmark)으로 사용됩니다. 선입선출 알고리즘 (FIFO: First-In, First-Out): 핵심 아이디어: 물리 메모리에 가장 먼저 들어온 페이지를 가장 먼저 내보냅니다. 즉, 메모리에 가장 오래 머물렀던 페이지를 희생시킵니다. 특징: 개념이 매우 단순하고 구현이 용이합니다. 페이지가 메모리에 들어온 순서대로 큐(queue)에 넣어 관리하면 되기 때문입니다. 하지만 페이지의 최근 사용 빈도나 중요도를 전혀 고려하지 않기 때문에, 오랫동안 자주 사용되던 중요한 페이지가 단지 오래되었다는 이유만으로 교체될 수 있어 비효율적인 상황이 발생할 수 있습니다. 심지어 프레임 수를 늘렸는데도 페이지 폴트가 증가하는 벨레이디의 모순(Belady's Anomaly) 현상이 나타날 수 있는 알고리즘이기도 합니다. 최근 최소 사용 알고리즘 (LRU: Least Recently Used): 핵심 아이디어: 가장 오랫동안 사용되지 않은 페이지를 교체합니다. 이는 \"과거에 오랫동안 사용되지 않았다면 미래에도 사용될 가능성이 적을 것이다\"라는 참조의 지역성(locality of principle) 원리에 기반합니다. 특징: 일반적으로 FIFO보다 훨씬 좋은 성능을 보이며, 최적 알고리즘에 근접하는 성능을 내는 경우가 많습니다. 하지만 실제로 각 페이지의 '가장 마지막 사용 시점'을 정확히 추적하는 것은 상당한 하드웨어 지원이나 소프트웨어적인 오버헤드를 필요로 하기 때문에, 완전한 LRU를 구현하는 것은 비용이 많이 들 수 있습니다. 따라서 실제 시스템에서는 LRU의 근사(approximation) 알고리즘들이 많이 사용됩니다. (예: LFU(Least Frequently Used), MFU(Most Frequently Used), NUR(Not Used Recently), Second-Chance Algorithm, Clock Algorithm 등) 이 슬라이드는 앞으로 논의될 페이지 교체 전략들의 대략적인 윤곽을 제시하며, 어떤 기준으로 이들을 평가해야 하는지에 대한 방향을 설정해 줍니다. 궁극적으로 모든 페이지 교체 알고리즘은 한정된 메모리라는 제약 하에서 어떻게 하면 디스크 I/O를 최소화하여 시스템 응답성과 처리율을 극대화할 수 있을까라는 근본적인 질문에 대한 각기 다른 해답이라고 볼 수 있습니다. Optimal Algorithm Original Text Optimal Algorithm Replace page that will not be used for longest period of time How do you know this? Can’t read the future Used for measuring how well your algorithm performs Korean Translation 최적 알고리즘 가장 오랜 기간 동안 사용되지 않을 페이지를 교체 이것을 어떻게 알 수 있는가? 미래를 읽을 수 없음 여러분의 알고리즘이 얼마나 잘 수행되는지 측정하는 데 사용됨 Detailed Explanation 이 슬라이드는 최적 페이지 교체 알고리즘(Optimal Page Replacement Algorithm), 종종 OPT 또는 MIN 알고리즘이라고도 불리는 기법에 대해 설명합니다. 이 알고리즘은 이름에서 알 수 있듯이 이론적으로 가장 이상적인 페이지 교체 성능을 제공합니다. 최적 알고리즘의 교체 전략: 미래 예측 최적 알고리즘의 핵심 전략은 매우 명확하고 강력합니다. 페이지 폴트가 발생하여 희생 페이지를 선택해야 할 때, \"현재 메모리에 있는 페이지들 중에서, 앞으로 가장 오랜 기간 동안 다시 참조되지 않을 페이지를 선택하여 교체한다.\" 좀 더 풀어서 설명하면 다음과 같습니다. 페이지 폴트 발생 시, 현재 물리 메모리에 있는 모든 페이지들을 살펴봅니다. 각 페이지에 대해, 해당 페이지가 미래의 어느 시점에 다시 참조될지를 예측합니다. (정확히는 프로그램의 이후 실행될 명령어 순서, 즉 참조 문자열(reference string)을 미리 알고 있다고 가정합니다.) 이 \"미래 참조 시점\"이 가장 멀리 있는 페이지를 희생양으로 선택합니다. 만약 어떤 페이지가 미래에 다시는 참조되지 않는다면, 그 페이지가 최우선적인 교체 대상이 됩니다. 만약 여러 페이지가 미래에 다시는 참조되지 않거나, 혹은 가장 먼 미래 참조 시점이 동일한 페이지가 여러 개 있다면, 그중 아무것이나 선택해도 무방합니다 (보통은 임의로 또는 FIFO 방식으로 선택). 이러한 전략을 사용하면, 가까운 미래에 다시 사용될 페이지는 최대한 메모리에 유지시키고, 당분간 사용되지 않거나 아예 사용되지 않을 페이지만을 골라서 내보내기 때문에, 결과적으로 가장 적은 수의 페이지 폴트를 발생시킵니다. 이것이 바로 최적 알고리즘이 \"최적\"이라고 불리는 이유입니다. 주어진 참조 문자열과 고정된 수의 프레임에 대해, 최적 알고리즘보다 더 적은 페이지 폴트를 발생시키는 알고리즘은 존재할 수 없습니다. 현실적인 한계: 미래를 알 수 없음 (Can't read the future) 슬라이드는 \"이것을 어떻게 알 수 있는가? (How do you know this?)\"라는 질문을 던지고, 그 답으로 \"미래를 읽을 수 없음 (Can’t read the future)\"이라고 명시합니다. 이것이 최적 알고리즘의 가장 큰 현실적인 한계입니다. 실제 운영 체제 환경에서 프로그램이 앞으로 어떤 순서로 메모리 페이지를 참조할지 미리 정확하게 아는 것은 불가능합니다. 사용자의 입력, 외부 이벤트, 프로그램 내부의 조건 분기 등 수많은 요인에 의해 프로그램의 실행 흐름은 동적으로 변하기 때문입니다. 따라서, 최적 알고리즘은 실제 운영 체제에서 페이지 교체 전략으로 직접 구현하여 사용할 수는 없습니다. 마치 일기 예보가 완벽하게 미래의 날씨를 맞출 수 없는 것과 같습니다. 만약 운영 체제가 미래의 모든 페이지 참조를 미리 알 수 있다면, 페이지 교체뿐만 아니라 CPU 스케줄링, 디스크 스케줄링 등 다른 많은 영역에서도 완벽한 결정을 내릴 수 있을 것입니다. 하지만 이는 현실적으로 불가능한 가정입니다. 최적 알고리즘의 용도: 성능 측정의 기준 (Benchmark) 그렇다면 구현도 불가능한 이 알고리즘을 왜 배우고 언급하는 것일까요? 슬라이드의 마지막 줄이 그 답을 제시합니다. \"여러분의 알고리즘이 얼마나 잘 수행되는지 측정하는 데 사용됨 (Used for measuring how well your algorithm performs)\" 최적 알고리즘은 실제 시스템에서 사용하기 위한 것이 아니라, 다른 현실적인 페이지 교체 알고리즘(예: FIFO, LRU 등)의 성능을 평가하고 비교하기 위한 이론적인 기준점 또는 상한선(upper bound) 역할을 합니다. 구체적으로는 다음과 같이 활용됩니다. 특정 참조 문자열 수집: 실제 프로그램 실행 중 발생하는 페이지 참조 순서(참조 문자열)를 기록하거나, 시뮬레이션을 위한 가상의 참조 문자열을 생성합니다. 최적 알고리즘 시뮬레이션: 수집된 참조 문자열과 특정 프레임 수에 대해 최적 알고리즘을 적용했을 때 발생하는 페이지 폴트 수를 계산합니다. 이는 해당 조건에서 달성 가능한 최소 페이지 폴트 수가 됩니다. 다른 알고리즘 시뮬레이션 및 비교: 동일한 참조 문자열과 프레임 수에 대해 우리가 실제로 구현하고자 하는 다른 페이지 교체 알고리즘(예: LRU, FIFO)을 적용하여 각각의 페이지 폴트 수를 계산합니다. 성능 평가: 이 결과를 최적 알고리즘의 결과와 비교합니다. 예를 들어, 최적 알고리즘이 10번의 폴트를 발생시켰고, LRU 알고리즘이 12번, FIFO 알고리즘이 15번의 폴트를 발생시켰다면, LRU가 FIFO보다 최적에 더 가깝고 따라서 더 효율적인 알고리즘이라고 평가할 수 있습니다. 이러한 방식으로, 연구자나 시스템 개발자들은 자신들이 고안한 새로운 페이지 교체 알고리즘이 얼마나 이상적인 성능에 근접하는지를 정량적으로 파악할 수 있습니다. 최적 알고리즘은 도달할 수 없는 별과 같지만, 다른 모든 알고리즘이 나아가야 할 방향을 제시해 주는 등대와 같은 역할을 하는 것입니다. 결론적으로 최적 페이지 교체 알고리즘은 그 자체로는 실용적인 구현체가 아니지만, 페이지 교체 연구 및 개발 분야에서 매우 중요한 이론적 도구입니다. 이는 \"만약 우리가 미래를 알 수 있다면 얼마나 잘할 수 있을까?\"라는 질문에 대한 답을 제공하며, 현실적인 알고리즘들이 얼마나 그 이상에 가까워지려고 노력하는지를 가늠하는 척도가 됩니다. Optimal Page Replacement Original Text Optimal Page Replacement (These slides typically depict a diagram or an example of the Optimal algorithm in action. Since no specific diagram content is provided in the prompt other than the title, the explanation will assume a common example reference string and walk through the algorithm's execution.) Korean Translation 최적 페이지 교체 (이 슬라이드들은 일반적으로 최적 알고리즘이 동작하는 예시나 다이어그램을 포함합니다. 제시된 프롬프트에는 제목 외에 특정 다이어그램 내용이 없으므로, 일반적인 참조 문자열 예시를 가정하고 알고리즘 실행 과정을 설명하겠습니다.) Detailed Explanation 이 슬라이드들은 최적 페이지 교체(Optimal Page Replacement) 알고리즘이 실제로 어떻게 동작하는지를 보여주는 예시 또는 다이어그램을 위한 자리입니다. 특정 다이어그램이 주어지지 않았으므로, 가장 널리 사용되는 방식인 참조 문자열(reference string)과 고정된 수의 프레임(frame)을 사용하여 최적 알고리즘의 단계별 실행 과정을 시뮬레이션하고, 그 결과를 통해 알고리즘의 특징을 상세히 설명하겠습니다. 시뮬레이션 설정 참조 문자열 (Reference String): 프로세스가 특정 순서로 페이지를 참조하는 것을 나타내는 숫자열입니다. 각 숫자는 페이지 번호를 의미합니다. 예를 들어, 다음과 같은 참조 문자열을 사용하겠습니다. 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1 사용 가능한 물리 메모리 프레임 수: 간단한 설명을 위해 3개의 프레임을 가정하겠습니다. 초기 상태: 모든 프레임은 비어있다고 가정합니다. 목표: 최적 알고리즘을 사용하여 페이지 폴트(Page Fault, PF) 발생 횟수를 최소화합니다. 최적 페이지 교체 알고리즘 실행 과정 (3 프레임) 각 단계에서 참조되는 페이지, 현재 프레임 상태, 페이지 폴트 발생 여부(H: Hit, F: Fault), 그리고 교체가 발생할 경우 어떤 페이지가 선택되는지를 살펴보겠습니다. 참조 페이지 프레임 1 프레임 2 프레임 3 결과 (PF 수) 교체 대상 (미래 참조 순서) 7 7 F (1) (빈 프레임 사용) 0 7 0 F (2) (빈 프레임 사용) 1 7 0 1 F (3) (빈 프레임 사용) 2 2 0 1 F (4) 7 교체 (7->17번째, 0->5번째, 1->14번째) 0 2 0 1 H 3 3 0 1 F (5) 2 교체 (2->9번째, 0->7번째, 1->14번째) 0 3 0 1 H 4 4 0 1 F (6) 3 교체 (3->10번째, 0->11번째, 1->14번째) 2 2 0 1 F (7) 4 교체 (4->없음, 0->11번째, 1->14번째) 3 2 0 3 F (8) 1 교체 (1->14번째, 0->11번째, 2->13번째) 0 2 0 3 H 3 2 0 3 H 2 2 0 3 H 1 1 0 3 F (9) 2 교체 (2->15번째, 0->16번째, 3->없음) 2 1 0 2 F (10) 3 교체 (3->없음, 0->16번째, 1->17번째) 0 1 0 2 H 1 1 0 2 H 7 7 0 2 F (11) 1 교체 (1->20번째, 0->19번째, 2->없음) 0 7 0 2 H 1 7 0 1 F (12) 2 교체 (2->없음, 0->없음, 7->없음. 임의로 가장 오래된 2) 최종 페이지 폴트 수: 12회 (위 예시에서 마지막 교체 대상 선정에 약간의 모호함이 있을 수 있으나, 일반적인 OPT 규칙을 따름) 실제 정확한 OPT 폴트 수는 참조열과 프레임 수에 따라, 그리고 타이 브레이킹 규칙에 따라 약간 달라질 수 있습니다. 위는 일반적인 과정을 보여주기 위한 예시입니다. 더 표준적인 예제(예: 7,0,1,2,0,3,0,4,2,3,0,3,2,1,2,0,1,7,0,1, 3프레임 시 9회 폴트가 나오는 경우)를 기준으로 설명하자면: 표준 예시 (3 프레임, 참조열: 7,0,1,2,0,3,0,4,2,3,0,3,2,1,2,0,1,7,0,1) 7: \\[7, , \\] (Fault: 1) 0: \\[7, 0, _]\\ (Fault: 2) 1: \\[7, 0, 1\\] (Fault: 3) 2: 참조 문자열에서 7은 18번째, 0은 5번째, 1은 14번째에 다시 나타남. 가장 먼 7을 교체. [2, 0, 1] (Fault: 4) 0: [2, 0, 1] (Hit) 3: 2는 9번째, 0은 7번째, 1은 14번째. 가장 먼 1을 교체. [2, 0, 3] (Fault: 5) 0: [2, 0, 3] (Hit) 4: 2는 9번째, 0은 11번째, 3은 10번째. 가장 먼 0을 교체 (또는 2, 3도 가능. 이후 참조가 없는 페이지가 있다면 우선). 미래 2,0,3 중 가장 먼 것은 0 (11번째), 2(9번째), 3(10번째) 입니다. 하지만 다음 참조 2,3,0,3,2,1,2,0,1,7,0,1 을 보면 페이지 4가 들어올때, 프레임 안의 [2,0,3] 중에서는 페이지 2는 다음 2에서, 페이지 0은 다음 0에서, 페이지 3은 다음 3에서 사용됩니다. 이 경우 0이 7번째 0에, 2가 9번째 2에, 3이 10번째 3에 있습니다. 가장 늦게 사용될 페이지는 3. (만약 다음 참조가 더 없다면 아무거나) 여기서 교체할 페이지는 미래 참조를 기반으로 결정됩니다. 2는 바로 다음 2, 0은 그 다음 0, 3은 그 다음 3. 페이지 0, 2, 3중 가장 나중에 참조되는 페이지를 교체한다. 페이지 4가 참조되면, 현재 [2,0,3]. 2의 다음참조는 9번째, 0의 다음참조는 11번째, 3의 다음참조는 10번째. 따라서 가장 나중인 0을 교체한다. [2, 4, 3] (Fault: 6) (0을 교체) 2: [2, 4, 3] (Hit) 3: [2, 4, 3] (Hit) 0: 2는 13번째, 4는 없음, 3은 12번째. 4를 교체. [2, 0, 3] (Fault: 7) 3: [2, 0, 3] (Hit) 2: [2, 0, 3] (Hit) 1: 2는 15번째, 0은 16번째, 3은 없음. 3을 교체. [2, 0, 1] (Fault: 8) 2: [2, 0, 1] (Hit) 0: [2, 0, 1] (Hit) 1: [2, 0, 1] (Hit) 7: 2는 없음, 0은 19번째, 1은 20번째. 2를 교체. [7, 0, 1] (Fault: 9) 0: [7, 0, 1] (Hit) 1: [7, 0, 1] (Hit) 최종 페이지 폴트 (표준 예시): 9회 분석 및 특징 미래 참조 기반 결정: 각 교체 시점에서, 최적 알고리즘은 현재 프레임에 있는 페이지들이 앞으로 언제 다시 사용될지를 살펴봅니다. 이 \"미래 예측\"이 핵심입니다. 페이지 폴트 최소화: 위 예시에서 9번의 페이지 폴트가 발생했습니다. 어떤 다른 알고리즘을 사용하더라도 이 참조 문자열과 3개의 프레임 조건에서는 9번보다 적은 페이지 폴트를 만들 수 없습니다. 이것이 최적 알고리즘의 강력함입니다. 불가능한 예측: 현실에서는 다음 참조될 페이지가 무엇일지 미리 알 수 없습니다. 프로그램의 실행 경로는 사용자 입력, 조건 분기 등 다양한 요인에 의해 결정되기 때문입니다. 따라서 최적 알고리즘은 실제 운영 체제에서 구현하여 사용할 수 없습니다. 비교 기준으로의 가치: 그럼에도 불구하고 최적 알고리즘은 매우 중요합니다. 성능의 상한선: 특정 조건에서 달성 가능한 이론적인 최상의 성능(최소 폴트 수)을 보여줍니다. 다른 알고리즘 평가: FIFO, LRU 등 실제 구현 가능한 알고리즘들이 얼마나 최적 알고리즘의 성능에 근접하는지를 비교함으로써 그 효율성을 평가할 수 있습니다. 예를 들어, 동일 조건에서 LRU가 12번, FIFO가 15번의 폴트를 발생시킨다면, LRU가 FIFO보다 더 우수한 알고리즘이라고 판단할 수 있습니다. 구현 시 고려사항 (이론적): 만약 미래를 알 수 있다면, 각 페이지마다 다음 참조까지의 거리를 계산하고, 이 거리가 가장 긴 페이지를 교체 대상으로 선택하면 됩니다. 만약 어떤 페이지가 미래에 더 이상 참조되지 않는다면, 그 페이지는 가장 우선적인 교체 대상이 됩니다. 4 프레임으로 확장 시의 변화 (개념적) 만약 사용 가능한 프레임 수가 3개에서 4개로 늘어난다면 어떤 변화가 있을까요? 일반적으로 프레임 수가 증가하면 페이지 폴트 횟수는 감소하거나 최소한 동일하게 유지됩니다 (벨레이디의 모순이 없는 대부분의 알고리즘에서). 최적 알고리즘의 경우, 프레임이 더 많아지면 더 많은 페이지를 메모리에 유지할 수 있으므로, 교체가 필요한 상황 자체가 줄어들거나, 교체 시 더 나은(더 먼 미래에 사용될) 페이지를 남겨둘 수 있는 선택지가 늘어나 페이지 폴트가 감소할 가능성이 큽니다. 예를 들어, 위의 3프레임 예시에서 4번째 프레임이 추가된다면, 초기 페이지 7, 0, 1, 2는 모두 폴트를 발생시키며 프레임에 순서대로 들어갈 것입니다. 그 이후의 교체 결정은 4개의 페이지 중에서 가장 먼 미래에 사용될 페이지를 선택하게 되므로, 3프레임일 때보다 더 적은 페이지 폴트를 기대할 수 있습니다. (예: 4프레임 시 7회 폴트) 이러한 예시들은 최적 알고리즘이 어떻게 각 단계마다 \"가장 이상적인\" 결정을 내리는지를 보여줍니다. 비록 현실 세계의 제약으로 인해 직접 사용할 수는 없지만, 이 알고리즘의 원리를 이해하는 것은 다른 실용적인 페이지 교체 알고리즘들의 설계 철학과 성능 한계를 파악하는 데 중요한 기초가 됩니다. 슬라이드 8.15와 8.16은 보통 이러한 과정을 그림으로 표현하여 직관적인 이해를 돕습니다. Slide 15 (8.17) Original Text FIFO Algorithm Replace page that is oldest How do you know this? FIFO queue Korean Translation FIFO 알고리즘 가장 오래된 페이지를 교체 이것을 어떻게 알 수 있는가? FIFO 큐 Detailed Explanation 이 슬라이드는 페이지 교체 알고리즘 중 가장 간단하고 직관적인 방법 중 하나인 FIFO(First-In, First-Out) 알고리즘에 대해 설명합니다. FIFO는 우리말로 선입선출 알고리즘이라고 하며, 그 이름에서 알 수 있듯이 메모리에 가장 먼저 들어온 페이지를 가장 먼저 내보내는 방식을 사용합니다. FIFO 알고리즘의 교체 전략: 도착 순서 기반 FIFO 알고리즘의 핵심 전략은 매우 단순합니다. 페이지 폴트가 발생하여 희생 페이지를 선택해야 할 때, \"현재 물리 메모리에 있는 페이지들 중에서 가장 먼저 메모리에 적재되었던 페이지, 즉 가장 오랫동안 메모리에 머물렀던 페이지를 선택하여 교체한다.\" 이는 마치 우리가 줄을 서서 서비스를 기다리는 것과 같습니다. 가장 먼저 줄을 선 사람(가장 먼저 메모리에 들어온 페이지)이 가장 먼저 서비스(교체 대상)를 받는 것입니다. 이 알고리즘은 페이지가 얼마나 자주 사용되었는지, 또는 얼마나 최근에 사용되었는지와 같은 페이지의 사용 패턴은 전혀 고려하지 않습니다. 오직 메모리 도착 시간(arrival time)만이 유일한 교체 기준이 됩니다. 구현 방법: FIFO 큐 (FIFO Queue) 슬라이드는 \"이것을 어떻게 알 수 있는가? (How do you know this?)\"라는 질문에 대한 답으로 \"FIFO 큐\"를 제시합니다. FIFO 알고리즘은 운영 체제가 페이지들의 도착 순서를 기억하기 위해 간단한 큐(Queue) 자료 구조를 사용하여 구현할 수 있습니다. 페이지 적재 시: 새로운 페이지가 페이지 폴트에 의해 물리 메모리의 빈 프레임으로 적재될 때, 해당 페이지의 번호를 FIFO 큐의 꼬리(tail)에 추가합니다. 희생 페이지 선택 시: 페이지 교체가 필요한 상황이 되면, FIFO 큐의 머리(head)에 있는 페이지를 제거합니다. 이 페이지가 바로 가장 오래전에 메모리에 들어온 페이지, 즉 교체될 희생 페이지입니다. 새로운 페이지 추가: 희생 페이지가 제거된 후, 새롭게 메모리로 들어오는 페이지는 다시 큐의 꼬리에 추가됩니다. 이러한 큐 관리는 구현하기가 매우 쉽고, 각 페이지에 대한 추가적인 시간 정보(예: 마지막 참조 시간)를 유지할 필요가 없어 시스템 오버헤드가 매우 적다는 장점이 있습니다. FIFO 알고리즘의 장점 단순성 및 낮은 오버헤드: 앞서 언급했듯이, FIFO 알고리즘은 이해하기 쉽고 구현하기가 매우 간단합니다. 페이지 교체 결정을 내리는 데 필요한 계산이 거의 없으므로, 알고리즘 자체의 실행 시간(오버헤드)이 매우 작습니다. 이는 시스템 자원이 제한적인 환경이나 빠른 결정이 요구되는 상황에서 유리할 수 있습니다. FIFO 알고리즘의 단점 및 한계 단순함에도 불구하고, FIFO 알고리즘은 심각한 성능 문제를 야기할 수 있는 몇 가지 중요한 단점을 가지고 있습니다. 페이지 사용 패턴 무시: FIFO는 페이지가 얼마나 중요하게, 또는 얼마나 자주 사용되는지를 전혀 고려하지 않습니다. 예를 들어, 프로그램의 핵심적인 기능을 수행하는 코드 페이지가 초기에 메모리에 적재되어 오랫동안 활발하게 사용되고 있었다고 가정해 봅시다. FIFO 알고리즘에서는 이 페이지가 단지 '오래되었다'는 이유만으로 교체될 수 있습니다. 만약 이 페이지가 교체된 직후 다시 필요해진다면, 즉시 또 다른 페이지 폴트가 발생하여 성능 저하를 초래합니다. 최적 성능과의 괴리: 일반적으로 FIFO 알고리즘의 페이지 폴트율은 최적 알고리즘이나 LRU 알고리즘에 비해 높게 나타납니다. 이는 페이지의 과거 사용 이력이나 미래 사용 가능성을 고려하지 않는 단순한 교체 전략 때문입니다. 벨레이디의 모순 (Belady's Anomaly): FIFO 알고리즘에서 나타날 수 있는 매우 특이하고 비직관적인 현상입니다. 대부분의 페이지 교체 알고리즘에서는 사용 가능한 물리 메모리 프레임의 수를 늘리면 페이지 폴트 횟수가 감소하거나 최소한 동일하게 유지되는 것이 일반적입니다. 하지만 FIFO 알고리즘에서는 프레임 수를 늘렸음에도 불구하고 오히려 페이지 폴트 횟수가 증가하는 경우가 발생할 수 있습니다. 이는 상식에 반하는 현상으로, FIFO 알고리즘의 비효율성을 단적으로 보여주는 예시입니다. (이 현상은 이후 슬라이드의 예시를 통해 더 자세히 설명될 수 있습니다.) 벨레이디의 모순 예시: 간단히 말해, 특정 참조 문자열에 대해 3개의 프레임으로 실행했을 때보다 4개의 프레임으로 실행했을 때 더 많은 페이지 폴트가 발생하는 상황입니다. 이는 프레임이 늘어남에 따라 페이지들이 메모리에 머무는 순서와 기간이 달라지고, 이로 인해 FIFO의 \"가장 오래된 페이지\"라는 선택 기준이 오히려 불리한 교체를 유도하기 때문입니다. FIFO 알고리즘의 사용 사례 이러한 단점들 때문에, 순수한 FIFO 알고리즘은 현대의 고성능 운영 체제에서 주된 페이지 교체 전략으로 널리 사용되지는 않습니다. 하지만 그 단순성 덕분에 특정 임베디드 시스템이나 매우 간단한 메모리 관리 기법이 요구되는 환경, 또는 다른 복잡한 알고리즘의 일부 구성 요소(예: 2차 기회 알고리즘의 기본 큐 관리)로 활용될 여지는 있습니다. 결론적으로, FIFO 페이지 교체 알고리즘은 구현의 용이성과 낮은 오버헤드라는 명확한 장점을 가지고 있지만, 페이지의 실제 사용 패턴을 고려하지 않아 성능이 떨어질 수 있으며, 특히 벨레이디의 모순과 같은 예측하기 어려운 행동을 보일 수 있는 한계를 지니고 있습니다. 이는 페이지 교체 알고리즘을 선택할 때 단순성 외에 다른 요소들(예: 효율성, 예측 가능성)도 중요하게 고려해야 함을 시사합니다. Slide 16 (8.18) & Slide 17 (8.19) Original Text FIFO Page Replacement (These slides typically depict a diagram or an example of the FIFO algorithm in action. Similar to the Optimal algorithm slides, the explanation will assume the same reference string to compare performance.) Korean Translation FIFO 페이지 교체 (이 슬라이드들은 일반적으로 FIFO 알고리즘이 동작하는 예시나 다이어그램을 포함합니다. 최적 알고리즘 슬라이드와 마찬가지로, 성능 비교를 위해 동일한 참조 문자열을 가정하여 설명하겠습니다.) Detailed Explanation 이 슬라이드들은 FIFO(First-In, First-Out) 페이지 교체 알고리즘이 실제로 어떻게 동작하는지를 보여주는 예시 또는 다이어그램을 위한 자리입니다. 최적 알고리즘 예시에서 사용했던 것과 동일한 참조 문자열(reference string)과 프레임 수를 사용하여 FIFO 알고리즘의 단계별 실행 과정을 시뮬레이션하고, 그 결과를 통해 알고리즘의 특징과 최적 알고리즘과의 성능 차이를 비교 분석하겠습니다. 시뮬레이션 설정 (최적 알고리즘과 동일) 참조 문자열 (Reference String): 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1 사용 가능한 물리 메모리 프레임 수: 3개의 프레임을 가정하겠습니다. 초기 상태: 모든 프레임은 비어있다고 가정합니다. FIFO 큐: 페이지가 메모리에 들어온 순서를 기록합니다. 큐의 머리(head)가 가장 먼저 들어온 페이지, 꼬리(tail)가 가장 최근에 들어온 페이지입니다. 목표: FIFO 알고리즘을 사용하여 페이지 폴트(Page Fault, PF) 발생 횟수를 계산합니다. FIFO 페이지 교체 알고리즘 실행 과정 (3 프레임) 각 단계에서 참조되는 페이지, 현재 프레임 상태 (메모리 도착 순으로 표시), FIFO 큐의 상태, 페이지 폴트 발생 여부(H: Hit, F: Fault)를 살펴보겠습니다. 참조 페이지 프레임 1 (가장 오래됨) 프레임 2 프레임 3 (가장 최신) FIFO 큐 (Head -> Tail) 결과 (PF 수) 7 7 7 F (1) 0 7 0 7, 0 F (2) 1 7 0 1 7, 0, 1 F (3) 2 0 1 2 0, 1, 2 F (4) 0 0 1 2 0, 1, 2 H 3 1 2 3 1, 2, 3 F (5) 0 2 3 0 2, 3, 0 F (6) 4 3 0 4 3, 0, 4 F (7) 2 0 4 2 0, 4, 2 F (8) 3 4 2 3 4, 2, 3 F (9) 0 2 3 0 2, 3, 0 F (10) 3 2 3 0 2, 3, 0 H 2 2 3 0 2, 3, 0 H 1 3 0 1 3, 0, 1 F (11) 2 0 1 2 0, 1, 2 F (12) 0 0 1 2 0, 1, 2 H 1 0 1 2 0, 1, 2 H 7 1 2 7 1, 2, 7 F (13) 0 2 7 0 2, 7, 0 F (14) 1 7 0 1 7, 0, 1 F (15) 최종 페이지 폴트 수 (FIFO, 3 프레임): 15회 분석 및 특징 단순한 교체 로직: FIFO 알고리즘은 각 교체 시점에서 단순히 큐의 맨 앞에 있는, 즉 가장 오래된 페이지만을 교체합니다. 페이지의 사용 빈도나 최근 사용 여부는 전혀 고려되지 않습니다. 예를 들어, 4번째 참조인 '2'가 들어올 때, 프레임에는 [7, 0, 1]이 있었고 FIFO 큐는 [7, 0, 1] (7이 가장 오래됨)이었습니다. 따라서 7이 교체됩니다. 6번째 참조인 '3'이 들어올 때, 프레임에는 [0, 1, 2]가 있었고 FIFO 큐는 [0, 1, 2] (0이 가장 오래됨)였습니다. 따라서 0이 교체됩니다. 이 0은 바로 직전(5번째 참조)에 사용되었음에도 불구하고 가장 오래되었다는 이유로 교체됩니다. 성능 비교 (vs. 최적 알고리즘): 동일한 참조 문자열과 3개의 프레임 조건에서, 최적 알고리즘은 9회의 페이지 폴트를 발생시켰습니다. FIFO 알고리즘은 15회의 페이지 폴트를 발생시켰습니다. 이 예시에서 FIFO는 최적 알고리즘보다 6번 더 많은 페이지 폴트를 유발했습니다. 이는 FIFO가 페이지 사용 패턴을 고려하지 않기 때문에 발생하는 비효율성을 보여줍니다. 예를 들어, FIFO는 자주 사용되는 페이지(위 예시에서 '0' 페이지)라도 단지 오래되었다는 이유로 교체하여, 이후 해당 페이지가 다시 참조될 때 불필요한 폴트를 발생시키는 경향이 있습니다. 벨레이디의 모순 (Belady's Anomaly) 가능성: FIFO 알고리즘은 프레임 수를 늘려도 페이지 폴트가 오히려 증가하는 '벨레이디의 모순' 현상이 나타날 수 있는 대표적인 알고리즘입니다. 이 특정 참조 문자열과 프레임 수에 대해서는 직접 계산해봐야 알 수 있지만, 이러한 현상의 존재 가능성 자체가 FIFO의 예측 불가능성과 비효율성을 나타냅니다. 벨레이디의 모순 예시 (다른 참조 문자열): 예를 들어 참조 문자열 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5 에 대해, 3개의 프레임으로 FIFO를 실행하면 9번의 페이지 폴트가 발생합니다. 4개의 프레임으로 FIFO를 실행하면 10번의 페이지 폴트가 발생합니다. 이처럼 프레임이 늘어났음에도 페이지 폴트가 증가하는 것은, 늘어난 프레임으로 인해 페이지들이 메모리에 머무는 순서와 기간이 변하고, 이것이 FIFO의 \"가장 오래된 페이지 교체\" 전략과 맞물려 오히려 더 안 좋은 교체 선택을 유도하기 때문입니다. 구현의 용이성: 앞서 설명했듯이, FIFO 큐를 사용하는 구현은 매우 간단하고 시스템 오버헤드가 적습니다. 각 페이지의 참조 시간을 기록하거나 복잡한 계산을 할 필요가 없습니다. 결론 이 예시들을 통해 FIFO 페이지 교체 알고리즘은 구현은 매우 간단하지만, 페이지의 실제 사용 패턴을 전혀 고려하지 않기 때문에 최적 알고리즘에 비해 훨씬 많은 페이지 폴트를 발생시킬 수 있음을 알 수 있습니다. 특히, 자주 사용되는 페이지라도 단지 오래되었다는 이유만으로 교체될 수 있다는 점과 벨레이디의 모순 발생 가능성은 FIFO 알고리즘의 주요 약점입니다. 슬라이드 8.18과 8.19는 보통 이러한 과정을 시각적인 다이어그램으로 표현하여, 페이지들이 프레임에 들어오고 나가는 순서와 그에 따른 큐의 변화를 명확하게 보여줌으로써 FIFO 알고리즘의 동작 방식을 쉽게 이해하도록 돕습니다. 이러한 시뮬레이션은 알고리즘의 장단점을 파악하고 다른 알고리즘과 비교 평가하는 데 유용합니다.",
      "frontmatter": {
        "tags": [
          "university",
          "operating-system"
        ],
        "date": "2025-03-18T09:29:00+09:00",
        "lastmod": "2025-06-05T17:41:23+09:00"
      }
    },
    "3장 트랜스포트 계층 (transport)": {
      "path": "/06.university/network/3장-트랜스포트-계층-transport/",
      "filename": "3장 트랜스포트 계층 (transport)",
      "content": "packet 이름 : segment udp 의 패킷을 특수하게 네트워크 계층에서 사용하는 패킷의 이름인 datagram 이라는 이름을 사용할 때도 있다 개괄 udp, tcp 공통 기능 프로세스대 프로세드 데이터 전달(multiplexing, demultiplexing) 오류 검출(check sum) tcp 특수 기능 신뢰적인 데이터 전송 흐름제어 순서번호 확인응답 타이머 혼잡제어 프로세스대 프로세드 데이터 트랜스포트 계층 다중화, 역 다중화 : 호스트대 호스트 전달을 프로세스대 프로세스 전달로 확장하는 과정 네트워크 소켓의 식별자 포멧 transport -> socket (-> application(process)) : depmultiplexing 적절한 소켓으로 데이터를 전송 : 분리하는 과정 socket -> transport (-> network) : multiplexing 소켓으로 부터 받은 데이터를 패킷화(segment) 해서 아래 계층으로 보낸다 udp 식별자 source port destination port 상황 : udp 소켓 19157 을 가진 호스트 A 의 프로세스가 호스트 B 의 UDP 소켓 46428 을 가진 프로세스에게 애플리케이션 데이터 전송을 원한다고 가정하자. 호스트 A 의 트랜스포트 계층은 애츨리케이션 데이터, 출발지 포트 번호(19157), 목적지 포트번호(46428), 그리고 2개의 다른값 #ModificationRequired 을 포함하는 트랜스포트계층 세그먼트를 생성한다. 트랜스포트 계층은 만들어진 세그먼트를 네트워크 계층으로 전달한다. 네트워크 계층은 세그먼트를 IP 데이터그램으로 캡술화하고 best-effort 서비스로 수신 호스트로 전달한다. ... 이동중 ... 수신호스트에서 네트워크 계층에서 받은 segment는 트랜스포트계층에서 목적지 포트번호 46428을 검사하고 그 세그먼트를 포트 46428로 식별되는 소켓에 전달한다 호스트 A의 소켓: (A의 IP 주소, 19157) 호스트 B의 소켓: (B의 IP 주소, 46428) checksum 헤더의 checksum 필드는은 16비트로 이루어져있음 송신자는 data의 모든 16비트 워드의 합산에 대해 다시 1의 보수를 수행해서 checksum 에 넣는다 합산시에는 윤회식 합산(wrap around)를 수행한다 수신자는 data의 모든 16비트 워드합산과 checksum 을 합치면 1111111111111111 이 나오면 오류가 없다고 판단한다(0이 한개라도 있으면 오류가 발생했다고 판단한다) 신뢰적인 data전송 rdt 1.0 완벽하게 신뢰적인 채널 상에서의 데이터 전송 비트오류 없음 패킷로스가 없음 Pasted image 20250410015692 rdt 2.0 (전송 후 대기 stop-and-wait) 비트 오류가 있는 채널상에서의 신뢰적인 데이터 전송 패킷로스가 없음 필요기능 오류검출 수신자 피드백 재전송 Pasted image 20250410020249 긍정확인응답(ACK), 부정확인응답(NAK) 만약 ack, nak 응답자체가 훼손되었을 경우는 어떻게 하는가? rdt 2.1 체크섬(checksum) 비트를 충분히 추가하여 송신자가 수신된 패킷의 오류를 검출할 뿐만 아니라, 오류를 직접 수정할 수 있도록 합니다. 송신자는 ack, nak 를 받지 않는 모든 패킷을 재전송 한다 중복 패킷 처리 문제 수신자는 ACK 또는 NAK가중복된 데이터 패킷이 도착할 가능성이 있습니다. 데이터 패킷에 새로운 필드를 추가하고 필드안에 순서번호(sequence number)를 삽입하는 방식으로 데이터 패킷에 송신자가 번호를 붙이는 것이다 1개의 비트 즉 2개의 상태번호만 있으면 된다 Pasted image 20250410022180 Pasted image 20250410052285 0 상태에서 2배 크기의 fsm 이다 sequence number 개수로 인해 nak 대신 이전패킷의 비정상 수신의 ack 를 보내도 되지 않을까? nak 대신 last recived ok rdt 2.2 ack 자체에 순서번호(sequence number)를 포함해서 보내도록 하고 nak 는 필요없다 Pasted image 20250410052788 Pasted image 20250410052896 rdt3.0 (alternating-bit protocol) 비트 오류와 패킷로스가 있는 채널상에서의 신뢰적인 데이터 전송 Pasted image 20250410055361 rdt3.0 송신자 측에서 (오류 발생이나 이전 패킷에) 대한 ack 가 발생하면 바로 재전송하지 않고 timeout 까지 기다리고 그렇게 하는 이유가 2번 연속으로 보내는 경우가 발생할 수 있다 인데 이것의 예시가 떠오르지 않아 질문드립니다 pipelining 필요기능 : sequence number 의 범위가 커져야 한다 프로토콜의 송신측과 수신측은 패킷 1개 이상을 버퍼링 해야 한다 L (Packet Size) : 패킷의 크기(packet size) R (Transmission Rate) : 송신 속도 (1Gbps) RTT (Round-Trip Time) : 데이터 패킷이 발신자에서 수신자로 전달된 후, ACK가 다시 발신자에게 돌아오는 데 걸리는 전체 왕복 시간 GBN (go back N) 특정 조건에서 송신자가 이미 전송한 여러 패킷(N개)을 다시 전송해야 하는 특징 Pasted image 20250410085576 Selective Repeat (SR) 손실된 패킷만 재전송하는 특징 🔄 1. GBN vs SR: 개요 요약 항목 Go-Back-N (GBN) Selective Repeat (SR) ACK 방식 누적적 ACK (Cumulative ACK) 개별 ACK 패킷 재전송 타임아웃된 패킷부터 window 내 모든 패킷 재전송 오직 손실/에러 난 패킷만 재전송 Receiver 버퍼링 불필요 (out-of-order 패킷 모두 폐기) 필요 (out-of-order 패킷 저장 후 순서 정렬) Window 크기 제한 N (window size) 최대 2^k-1 / 2 (sequence number space 절반) 🧱 2. 시나리오 설정 (공통) 최대 SEQ 번호: 7 (즉, k=3 bits) Window Size (N): 4 전송해야 할 패킷: pkt0, pkt1, pkt2, pkt3, pkt4, pkt5 pkt2 유실 ACK 수신 순서: ACK0, ACK1, ACK2 지연됨 → timeout 발생 🟩 3. Go-Back-N (GBN) 시뮬레이션 ✅ Sender 측 Event Window (base ~ nextseqnum - 1) Sent Packets Status Notes 초기 상태 [0, 0] 없음 대기 base = 0, nextseqnum = 0 rdt_send(data0) [0, 0] → [0, 0] pkt0 보냄 전송 nextseqnum = 1 rdt_send(data1) [0, 1] pkt1 보냄 전송 nextseqnum = 2 rdt_send(data2) [0, 2] pkt2 보냄 전송 nextseqnum = 3 rdt_send(data3) [0, 3] pkt3 보냄 전송 nextseqnum = 4 ACK0 수신 [1, 3] - base = 1 타이머 재시작 ACK1 수신 [2, 3] - base = 2 타이머 재시작 Timeout (pkt2 미수신) [2, 3] pkt2~pkt3 재전송 재전송 타이머 리셋 ACK2 수신 [3, 3] - base = 3 타이머 중지 ACK3 수신 [4, 3] → [4, 4] - base = 4 다음 패킷 보낼 수 있음 🔁 GBN 특징: 누적 ACK를 기준으로 window sliding하며, 한번 유실되면 window 내 모든 패킷 재전송. ❌ Receiver 측 수신 패킷 expectedseqnum 결과 ACK 전송 pkt0 0 in-order ACK0 pkt1 1 in-order ACK1 pkt3 2 out-of-order ACK1 재전송 pkt4 2 out-of-order ACK1 재전송 pkt5 2 out-of-order ACK1 재전송 pkt2 2 in-order (마침내) ACK2 전송 pkt3 3 in-order ACK3 전송 pkt4 4 in-order ACK4 전송 pkt5 5 in-order ACK5 전송 ⚠️ GBN Receiver: 잘못된 순서의 패킷은 무시하고 마지막으로 받은 in-order 패킷에 대한 ACK 계속 송신. 🟦 4. Selective Repeat (SR) 시뮬레이션 ✅ Sender 측 Event Window (base ~ nextseqnum - 1) Sent Packets Status Notes 초기 상태 [0, 0] 없음 대기 base = 0, nextseqnum = 0 rdt_send(data0) [0, 0] → [0, 0] pkt0 보냄 전송 nextseqnum = 1 rdt_send(data1) [0, 1] pkt1 보냄 전송 nextseqnum = 2 rdt_send(data2) [0, 2] pkt2 보냄 전송 nextseqnum = 3 rdt_send(data3) [0, 3] pkt3 보냄 전송 nextseqnum = 4 ACK0 수신 [1, 3] - base = 1 타이머 재시작 ACK1 수신 [2, 3] - base = 2 타이머 재시작 ACK3 수신 [2, 3] - 아직 base = 2 pkt2는 미수신 Timeout (pkt2) [2, 3] pkt2 재전송 only pkt2 재전송 타이머 리셋 ACK2 수신 [3, 3] - base = 3 모든 ack 받음 rdt_send(data4) [3, 4] pkt4 보냄 전송 nextseqnum = 5 📦 SR 특징: 누적 ACK 아님 → 각각의 패킷에 대해 독립적으로 확인 가능. loss가 하나뿐이라면 그 하나만 재전송. ✅ Receiver 측 수신 패킷 expectedseqnum 결과 ACK 전송 버퍼 내용 pkt0 0 in-order ACK0 - pkt1 1 in-order ACK1 - pkt3 2 out-of-order ACK3 pkt3 save pkt4 2 out-of-order ACK4 pkt4 save pkt5 2 out-of-order ACK5 pkt5 save pkt2 2 in-order ACK2 pkt3, pkt4, pkt5 deliver 가능 (after ACK2) 6 - - buffer empty 💡 SR Receiver: out-of-order 패킷을 버퍼링하여 순서 복구 후 일괄 전달 (in-order delivery) 📊 5. 주요 비교 요약 항목 Go-Back-N Selective Repeat ACK 종류 누적 ACK 개별 ACK 재전송 범위 window 내 전체 손실된 패킷만 Receiver 버퍼 필요 없음 필요함 채널 효율성 packet error 많으면 낮음 packet error 많아도 높음 복잡도 낮음 높음 사용 예 UDP 기반 간단한 RDT TCP (유사 구조) 🧾 결론 GBN은 간단하지만 비효율적입니다. 하나의 패킷 유실 시 window 내 모든 패킷을 다시 보내야 해서 네트워크 과부하를 유발할 수 있습니다. SR은 더 복잡하지만 효율적입니다. 손실된 패킷만 재전송하고, receiver 측에서 out-of-order 패킷을 적절히 버퍼링하여 순서 정렬 후 전달합니다. RTT Sample RTT 대략적인 RTT RTT 문제 아래는 Exponential Weighted Moving Average (EWMA)와 관련된 TCP의 Round-Trip Time (RTT) 추정 및 Timeout Interval 계산을 다루는 문제 3개입니다. 각 문제는 초기값과 샘플 데이터를 제공하며, 여러분이 주어진 공식을 적용하여 값을 계산하도록 설계되었습니다. 문제 1: EstimatedRTT 계산 주어진 정보: 초기 EstimatedRTT = 100ms α (가중치 계수) = 0.125 (즉, \\( $\\alpha = 1/8$ \\)) SampleRTT 값: 첫 번째 SampleRTT = 120ms 두 번째 SampleRTT = 90ms 세 번째 SampleRTT = 110ms 질문: 첫 번째 SampleRTT를 기반으로 업데이트된 EstimatedRTT를 계산하세요. 두 번째 SampleRTT를 기반으로 다시 업데이트된 EstimatedRTT를 계산하세요. 세 번째 SampleRTT를 기반으로 최종 EstimatedRTT를 계산하세요. 문제 2: DevRTT (RTT 변동성) 계산 주어진 정보: 초기 DevRTT = 0ms β (가중치 계수) = 0.25 위에서 계산한 EstimatedRTT 값들: 첫 번째 EstimatedRTT = 101.25ms 두 번째 EstimatedRTT = 100.625ms 세 번째 EstimatedRTT = 101.094ms SampleRTT 값: 첫 번째 SampleRTT = 120ms 두 번째 SampleRTT = 90ms 세 번째 SampleRTT = 110ms 질문: 첫 번째 SampleRTT를 기반으로 업데이트된 DevRTT를 계산하세요. 두 번째 SampleRTT를 기반으로 다시 업데이트된 DevRTT를 계산하세요. 세 번째 SampleRTT를 기반으로 최종 DevRTT를 계산하세요. 문제 3: TimeoutInterval 계산 주어진 정보: 위에서 계산한 최종 EstimatedRTT = 101.094ms 위에서 계산한 최종 DevRTT = 2.34375ms TimeoutInterval 공식: $\\text{TimeoutInterval} = \\text{EstimatedRTT} + 4 \\cdot \\text{DevRTT}$ 질문: 최종 TimeoutInterval을 계산하세요. 만약 Timeout이 발생했다면, TimeoutInterval은 어떻게 업데이트되나요? (초기 TimeoutInterval = 1초) Timeout 이후에 새로운 SampleRTT = 105ms가 측정되었고, EstimatedRTT와 DevRTT가 업데이트되었다고 가정할 때, TimeoutInterval을 재계산하세요. 풀이 방법 안내 EstimatedRTT 계산: 아래 공식을 사용합니다. $\\text{EstimatedRTT} = (1 - \\alpha) \\cdot \\text{EstimatedRTT} + \\alpha \\cdot \\text{SampleRTT}$ DevRTT 계산: 아래 공식을 사용합니다. $\\text{DevRTT} = (1 - \\beta) \\cdot \\text{DevRTT} + \\beta \\cdot \\text{SampleRTT} - \\text{EstimatedRTT} $ TimeoutInterval 계산: 아래 공식을 사용합니다. $\\text{TimeoutInterval} = \\text{EstimatedRTT} + 4 \\cdot \\text{DevRTT}$ 힌트 계산 과정에서 소수점 처리를 정확히 해야 합니다. TimeoutInterval이 두 배로 증가하는 경우는 Timeout이 발생했을 때만 해당됩니다. 모든 계산은 단계별로 진행하고, 중간 결과를 기록하세요. 예상 답안 형식 문제 1 예시 답안: 첫 번째 EstimatedRTT: $\\text{EstimatedRTT} = 0.875 \\cdot 100 + 0.125 \\cdot 120 = 101.25 \\, \\text{ms}$ 두 번째 EstimatedRTT: $\\text{EstimatedRTT} = 0.875 \\cdot 101.25 + 0.125 \\cdot 90 = 100.625 \\, \\text{ms}$ 세 번째 EstimatedRTT: $\\text{EstimatedRTT} = 0.875 \\cdot 100.625 + 0.125 \\cdot 110 = 101.094 \\, \\text{ms}$ 문제 2 예시 답안: 첫 번째 DevRTT: $\\text{DevRTT} = 0.75 \\cdot 0 + 0.25 \\cdot 120 - 101.25 = 2.5 \\, \\text{ms}$ 두 번째 DevRTT: $\\text{DevRTT} = 0.75 \\cdot 2.5 + 0.25 \\cdot 90 - 100.625 = 2.1875 \\, \\text{ms}$ 세 번째 DevRTT: $\\text{DevRTT} = 0.75 \\cdot 2.1875 + 0.25 \\cdot 110 - 101.094 = 2.34375 \\, \\text{ms}$ 문제 3 예시 답안: 최종 TimeoutInterval: $\\text{TimeoutInterval} = 101.094 + 4 \\cdot 2.34375 = 110.5875 \\, \\text{ms}$ Timeout 발생 시: $\\text{TimeoutInterval} = 2 \\cdot 110.5875 = 221.175 \\, \\text{ms}$ Timeout 이후 재계산: (새로운 EstimatedRTT와 DevRTT를 기반으로 다시 계산) 이 문제들은 EWMA와 TCP 타임아웃 메커니즘을 이해하고 실제 계산 능력을 테스트하기 위해 설계되었습니다. 문제를 풀면서 주어진 공식을 잘 적용해 보세요! 😊 TCP TCP의 기본 동작 원리 TCP는 세 가지 주요 이벤트를 처리하며, 각 이벤트에 따라 특정 작업을 수행합니다: (1) 애플리케이션으로부터 데이터 수신 TCP는 애플리케이션 계층으로부터 데이터를 받아 이를 TCP 세그먼트로 캡슐화합니다. 각 세그먼트에는 시퀀스 번호(sequence number)가 포함됩니다. 이는 세그먼트 내 첫 바이트의 바이트 스트림 번호를 나타냅니다. 타이머가 이미 실행 중이 아니라면, 세그먼트를 IP 계층으로 전달할 때 타이머를 시작합니다. (2) 타임아웃 발생 타임아웃이 발생하면, TCP는 해당 세그먼트를 재전송합니다. 이후 타이머를 다시 시작합니다. (3) ACK(Acknowledgment) 수신 수신자로부터 ACK가 도착하면, TCP는 ACK 필드 값 y 와 SendBase 를 비교합니다. SendBase : 가장 오래된 미확인 바이트의 시퀀스 번호. y > SendBase 라면, 하나 이상의 세그먼트가 성공적으로 전달되었음을 의미합니다. TCP는 SendBase 를 업데이트하고, 아직 확인되지 않은 세그먼트가 남아 있다면 타이머를 다시 시작합니다. TCP의 신뢰성 보장 메커니즘 TCP는 데이터 전송의 신뢰성을 보장하기 위해 다양한 기법을 사용합니다. 이 과정에서 TCP는 Go-Back-N(GBN)과 Selective Repeat(SR) 프로토콜의 특징을 혼합한 하이브리드 방식으로 동작합니다. 이를 아래와 같이 상세히 설명하겠습니다. (1) 타임아웃 및 재전송 TCP는 각 세그먼트에 대해 타임아웃 간격( TimeoutInterval ) 을 설정합니다. TimeoutInterval 은 EstimatedRTT (예상 왕복 시간)과 DevRTT (왕복 시간 편차)를 기반으로 계산됩니다. 만약 특정 세그먼트에 대한 ACK가 타임아웃 내에 도착하지 않으면, 해당 세그먼트를 재전송합니다. 지수 백오프 알고리즘: 타임아웃이 발생할 때마다 다음 타임아웃 간격은 이전 값의 두 배로 설정됩니다. 예를 들어, 초기 타임아웃 간격이 0.75초라면, 첫 번째 재전송 후에는 1.5초, 두 번째 재전송 후에는 3.0초로 증가합니다. 이는 네트워크 혼잡 상태를 완화하기 위한 제한된 형태의 혼잡 제어입니다. (2) 누적 ACK(Cumulative Acknowledgment) TCP는 기본적으로 누적 ACK 방식을 사용하여 수신자가 받은 데이터를 확인합니다. 예: 수신자가 ACK = y 를 보내면, 이는 \"바이트 번호 y 이전의 모든 데이터가 성공적으로 수신되었음\"을 의미합니다. 이를 통해 송신자는 손실된 세그먼트를 빠르게 감지하고 재전송할 수 있습니다. 하이브리드 특징: 일부 TCP 구현은 순서가 맞지 않는(out-of-order) 세그먼트를 버퍼링하며, 선택적 ACK(SACK, Selective Acknowledgment) [RFC 2018]를 사용하여 특정 세그먼트를 개별적으로 ACK할 수 있습니다. 이를 통해 SR 스타일의 선택적 재전송이 가능해집니다. (3) 단일 타이머 관리 효율성을 위해 TCP는 단일 타이머만 사용합니다. 이 타이머는 가장 오래된 미확인 세그먼트와 연결됩니다. 새로운 ACK가 도착하거나 타임아웃이 발생하면 타이머를 조정합니다. Fast Retransmit: 타임아웃을 기다리지 않고도 중복 ACK(duplicate ACK)를 활용하여 손실된 세그먼트를 빠르게 재전송할 수 있습니다. 예를 들어, 동일한 ACK가 3번 이상 도착하면 송신자는 해당 세그먼트가 손실되었음을 판단하고 즉시 재전송합니다. TCP는 GBN인가, SR인가? TCP의 에러 복구 메커니즘은 GBN과 SR의 혼합형이라고 할 수 있습니다. 이를 구체적으로 살펴보겠습니다: (1) GBN과 유사한 특징 TCP는 누적 ACK를 사용하며, 중간에 손실된 세그먼트가 있더라도 이후 세그먼트를 개별적으로 ACK하지 않습니다. TCP 송신자는 가장 오래된 미확인 바이트( SendBase )와 다음에 보낼 바이트( NextSeqNum )만 추적합니다. 이는 GBN의 특징과 유사합니다. (2) SR과 유사한 특징 일부 TCP 구현은 순서가 맞지 않는 세그먼트를 버퍼링합니다. 이는 SR의 핵심 특징 중 하나입니다. 예를 들어, 송신자가 세그먼트 1, 2, ..., N을 전송했고, 세그먼트 n의 ACK가 손실되었지만 나머지 ACK가 타임아웃 전에 도착했다고 가정합니다. 이 경우: GBN은 세그먼트 n부터 모든 세그먼트(n, n+1, ..., N)를 재전송합니다. TCP는 최대 한 개의 세그먼트(n)만 재전송하며, 세그먼트 n+1의 ACK가 타임아웃 전에 도착했다면 세그먼트 n조차 재전송하지 않을 수 있습니다. (3) 선택적 ACK(SACK) 선택적 ACK(SACK)를 사용하면 수신자가 특정 세그먼트를 개별적으로 ACK할 수 있습니다. 이를 통해 송신자는 이미 수신된 세그먼트를 재전송하지 않으며, SR 스타일의 선택적 재전송이 가능해집니다. 결론 TCP의 에러 복구 메커니즘은 기본적으로 GBN 스타일로 동작하지만, SACK와 Fast Retransmit을 통해 SR 스타일의 기능을 추가로 제공합니다. 따라서 TCP는 GBN과 SR의 하이브리드 모델로 분류됩니다. 핵심 요약: 기본적으로 GBN과 유사: 누적 ACK, 단일 타이머 사용. 선택적 ACK(SACK)와 Fast Retransmit을 통해 SR의 장점을 통합. 결과적으로 TCP는 GBN과 SR의 장점을 결합한 유연한 프로토콜입니다. 3way handshake vs 2way handshake 2-Way Handshake의 문제를 설명한 내용을 다시 정리하고, 핵심적인 문제점과 그로 인해 발생하는 상황을 상세히 설명하겠습니다. 2-Way Handshake의 동작 방식 2-Way Handshake는 연결 설정을 위해 두 번의 메시지 교환만으로 완료됩니다: 클라이언트 → 서버: SYN(연결 요청) 서버 → 클라이언트: ACK(연결 수락) 이 과정에서 서버는 클라이언트의 연결 요청(SYN)을 받고, 즉시 연결이 확립되었다고 간주합니다. 하지만 이 방식은 신뢰성 있는 연결 설정을 보장하지 못하며, 특히 네트워크 지연이나 중복 패킷(Duplicate Packet)로 인해 문제가 발생할 수 있습니다. 문제 발생 시나리오 시나리오: Old Duplicate SYN 패킷 클라이언트가 서버에 연결 요청(SYN)을 보냄 클라이언트는 서버와의 연결을 위해 SYN 패킷을 전송합니다. 그러나 이 패킷은 네트워크 지연 또는 손실로 인해 서버에 도착하지 않거나, 재전송되며 지연됩니다. 서버가 SYN 패킷을 수신하고 연결 수락(ACK) 서버는 클라이언트의 SYN 패킷을 수신하고, 연결 요청을 수락하기 위해 ACK 패킷을 클라이언트에게 전송합니다. 이로써 서버는 연결이 성공적으로 확립되었다고 판단합니다. 클라이언트와 서버가 데이터를 주고받으며 연결 종료 클라이언트와 서버는 데이터 통신을 완료하고 연결을 종료합니다. 클라이언트는 더 이상 해당 연결을 사용하지 않습니다. 지연된 SYN 패킷이 서버에 도착 초기에 전송되었던 SYN 패킷이 네트워크 지연으로 인해 이제야 서버에 도착합니다. 서버는 이 패킷을 새로운 연결 요청으로 오인합니다. 서버가 잘못된 연결 상태를 유지 서버는 지연된 SYN 패킷에 대해 ACK를 전송하며, 새로운 연결이 확립되었다고 판단합니다. 하지만 실제로 클라이언트는 이미 연결을 종료했으므로, 서버는 \"상대방이 없는 커넥션\"을 유지하게 됩니다. 발생하는 문제 하프 오픈(Half-Open) 상태 서버는 연결이 활성화된 상태로 판단하고 리소스를 할당하지만, 클라이언트는 이미 연결을 종료한 상태입니다. 이로 인해 서버는 불필요한 자원을 낭비하며, \"하프 오픈\" 상태의 커넥션을 유지하게 됩니다. 데이터 송수신 실패 서버는 클라이언트로부터 추가 데이터를 기다리지만, 클라이언트는 이미 연결을 종료했으므로 데이터를 보내지 않습니다. 결과적으로 서버는 데이터를 받지 못하고, 결국 타임아웃(Time-out) 후 연결을 강제로 종료해야 합니다. 재전송(Retransmission) 문제 클라이언트는 초기 SYN 패킷이 서버에 도착하지 않았다고 판단하여 재전송을 시도할 수 있습니다. 이러한 재전송된 SYN 패킷이 서버에 도착하면, 서버는 이를 또 다른 새로운 연결 요청으로 오인할 가능성이 큽니다. 이로 인해 서버는 여러 개의 유효하지 않은 연결을 생성하며, 자원을 더욱 낭비하게 됩니다. 왜 이런 문제가 발생할까? 2-Way Handshake는 단순히 \"요청 → 응답\"의 구조로 동작하기 때문에, 연결 요청이 실제로 현재 유효한 요청인지 확인할 방법이 없습니다. 특히 네트워크 지연이나 중복 패킷이 존재할 경우, 다음과 같은 이유로 문제가 발생합니다: Old Duplicate Packet 처리 불가능: 2-Way Handshake는 지연된 패킷을 구분할 방법이 없어, 이를 새로운 연결 요청으로 오인합니다. 상태 동기화 부족: 클라이언트와 서버 간의 연결 상태가 완벽히 동기화되지 않아, 한쪽이 연결을 종료한 상태에서도 다른 쪽이 연결이 활성화된 것으로 판단할 수 있습니다. 해결책: 3-Way Handshake 3-Way Handshake는 2-Way Handshake의 문제를 해결하기 위해 추가적인 단계를 포함합니다: 클라이언트 → 서버: SYN(연결 요청) 서버 → 클라이언트: SYN+ACK(연결 수락 및 확인 요청) 클라이언트 → 서버: ACK(최종 확인) 이 과정에서 클라이언트와 서버는 서로의 상태를 명확히 동기화하며, 다음과 같은 장점을 제공합니다: Old Duplicate Packet 무시: 지연된 SYN 패킷이 도착해도, 최종 ACK 단계가 없으면 연결이 확립되지 않습니다. 상태 동기화 보장: 클라이언트와 서버가 모두 연결 확립을 확인하므로, 하프 오픈 상태를 방지합니다. 신뢰성 있는 연결 설정: 재전송된 패킷을 유효하게 처리하거나 무시할 수 있어, 혼란을 줄입니다. 결론 2-Way Handshake는 간단하지만, 네트워크 지연 또는 중복 패킷으로 인해 문제가 발생할 수 있습니다. 특히, 하프 오픈 상태와 재전송 문제로 인해 서버 자원이 낭비되고, 데이터 송수신 실패 등의 문제가 생길 수 있습니다. 따라서 TCP에서는 3-Way Handshake를 통해 신뢰성 있는 연결 설정을 보장하며, 이러한 문제를 효과적으로 해결합니다. TCP flow control and congestion control 목적1 : 혼잡제어",
      "frontmatter": {
        "aliases": [
          "전송 계층"
        ],
        "tags": [
          "network"
        ],
        "series": "네트워크 하향식 접근 책",
        "series_weight": "3",
        "date": "2025-04-08T11:00:00+09:00",
        "lastmod": "2025-06-03T06:14:08+09:00"
      }
    },
    "4계층 네트워크 계층 (network)": {
      "path": "/06.university/network/4계층-네트워크-계층-network/",
      "filename": "4계층 네트워크 계층 (network)",
      "content": "data plain : 단일 기기에서 어떤 input -> output control plain : 라우팅 , 전체 경로 결정 전통적 방법 SDN hello world dhll eowld hello world hello world DATA PLAIN",
      "frontmatter": {
        "tags": [
          "network"
        ],
        "date": "2025-06-02T12:00:00+09:00",
        "lastmod": "2025-06-03T06:14:10+09:00"
      }
    },
    "60222100 신년기 네트워크 과제 1": {
      "path": "/06.university/network/60222100-신년기-네트워크-과제-1/",
      "filename": "60222100 신년기 네트워크 과제 1",
      "content": "SMTP (Simple Mail Transfer Protocol) 프로토콜 목적 주요 포트 특징 SMTP 이메일 전송 25, 465, 587 메일 서버 간 또는 클라이언트-서버 간 메일 전달 목적: 이메일 전송을 위한 프로토콜입니다. 메일 클라이언트에서 메일 서버로, 또는 메일 서버 간에 이메일을 전달하는 데 사용됩니다. 특징: TCP 기반의 7계층(응용 계층) 프로토콜입니다. 메일 전송 과정에서 신뢰성 있는 연결(TCP)을 제공합니다. 메일 전송자 → 메일 서버 → 수신자 메일 서버로의 경로를 책임집니다. 포트별 차이: 25번 포트: 기본 SMTP 포트로, 서버 간 메일 전송에 주로 사용됩니다. 하지만 스팸 메일 문제로 인해 일부 ISP에서는 차단할 수 있습니다. 465번 포트: SSL/TLS 암호화를 사용한 SMTP 전송을 위해 사용되지만, 비공식적으로 사용되는 포트입니다. 587번 포트: 현재 표준으로 권장되는 포트로, 메일 클라이언트가 메일 서버로 이메일을 보내는 데 사용됩니다. STARTTLS 명령을 통해 암호화된 통신이 가능합니다. 사용 사례: Gmail, Outlook 등 메일 서비스 제공자가 사용. SSH (Secure Shell) 프로토콜 목적 주요 포트 특징 SSH 원격 서버 관리 22 암호화된 연결로 보안성 제공 목적: 원격 서버에 안전하게 접속하고 관리하기 위한 프로토콜입니다. 특징: TCP 기반으로 데이터를 암호화하여 보안성을 제공합니다. 원격 서버에서 명령어 실행, 터널링 등의 작업을 지원합니다. 기본 포트는 22번 보안 강화: 패스워드 기반 인증 외에도 공개키/비밀키 기반 인증을 지원하여 보안성을 높입니다. 중간자 공격(Man-in-the-Middle Attack) 방지 기능 포함. 사용 사례: 리눅스 서버 관리. AWS, Google Cloud 등의 클라우드 서버에 접근. WebSocket 프로토콜 목적 주요 포트 특징 WebSocket 실시간 양방향 통신 80, 443 HTTP 기반 초기 연결, 이후 지속적 연결 유지 목적: 실시간 양방향(full-duplex) 통신을 지원하는 프로토콜입니다. 특징: 초기 연결은 HTTP/HTTPS 프로토콜을 사용하지만, 이후 WebSocket 핸드셰이크 과정을 통해 지속적인 연결(Persistent Connection)을 유지합니다. HTTP와 달리 요청-응답 방식이 아닌, 서버와 클라이언트 간 실시간 데이터 교환이 가능합니다. 기본적으로 80번(HTTP)과 443번(HTTPS) 포트를 사용하므로 방화벽 우회가 용이합니다. 사용 사례: 채팅 애플리케이션(실시간 메시지 전송). 실시간 주식 거래 플랫폼. 온라인 게임에서의 실시간 데이터 동기화. XRDP 프로토콜 목적 주요 포트 특징 XRDP 원격지 GUI 제어 3389 RDP 기반으로 리눅스/Windows 모두 지원 목적: 원격지에서 GUI 환경을 통해 서버를 제어하기 위한 프로토콜입니다. 특징: RDP(Remote Desktop Protocol)를 기반으로 하며, 리눅스 환경에서도 사용 가능하도록 구현되었습니다. 기본 포트는 3389번이며, Windows의 RDP와 호환됩니다. 원격지에서 마우스, 키보드 입력을 전달하고, 서버의 화면을 실시간으로 전송받습니다. 사용 사례: 리눅스 서버에 GUI 도구를 사용하여 접근. 가상 데스크톱 환경에서 원격 작업 수행. FTP (File Transfer Protocol) 프로토콜 목적 주요 포트 특징 FTP 파일 전송 20, 21 데이터 전송 신뢰성 보장, 기본 보안 취약 목적: 파일 전송을 위한 프로토콜입니다. 특징: TCP 기반으로 데이터 전송의 신뢰성을 보장합니다. 두 개의 포트를 사용합니다: 21번 포트: 제어 연결(Control Connection) - 명령어 및 인증 처리. 20번 포트: 데이터 연결(Data Connection) - 실제 파일 전송. 기본적으로 암호화되지 않으므로 보안성이 취약합니다. 보안 강화: FTPS(File Transfer Protocol Secure): SSL/TLS를 사용하여 암호화. SFTP(Secure File Transfer Protocol): SSH를 통해 암호화된 파일 전송. 사용 사례: 웹사이트 운영 시 파일 업로드/다운로드. 대용량 파일 공유.",
      "frontmatter": {}
    },
    "network 2차 과제": {
      "path": "/06.university/network/network-2차-과제/",
      "filename": "network 2차 과제",
      "content": "🌐 HTTP 0.9 ~ HTTP 1.1 까지 알아보는 통신 기술 http-protocol-history http-0.9 HTTP의 시작은 1989년 팀 버너 리(Tim Berners-LEE)에 의해 제안된 인터넷의 하이퍼 텍스트 시스템이다. 초기 버전인 HTTP/0.9는 매우 단순한 프로토콜이었다. 가능한 메서드는 하이퍼텍스트 문서(html)를 가져오기만 하는 GET 동작이 유일했으며, 헤더(header)도 없어 요청과 응답이 극히 단순 명료 하였다. 또한 상태 코드(status code)도 없었기 때문에 문제가 발생한 경우 특정 html 파일을 오류에 대한 설명과 함께 보내졌다. http-0.9 <HTML> A very simple HTML page </HTML> HTTP 0.9 스펙을 요약하면 다음과 같다. TCP/IP 링크 위에서 동작하는 ASCII 프로토콜 Get 메서드만 지원 HTTP 헤더 X, 상태 코드 X 응답도 HTML 파일 자체만 보내줌 서버와 클라이언트 간의 연결은 모든 요청 후에 닫힘(closed) 사실 초기에는 버전 번호가 존재하지 않았지만, 이후에 다른 http 버전들과 구분하기 위해서 0.9라는 버전을 붙이게 되었다고 한다. HTTP는 이러한 비교적 단순한 형태로 1991년에 시작되어, 이후 빠르게 진화하고 발전하게 되어지기 시작 했다. HTTP / 1.0 인터넷의 성장이 날이 갈수록 거대해지면서, 1994년 W3C가 만들어지며 HTML의 발전을 도모하게 되었고, 이와 비슷하게 HTTP 프로토콜 개선에 초점을 맞추기 위해 HTTP-WG(HTTP Working Group)가 설립되었다. 웹 브라우저, 인터넷 인프라가 빠르게 진화하며 이제는 단순히 하이퍼텍스트 문서 뿐만 아니라 멀티미디어 데이터나 메타데이터 등 다양하고 상세한 컨텐츠가 필요해짐으로써, 기존의 HTTP 0.9로는 다양한 요구사항들을 채울수 없는 한계에 봉착하게 되었다. 그러다 1996년 HTTP-WG는 HTTP/1.0 구현의 일반적인 사용을 문서화한 RFC 1945를 발표하게 된다. RFC 1945는 어렵게 생각할 필요없이 HTTP 1.0 프로토콜 통신 스펙에 관한 기술 문서 정도로 생각하면 된다. 컨텐츠 인코딩, 다양한 글자 지원, 멀티파트 타입, 인가, 캐싱, 프록시, 날짜 형식 등을 문서화 하였다. 이는 다음과 같은 익숙한 형태의 요청과 응답 포맷으로 구성되었다. Request 메세지에는 GET 요청이 시작되는 줄에 PATH와 HTTP 버젼 그리고 다음 줄로 이어지는 헤더값을 가지며, Response 메세지에는 200 OK 이후 응답 상태로 이어지는 응답 헤더값을 가지는 걸 볼 수 있다. http-1.0 http-1.0 이른바 HTTP 포맷 형태의 시초라고 보면 된다. 이렇게 발표된 HTTP 1.0 스펙을 요약하면 다음과 같다. 기본적인 HTTP 메서드와 요청/응답 헤더 추가 HTTP 버전 정보가 각 요청 사이내로 전송되기 시작 (HTTP/1.0 이 GET 라인에 붙은 형태로) 상태 코드(status code)가 응답의 시작 부분에 붙어 전송되어, 브라우저가 요청에 대한 성공과 실패를 알 수 있고 그 결과에 대한 동작을 할 수 있게 되었다. (특정 방법으로 로컬 캐시를 갱신하거나 ..등) 응답 헤더의 Content-Type 덕분에 HTML 파일 형식 외에 다른 문서들을 전송하는 기능이 추가되었다. 단기커넥션 : connection 하나당 1 Request & 1 Response 처리 가능 HTTP 1.0 문제점 Short-lived Connection HTTP 1.0의 문제점은 비연결성(connectionless)로 인한 단기 커넥션(Short-lived connenction) 특징이다. 즉, 커넥션 하나당 하나의 요청 하나의 응답 처리가 가능한 것을 말하는데, 서버에 자원을 요청할때마다 매번 새로운 연결을 해주어야 했다. 1 Request & 1 response 매번 새로운 연결로 성능 저하 매번 새로운 연결로 서버 부하 비용 증가 단기커넥션 예를들어 웹페이지를 요청하면 html과 그에 딸린 css나 js 및 이미지 등등 수 많은 자원들이 다운로드되어 화면에 띄울 텐데, 각 자원들을 따로 따로 매번 TCP 연결하고 다운받고 연결 끊고 다시 연결하고 다운 받고 연결 끊는 것이다. 그래서 HTTP 초기에는 모든 자료에 대해서 비연결성으로 각각의 자원에 대해 연결/응답/종료를 반복하다보니 느렸다. HTTP / 1.1 HTTP 1.0의 몇가지 단점을 커버하기 위해 HTTP 1.0이 출시된지 6개월 만에 1997년 1월에 공식적으로 HTTP/1.1이 릴리즈 되게 된다. HTTP 1.1은 현재 가장 많이 쓰이는 프로토콜 버젼이며, 우리가 HTTP를 학습할때 배우는 기본 베이스 지식이기도 하다. HTTP 1.1 표준은 이전 버전에서 발견 된 많은 프로토콜 모호성을 해결하고 몇 가지 크리티컬한 성능 개선을 도입했다. 좀더 보완된 특징은 다음과 같다. 지속 연결(Persistent connection) : 지정한 timeout 동안 연속적인 요청 사이에 커넥션을 닫지 않음. 기존 연결에 대해서 handshake 생략 가능 파이프 라이닝(pipelining) : 이전 요청에 대한 응답이 완전히 전송되기 전에 다음 전송을 가능하게 하여, 여러 요청을 연속적으로 보내 그 순서에 맞춰 응답을 받는 방식으로 지연 시간을 줄이는 방식 (불안정하여 사장됨) HOST 헤더 추가 : 동일 IP 주소에 다른 도메인을 호스트하는 기능 가능 Chunk Encoding 전송 : 응답 조각 바이트 범위 요청 캐시 제어 메커니즘 도입 Persistent Connection (keep-alive) HTTP는 TCP 연결 기반 위에서 동작하는 프로토콜로 신뢰성 확보를 위해 연결을 맺고 끊는 데 있어서 3 way Handshake 가 이루어진다. 그런데 HTTP는 기본적으로 비연결성(connecitonless) 프로토콜이기 때문에 한 번의 요청과 응답을 하고 응답이 끝나면 연결을 끊어 버리는데, 자원을 요청할때 마다 연결을 맺고 끊어버려 오버헤드(overhead))가 생기게 된다. 그래서 HTTP/1.1에서 Persistent Connection 기능이 추가됨으로써, 한 번 맺어졌던 연결을 끊지 않고 지속적으로 유지하여 불필요한 Handshake를 줄여 성능을 개선하였다. 연결을 유지함으로써 Handshake 과정을 생략해 빠르게 자원을 받아올 수 있다. 불필요한 연결의 맺고 끊음을 최소화시켜 네트워크 부하를 줄 일 수 있다. 클라이언트 측에서 요청에 keep-alive 헤더를 담아 보내야 한다. 정확한 Content-length 헤더를 사용해야 한다. 하나의 connection을 계속해서 재사용해야 하는데, 특정 요청의 종료를 판단할 수 없기 때문이다. Connection 헤더를 지원하지 않는 proxy에는 사용할 수 없다. Persistent Connection 가끔 HTTP 지속 연결을 persistent connection 혹은 keep-alive connection 으로 용어를 혼재하는데, 정확히는 persistent connection이 맞다. keep-alive는 HTTP 1.0+�� persistent connection을 연결하기 위해, 헤더에 명시해 사용하는 단어라고 보면 된다. keep-alive 동작 과정 Keep-Alive는 원리는 단순하다. 지정한 timeout동안 연결을 끊지 않게 지정해서, HTTP 요청과 응답 시 다수의 TCP 연결 handshake를 줄이는 것에 초점을 둔다. HTTP/1.1부터는 keep-alive가 기본으로 세팅되어 자동으로 Persistent Connection 연결이 된다. 하지만 기본적으로 HTTP/1.0 connection은 하나의 request에 응답할 때마다 connection을 close하도록 설정돼있다. keep-alive 따라서 HTTP/1.0+ 기반에서 TCP 연결의 재사용을 원할때 아래처럼 요청 헤더 Connection 속성에 keep-alive를 세팅해야 한다는 특징이 있다. keep-alive request header 만약 서버에서 keep-alive connection을 지원하는 경우에는 동일한 헤더를 response에 담아 보내주고, 지원하지 않으면 헤더에 담아 보내주지 않는다. 만약 서버의 응답에 해당 헤더가 없을 경우 client는 지원하지 않는다고 가정하고 connection을 재사용하지 않게 된다. keep-alive max : keep-alive을 통해서 주고받을 수 있는 request의 최대 갯수. 이 수보다 더 많은 요청을 주고 받을 경우에는 connection은 close된다. timeout : keep-alive가 얼마동안 유지될 것인가를 의미한다. 이 시간이 지날 동안 request가 없을 경우에 connection은 close된다 keep-alive를 이용한 통신은 위의 설정에 따라 클라이언트나 서버 중 한쪽이 다음 헤더를 부여해 접속을 끊거나 타임아웃될 때까지 연결이 유지된다. 그래서 만일 필요한 자원을 모두 할당받고 더이상 keep-alive 연결을 유지할 필요가 없을 경우 요청 헤더에서 Connection 속성을 close로 설정해 서버로 보내게 되면, TCP 지속 연결을 끊게 된다. keep-alive keep-alive 메세지 통신 다음은 두개의 요청에 대한 HTTP 메세지 예시이다. 먼저 HTML 페이지에 대한 요청을 하고 그다음 아이콘 이미지에 대한 요청을 한다. 이 2가지 요청은 모두 한 개의 keep-alive 연결을 통해 전달된다. HTML 파일 요청 (인코딩, charset과 쿠키 메타데이터와 함께) keep-alive HTML 요청에 대한 응답 keep-alive 동일한 TCP 연결에 발생한 icon 파일 요청 (icon 파일을 받고나면 서버에게 해당 연결이 재사용되지 않을 것임을 알리기 위에 Connection 헤더값을 close로 설정) keep-alive icon 응답과 연결 종료 keep-alive Pipelining 파이프 라이닝은 여러개의 요청을 보낼때 처음 요청이 응답될 때까지 기다리지 않고 바로 요청을 한꺼번에 보내는 것을 의미한다. 즉, 여러개의 요청을 한꺼번에 보내서 응답을 받음으로서 대기시간을 줄이는 기술이다. keep-alive를 전제로 하며, 서버 간 요청의 응답속도를 개선시키기 위해 적용 서버는 요청이 들어온 순서대로(FIFO) 응답을 반환한다. 하지만 응답 순서를 지키기 위해 응답 처리를 미루기 때문에 Head Of Line Blocking 문제가 발생하여, 그래서 모던 브라우저들은 대부분 파이프라이닝을 사용하지 못하도록 막아 놓았다. HTTP 2에서는 멀티플렉싱 알고리즘으로 대체되었다. Pipelining Domain Sharding 파이프라이닝을 대체하기 위한 차선책으로 나온 기술이며, 브라우저들은 하나의 도메인에 대해 여러 개의 Connection을 생성해서 병렬로 요청을 보내고 받는 방식으로 성능을 개선했다. 한 도메인당 6~13개의 TCP 연결들을 동시 생성해 여러 리소스를 한 번에 다운로드 하는 것이다. 이를 Domain Sharding이라고 부른다. Domain Sharding 하지만 도메인의 주소를 찾기 위해 DNS Lookup 과정에서 시간을 잡아먹을수도 있으며, 브라우저별로 Domain당 Connection 개수의 제한이 존재하여 근본적인 해결책은 아니었다. Domain Sharding HTTP/1.1 문제점 HOLB (Head Of Line Blocking) Head Of Line Blocking 위에서 소개한 파이프 라이닝은 어찌보면 정말 혁신적인 기술이지만, 보낸 요청 순서대로 응답을 받아야하는 규칙 부분에서 문제가 생기게 된다. 마치 FIFO(선입선출) 처럼 생각하면 되는데, 문제는 요청하는 데이터의 크기는 제각각 이기 때문에, 첫번째로 요청한 데이터가 용량이 큰 데이터라면, 두번째, 세번째 데이터가 아무리 빨리 처리되어도 우선순위 원칙에 따라 첫번째 데이터의 응답 속도가 늦어지면 후 순위에 있는 데이터 응답속도도 덩달아 늦어지게 되는 것이다. 이해가 잘 되지 않는다면 아래 그림을 살펴보자. 첫번째 http request에서는 하나의 요청당 응답을 받아야 다음 요청을 보내는 오래된 방법으로 time 길이를 보면 오래 걸려 길다. 그래서 pipelining을 통해 동시 요청을 통해 time을 감소 시켰지만, 문제는 첫번째 요청에 대한 응답이 오래걸릴 경우 그 뒤의 응답도 같이 늦게 되서 결과적으로 총 time이 길어지게 되는 비효율적인 상황이 발생하게 되는 것이다. Head Of Line Blocking 따라서 위의 문제점과 더불어 구현 복잡성에 의해 파이프 라이닝은 활용이 매우 제한적이었으며, 대부분의 브라우저에서는 여러개의 tcp 연결을 만들어 병렬적으로 이용하는 방식을 많이 사용하였지만 이 역시 추가 메모리와 리소스를 낭비하는 단점이 있었다. RTT (Round Trip Time) RTT(Round Trip Time)란, 요청(SYN)을 보낼 때부터 요청에 대한 응답(SYN+ACK)을 받을 때까지의 왕복 시간을 의미한다. 즉, 아무리 keep-alive 라고 하지만 결국 TCP상에서 동작하는 HTTP의 특성상 Handshake 가 반복적으로 일어나게 되어 불필요한 RTT증가로 인해 네트워크 지연을 초래하여 성능이 저하되게 된다. 예전에는 컨텐츠가 지금처럼 많지 않았기에 큰 부담은 아니었지만, 점점 컨텐츠가 증가하면서 이러한 레이턴시도 부담스러워 졌다. RTT 무거운 헤더 구조와 중복 http/1.1의 헤더에는 많은 메타정보들이 저장되어져 있다. 또한 해당 도메인에 설정된 cookie정보도 매 요청시 마다 헤더에 포함되어 전송되기 때문에 오히려 전송하려는 값보다 헤더 값이 더 큰 경우가 비일비재 하였다. 그리고 지속 커넥션 속에서 주고 받는 연속된 요청 데이터가 중복된 헤더값를 가지고 있는 경우가 많아 쓸데없는 메모리 자원도 낭비하게 되는 꼴이 되었다. header-http HTTP 1.1을 개선한 HTTP 2.0 HTTP 2.0은 기존 HTTP 1.1 버전의 성능 향상에 초점을 맞춘 프로토콜이다. 기존의 HTTP 1.1의 내부적인 통신 구조를 다른 개념으로 송두리째 바꿔버렸는데, 웹 응답 속도가 HTTP/1/1에 비해 15~50% 향상 되었다. 아래 그림을 보면 고용량 이미지에 대해서 응답속도 비교를 한눈에 볼 수 있다. HTTP/2 🌐 HTTP 2.0 소개 & 통신 기술 알아보기 http-protocol-history HTTP 2.0은 기존 HTTP 1.1 버전의 성능 향상에 초점을 맞춘 프로토콜이다. 인터넷 프로토콜 표준의 대체가 아닌 확장으로써, HTTP 1.1의 성능 저하 부분과 비효율적인 것들을 개선되어 탄생한 것이 HTTP 2.0라고 생각하면 된다. HTTP 1.1까지는 한번에 하나의 파일만 전송이 가능했다. 비록 파이프라이닝 기술이 있었지만, 여러 파일을 전송할 경우 선행하는 파일의 전송이 늦어지면 HOLB(Head Of Line Blocking)이 발생하였다. 따라서 HTTP 2.0에서는 이 문제를 해결하기 위해 여러 파일을 한번에 병렬로 전송한다. http-protocol-history 그래서 일반적으로 HTTP/2를 사용만해도 웹 응답 속도가 HTTP/1/1에 비해 15~50% 향상 된다고 한다. 아래는 동일 이미지를 웹사이트에 로딩시켜 HTTP/1.1과 HTTP/2의 속도를 비교한 결과이다. http-2.0 이러한 혁신적인 속도에 대부분의 사이트들은 HTTP 2를 지원한다. 크롬 개발자 도구 네트워크 탭에서 우측 클릭하고 Protocol 탭을 활성화 시키면 각 요청에 대한 프로토콜을 볼 수 있다. http-protocol-history http-protocol-history h2 가 http/2.0 약자라고 보면 된다 SPDY 프로토콜 사실 HTTP/2.0의 원조는 구글이 만든 새로운 프로토콜인 2009년 중반에 발표된 SPDY(스피디) 이다. HTTP/1.1의 메시지 포맷은 구현의 단순성과 접근성에 주안점을 두고 최적화 된 프로토콜이다 보니 성능은 어느 정도 희생시키지 않을 수 없었다. 때문에 더 효율적이고 빠른 HTTP가 필요했고, 이러한 요구에 만들어진 것이 구글의 SPDY 프로토콜이다. SPDY는 HTTP를 대체하는 프로토콜이 아니고 HTTP를 통한 전송을 재 정의하는 형태로 구현 되었다. 그래서 전송 계층의 구현만 변경하면 기존 HTTP 서버 프로그램을 그대로 SPDY에서 사용할 수 있었다. SPDY 혁신적인 성능 향상에 힘입어 SPDY를 사용하는 사이트가 늘어나게 되었고, 이러한 상황을 주시하고 있던 HTTP-WG(HTTP working group)는 HTTP/2 표준을 선보이려는 노력을 했고 이 프로토콜의 초안을 SPDY 프로토콜을 채택하였다. 이렇게 2012년부터 2015년까지 3년간의 노력으로 HTTP/2 표준이 발행되게 되었다. 그리고 몇년간 함께 발전해온 SPDY는 지원을 중단하며, HTTP2가 널리 채택된다는 말을 남기고 사라지게 되었다. SPDY HTTP 2.0 개선점 Binay Framing Layer HTTP 1.1과 HTTP 2.0의 주요한 차이점은 HTTP 메세지가 1.1에서는 text로 전송되었던 것과 달리, 2.0에서는 binary frame로 인코딩되어 전송된다는 점이다. 기존 text 방식으로 HTTP 메세지를 보내는 방식은, 본문은 압축이 되지만 헤더는 압축이 되지 않으며 헤더 중복값이 있다는 문제 때문에 HTTP 2.0에서는 바이너리로 변경 되었다. 또한 HTTP 헤더에 대해서 배웠을때 헤더와 바디를 \\\\r 이나 \\\\n 과 같은 개행 문자로 구분한다고 하였는데, HTTP/2.0에서 부터는 헤더와 바디가 layer로 구분된다. 이로인해 데이터 파싱 및 전송 속도가 증가하였고 오류 발생 가능성이 줄어들었다. http-2.0 Stream 과 Frame 단위 HTTP/1.1에서는 HTTP 요청와 응답은 통짜 텍스트 Message 단위로 구성되어 있었다. HTTP/2 로 오면서 Message라는 단위 외에 Frame, Stream이라는 단위가 추가되었다. Frame : HTTP/2에서 통신의 최소 단위이며, Header 혹은 Data 가 들어있다. Message : HTTP/1.1과 마찬가지로 요청 혹은 응답의 단위이며 다수의 Frame으로 이루어진 배열 라인 Stream : 연결된 Connection 내에서 양방향으로 Message를 주고 받는 하나의 흐름 즉, HTTP/2 는 HTTP 요청을 여러개의 Frame들로 나누고, 이 frame들이 모여 요청/응답 Message가 되고, 그리고 Message는 특정 Stream에 속하게 되고, 여러개의 Stream은 하나의 Connection에 속하게 되는 구조이다. http-2.0 frame - message - stream - connection 이 처럼 프레임 단위로 이루어진 요청과 응답 메세지는 하나의 스트림을 통해 이루어지며, 이러한 스트림들이 하나의 커넥션 내에서 병렬적로 처리된다. 하나의 커넥션에서 여러개의 스트림이 동시에 열리니 속도가 빠를수밖에 없다. 좀더 Stream 통신 방식에 대해 깊이 파보자면, 모든 스트림은 31비트의 무부호 정수로 된 고유한 식별자를 갖는데, 스트림이 클라이언트에 의해 초기화되었다면 이 식별자는 반드시 홀수여야 하며 서버라면 짝수를 갖는 식으로 요청 스트림인지 응답 스트림인지 구분을 둔다. 새로 만들어지는 스트림의 식별자는 이전에 만들어졌거나 예약된 스트림들의 식별자보다 커야 한다. 한번 사용한 스트림 식별자는 다시 사용할 수 없다. 하나의 커넥션에서 오래 스트림을 사용하다보면 스트림에 할당될 수 있는 식별자가 고갈되기도 하는데, 그런 경우 커넥션을 다시 맺는 식으로 처리한다. Multiplexing Multiplexing 바로 위에서 frame - message - stream - connection 그림에서 봤듯이, HTTP 헤더 메세지를 바이너리 형태의 프레임으로 나누고 하나의 커넥션으로 동시에 여러개의 메세지 스트림을 응답 순서에 상관없이 주고 받는 것을 멀티플렉싱(multiplexing)이라고 한다. HTTP/1.1의 Connection Keep-Alive, Pipelining, Head Of Line Blocking을 개선했다. latency만 줄여주는게 아니라 결국 네트워크를 효율적으로 사용할 수 있게 하고 그 결과 네트워크 비용을 줄여준다. 특히 클라우드 시스템을 이용한다면 비용과 직결된다. HTTP 1.1 통신 과정 HTTP 1.1에서는 한 TCP 커넥션을 통해 요청을 보냈을 때, 그에 대한 응답이 도착하고 나서야 같은 TCP 커넥션으로 다시 요청을 보낼 수 있다. 따라서 웹브라우저들은 회전 지연을 줄이기 위해 여러 개의 TCP 커넥션을 만들어 동시에 여러 개의 요청을 보내는 방법을 사용하였다. 그러나 그렇다고 TCP 커넥션을 무한정 만들 수는 없기에, 한 페이지에 보내야 할 요청이 수십개에서 수백개에 달하는 요즘 시대에는 한계가 있었다. Multiplexing Request 1을 전송 받기 위해 하나의 TCP Connection 1 을 열고 요청/응답한다. 다음으로 Request 2, 3, 4을 요청하는데 빠르게 전송받기 위해 여러개의 커넥션 TCP Connection 2 와 TCP Connection 3을 만들어 요청/응답한다. 하지만 커넥션을 무한정으로 만들수없어 이러한 방식은 한계가 존재한다. HTTP 2.0 통신 과정 반면, HTTP 2에서는 하나의 커넥션에 여러 개의 스트림이 동시에 요청/응답 한다. HTTP 1.1은 요청과 응답이 메시지라는 단위로 구분되어 있었지만, HTTP 2부터는 Stream을 통해 요청과 응답이 묶일 수 있어 다수 개의 요청을 병렬적으로 처리가 가능해졌다. 따라서 응답 프레임들은 요청 순서에 상관없이 먼저 완료된 순서대로 클라이언트에 전달이 가능하다. Multiplexing Request 1을 전송 받기 위해, 우선 Framing Layer을 통해 바이너리 프레임 단위로 쪼개고 하나의 TCP Connection을 만들고 통신한다. 다음으로 Request 2, 3, 4을 요청하는데 기존의 커넥션을 이용하며, 쪼개진 프레임들은 메세지 통로를 통해 동시다발적으로 요청/응답 받는다. 커넥션 낭비도 없고 병렬적으로 자원이 전송받기에 매우 빠르다. Server Push PUSH_PROMISE HTTP 2.0에서는 클라이언트의 요청에 대해 미래에 필요할것 같은 리소스를 똑똑하게 미리 보낼 수 있다. 예를 들어 클라이언트로부터 HTML 문서를 요청하는 하나의 HTTP 메세지를 받은 서버는 그 HTML 문서가 링크하여 사용하고 있는 이미지, CSS 파일, JS 파일 등의 리소스를 스스로 파악하여 클라이언트에게 미리 push해�� 미리 브라우저의 캐시에 가져다 놓는다. 즉, 서버는 요청하지도 않은 리소스를 미리 보내어 가까운 미래에 특정 개체가 필요할때 바로 사용 되도록 성능 향상을 이끌어 내는 것이다. 그래서 클라이언트가 HTML 문서를 파싱해서 필요한 리소스를 다시 요청하여 발생하게 되는 트래픽과 회전 지연을 줄여준다는 장점이 있다. HTTP 2.0 + Push 통신 과정 PUSH_PROMISE 서버가 클라이언트로부터 Request 1을 전송 받으면, index.html 에 있는 자원들을 파싱한다. 클라이언트가 따로 요청하지 않아도, 서버가 알아서 미리 자원들을 클라이언트에 보낸다. 따라서 총 로드 시간이 줄어드는 이점이 있다. Stream Prioritization HTTP 1.1에서 파이프라이닝 이라는 혁신적인 기술이 있었지만, 우선 순위 문제 때문에 HOLB(Head Of Line Blocking)가 발생하여 사장되었다고 HTTP 1.1 글에서 소개했었다. HTTP 2에서는 리소스간 의존관계(우선순위)를 설정하여 이런 문제를 해결하였다. 위에서 봤던 것 처럼 HTTP 메세지가 개별 바이너리 프레임으로 분할되고, 여러 프레임을 멀티플렉싱 할 수 있게 되면서 요청과 응답이 동시에 이루어져 비약적인 속도 향상이 되었다. 하지만 하나의 연결에 여러 요청과 응답이 뒤섞여 버려 패킷 순서가 엉망 징창이 되었다. 따라서 스트림들의 우선순위를 지정할 필요가 생겼는데, 클라이언트는 우선순위 지정 트리를 사용하여 스트림에 식별자를 설정함으로써 해결 하였다. 각각의 스트림은 1-256 까지의 가중치를 갖음 하나의 스트림은 다른 스트림에게 명확한 의존성을 갖음 Stream Prioritization 우선순위 지정 트리 스트림 우선순위 통신 과정 Stream Prioritization 클라이언트는 서버에게 스트림을 보낼때, 각 요청 자원에 가중치 우선순위를 지정하고 보낸다. 그렇게 요청 받은 서버는 우선순위가 높은 응답이 클라이언트에 우선적으로 전달될 수 있도록 대역폭을 설정한다. 응답 받은 각 프레임에는 이것이 어떤 스트림인지에 대한 고유한 식별자가 있어, 클라이언트는 여러개의 스트림을 interleaving을 통해 서로 끼워놓는 식으로 조립한다. 최신 브라우저들은 자원의 종류, 페이지가 로드된 위치 그리고 이전 페이지 방문에서 학습한 결과에 따라 자원 요청의 우선순위를 결정하기도 한다. HTTP Header Data Compression HTTP 1.1 에서 헤더는 아무런 압축 없이 그대로 전송되었다. 이를 개선하기 위해 HTTP 2.0에서는 HTTP 메시지의 헤더를 압축하여 전송한다. 또한 HTTP 1.1 에서는 연속적으로 요청되는 HTTP 메세지들에게서 헤더값이 중복되는 부분이 많아 역시 메모리가 낭비되었는데, HTTP 2.0 에서는 이전 Message의 헤더의 내용 중 중복되는 필드를 재전송하지 않도록하여 데이터를 절약할 수 있게 되었다. HTTP Header Data Compression 만일 메세지 헤더에 중복값이 존재하는 경우, 위의 그림에서 Static / Dynamic Header Table 개념을 사용하여 중복 헤더를 검출하고, 중복된 헤더는 index값만 전송하고 중복되지 않은 Header 정보의 값은 호프만 인코딩(Huffman Encoding) 기법을 사용하는 HPACK 압축 방식으로 인코딩 처리 하여 전송하여, 데이터 전송 효율을 높였다고 보면 된다. HTTP 2.0 문제점 여전한 RTT (Round Trip Time) 아무리 혁신적으로 개선되었다 하더라도, HTTP 1.1 이나 HTTP 2는 여전히 TCP를 이용하기 때문에 Handshake의 RTT(Round Trip Time)로인한 지연 시간(Latency)이 발생한다. 결국 원초적으로 TCP로 통신하는게 문제인 것이다. TCP 자체의 HOLB (Head Of Line Blocking) 분명 HTTP 2에서 HTTP 1.1의 파이프라이닝 HOLB 문제를 멀티플렉싱(Multiplexing)을 통해 해결했다고 하였다. 하지만 기본적으로 TCP는 패킷이 유실되거나 오류가 있을때 재전송하는데, 이 재전송 과정에서 패킷의 지연이 발생하면 결국 HOLB 문제가 발생된다. TCP/IP 4 계층을 보면, 애플리케이션 계층(L4)에서 HTTP HOLB를 해결하였다 하더라도, 전송 계층(L3)에서의 TCP HOLB 를 해결한건 아니기 때문이다. HOLB 중개자 캡슐화 공격 위에서 배웠듯이 HTTP 2.0은 헤더 필드의 이름과 값을 바이너리로 인코딩한다. 이를 다르게 말하면 HTTP 2.0 이 헤더 필드로 어떤 문자열이든 사용할 수 있게 해준다는 뜻이다. 그래서 이를 악용하면 HTTP 2.0 메시지를 중간의 Proxy 서버가 HTTP 1.1 메시지로 변환할 때 메시지를 불법 위조할수 있다는 위험성이 있다. 다행히 거꾸로 HTTP/1.1 메시지를 HTTP/2.0 메시지로 번역하는 과정에서는 이런 문제가 발생하지 않는다. 길다란 커넥션 유지로 인한 개인정보 누출 우려 HTTP 2.0은 기본적으로 성능을 위해 클라이언트와 서버 사이의 커넥션을 오래 유지하는 것을 염두에 두고 있다. 하지만 이것은 개인 정보의 유출에 악용될 가능성이 있다. 이는 HTTP/1.1에서의 Keep-Alive도 가지고 있는 문제이기도 하다. HTTP 2.0을 개선한 HTTP 3.0 위의 HTTP 2.0의 문제점을 한마디로 요약하자면 TCP가 문제이다. (HTTP는 TCP 기반 위에서 동작된다) 최근에 나온 HTTP 3.0 버전은 TCP를 버리고 UDP를 채택하였다. 정확히 말하면 UDP를 개조한 QUIC 라는 프로토콜을 새로 만들었다. 기존 TCP는 클라이언트와 서버 간에 세션을 설정하기 위해 핸드쉐이크가 필요하며, 인증서인 TLS도 세션이 보호되도록 자체 핸드셰이크도 필요하다. 하지만 QUIC는 보안 세션을 설정하기 위해 한 번의 핸드셰이크만 필요하다. 아래 그림만 봐도 한번 통신하는데 드는 시간 세로축 차이가 어마어마하게 난다는 것을 볼 수 있다. HTTP 3.0에 대한 자세한 스펙을 보려면 아래 포스팅을 참고하길 바란다. http-3.0 🌐 HTTP 3.0 소개 & 통신 기술 알아보기 http-protocol-history HTTP 2.0 의 등장과 함께 기존의 프로토콜 데이터 체계를 프레임과 스트림 개념으로 재구축한 결과 기존 보다 혁신적으로 성능이 향상되게 되었다. 하지만 여전히 HTTP는 TCP 기반 위에서 동작되기 때문에, TCP 자체의 핸드쉐이크 과정에서 발생하는 지연 시간과, 기본적으로 TCP는 패킷이 유실되거나 오류가 있을때 재전송을하는데 이 재전송하는 패킷에 지연이 발생하면 결국 HOLB(Head Of Line Blocking) 문제가 발생되었다. 즉, HTTP 2.0은 TCP/IP 4 계층의 애플리케이션 계층(L4)에서 HTTP의 HOLB를 해결하였지만, 전송 계층(L3)에서의 TCP HOLB 를 해결한건 아니기 때문이다. 애초에 TCP로 인터넷 통신을 하는 것이 발목을 잡은 것이다. 점점 기술이 발전하고 다채로운 휴대 통신 기기가 널리 보급되면서 기업들은 다양한 컨텐츠를 여러 기기에 신속하게 전달하기 위해 TCP의 한계를 극복하고 최적화하는 것이 급선무의 과제였다. 그러자 IT 기업의 선두주자인 구글은 SPDY 프로토콜에 이어 새로운 UDP 기반인 QUIC 프로토콜을 고안하게 된다. 그리고 이 새로운 QUIC 프로토콜이 TCP/IP 4계층에도 동작시키기 위해 설계된 것이 바로 HTTP 3.0 이다. 즉, HTTP/1.1과 HTTP/2는 TCP를 전송에 사용하지만, HTTP/3은 UDP(QUIC)를 사용한다고 보면 된다. http-3.0 HTTP 3.0은 HTTP 2.0가 가지는 장점들을 모두 가지면서 TCP가 가지는 원초적인 단점을 보완하는데 중점으로 개발되었다. 그래서 지금까지 거론되었던 HTTP/2 의 문제를 거의 해결하였다고 보면 된다. RTT(Round Trip Time)를 제로 수준으로 줄였고, 패킷 손실에 대한 빠른 대응, 사용자 IP가 바뀌어도 연결이 유지되는 것이 특징이다. 통신 인프라가 빈약한 나라에서는 큰 차이가 느껴질지도 모르겠지만, 사실 한국에서 HTTP/2를 사용하든 HTTP/3를 사용하든 워낙에 땅이 좁은데다 통신 인프라는 세계에서 끝내주게 잘되어 있기 때문에 소비자들은 체감을 못 할 것이다. 그렇지만 2022년 11월 15일 한국 최초로 네이버가 HTTP/3을 도입하였다고 한다. (당연히 구글은 이미 도입했다) http-3.0 https://n.news.naver.com/mnews/article/022/0003754517?sid=101 개발자 도구의 네트워크 탭에서 표 항목에 Protocol을 활성화하면 각 요청에 대한 프로토콜을 볼 수 가 있다. 사진상에서 h3이라고 쓰여져 있는 것이 http 3.0 이며 http 2.0 과 간혹 http 1.1 도 보인다. http-3.0 개발자 도구 네트워크 탭에서 우측 클릭하고 Protocol 탭을 활성화 시킨다 http-3.0 http-3.0 QUIC 프로토콜 QUIC 프로토콜 HTTP/3의 가장 큰 특징은 기존의 HTTP/1, HTTP/2와는 다르게 UDP 기반의 프로토콜인 QUIC(Quick UDP Internet Connections)을 사용하여 통신하는 프로토콜이라는 점이다. 'Quick UDP Internet Connections' 라는 이름에서 알수 있듯이 말 그대로 UDP를 사용하여 빠르게 인터넷 연결을 하는 새로운 프로토콜이다. (참고로 '퀵' 이라고 읽는다) HTTP/2의 기반이 되는 SPDY는 사장되었지만, HTTP/3의 기반이 되는 QUIC는 RFC 9000으로 표준화되어 있다는 점도 다르다. QUIC의 계층 위치 QUIC-udp 위의 TCP/IP 4 Layer에서 볼 수 있듯이 HTTP/3은 계층 형태는 약간 특이하다. 왜냐하면 QUIC은 TCP + TSL + HTTP의 기능을 모두 구현한 프로토콜이기 때문이다. TCP의 프로토콜의 무결성 보장 알고리즘과 SSL이 이식됨으로써 높은 성능과 동시에 신뢰성을 충족시켰다고 보면 된다. 그래서 계층 위치도 약간 비스듬하게 걸쳐 있게 표현된 것이다. 쉽게 말하자면, Application 계층의 HTTP/3은 QUIC를 동작시키기 위해 있는 것이라고 보면 되고, 위에서 배웠다시피 QUIC는 UDP 기반으로 만들어졌기에 Transport 계층의 UDP 위에서 동작한다고 보면된다. 어째서 TCP가 아닌 UDP인가 ��� 💬 TCP는 구조상 한계로 개선해도 여전히 느리다 사실 TCP는 인류가 지금과 같이 엄청난 속도로 발전할 것이라곤 상상 할 수 없는 시기에 만들어졌다. TCP가 만들어지던 시절에 클라이언트와 서버가 동시 다발적으로 여러 개 파일의 데이터 패킷을 교환할 것이라고 상상하지 못했기 때문이다. 그래서 모바일 기기와 같이 네트워크 환경을 바꾸어가면서 서버와 클라이언트가 소통할 수 있을 것이라고 생각하지 못했다. 그 때문에 와이파이를 바꾸면 다시 새로운 커넥션을 맺어야 되서 끊김 현상이 일어나는 것이다. 또한 TCP를 사용한 통신에선 패킷은 신뢰성을 위해 무조건 순서대로 처리되어야 한다. 또한 패킷이 처리되는 순서 또한 정해져있으므로 이전에 받은 패킷을 파싱하기 전까지는 다음 패킷을 처리할 수도 없다. 만일 중간에 패킷이 손실되어 수신 측이 패킷을 제대로 받지 못했으면 다시 보내야 한다. 이렇게 패킷이 중간에 유실되거나 수신 측의 패킷 파싱 속도가 느리다면 통신에 병목이 발생하게 되는데, 이러한 현상을HOLB(Head of line Blocking)라고 부른다. 이 HOLB는 TCP 설계도상 어쩔수 없이 발생하는 문제이기 때문에 HTTP/1.1 뿐만 아니라 HTTP/2도 가지고 있는 아주 고질적인 문제였다. 따라서 이런 고질적인 문제들을 해결하기 위해 HTTP/3는 TCP를 버리고 UDP를 선택하였다. 💬 UDP는 신뢰성이 없는게 아니라 탑재를 안했을 뿐이다 처음 TCP와 UDP에 대해서 배웠을때, UDP는 하얀 도화지 같이 기능이 거의 없어서 빠르지만 대신에 신뢰성이 낮기 때문에, 인터넷 통신에선 조금 느리더라도 신뢰성이 높은 TCP를 사용한다라고 배웠을 것이다. TCP UDP 연결 방식 연결 지향형 프로토콜 비 연결 지향형 프로토콜 전송 순서 보장 보장하지 않음 신뢰성 높음 낮음 전송속도(상대적) 느림 빠름 혼잡제어 O X 헤더 크기 20바이트 8 바이트 UDP는 User Datagram Protocol이라는 이름에서도 알 수 있듯이 데이터그램 방식을 사용하는 프로토콜이기 때문에 패킷의 목적지만 정해져있다면 중간 경로는 신경쓰지 않기 때문에 핸드쉐이크 과정이 필요없다. QUIC-udpQUIC-udp TCP(좌) - UDP(우) 결론으로는 UDP는 TCP가 신뢰성을 얻기 위해 내제된 과정을 거치지 않기 때문에 속도가 더 빠를 수 밖에 없다는 것인데, 그렇다면 UDP를 사용하게되면 빠르지만 신뢰성과 패킷의 무결성을 보증할 없다는 뜻인데 이것을 인터넷 통신에 사용해도 문제가 없는 걸까? 이부분은 오해인것이, UDP는 신뢰성이 없는게 아니라 탑재를 안했을 뿐이다. UDP의 진짜 장점은 커스터마이징이 가능하다는 점이다. 즉, 아래 사진과 같이 UDP 자체는 헤더에 들은게 없어 신뢰성이 낮고 제어 기능도 없지만, 이후 개발자가 애플리케이션에서 구현을 어떻게 하냐에 따라서 TCP와 비슷한 수준의 기능을 가질 수도 있다는 말이다. QUIC-udp UDP 헤더 구성 💬 아예 새로운 프로토콜은 안되는가? TCP가 문제이고 UDP도 애매하면 아예 다른 프로토콜을 만들거나 채용한다는 선택지도 있을 것이다. 이론적으로도 네트워크 스택에서 UDP와 TCP 옆에 새로운 전송 프로토콜을 만들 수 있다. 아니면 이미 있는 전송 프로토콜인 SCTP를 사용할 수도 있다. 그러나 새 프로토콜 배포는 그렇게 녹록치만은 않은데, 사용자와 서버 사이에 있는 TCP와 UDP만 허용하는 방화벽, NAT, 라우터 등의 설정에 따라 차단 될 수 있기 때문이다. 이를 프로토콜 고착화(ossification)라고 부른다. 게다가 네트워크 스택의 전송 프로토콜 계층에서 뭔가를 바꾼다는 것은 새로운 운영체제 커널을 갱신하고 프로토콜을 구현해 배포하는 것은 상당한 노력이 필요한 과정이다. 그래서 이미 표준화된 수많은 TCP 개선사항도 광범위하게 지원되지 않아서 널리 배포되거나 사용되지 않고 있는 것이다. HTTP 3.0 개선점 연결 시 레이턴시 감소 기존 TLS+TCP에서는 TLS 연결을 위한 핸드쉐이크와 TCP를 위한 핸드쉐이크가 각각 발생했다. 그래서 TCP는 연결을 생성하기 위해 기본적으로 1 RTT 가 필요하고, 여기에 TLS를 이용한 암호화 통신까지 한다면 총 3 RTT가 필요하게 된다. RTT (Round Trip Time) RTT(Round Trip Time)란, 요청(SYN)을 보낼 때부터 요청에 대한 응답(SYN+ACK)을 받을 때까지의 왕복 시간을 의미한다. QUIC-http3 QUIC에서는 이를 한단계로 줄였다. UDP 위에서 동작하는 QUIC는 통신을 시작할 때 3 Way Handshake 과정을 거치지 않아도 되기 때문에 첫 연결 설정에 1 RTT만 소요된다. 그 이유는 연결 설정에 필요한 정보와 함께 데이터도 보내버리기 때문이다. QUIC 내에 아예 TLS 인증서를 내포하고 있기 때문에, 최초의 연결 설정에서 필요한 인증 정보와 데이터를 함께 전송한다. 그래서 클라이언트가 서버에 어떤 신호를 한번 주고, 서버도 거기에 응답하기만 하면 바로 본 통신을 시작할 수 있다는 것이다 QUIC-http3 최초 Connection 시 위의 그림에서 볼 수 있듯이 TCP+TLS는 서로 자신의 세션 키를 주고 받아 암호화된 연결을 성립하는 과정을 거치고 나서야 세션 키와 함께 데이터를 교환하기 때문에 핸드쉐이크 과정이 여러번 발생하게 된다. 하지만 QUIC은 서로의 세션 키를 교환하기도 전에 데이터를 교환할 수 있기 때문에 연결 설정이 더 빠르다. 다만, 최초의 요청을 보낼 때는 클라이언트는 서버의 세션 키를 모르는 상태이기 때문에, 목적지인 서버의 Connection ID를 사용하여 생성한 특별한 키인 초기화 키(Initial Key)를 사용하여 통신을 암호화 한다. 그리고 한번 연결에 성공했다면 서버는 그 설정을 캐싱해놓고 있다가, 다음 연결 때 캐시를 불러와 바로 연결을 하기 때문에 추가적인 핸드 쉐이크 없이 0 RTT만으로 바로 통신을 시작할 수도 있다는 장점도 있다. QUIC-http3 재연결 Connection 시 잔존하던 HOLB 현상을 해결 HTTP의 HOLB (Head Of Line Blocking) 기존 HTTP/1.1 같은 경우 파이프라인(pipeline) 기술을 통해 병렬적으로 리소스를 빠르게 얻도록 하려고 하였지만, 만일 첫번째 요청에 딜레이가 생기면 나머지 요청이 빨리 처리됬음에도 불구하고 딜레이가 되는 심각한 현상이 있었다. 예를들어 3개의 이미지 a.png, b.png, c.png 를 받는다고 가정한다면 다음과 같이 첫번째 a.png 를 받는 과정에서 오래걸리게 된다면 b와 c 이미지가 아무리 빨리 처리되더라도 결과적으로 늦게 받게 되게 된다. QUIC-holb 그래서 이를 극복하기 위해 HTTP/2 에서는 리소스들을 하나의 커넥션에서 병렬적으로 보내도록 개선하였다. 따라서 a.png 가 시간이 걸리더라도 b 와 c 이미지는 먼저 받아서 보여줄 수 있었다. QUIC-holb TCP의 HOLB (Head Of Line Blocking) 이처럼 HTTP 레이어의 HOL Blocking 은 해결됬지만, 문제는 TCP 레이어의 HOL Blocking 문제가 여전히 잔존해 있었던 것이다. HTTP/2를 사용하는 일반적인 브라우저는 TCP 연결 한개로 수십, 수백 개의 스트림 데이터를 병렬 전송을 한다. 그런데 만일 두 엔드포인트 사이 네트워크 어딘가에서 하나의 패킷이 빠지거나 없어진다면, 없어진 패킷을 다시 전송하고 목적지를 찾는 동안 전체 TCP 연결이 중단되게 된다. 즉, HTTP/2에서 스트림에서 여러가지 프레임들이 뒤 섞여 이동되게 되는데, 만일 어느 하나의 프레임에 문제가 ���기면 상관없는 그 뒤의 프레임까지 영향이 가게 된다. 따라서 결국은 HTTP의 HOLB처럼 스트림 내 패킷들은 전체가 지연이 되게 된다. QUIC-holb 거기다가 HTTP/2는 1개의 TCP 커넥션으로 전부를 처리하고 있기 때문에 패킷 손실률이 증가하면 여러 개의 TCP를 사용하는 HTTP/1.1보다 성능 저하가 커질 수 있다. 독립 스트림으로 HOLB 단축 그래서 TCP를 버려버리고 새로 QUIC 프로토콜로 구축해서 아예 스트림 자체를 독립적으로 여러개로 나누어서 처리하도록 하였다. 이를 독립 스트림이라고 한다. QUIC 연결을 통해 두 가지 다른 스트림을 설정했을 때, 이들을 독립적으로 다루므로 만약 특정 스트림에서 HOLB가 발생하더라도, 다른 스트림에는 영향을 미치지 않는다. QUIC-holb 패킷 손실 감지에 걸리는 시간 단축 HOLB 해결에 이어 QUIC는 흐름 제어하는 시간까지 단축하였다. QUIC도 TCP와 마찬가지로 전송하는 패킷에 대한 흐름 제어를 해야한다. QUIC는 기본적으로 TCP와 유사한 방법으로 패킷 손실을 탐지하지만 여기에 몇 가지 알고리즘 개선 사항을 추가하였다. 예를들어 HTTP 2.0에서는 아래 그림과 같이 하나의 스트림에 A, B, C 패킷 프레임들이 비순서대로 전달될때, 만일 세번째 프레임에서 패킷 손실이 일어나면, 패킷 B만 중지되어야 하지만 위에서 배운바와 같이 전혀 연관없는 패킷 A와 C도 모두 막혀 대기를 해야된다. QUIC-holb 이러한 문제를 해결하기 위해 QUIC는 헤더에 패킷의 전송 순서를 나타내는 별도의 패킷 번호 공간을 부여했다. 이를 이용해 QUIC는 패킷 번호를 파악해 개별 파일을 구분하여 중간에 패킷 로스가 발생해도 해당 파일의 스트림만 정지가 되도록 할 수 있다. 하나의 스트림에서 문제가 발생한다고 해도 다른 스트림은 지킬 수 있게 되어 이런 문제에서 자유로워 졌다. QUIC-holb 더욱 향상된 멀티플렉싱 HTTP/3도 당연히 HTTP/2와 같은 멀티플렉싱을 지원한다. 그리고 독립 스트림 방식으로 기존의 멀티플렉싱을 더욱 강화시켰다고 보면 된다. QUIC-http3 보안을 더욱 강화 HTTP/3와 그 기반 기술인 QUIC은 TLS 암호화를 기본적으로 사용한다. 물론 UDP와 TLS가 결합된 기술로는 DTLS라는 기술도 있지만 'TCP의 재구현'이 목표 중 하나인 QUIC와는 지향하는 바가 다르다. 이처럼 기본적으로 QUIC 내에 TLS 이 포함되어있기 때문에 TCP와 달리 헤더 영역도 같이 암호화된다. QUIC-http3 기존에 암호화되지 않던 영역까지 암호화에 포함해 보안성을 더 강화하였다. 네트워크가 변경 되도 연결이 유지 TCP의 경우 클라이언트와 서버가 서로를 구분하기 위해서는 클라이언트 IP, 클라이언트 PORT, 서버 IP, 서버 PORT, 이렇게 네 가지가 필요하다. 그래서 클라이언트의 IP가 바뀌는 상황이 발생하면 연결이 끊어져 버린다. 우리가 핸드폰을 들고 와이파이존에서 LTE 데이터를 사용하게 됐을 때, 동영상 끊김과 같이 일시적 지연이 일어나는 이유는 클라이언트 IP가 이때 바뀌기 때문이다. 그래서 다시 연결을 생성하기 위해 결국 핸드쉐이크 과정을 다시 거쳐야한다는 것이고, 이 과정에서 다시 지연시간이 발생하게 되는 것이다. QUIC-http3 Connection ID 반면 QUIC은 Connection ID를 사용하여 서버와 연결을 생성한다. Connction ID는 각 연결은 연결 식별자나 연결 ID를 가지므로 이를 통해 연결을 식별한다. QUIC-http3 Connection ID는 랜덤한 값일 뿐, 클라이언트의 IP와는 전혀 무관한 데이터이기 때문에 클라이언트의 IP가 변경되더라도 기존의 연결을 계속 유지할 수 있다. 그래서 새로 연결을 생성할 때 거쳐야하는 핸드쉐이크 과정을 생략할 수 있다. 따라서 휴대폰으로 인터넷을 할 때, 중간에 와이파이에서 LTE로 변경해도 스트림이 계속 유지가 된다. QUIC-http3 하지만 똑같은 Connection ID만 사용한다면 해커가 네트워크를 통해 사용자를 추적하여 보안 문제가 일어날 수도 있을 것이다. 그래서 QUIC는 새 네트워크가 사용될 때마다 Connection ID를 변경한다. 위의 말과 행동이 다르겠다고 생각하겠지만, 내부적으로 클라이언트와 서버가 모두 연결을 위해 무작위로 생성한 Connection ID에 대해 인지하고 있고, 네트워크가 바뀔때 Connection ID를 바꾸더라도 이게 이전 Connectin ID와 동일하다고 인지하여 연결을 유지하는 것이다. HTTP 3.0 우려점 HTTP 3.0 과 QUIC 프로토콜이 이렇게 좋은 많은 것들을 제공해 주고 있지만, 아직 전세계의 기업들이 이를 막상 도입하지 않는 현실적인 이유가 존재한다. 기존 체계 호환성 문제 HTTP/1.1 이나 HTTP/2 기반의 프론트엔드단 최적화를 이미 적용한 기업의 경우 QUIC 도입에 부담스러울 수 있다. 예를들어 브라우저의 병렬 다운로드를 통해 리소스를 빠르게 받아오는 도메인 분할(domain sharding) 기법을 이미 적용하여 최적화를 시킨 기업은 오히려 멀티플렉싱 기반의 HTTP/2 혹은 HTTP/3에서 성능이 반감될 수 있다. 또한 브라우저의 콘텐츠 Prefetch 기능을 적용한 경우, 이를 Server Push 기능으로 변경해야 할지에 대한 기술적인 판단과 충분한 성능 비교 테스트가 필요하게 된다. ​ 암호화로 네트워크 제어가 힘듬 QUIC는 기존에는 암호화하지 않던 헤더 필드도 암호화한다. 그래서 이런 헤더의 정보를 사용하는 ISP나 네트워크 중계회사들은 기존에 암호화하지 않던 헤더 필드 영역들을 읽을 수 없어 네트워크 혼잡을 관리하기 위한 네트워크를 최적화하기 힘들다. 예를 들어 패킷이 ACK인지 재전송인지 알 수 없다. RTT 추정 은 더 어렵다. 이러한 이유로 기업들이 HTTP 3 도입을 주저하고 있다. 암호화로 리소스가 많이 듬 QUIC은 패킷별로 암호화를 한다. 이는 기존의 TLS-TCP에서 패킷을 묶어서 암호화하는 것보다 더 큰 리소스 소모를 불러올 수 있다는 단점이 있다. QUIC는 CPU를 너무 사용함 QUIC은 너무 많은 CPU 시간을 차지한다. 따라서 보급형 스마트폰과 IoT 장치같은 마이크로 애플리케이션들은 이용에 어려움을 겪을 수도 있다. 물론 시간이 지나면 개선될수도 있다. 다만 문제는 추가적인 CPU 사용이 배포자에게 얼마나 영향을 끼치는가 이다. UDP의 보안적인 문제 DNS에서 TCP나 UDP를 53포트를 이용해 통신하게 되는데, 53 포트가 아닌 UDP 트래픽이 최근에는 도스 공격에 주로 사용되기 때문에 많은 서비스들에서 차단하거나 속도를 제한하고 있다. 그래서 QUIC에서는 초기 패킷이 최소 1200바이트여야 한다는 조건과 서버가 클라이언트로부터 응답 패킷을 받지 않으면 요청 크기의 3배 이상은 절대 보내면 안 된다는 프로토콜의 제약사항으로 이를 해결하려고 노력중이다. 기존��도 HTTP/2의 여러 보안 취약점이 발견되어 모든 업체가 이에 대한 보안 패치를 적용한 사례가 있듯이, 이처럼 새로운 기술이 나오면 보안 문제는 항상 대두되길 마련이다. 물론 이러한 우려점들은 QUIC도 버전이 업데이트 됨에 따라 극복될 가능성이 있다. HTTP/3도 점점 발전해 나갈것이고, QUIC가 사장되고 또다른 프로토콜이 나온다고 할지라도, 이제는 더이상 HTTP는 TCP라는 이야기도 바뀌지 않을꺼라 생각된다.",
      "frontmatter": {
        "date": "2025-06-03T06:05:16+09:00",
        "lastmod": "2025-06-03T06:40:34+09:00"
      }
    },
    "university network quiz": {
      "path": "/06.university/network/university-network-quiz/",
      "filename": "university network quiz",
      "content": "Pasted image 20250320150681 Pasted image 20250327150571 5번은 iterated query, recursive query 를 사용 Pasted image 20250403150631 Pasted image 20250410150653 Pasted image 20250424150824 중간고사 ack, nak : 비트 에러 대비용 seq : 순서 확인용 timeout : 패킷 손실 대비용 tcp 에서 timeout 이 발생하면 timeout 이 발생한 패킷만 재전송한다( 잘 도착한 패킷은 버퍼링하기 때문에 필요없음 ) GBN, SR 의 원형은 받은 패킷번호를 ack vs TCP 는 받고난후 다음에 받을 패킷번호를 ack TCP RENO Pasted image 20250508150413 Pasted image 20250521010187 Pasted image 20250521010190 Pasted image 20250522150525 32 - 14 = 18 18 -> 2\\^18 NAT (1).jpg)1. TDMA RAandom access csma/cd binary ... 48",
      "frontmatter": {
        "tags": [
          "university"
        ],
        "date": "2025-06-03T06:05:16+09:00",
        "lastmod": "2025-06-03T06:40:39+09:00"
      }
    },
    "university network": {
      "path": "/06.university/network/university-network/",
      "filename": "university network",
      "content": "전송 4계층 TCP 3계층 FCFS Priority RR(round robin) weighted fair network neutality ( 망 중립성 ) 각 ISP 각 자원을 어떻게 분배할 것인가? protecting free speech encouraging innovation, compatition link s chapter4 보장된 전달 지연 제한 이내의 보장된 전달 순서화 패킷 전달 최소 대역폭 보장 보안 서비스",
      "frontmatter": {
        "tags": [
          "university"
        ],
        "date": "2025-03-18T16:00:00+09:00",
        "lastmod": "2025-06-03T06:40:40+09:00"
      }
    },
    "네트워크 족보 문제 풀이": {
      "path": "/06.university/network/네트워크-족보-문제-풀이/",
      "filename": "네트워크 족보 문제 풀이",
      "content": "네트워크 라우터의 패킷 서비스 순서 문제 패킷 크기는 모두 동일하며, 다음 순서로 도착하여 큐에 이미 들어가 있다고 가정합니다: P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12 각 패킷은 아래와 같이 클래스(Class) 별로 분류되어 있습니다: Class 1: P2, P4, P7, P12 Class 2: P3, P5, P10 Class 3: P1, P6, P8, P9, P11 Round Robin Scheduling (5점) 서비스는 Class 3 큐부터 시작됩니다. 각 클래스 내부에서는 패킷 번호 순서대로 서비스가 진행됩니다. 요구사항: Round Robin 스케줄링 방식에 따라 패킷이 실제로 서비스되는 순서를 기술하시오. Weighted Fair Queueing (WFQ) Scheduling (5점) 각 클래스의 가중치: Class 1: 3 Class 2: 2 Class 3: 4 패킷 전송 시간은 동일하다고 가정합니다. 가중치 계산 결과가 동일한 경우, 번호가 작은 패킷이 우선으로 서비스됩니다. 요구사항: WFQ 스케줄링 방식에 따라 패킷이 실제로 서비스되는 순서를 기술하시오. 각 스케줄링 기법에 따른 패킷 서비스 순서는 다음과 같습니다. 라운드 로빈(Round Robin) 스케줄링 서비스 순서: P1, P2, P3, P6, P4, P5, P8, P7, P10, P9, P12, P11 설명 라운드 로빈 스케줄링은 각 클래스 큐를 순서대로 돌아가며 공평하게 한 번씩 서비스하는 방식입니다. 문제의 조건에 따라 Class 3부터 시작하여 Class 1, Class 2 순서(3 → 1 → 2)로 순환하며 각 큐의 첫 번째 패킷을 서비스합니다. 1차 순환: Class 3의 P1, Class 1의 P2, Class 2의 P3 2차 순환: Class 3의 P6, Class 1의 P4, Class 2의 P5 3차 순환: Class 3의 P8, Class 1의 P7, Class 2의 P10 (이후 Class 2는 비어있음) 4차 순환: Class 3의 P9, Class 1의 P12 (이후 Class 1은 비어있음) 5차 순환: Class 3에 남은 P11을 서비스합니다. WFQ(Weighted Fair Queueing) 스케줄링 서비스 순서: P1, P2, P3, P6, P4, P8, P5, P7, P9, P11, P12, P10 설명 WFQ 스케줄링은 가중치를 기반으로 각 패킷의 가상 종료 시간(Virtual Finish Time)을 계산하고, 이 값이 가장 작은 패킷을 우선적으로 서비스합니다. 가상 종료 시간은 이전 패킷의 종료 시간 + (패킷 전송 시간 / 가중치) 공식으로 계산됩니다. 모든 패킷의 크기가 동일하므로 '패킷 전송 시간'을 임의의 값(예: 12)으로 가정하여 계산할 수 있습니다. 주어진 가중치: Class 1 = 3, Class 2 = 2, Class 3 = 4 시작: 각 큐의 첫 패킷(P1, P2, P3)의 가상 종료 시간을 계산합니다. P1(C3, w=4): 12 / 4 = 3 P2(C1, w=3): 12 / 3 = 4 P3(C2, w=2): 12 / 2 = 6 ➡️ P1 선택 (값이 가장 작음) 다음: P1이 서비스된 Class 3의 다음 패킷(P6)의 가상 종료 시간을 계산합니다. P6(C3): 3 (P1의 값) + 12 / 4 = 6 P2(C1): 4 P3(C2): 6 ➡️ P2 선택 다음: P2가 서비스된 Class 1의 다음 패킷(P4)을 계산합니다. P4(C1): 4 (P2의 값) + 12 / 3 = 8 P6(C3): 6 P3(C2): 6 ➡️ P3와 P6의 값이 같으므로 패킷 번호가 낮은 P3 선택 이와 같은 방식으로 모든 패킷의 가상 종료 시간을 순차적으로 계산하고 비교하여 전체 서비스 순서를 결정합니다. 네트워크 최단 경로 문제 네트워크 정보 주어진 간선(링크)와 비용(Cost): ut : 5 uv : 6 ux : 2 tw : 2 vx : 2 xw : 2 xw : 4 (중복된 링크, 동일 노드 간 두 개의 연결 존재) xy : 3 xz : 2 wz : 1 yz : 1 각 점에서의 least cost path를 찾는 문제입니다. 총 20점 (문제 7: 10점, 문제 8: 10점) Dijkstra’s Algorithm (10점) 요구사항: node u 에서 출발하여 Dijkstra’s algorithm을 사용해 각 노드( t, v, w, x, y, z )까지의 least cost path를 찾는 과정을 다음 표에 작성하시오. 만약 동일 Step에서 비용이 같은 경우, 알파벳 순서가 빠른 노드를 먼저 선택합니다. 제시된 표 형식: Step N' D(t),p(t) D(v),p(v) D(w),p(w) D(x),p(x) D(y),p(y) D(z),p(z) 0 u 1 2 3 4 5 6 D(n) : node n 까지의 누적 최소 비용 p(n) : 이전(predecessor) 노드 Bellman-Ford Algorithm (10점) 요구사항: node u 에서 출발하여 Bellman-Ford algorithm을 사용하여 hop 수를 증가시키며 각 노드( t, v, w, x, y, z )까지의 least cost path를 찾는 과정을 다음 표에 작성하시오. 각 단계에서 어떤 이웃(neighbor)에게 전달해야 하는지도 함께 기술하시오. 제시된 표 형식: From u Cost to, (to neighbor of u) t v w x y z up to 1 hop up to 2 hops up to 3 hops up to 4 hops 네트워킹 관련 문제 (총 20점) 서브넷팅 (Subnetting) 문제 (5점) 문제: 212.20.100.0/25 네트워크에서 각 서브넷당 20개의 호스트가 사용할 수 있도록 서브넷팅(Subnetting)을 수행하려고 합니다. 이때, 최대한 많은 서브넷을 생성하기 위한 조건에 따라 다음 질문에 답하시오. 질문: 생성되는 최대 서브넷의 개수는 얼마입니까? 이렇게 만든 모든 서브넷을 사용할 경우, 실제 사용 가능한 최대 호스트의 개수는 몇 개인가요? 슈퍼네팅 (Supernetting) 문제 (5점) 문제: 212.20.100.0/25 네트워크에 다른 네트워크를 합쳐서 약 500대 규모의 단일 네트워크(호스트 수 약 500개)를 만들기 위해 슈퍼네팅(Supernetting)을 수행하려고 합니다. 질문: 슈퍼네팅에 필요한 다른 네트워크 주소와 합쳐진 단일 네트워크 주소를 CIDR 표기법으로 각각 제시하시오. SDN/OpenFlow Flow Table 문제 (5점) 문제: SDN 환경에서 다음과 같은 네트워크 구성이 있습니다: 호스트: h1 , h2 , h3 , h4 스위치: s1 , s2 , s3 호스트 h1 과 h2 에서 보낸 데이터그램들이 s3 스위치를 거쳐 s2 로 보내져, 최종적으로 h3 또는 h4 로 전달되도록 하고자 합니다. s3 스위치의 OpenFlow Flow Table에 이 기능을 처리할 수 있는 2개의 엔트리(match & action)를 작성하시오. NAT (Network Address Translation) 문제 (5점) 문제: 다음 그림과 같은 NAT 라우터가 있다고 가정합니다. 내부 네트워크 IP 대역: 192.168.0.0/24 외부 공인 IP 주소: 210.100.10.5 포트 번호 할당 범위: 4000 ~ 65535 라우터는 아래와 같은 방식으로 연결된 세 호스트의 패킷을 처리하고 있습니다: 호스트 내부 IP 외부 목적지 IP H1 192.168.0.1 172.16.0.10 H2 192.168.0.2 10.10.10.10 H3 192.168.0.3 8.8.8.8 질문: 각 호스트가 동시에 동일한 외부 서버(예: 8.8.8.8 )에 접속할 때, NAT 테이블에 저장되어야 하는 정보를 포함한 예시를 작성하시오. NAT 라우터가 외부에서 들어오는 응답 패킷을 올바른 내부 호스트로 전달하는 방법을 설명하시오. 📝 데이터 전송 시 CRC(Error Detection) 계산 문제 문제 내용 데이터 D = 10111001010 을 전송할 때, 순환 중복 검사(CRC, Cyclic Redundancy Check)를 사용하여 오류를 탐지하려고 합니다. 이때 사용하는 생성자(Generator Polynomial)는 G = 1001 입니다. 요구 사항 이 데이터에 CRC 값을 추가하기 위해 필요한 체크섬(FCS, Frame Check Sequence) 필드 값을 계산하시오 계산 과정과 결과 값을 명확히 보이시오 (5점) 🔍 예시 형식 데이터: D = 10111001010 생성자: G = 1001 (4비트) CRC 길이: G의 비트 수 - 1 = 3비트 따라서 데이터 뒤에 3개의 0을 붙여서 나눗셈 수행 나머지를 CRC 값으로 하여 데이터에 첨가 📝 네트워킹 관련 문제 정답 및 풀이 서브넷팅 (Subnetting) 문제 최대 서브넷 개수: 4개 최대 사용 가능 호스트 개수: 120개 📜 풀이 과정 호스트 비트 계산: 각 서브넷에 20개의 호스트가 필요하므로, 2^h - 2 ≥ 20 공식을 만족하는 호스트 비트( h )를 찾아야 합니다. 2^h ≥ 22 이므로, h=5 가 됩니다 (2^5 = 32). 즉, 각 서브넷은 5개의 호스트 비트를 가져야 합니다. 서브넷 비트 계산: 주어진 네트워크 212.20.100.0/25 는 기본적으로 7개의 호스트 비트( 32-25=7 )를 가집니다. 여기서 새로운 호스트 비트로 5개를 사용하면, 서브넷을 만드는 데 사용할 수 있는 비트는 7 - 5 = 2 비트입니다. 최종 계산: 최대 서브넷 수: 2^2 = 4 개 서브넷당 사용 가능 호스트 수: 2^5 - 2 = 30 개 전체 사용 가능 호스트 수: 30개/서브넷 * 4개 서브넷 = 120 개 슈퍼네팅 (Supernetting) 문제 필요한 다른 네트워크 주소: 212.20.101.0/24 합쳐진 단일 네트워크 주소: 212.20.100.0/23 📜 풀이 과정 필요한 호스트 수 계산: 약 500개의 호스트를 지원하려면 2^H ≥ 500 을 만족하는 호스트 비트( H )가 필요합니다. 2^9 = 512 이므로, 총 9개의 호스트 비트가 필요합니다. 슈퍼넷 마스크 결정: 9개의 호스트 비트를 사용하면 새로운 네트워크의 프리픽스는 32 - 9 = 23 이 됩니다. 즉, /23 네트워크를 만들어야 합니다. 네트워크 통합: /23 네트워크는 512개의 IP 주소를 포함하며, 이는 2개의 /24 네트워크를 합친 크기입니다. 주어진 212.20.100.0/25 는 212.20.100.0/24 블록에 포함됩니다. 212.20.100.0/24 와 인접한 블록인 212.20.101.0/24 를 합치면 212.20.100.0/23 슈퍼넷이 생성됩니다. SDN/OpenFlow Flow Table 문제 s3 스위치의 Flow Table에 다음과 같은 2개의 엔트리를 추가합니다. (h1 IP: 10.0.0.1 , h2 IP: 10.0.0.2 로 가정, 포트는 상황에 맞게 가정) Entry Match (매치 조건) Action (동작) 1 in_port=1 , eth_type=0x0800 , ipv4_src=10.0.0.1 output:2 2 in_port=1 , eth_type=0x0800 , ipv4_src=10.0.0.2 output:2 📜 설명 Match: s3 스위치로 들어오는 패킷을 검사하는 조건입니다. in_port=1 : h1, h2가 연결된 방향의 포트에서 들어온 패킷. eth_type=0x0800 : IPv4 패킷. ipv4_src : 출발지 IP 주소가 h1 또는 h2 인 경우. Action: Match 조건이 일치할 때 수행할 동작입니다. output:2 : s2 스위치와 연결된 2번 포트로 패킷을 전달(Forwarding)합니다. NAT (Network Address Translation) 문제 NAT 테이블 예시: (3개 호스트가 동시에 8.8.8.8:443 에 접속하는 경우) 내부 출발지 IP:Port 외부(공인) 출발지 IP:Port 외부 목적지 IP:Port 192.168.0.1:1001 210.100.10.5:4000 8.8.8.8:443 192.168.0.2:2002 210.100.10.5:4001 8.8.8.8:443 192.168.0.3:3003 210.100.10.5:4002 8.8.8.8:443 응답 패킷 전달 방법: 외부 서버에서 온 응답 패킷이 NAT 라우터의 공인 IP( 210.100.10.5 )로 도착하면, 라우터는 패킷의 목적지 포트 번호를 확인합니다. 예를 들어, 목적지 포트가 4001 이라면, 라우터는 NAT 테이블을 조회하여 이 포트가 내부 호스트 192.168.0.2:2002 와 매핑된 것을 확인합니다. 그 후, 패킷의 목적지 IP와 포트를 192.168.0.2:2002 로 변환하여 해당 내부 호스트에게 정확히 전달합니다. CRC(Error Detection) 계산 문제 계산된 CRC 체크섬(FCS) 값: 001 📜 계산 과정 준비: 생성자 G = 1001 는 4비트이므로 데이터 D = 10111001010 뒤에 4 - 1 = 3 개의 0 을 추가합니다. 나눗셈 대상 데이터: 10111001010000 이진 나눗셈 (XOR 연산): 10101100011 <- 몫 (중요하지 않음) ____________ 1001 | 10111001010000 1001 ---- 01010 1001 ----- 00111 0000 ----- 01110 1001 ----- 01110 1001 ----- 01111 1001 ----- 01100 1001 ----- 01010 1001 ----- 00110 0000 ----- 01100 1001 ----- 0101 <- 최종 나머지 (Remainder) 수정된 계산: 10111001010000 G= 1001 XOR 1001 ---- 001010 0000 ---- 1010 1001 ---- 00110 0000 ---- 1100 1001 ---- 1011 1001 ---- 0100 000 ---- 1000 1001 ---- 001 <- 최종 나머지 (Remainder) 결과: 계산된 최종 나머지 001 이 CRC 체크섬(FCS) 값입니다. 이 값을 원본 데이터 뒤에 붙여 10111001010**001** 을 전송합니다. 다음은 주어진 내용을 바탕으로 정리된 마크다운 형식의 문제입니다: 네트워크 구성 및 통신 관련 문제 (총 30점) 주어진 조건 요약 S3: 다른 호스트들이 사용하는 서버 S1: DNS 서버 S2: 웹 서버 ( www.neighbor.com ) D: 호스트가 subnet mask 255.255.254.0 인 네트워크에 새로 접속 D는 웹 브라우저에서 www.neighbor.com 을 요청하는 상황을 기반으로 문제 해결 네트워크 설정 정보 확인 문제 (5점) D 호스트가 새로 접속한 네트워크에서 인터넷 사용을 위해 네트워크 관련 설정을 시행하고 있습니다. 질문: 호스트가 속한 네트워크 주소(실제 숫자)를 쓰시오. 외부 인터넷과 데이터그램 송수신을 담당할 기기 인터페이스의 주소를 CIDR 표기법으로 쓰시오. DHCP Request Frame 전송 문제 (5점) D 호스트가 새로 접속한 네트워크는 이더넷이며, 인터넷 사용을 위해 DHCP를 사용하여 네트워크 관련 설정을 수행합니다. 질문: 이때 전송되는 DHCP Request 메시지가 담긴 Layer 2 프레임의 destination 주소는 무엇입니까? ARP Protocol 활용 문제 (5점) D 호스트가 www.neighbor.com 의 IP 주소를 알아내기 위해 응용 계층(Application Layer)의 질의를 보낸 후, 해당 질의가 링크 계층(Link Layer)까지 도달했습니다. 이제 이 질의를 외부 인터넷으로 내보내야 하는 상황입니다. D 호스트는 외부 인터넷과 데이터그램 송수신을 담당할 기기 인터페이스의 IP 주소는 알고 있지만, 해당 주소에 대응되는 Layer 2 주소(MAC 주소)는 모르고 있습니다. 질문: 이 문제를 해결하기 위해 사용하는 프로토콜의 정식 명칭을 쓰시오. 해당 프로토콜을 사용해서 알아낸 Layer 2 주소(MAC 주소)를 쓰시오. DNS 질의 응답 패킷의 Source IP 문제 (5점) D 호스트가 www.neighbor.com 의 IP 주소를 알아내기 위해 보낸 DNS 질의에 대한 응답 패킷이 라우터 R2에 도착했습니다. 이후 R2에서 해당 네트워크 계층(IP Datagram) 패킷을 외부로 보낼 때 사용하는 source IP 주소는 무엇입니까? TCP 연결 설정 시 ACK Segment의 Source Layer 2 주소 문제 (5점) D 호스트가 www.neighbor.com 의 IP 주소를 알아낸 후, 홈페이지 요청을 위한 TCP 연결 설정을 시작했습니다. 이때 전송된 ACK 세그먼트(TCP Segment)가 라우터 R3에 도착했고, 이후 R3에서 외부로 보내는 링크 계층(Layer 2) 프레임에서 사용되는 source 주소는 무엇입니까? 웹 서버 응답 패킷의 Destination Layer 2 주소 문제 (5점) D 호스트의 홈페이지 요청에 대해, 웹 서버( www.neighbor.com )로부터 온 응답 패킷이 라우터 R2에 도착했습니다. 이후 R2에서 D 호스트로 보내는 링크 계층(Layer 2) 프레임에서 사용되는 destination 주소는 무엇입니까? 필요하시면 각 문제에 대한 풀이 예시 및 설명도 함께 제공해 드릴 수 있습니다. 풀이 원하시는 문제 번호를 알려주세요 😊 네, 주어진 네트워크 구성도와 문제를 바탕으로 각 질문에 대한 답변을 정리해 드립니다. 네트워크 설정 정보 확인 문제 호스트가 속한 네트워크 주소: 호스트 D의 IP 주소 110.0.0.2 와 서브넷 마스크 255.255.254.0 를 AND 연산하면 네트워크 주소를 알 수 있습니다. 정답: 110.0.0.0 외부 인터넷과 데이터그램 송수신을 담당할 기기(게이트웨이) 인터페이스의 주소: 호스트 D가 속한 네트워크의 게이트웨이는 라우터 R3입니다. R3의 해당 인터페이스 IP 주소는 110.0.0.1 이며, 서브넷 마스크 255.255.254.0 는 23비트를 사용하므로 CIDR 표기법으로는 /23 입니다. 정답: 110.0.0.1/23 DHCP Request Frame 전송 문제 DHCP를 통해 IP 주소를 할당받기 전이므로, 호스트 D는 네트워크에 있는 모든 기기에게 요청을 보내기 위해 브로드캐스트 주소를 사용해야 합니다. Layer 2(이더넷) 환경에서의 브로드캐스트 주소는 정해져 있습니다. 정답: FF-FF-FF-FF-FF-FF ARP Protocol 활용 문제 프로토콜의 정식 명칭: IP 주소를 이용해 동일 네트워크 내의 상대방 MAC 주소(Layer 2 주소)를 알아내기 위해 사용하는 프로토콜은 주소 결정 프로토콜(ARP)입니다. 정답: Address Resolution Protocol (ARP) 알아낸 Layer 2 주소: D 호스트는 게이트웨이인 R3의 IP 주소( 110.0.0.1 )에 해당하는 MAC 주소를 알아내야 합니다. 구성도에서 R3의 해당 인터페이스 MAC 주소를 확인할 수 있습니다. 정답: 2C-DA-5B-FF-E6-AB DNS 질의 응답 패킷의 Source IP 문제 DNS 서버 S1( 100.0.0.5 )이 보낸 응답 패킷은 라우터를 거치면서 Layer 2 헤더(MAC 주소)는 계속 바뀌지만, 일반적인 라우팅 환경(NAT가 없다는 가정 하에)에서 Layer 3 헤더의 Source IP와 Destination IP는 바뀌지 않습니다. 따라서 R2를 통과할 때도 패킷의 출발지 IP 주소는 S1의 주소입니다. 정답: 100.0.0.5 TCP 연결 설정 시 ACK Segment의 Source Layer 2 주소 문제 ACK 세그먼트가 R3에 도착하여 외부 인터넷(R2 방향)으로 나갈 때, R3는 새로운 Layer 2 프레임을 만듭니다. 이때 프레임의 출발지(source) 주소는 패킷을 내보내는 R3 인터페이스의 MAC 주소가 됩니다. 정답: 00-A0-CC-23-AF-4A 웹 서버 응답 패킷의 Destination Layer 2 주소 문제 문제의 시나리오(웹 서버 S2의 응답이 R2에 도착)는 실제 구성과 맞지 않지만, 질문의 의도(\"라우터를 거쳐 D 호스트에게 최종적으로 전달되는 프레임의 목적지\")를 고려하여 풀이합니다. 외부에서 온 패킷이 마지막 라우터(R3)를 거쳐 최종 목적지인 호스트 D에게 전달될 때, 이 마지막 구간에서 사용되는 Layer 2 프레임의 목적지 주소는 호스트 D의 MAC 주소입니다. 정답: 88-82-2F-54-1A-0F",
      "frontmatter": {
        "tags": [
          "network",
          "university"
        ],
        "date": "2025-06-13T13:46:10+09:00",
        "lastmod": "2025-06-13T14:53:05+09:00"
      }
    },
    "leetcode sql 문제": {
      "path": "/leetcode-sql-문제-풀기/leetcode-sql-문제/",
      "filename": "leetcode sql 문제",
      "content": "1581 문제 not in 으로 풀기 SELECT customer_id, count(*) 'count_no_trans' FROM visits as vi where visit_id not in ( select visit_id from Transactions ) group by customer_id left join 으로 풀기 select * from Visits as vi left join Transactions as tr on vi.visit_id = tr.visit_id where tr.visit_id is null --거래기록이 없는 사람 제외 group by customer_id 1280 참여한 시험이 0개를 어떻게 표현하지 join 풀이 select st.student_id, st.student_name, su.subject_name, COUNT(e.subject_name) AS attended_exams from Students AS st CROSS JOIN Subjects AS su left JOIN Examinations AS e ON st.student_id = e.student_id and su.subject_name = e.subject_name group by st.student_name, su.subject_name order by st.student_id, su.subject_name Coalese 함수 사용 속도와 null 의 적절한 처리를 위해 미리 exam 테이블의 개수를 세서 조인한다 또한 Coalese 함수 또는 NVL 사용해서 적절한 null 값 처리를 한다 SELECT s.student_id, s.student_name, sub.subject_name, COALESCE(e.attended_exams, 0) AS attended_exams FROM Students s CROSS JOIN Subjects sub LEFT JOIN ( SELECT student_id, subject_name, COUNT(*) AS attended_exams FROM Examinations GROUP BY student_id, subject_name ) e ON s.student_id = e.student_id AND sub.subject_name = e.subject_name ORDER BY s.student_id, sub.subject_name; 570 미리 계산 with cte as (select t.id, count(s.id) as cnt from Employee as s join Employee as t on s.managerId = t.id group by s.managerId) select name from Employee as e join cte on e.id = cte.id where cte.cnt is not null join & group by select t.name from Employee as s join Employee as t on s.managerId = t.id group by s.managerId having count(s.id) is not null and count(s.id) >= 5 미친 깔끔 SELECT name FROM Employee WHERE id IN ( SELECT managerId FROM Employee GROUP BY managerId HAVING COUNT(*) >= 5) 1934 개같이 멸망 요청조차 하지 않은 사용자의 비율 계산처리?? select s.user_id, round(avg (case when action = 'confirmed' then 1 else 0 end), 2) as confirmation_rate from signups as s left join confirmations as c on s.user_id = c.user_id group by s.user_id round 소수점 숫자 정확도 정해주기 avg 를 사용하는 방법임 나는 나누기로 할려고 했지만... SELECT s.user_id, COALESCE( ROUND( COUNT ( CASE WHEN c.action = 'confirmed' THEN 1 END) * 1.0 / COUNT(c.user_id),2) ,0) AS confirmation_rate FROM signups AS s LEFT JOIN confirmations AS c ON s.user_id = c.user_id GROUP BY s.user_id; 끝까지 나누기로 풀기 1251 멸망 가격의 평균을 구하되 units 의 개수에 따른 가중치를 계산해주어야 한다 select product_id , round(avg(up)/sum(units),2) as average_price from (select p.product_id , units , units *price from prices as p join unitssold as u on p.product_id = u.product_id and u.purchase_date BETWEEN start_date and end_date group by p.product_id, units , price) as temp(product_id, units, up) group by product_id SELECT p.product_id, round(SUM(p.price * u.units) / SUM(u.units),2) AS average_price FROM prices AS p JOIN unitssold AS u ON p.product_id = u.product_id AND u.purchase_date BETWEEN p.start_date AND p.end_date GROUP BY p.product_id; SELECT p.product_id, ROUND(SUM(p.price * u.units) OVER (PARTITION BY p.product_id) / SUM(u.units) OVER (PARTITION BY p.product_id), 2) AS average_price FROM prices AS p JOIN unitssold AS u ON p.product_id = u.product_id AND u.purchase_date BETWEEN p.start_date AND p.end_date; productid startdate enddate price productid purchase_date units 1 2019-02-17 2019-02-28 5 1 2019-02-25 100 1 2019-03-01 2019-03-22 20 1 2019-03-01 15 2 2019-02-01 2019-02-20 15 2 2019-02-10 200 2 2019-02-21 2019-03-31 30 2 2019-03-22 30 1633 null 이 존재하는 culumn 의 속성을 group 으로 묶으면? 애초에 고민을 할 필요가 없는 문제 join 할 필요가 없다 select contest_id, round( count(user_id) / ( select count(*) from users) , 4) * 100 as percentage from register group by contest_id order by percentage desc, contest_id 1211 그룹으로 묶인 것의 선택적으로 조건을 적용 select query_name, round( avg(rating/position),2 ) as quality, round( avg( (case when rating < 3 then 1 else 0 end ) ) * 100 ,2) as poor_query_percentage from Queries group by query_name having query_name is not null month country transcount approvedcount transtotalamount approvedtotalamount",
      "frontmatter": {
        "date": "2025-06-03T06:05:16+09:00",
        "lastmod": "2025-09-04T20:49:28+09:00"
      }
    },
    "대장균 자식 수 구하기": {
      "path": "/leetcode-sql-문제-풀기/대장균-자식-수-구하기/",
      "filename": "대장균 자식 수 구하기",
      "content": "https://school.programmers.co.kr/learn/courses/30/lessons/299305 join 방식으로 풀기 SELECT a.id, COUNT(b.parent_id) AS child_count FROM ecoli_data a LEFT JOIN ecoli_data b ON a.ID = b.parent_id GROUP BY a.ID ORDER BY a.id subquery SELECT E.ID, ( SELECT COUNT(*) FROM ECOLI_DATA AS ED WHERE ED.PARENT_ID = E.ID ) AS CHILD_COUNT FROM ECOLI_DATA E ORDER BY E.ID; CTE",
      "frontmatter": {
        "series": "sql programmers 문제",
        "series_weight": "1",
        "date": "2024-04-14T13:26:00+09:00",
        "lastmod": "2025-09-04T20:49:08+09:00"
      }
    },
    "c 선언부 문법 읽기": {
      "path": "/01.publish/c-선언부-문법-읽기/",
      "filename": "c 선언부 문법 읽기",
      "content": "쉬운 선언 int foo[5]; // foo는 5개의 정수로 구성된 배열입니다. char *foo; // foo는 char에 대한 포인터입니다. double foo(); // foo는 double을 반환하는 함수입니다. 그러나 선언이 좀 더 복잡해지면 보고 있는 내용을 정확히 아는 것이 더 어려워진다 char *(*(**foo[][8])())[]; //??? 위의 구문처럼 코딩을 하는 경우는 잘 없지만 하나하나 해석하면서 선언부 문법을 완벽하게 이해해 보자 규칙 long **foo[7]; 어떤 영문 자료에서 확인한 자료이다 언제나 먼저 변수 이름으로 시작합니다 => foo 는 ... 언제나 마지막은 type 입니다 => foo 는 char 입니다 가운데는 \"go right when you can, go left when you must\" 갈 수 있다면 오른쪽으로 왼쪽으로는 가야할때만!! [!참조] c, cpp 연산자 우선순위 를 참고 \\* 보다 \\[] 이 우선한다 5가지 c 언어에서 어렵다고 소문난 내용을 이해해 보자 포인터 배열과 배열 포인터 int (*a)[7] int *a[7] 의 차이 포인터 배열 int *a[7] a : 변수 a a[7] : 개수가 7인 배열 a *a[7] : 메모리 주소를 저장하는 크기가 7인 배열 a int *a[7] : int 형 메모리 주소를 저장하는 개수가 7인 배열 a 배열 포인터 int (*a)[7] a : 변수 a (*a) : 메모리 주소를 저장하는 a (*a)[7] : 개수가 7인 배열의 메모리 주소를 저장하는 a int (*a)[7] : int 형 변수를 저장하는 개수가 7인 배열의 메모리 주소를 저장하는 a 이중 포인터 int **a a : 변수 a *a : 메모리 주소를 저장하는 변수 a **a : 메모리 주소를 저장하는 포인터형 변수의 메모리 주소를 저장하는 변수 a int **a : int형 변수의 메모리 주소를 저장하는 포인터형 변수의 메모리 주소를 저장하는 변수 a 다차원 배열 int a[5][7]; a : 변수 a a[5] : 개수가 5인 배열 a a[5][7] : 개수가 7인 배열을 원소로 가지는 크기가 5인 배열 a int a[5][7] : int형 변수를 원소로 가지는 크기가 7인 배열을 원소로 가지는 크기가 5인 배열 a 함수포인터 int(*a)(int,int); a : 변수 a (*a) : 메모리 주소를 저장하는 포인터 변수 a (*a)(int,int) : 두 개의 int형 변수를 인자로 받는 함수의 메모리 주소를 저장하는 포인터 변수 a int(*a)(int,int) : int 형 값을 반환하고 두 개의 int형 변수를 인자로 받는 함수의 메모리 주소를 저장하는 포인터 변수 a 결론 char *(*(**foo[][8])())[]; //??? 처음에 보았던 난해한 선언이다 foo : 변수 foo foo[] : 배열 foo foo[][8] : 개수가 8인 배열을 원소로 가지는 배열 foo *foo[][8] : 메모리 주소를 저장하는 개수가 8일 배열을 원소로 가지는 배열 foo **foo[][8] : 메모리 주소를 저장하는 포인터형 변수의 메모리 주소를 저장하는 개수가 8인 배열을 원소로 가지는 배열 foo *(**foo[][8])() : 반환값이 메모리 주소를 저장하고 인수가 없는 함수의 메모리 주소를 저장하는 포인터형 변수의 메모리 주소를 저장하는 개수가 8일 배열을 원소로 가지는 배열 foo (*(**foo[][8])())[] : 크기가 정해지지 않은 배열의 메모리 주소를 저장하는 포인터형 변수를 반환값으로 가지고 인수가 없는 함수의 메모리 주소를 저장하는 포인터형 변수의 메모리 주소를 저장하는 개수가 8일 배열을 원소로 가지는 배열 foo char *(*(**foo[][8])())[] : char를 저장하는 크기가 정해지지 않은 배열의 메모리 주소를 저장하는 포인터형 변수를 반환값으로 가지고 인수가 없는 함수의 메모리 주소를 저장하는 포인터형 변수의 메모리 주소를 저장하는 개수가 8일 배열을 원소로 가지는 배열 foo foo 는 아무 인자를 받지 않고 문자 포인터의 배열을 반환하는 함수를 가리키는 이중 포인터들의 2차원 배열 쉽지 않네...",
      "frontmatter": {
        "tags": [
          "c",
          "cpp",
          "language",
          "explanation"
        ],
        "date": "2023-12-22T10:21:00+09:00",
        "lastmod": "2025-10-06T04:30:41+09:00",
        "share_link": "https://share.note.sx/mjzeq6xi#5sX5dVRXnx4vHvMWNFtDh5m1Eg3GPJXOebapYaDOzFs",
        "share_updated": "2025-07-01T23:01:11+09:00"
      }
    },
    "Emojify": {
      "path": "/copilot/copilot-custom-prompts/emojify/",
      "filename": "Emojify",
      "content": "Add relevant emojis to enhance {}. Follow these rules: Insert emojis at natural breaks in the text Never place two emojis next to each other Keep all original text unchanged Choose emojis that match the context and tone Return only the emojified text.",
      "frontmatter": {
        "date": "2025-10-17T22:42:39+09:00",
        "lastmod": "2025-10-17T22:46:26+09:00",
        "copilot-command-context-menu-enabled": "true",
        "copilot-command-slash-enabled": "true",
        "copilot-command-context-menu-order": "1050",
        "copilot-command-model-key": "",
        "copilot-command-last-used": "0"
      }
    },
    "Explain like I am 5": {
      "path": "/copilot/copilot-custom-prompts/explain-like-i-am-5/",
      "filename": "Explain like I am 5",
      "content": "Explain {} in simple terms that a 5-year-old would understand: Use basic vocabulary Include simple analogies Break down complex concepts Return only the simplified explanation.",
      "frontmatter": {
        "date": "2025-10-17T22:42:39+09:00",
        "lastmod": "2025-10-17T22:46:29+09:00",
        "copilot-command-context-menu-enabled": "true",
        "copilot-command-slash-enabled": "true",
        "copilot-command-context-menu-order": "1040",
        "copilot-command-model-key": "",
        "copilot-command-last-used": "0"
      }
    },
    "Fix grammar and spelling": {
      "path": "/copilot/copilot-custom-prompts/fix-grammar-and-spelling/",
      "filename": "Fix grammar and spelling",
      "content": "Fix the grammar and spelling of {}. Preserve all formatting, line breaks, and special characters. Do not add or remove any content. Return only the corrected text.",
      "frontmatter": {
        "date": "2025-10-17T22:42:39+09:00",
        "lastmod": "2025-10-17T22:46:29+09:00",
        "copilot-command-context-menu-enabled": "true",
        "copilot-command-slash-enabled": "true",
        "copilot-command-context-menu-order": "1000",
        "copilot-command-model-key": "",
        "copilot-command-last-used": "0"
      }
    },
    "Generate glossary": {
      "path": "/copilot/copilot-custom-prompts/generate-glossary/",
      "filename": "Generate glossary",
      "content": "Create a glossary of important terms, concepts, and phrases from {}. Format each entry as \"Term: Definition\". Sort entries alphabetically. Return only the glossary.",
      "frontmatter": {
        "date": "2025-10-17T22:42:39+09:00",
        "lastmod": "2025-10-17T22:46:36+09:00",
        "copilot-command-context-menu-enabled": "false",
        "copilot-command-slash-enabled": "false",
        "copilot-command-context-menu-order": "1090",
        "copilot-command-model-key": "",
        "copilot-command-last-used": "0"
      }
    },
    "Generate table of contents": {
      "path": "/copilot/copilot-custom-prompts/generate-table-of-contents/",
      "filename": "Generate table of contents",
      "content": "Generate a hierarchical table of contents for {}. Use appropriate heading levels (H1, H2, H3, etc.). Include page numbers if present. Return only the table of contents.",
      "frontmatter": {
        "date": "2025-10-17T22:42:39+09:00",
        "lastmod": "2025-10-17T22:46:37+09:00",
        "copilot-command-context-menu-enabled": "false",
        "copilot-command-slash-enabled": "false",
        "copilot-command-context-menu-order": "1080",
        "copilot-command-model-key": "",
        "copilot-command-last-used": "0"
      }
    },
    "Make longer": {
      "path": "/copilot/copilot-custom-prompts/make-longer/",
      "filename": "Make longer",
      "content": "Expand {} to twice its length by: Adding relevant details and examples Elaborating on key points Maintaining the original tone and style Return only the expanded text.",
      "frontmatter": {
        "date": "2025-10-17T22:42:39+09:00",
        "lastmod": "2025-10-17T22:46:38+09:00",
        "copilot-command-context-menu-enabled": "true",
        "copilot-command-slash-enabled": "true",
        "copilot-command-context-menu-order": "1070",
        "copilot-command-model-key": "",
        "copilot-command-last-used": "0"
      }
    },
    "Make shorter": {
      "path": "/copilot/copilot-custom-prompts/make-shorter/",
      "filename": "Make shorter",
      "content": "Reduce {} to half its length while preserving these elements: Main ideas and key points Essential details Original tone and style Return only the shortened text.",
      "frontmatter": {
        "copilot-command-context-menu-enabled": "true",
        "copilot-command-slash-enabled": "true",
        "copilot-command-context-menu-order": "1060",
        "copilot-command-model-key": "",
        "copilot-command-last-used": "0"
      }
    },
    "Remove URLs": {
      "path": "/copilot/copilot-custom-prompts/remove-urls/",
      "filename": "Remove URLs",
      "content": "Remove all URLs from {}. Preserve all other content and formatting. URLs may be in various formats (http, https, www). Return only the text with URLs removed.",
      "frontmatter": {
        "date": "2025-10-17T22:42:39+09:00",
        "lastmod": "2025-10-17T22:46:39+09:00",
        "copilot-command-context-menu-enabled": "false",
        "copilot-command-slash-enabled": "false",
        "copilot-command-context-menu-order": "1100",
        "copilot-command-model-key": "",
        "copilot-command-last-used": "0"
      }
    },
    "Rewrite as tweet thread": {
      "path": "/copilot/copilot-custom-prompts/rewrite-as-tweet-thread/",
      "filename": "Rewrite as tweet thread",
      "content": "Convert {} into a Twitter thread following these rules: Each tweet must be under 240 characters Start with \"THREAD START\" on its own line Separate tweets with \" \" End with \"THREAD END\" on its own line Make content engaging and clear Return only the formatted thread.",
      "frontmatter": {
        "date": "2025-10-17T22:42:39+09:00",
        "lastmod": "2025-10-17T22:46:42+09:00",
        "copilot-command-context-menu-enabled": "false",
        "copilot-command-slash-enabled": "false",
        "copilot-command-context-menu-order": "1120",
        "copilot-command-model-key": "",
        "copilot-command-last-used": "0"
      }
    },
    "Rewrite as tweet": {
      "path": "/copilot/copilot-custom-prompts/rewrite-as-tweet/",
      "filename": "Rewrite as tweet",
      "content": "Rewrite {} as a single tweet with these requirements: Maximum 280 characters Use concise, impactful language Maintain the core message Return only the tweet text.",
      "frontmatter": {
        "date": "2025-10-17T22:42:39+09:00",
        "lastmod": "2025-10-17T22:46:40+09:00",
        "copilot-command-context-menu-enabled": "false",
        "copilot-command-slash-enabled": "false",
        "copilot-command-context-menu-order": "1110",
        "copilot-command-model-key": "",
        "copilot-command-last-used": "0"
      }
    },
    "Simplify": {
      "path": "/copilot/copilot-custom-prompts/simplify/",
      "filename": "Simplify",
      "content": "Simplify {} to a 6th-grade reading level (ages 11-12). Use simple sentences, common words, and clear explanations. Maintain the original key concepts. Return only the simplified text.",
      "frontmatter": {
        "date": "2025-10-17T22:42:39+09:00",
        "lastmod": "2025-10-17T22:46:42+09:00",
        "copilot-command-context-menu-enabled": "true",
        "copilot-command-slash-enabled": "true",
        "copilot-command-context-menu-order": "1030",
        "copilot-command-model-key": "",
        "copilot-command-last-used": "0"
      }
    },
    "Summarize": {
      "path": "/copilot/copilot-custom-prompts/summarize/",
      "filename": "Summarize",
      "content": "Create a bullet-point summary of {}. Each bullet point should capture a key point. Return only the bullet-point summary.",
      "frontmatter": {
        "date": "2025-10-17T22:42:39+09:00",
        "lastmod": "2025-10-17T22:46:43+09:00",
        "copilot-command-context-menu-enabled": "true",
        "copilot-command-slash-enabled": "true",
        "copilot-command-context-menu-order": "1020",
        "copilot-command-model-key": "",
        "copilot-command-last-used": "0"
      }
    },
    "Translate to Chinese": {
      "path": "/copilot/copilot-custom-prompts/translate-to-chinese/",
      "filename": "Translate to Chinese",
      "content": "Translate {} into Chinese: Preserve the meaning and tone Maintain appropriate cultural context Keep formatting and structure Return only the translated text.",
      "frontmatter": {
        "date": "2025-10-17T22:42:39+09:00",
        "lastmod": "2025-10-19T16:12:05+09:00",
        "copilot-command-context-menu-enabled": "true",
        "copilot-command-slash-enabled": "true",
        "copilot-command-context-menu-order": "1010",
        "copilot-command-model-key": "",
        "copilot-command-last-used": "0"
      }
    },
    "markdown systex": {
      "path": "/markdown-test/markdown-systex/",
      "filename": "markdown systex",
      "content": "아래는 Markdown 기본 구문(Basic Syntax)과 확장 구문(Extended Syntax)을 모두 포함한 테스트용 Markdown 파일입니다. 이 파일을 .md 확장자로 저장한 후, 다양한 Markdown 렌더러(예: VS Code, Obsidian, Typora, GitHub 등)에서 열어 각 기능이 제대로 작동하는지 확인할 수 있습니다. Markdown 전체 구문 테스트 문서 이 문서는 Markdown 기본 구문과 확장 구문을 모두 포함합니다. 제목 (Headings) Heading level 1 Heading level 2 Heading level 3 Heading level 4 Heading level 5 Heading level 6 또는 대체 문법: Heading level 1 == Heading level 2 단락과 줄바꿈 (Paragraphs & Line Breaks) 이것은 첫 번째 단락입니다. 이것은 두 번째 단락입니다. 줄바꿈 테스트: 두 칸 이상의 공백으로 줄바꿈 → 다음 줄입니다. 또는 <br> 태그 사용: 첫 줄두 번째 줄 강조 (Emphasis) 굵게: **굵게** 또는 __굵게__ 기울임*: *기울임* 또는 _기울임_ 굵게 + 기울임*: ***굵게 + 기울임*** 또는 ___굵게 + 기울임___ 단어 중간 강조: Loveisbold, Acatmeow 인용문 (Blockquotes) 이것은 인용문입니다. 이것은 여러 줄 인용문입니다. 인용문 안에 중첩 인용문 인용문 안에 다른 요소: 인용 내 제목 목록 항목 또 다른 항목 굵게, 기울임, 코드 목록 (Lists) 순서 있는 목록 (Ordered) 첫 번째 항목 두 번째 항목 세 번째 항목 순서 없는 목록 (Unordered) 항목 1 항목 2 중첩 항목 또 다른 중첩 항목 더 깊은 중첩 항목 더 깊은 중첩 항목 더더 깊은 중첩 항목 더더 깊은 중첩 항목 항목 (별표) 항목 (플러스) 목록 내 요소 목록 항목 다음 단락을 넣으려면 4칸 들여쓰기: 이건 들여쓴 단락입니다. 코드 블록 포함: console.log(\"Hello, world!\"); 이미지 포함: Tux 코드 (Code) 인라인 코드: console.log(\"Hello\") 백틱 포함 코드: ` code ### 코드 블록 (기본 문법) <html> <body> <p>Hello</p> </body> </html> ### 코드 블록 (펜스 문법, 확장) \u0002PROTECTED0\u0003\u0002PROTECTED1\u0003 --- ## 7. 수평선 (Horizontal Rules) --- *** ___ --- ## 8. 링크와 이미지 (기본 문법) [Markdown Guide](https://www.markdownguide.org) ![Tux](https://mdg.imgix.net/assets/images/tux.png?auto=format&fit=clip&q=40&w=100) 자동 URL 링크: https://www.example.com 비활성화된 자동 링크: https://www.example.com --- ## 9. 표 (Tables, 확장) | 좌정렬 | 중앙정렬 | 우정렬 | | :----- | :------: | -----: | | 왼쪽 | 가운데 | 오른쪽 | | 데이터 | 데이터 | 데이터 | 코드 파이프 문자 이스케이프: &#124; --- ## 10. 각주 (Footnotes, 확장) 간단한 각주.[^2] 긴 각주.[^1] [^1]: 여러 단락을 가진 각주입니다. 들여쓰면 같은 각주에 포함됩니다. 코드 --- ## 11. 제목 ID 및 앵커 링크 (Heading IDs, 확장) ### 이 제목은 ID를 가집니다 {#custom-id} [위 제목으로 이동](#custom-id) --- ## 12. 정의 목록 (Definition Lists, 확장) 사과 : 달콤하고 바삭한 과일. 오렌지 : 비타민 C가 풍부한 감귤류. : 주스로도 즐깁니다. --- ## 13. 취소선 (Strikethrough, 확장) ~~이 문장은 취소되었습니다~~. --- ## 14. 작업 목록 (Task Lists, 확장) - [x] 완료된 작업 - [ ] 미완료 작업 - [ ] 또 다른 미완료 작업 --- ## 15. 이모지 (Emoji, 확장) 복사-붙여넣기: 🎉🚀 이모지 쇼트코드: :smile: :rocket: :tada: --- ## 16. 하이라이트 (Highlight, 확장) ==이 텍스트는 하이라이트됩니다==. 또는 HTML: <mark>하이라이트</mark> --- ## 17. 첨자와 위첨자 (Subscript/Superscript, 확장) - 아랫첨자: H~2~O 또는 HTML: H<sub>2</sub>O - 윗첨자: E = mc^2^ 또는 HTML: E = mc<sup>2</sup> --- ## 18. 자동 URL 링크 (Automatic URL Linking, 확장) https://www.markdownguide.org https://www.markdownguide.org --- ✅ 이 파일은 Markdown 기본 및 확장 구문을 모두 테스트하기 위해 작성되었습니다. 사용 중인 Markdown 렌더러에 따라 일부 확장 기능이 지원되지 않을 수 있습니다. --- HTML 태그도 사용할 수 있습니다: <p style=\"color: blue;\">이 텍스트는 파란색입니다.</p> <!-- 이건 주석입니다 --> --- 수식은 일부 도구에서 지원됩니다. $$ E = mc^2 $$ 인라인 수식: $a^2 + b^2 = c^2$ ### link to heading 보여질 이름 [수식 테스트](../수식%20테스트.md) ### footnote (각주) 이것은 간단한 각주[^2]이고, 이것은 더 긴 각주입니다.[^3] [^2]: 이것은 첫 번째 각주입니다. [^3]: 여러 단락과 코드가 포함된 각주입니다. 각주에 단락을 포함시키려면 들여쓰기를 해야 합니다. { 내 코드 } 원하는 만큼 단락을 추가할 수 있습니다. #### 설명: 과 :` 형식으로 시작합니다. 여러 단락을 각주 안에 넣고 싶다면 들여쓰기(indent)를 해야 합니다. 코드나 다른 형식도 각주 안에 포함시킬 수 있습니다.",
      "frontmatter": {
        "date": "2025-09-05T00:38:30+09:00",
        "lastmod": "2025-10-31T22:51:14+09:00"
      }
    },
    "marmaid test": {
      "path": "/markdown-test/marmaid-test/",
      "filename": "marmaid test",
      "content": "Mermaid는 마크다운 기반의 다이어그램 생성 라이브러리로, 텍스트 기반으로 다양한 시각적 다이어그램을 쉽게 만들 수 있습니다. 현재 Mermaid는 다음과 같은 주요 다이어그램(차트) 유형들을 지원합니다: Flowchart (플로우차트) 순차적, 조건부, 반복적인 프로세스를 시각화. 방향: 상하( TD / TB ), 좌우( LR ), 우좌( RL ) 등 설정 가능. 노드 스타일, 연결선 스타일 커스터마이징 가능. flowchart LR A --> B --> C Sequence Diagram (시퀀스 다이어그램) 객체 간의 시간 흐름에 따른 상호작용을 표현. 주로 소프트웨어 설계나 API 통신 흐름에 사용. sequenceDiagram Alice->>John: Hello John! John-->>Alice: Hi Alice! Class Diagram (클래스 다이어그램) 객체지향 설계에서 클래스, 속성, 메서드 및 관계를 표현. 상속, 집합, 연관 등 다양한 관계 표현 가능. classDiagram Animal <|-- Dog Animal : +String name Dog : +bark() State Diagram (상태 다이어그램) 시스템 또는 객체의 상태 변화를 표현. 상태 전이(transition), 조건, 이벤트 등을 명시 가능. stateDiagram-v2 [*] --> Still Still --> Moving : push Moving --> Still : stop Entity Relationship Diagram (ERD, 엔터티 관계 다이어그램) 데이터베이스 구조를 시각화. 엔터티, 속성, 관계 및 카디널리티 표현. erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains User Journey Diagram (사용자 여정 다이어그램) 사용자가 제품이나 서비스를 경험하는 흐름을 단계별로 시각화. 각 단계의 경험 점수도 표시 가능. journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Gantt Chart (간트 차트) 프로젝트 일정 관리에 사용. 작업, 시작일, 기간, 의존성 등을 표현. gantt title Project Timeline dateFormat YYYY-MM-DD section Phase 1 Task A :a1, 2025-11-01, 5d Task B :after a1, 3d Pie Chart (파이 차트) 간단한 비율 시각화. 라벨과 수치만으로 구성. pie title Sales by Product \"Product A\" : 40 \"Product B\" : 30 \"Product C\" : 30 Quadrant Chart (사분면 차트) 두 축을 기준으로 항목을 4분면에 배치. 예: 중요도 vs 긴급도, 인기 vs 난이도 등. quadrantChart title Product Evaluation x-axis Low Reach --> High Reach y-axis Low Engagement --> High Engagement quadrant-1 We should expand quadrant-2 Need to promote quadrant-3 Re-evaluate quadrant-4 May be improved \"Product A\": [0.7, 0.8] Git Graph (깃 커밋 히스토리) Git 브랜치 및 커밋 흐름을 시각화. 실제 Git 명령어 기반은 아니지만, 개념적 표현 가능. gitGraph commit branch develop checkout develop commit checkout main commit Requirement Diagram (요구사항 다이어그램) (실험적 기능) 시스템 요구사항과 그 관계를 표현. ISO/IEC/IEEE 29148 표준 기반. requirementDiagram requirement Login { id: 1 text: The user shall be able to log in. } functionalRequirement Login Mermaid는 지속적으로 발전 중이며, 일부 차트는 실험적(experimental) 상태일 수 있습니다. 최신 기능은 공식 Mermaid 문서에서 확인하는 것이 좋습니다. 필요한 다이어그램 유형이 있다면, 예제나 사용법도 도와드릴 수 있어요!",
      "frontmatter": {
        "date": "2025-11-01T23:56:29+09:00",
        "lastmod": "2025-11-01T23:59:48+09:00"
      }
    },
    "math-test": {
      "path": "/markdown-test/math-test/",
      "filename": "math-test",
      "content": "수학 표현식 테스트 이 문서는 KaTeX 수학 렌더링 기능을 테스트하기 위한 파일입니다. 인라인 수학 표현식 다음은 인라인 수학 표현식입니다: $E = mc^2$ 그리고 또 다른 예: $\\sum{i=1}^{n} xi = x1 + x2 + \\cdots + x_n$ 블록 수학 표현식 아래는 블록 수학 표현식입니다: $$ \\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi} $$ 복잡한 방정식: $$ \\frac{d}{dx}\\left( \\int_{a}^{x} f(t) \\, dt\\right) = f(x) $$ 행렬: $$ A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} $$ 그리스 문자와 기호들 알파($\\alpha$), 베타($\\beta$), 감마($\\gamma$), 델타($\\delta$) 그리고 더 복잡한 표현식: $$ \\lim_{x \\to 0} \\frac{\\sin x}{x} = 1 $$ 분수와 루트 분수: $\\frac{a}{b}$, $\\frac{x^2 + y^2}{z^2}$ 루트: $\\sqrt{x}$, $\\sqrt[3]{x}$, $\\sqrt{x^2 + y^2}$ 합과 곱 $$ \\sum_{k=1}^{\\infty} \\frac{1}{k^2} = \\frac{\\pi^2}{6} $$ $$ \\prod_{p \\text{ prime}} \\frac{1}{1-p^{-s}} = \\zeta(s) $$ 이 테스트를 통해 수학 표현식이 올바르게 렌더링되는지 확인할 수 있습니다.",
      "frontmatter": {}
    },
    "resource test": {
      "path": "/markdown-test/resource-test/",
      "filename": "resource test",
      "content": "markdown test only anchor : anchor Home 가변인자(variadic).md) 초기 설정 with anchor image test markdown, inside markdown outside : html inside : html outside : page resource video test markdown inside : markdown outside : html inside : html outside : audio markdown inside markdown outside html inside : html outside : asciinema markdown inside : markdown outside : html outside 1 : text file youtube more test :",
      "frontmatter": {
        "date": "2025-10-23T06:26:45+09:00",
        "lastmod": "2025-10-29T13:28:01+09:00"
      }
    },
    "test-code-highlight": {
      "path": "/markdown-test/test-code-highlight/",
      "filename": "test-code-highlight",
      "content": "코드 구문 강조 테스트 JavaScript 코드 function hello(name) { console.log(`Hello, ${name}!`); return true; } const users = ['Alice', 'Bob', 'Charlie']; users.forEach(user => hello(user)); Python 코드 def fibonacci(n): if n <= 1: return n return fibonacci(n-1) + fibonacci(n-2) # 리스트 컴프리헨션 예제 squares = [x**2 for x in range(10)] print(squares) SQL 코드 SELECT u.name, COUNT(o.id) as order_count FROM users u LEFT JOIN orders o ON u.id = o.user_id WHERE u.created_at > '2023-01-01' GROUP BY u.id, u.name ORDER BY order_count DESC; Bash 스크립트 #!/bin/bash # 환경 변수 설정 export NODE_ENV=production # 패키지 설치 및 빌드 npm install npm run build echo \"배포 완료!\" 인라인 코드 일반 텍스트 중에 console.log(\"inline code\") 같은 인라인 코드도 포함할 수 있습니다. Java 코드 public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello, World!\"); List<String> names = Arrays.asList(\"Alice\", \"Bob\", \"Charlie\"); names.stream() .map(String::toUpperCase) .forEach(System.out::println); } }",
      "frontmatter": {
        "date": "2025-09-10T12:34:37+09:00",
        "lastmod": "2025-09-11T07:14:15+09:00"
      }
    },
    "간단한 수식 테스트": {
      "path": "/markdown-test/간단한-수식-테스트/",
      "filename": "간단한 수식 테스트",
      "content": "간단한 수식 테스트 인라인 수식: $E = mc^2$ 블록 수식: $$ E = mc^2 $$ 끝.",
      "frontmatter": {
        "date": "2025-09-11T09:54:47+09:00",
        "lastmod": "2025-10-07T23:35:06+09:00"
      }
    },
    "마크다운 문법": {
      "path": "/markdown-test/마크다운-문법/",
      "filename": "마크다운 문법",
      "content": "",
      "frontmatter": {
        "aliases": [
          "markdown"
        ],
        "tags": [
          "잡지식"
        ],
        "date": "2025-05-26T14:05:00+09:00",
        "lastmod": "2025-05-26T14:05:00+09:00"
      }
    },
    "05. Share Note": {
      "path": "/00.data-view-and-bases/05.-share-note/",
      "filename": "05. Share Note",
      "content": "TABLE WITHOUT ID link(file.path, truncate(file.name, 28)) as Note, dateformat(share_updated, \"yyyy-MM-dd\") as \"Shared on\", elink(share_link, regexreplace(share_link, \"^.*?(\\w+)(#.+?|)$\", \"$1\")) as Link, choice(regextest(\"#\", share_link), \"🔒\", \"\") as \"\" WHERE share_link",
      "frontmatter": {
        "date": "2025-05-26T00:14:03+09:00",
        "lastmod": "2025-07-05T10:09:54+09:00"
      }
    },
    "11. Data View all tag": {
      "path": "/00.data-view-and-bases/11.-data-view-all-tag/",
      "filename": "11. Data View all tag",
      "content": "TABLE WITHOUT ID tag AS Tags, length(rows.file.link) AS \"File Count\" FROM \"\" WHERE file.tags FLATTEN file.tags AS tag GROUP BY tag SORT length(rows.file.link) DESC TABLE WITHOUT ID (tag + \"(\" + length(rows.file.link) + \")\") AS Tags, join(rows.file.link, \" \") AS Files FROM \"\" WHERE file.tags FLATTEN file.tags AS tag GROUP BY tag SORT length(rows.file.link) DESC",
      "frontmatter": {
        "date": "2025-05-26T03:26:46+09:00",
        "lastmod": "2025-06-09T16:49:04+09:00"
      }
    },
    "12. Data View recent modified": {
      "path": "/00.data-view-and-bases/12.-data-view-recent-modified/",
      "filename": "12. Data View recent modified",
      "content": "table modified as \"최근 수정 일시\", created as \"생성일\" from \"\" where modified sort modified desc",
      "frontmatter": {
        "date": "2025-06-26T00:28:59+09:00",
        "lastmod": "2025-06-26T00:41:23+09:00"
      }
    },
    "따라IT": {
      "path": "/00.data-view-and-bases/따라it/",
      "filename": "따라IT",
      "content": "TABLE index WHERE source = \"따라IT Network\" sort index",
      "frontmatter": {
        "date": "2024-05-12T22:27:40+09:00",
        "lastmod": "2025-06-29T04:36:56+09:00"
      }
    },
    "이것이 코딩테스트다 with python": {
      "path": "/00.data-view-and-bases/이것이-코딩테스트다-with-python/",
      "filename": "이것이 코딩테스트다 with python",
      "content": "TABLE index WHERE source = \"이것이 코딩테스트다 with python\" SORT index",
      "frontmatter": {
        "date": "2024-05-12T22:27:40+09:00",
        "lastmod": "2025-06-29T04:36:57+09:00"
      }
    },
    "학교 database 강의": {
      "path": "/00.data-view-and-bases/학교-database-강의/",
      "filename": "학교 database 강의",
      "content": "table sequence where source = \"database(university)\" and (\"00\" < sequence and \"10\" > sequence) sort sequence table sequence where source = \"database(university)\" and (\"10\" <= sequence and \"20\" > sequence) sort sequence",
      "frontmatter": {}
    }
  }
}